{"data": {"doi": "10.1109/pacificvis.2014.61", "year": "2014", "title": "Color Tunneling: Interactive Exploration and Selection in Volumetric Datasets", "conferenceName": "PacificVis", "authors": "Christophe Hurter;Russ Taylor;Sheelagh Carpendale;Alexandru Telea", "citationCount": "13", "affiliation": "Hurter, C (Corresponding Author), Univ Toulouse, ENAC, Toulouse, France.\nHurter, C., Univ Toulouse, ENAC, Toulouse, France.\nTaylor, A. R.; Carpendale, S., Univ Calgary, Calgary, AB T2N 1N4, Canada.\nTelea, A., Univ Groningen, NL-9700 AB Groningen, Netherlands.\nTelea, A., Univ Carol Davila, Bucharest, Romania.", "countries": "Canada;France;Netherlands;Romania", "abstract": "Interactive data exploration and manipulation are often hindered by dataset sizes. For 3D data, this is aggravated by occlusion, important adjacencies, and entangled patterns. Such challenges make visual interaction via common filtering techniques hard. We describe a set of realtime multi-dimensional data deformation techniques that aim to help users to easily select, analyze, and eliminate spatial-and-data patterns. Our techniques allow animation between view configurations, semantic filtering and view deformation. Any data subset can be selected at any step along the animation. Data can be filtered and deformed to reduce occlusion and ease complex data selections. Our techniques are simple to learn and implement, flexible, and real-time interactive with datasets of tens of millions of data points. We demonstrate our techniques on three domain areas: 2D image segmentation and manipulation, 3D medical volume exploration, and astrophysical exploration.", "keywords": "I.3.6 {[}Methodology and Techniques]: Interaction techniques", "link": "https://doi.org/10.1109/PacificVis.2014.61", "refList": ["10.1109/38.626971", "10.1109/tvcg.2008.162", "10.1109/visual.2003.1250400", "10.1145/2380116.2380150", "10.1145/2254556.2254652", "10.1086/375301", "10.1111/j.1467-8659.2009.01440.x", "10.1117/12.704612", "10.1109/infvis.2004.66", "10.1145/2024156.2024165", "10.1109/tvcg.2007.70565", "10.1109/sibgrapi.2010.18", "10.1109/tvcg.2006.144", "10.1109/tvcg.2006.39", "10.1109/tvcg.2002.1021579", "10.1109/visual.1994.346302", "10.1145/1101616.1101643", "10.1109/tvcg.2011.223", "10.1109/pacificvis.2009.4906857", "10.1109/tvcg.2009.145", "10.1109/visual.1996.568148", "10.1016/0141-5425(92)90057-r", "10.1109/tvcg.2010.78", "10.3322/canjclin.35.3.130", "10.1145/22339.22342", "10.1109/tvcg.2011.261"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2015.2467202", "title": "CAST: Effective and Efficient User Interaction for Context-Aware Selection in 3D Particle Clouds", "year": "2015", "conferenceName": "SciVis", "authors": "Lingyun Yu;Konstantinos Efstathiou;Petra Isenberg;Tobias Isenberg", "citationCount": "21", "affiliation": "Yu, LY (Corresponding Author), Hangzhou Dianzi Univ, Hangzhou, Zhejiang, Peoples R China. Yu, Lingyun, Hangzhou Dianzi Univ, Hangzhou, Zhejiang, Peoples R China. Efstathiou, Konstantinos, Univ Groningen, NL-9700 AB Groningen, Netherlands.", "countries": "China;Netherlands", "abstract": "We present a family of three interactive Context-Aware Selection Techniques (CAST) for the analysis of large 3D particle datasets. For these datasets, spatial selection is an essential prerequisite to many other analysis tasks. Traditionally, such interactive target selection has been particularly challenging when the data subsets of interest were implicitly defined in the form of complicated structures of thousands of particles. Our new techniques SpaceCast, TraceCast, and PointCast improve usability and speed of spatial selection in point clouds through novel context-aware algorithms. They are able to infer a user's subtle selection intention from gestural input, can deal with complex situations such as partially occluded point clusters or multiple cluster layers, and can all be fine-tuned after the selection interaction has been completed. Together, they provide an effective and efficient tool set for the fast exploratory analysis of large datasets. In addition to presenting Cast, we report on a formal user study that compares our new techniques not only to each other but also to existing state-of-the-art selection methods. Our results show that Cast family members are virtually always faster than existing methods without tradeoffs in accuracy. In addition, qualitative feedback shows that PointCast and TraceCast were strongly favored by our participants for intuitiveness and efficiency.", "keywords": "Selection, spatial selection, structure-aware selection, context-aware selection, exploratory data visualization and analysis, 3D interaction, user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2015.2467202", "refList": ["10.1016/j.ijhcs.2013.03.003", "10.2312/eurovisshort.20141164", "10.1109/tvcg.2008.153", "10.1145/2559206.2578881", "10.1145/1166253.1166260", "10.1109/tvcg.2013.126", "10.1109/pacificvis.2014.61", "10.1109/mc.2013.178", "10.2312/pe/eur0vissh0rt/eur0vissh0rt2012/007-011", "10.1177/0956797613504966", "10.1109/mcg.2010.30", "10.1016/0097-8493(94)90062-0", "10.1007/978-3-319-07731-4\\_2", "10.3758/s13428-013-0330-5", "10.1145/253284.253303", "10.1051/0004-6361/201116878", "10.1111/j.1365-2966.2008.14066.x", "10.1136/bmj.292.6522.746", "10.1117/12.497665", "10.1016/0169-2607(95)01628-7", "10.1145/2669557", "10.1145/1053427.1053445", "10.1038/nature03597", "10.1109/3dui.2011.5759219", "10.1007/bf01900346", "10.1007/978-3-540-85412-8\\_4", "10.1007/s12650-014-0206-5", "10.1145/37402.37422", "10.1109/mcg.2009.117", "10.1016/j.cag.2012.12.006", "10.2312/egve/ipt\\_egve2005/201-209", "10.1109/tvcg.2012.217", "10.1007/978-3-319-16766-4\\_20", "10.1109/tvcg.2010.157", "10.1109/tvcg.2012.292", "10.1109/visual.1999.809932", "10.1111/cgf.12406"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2016.2599049", "title": "GlyphLens: View-Dependent Occlusion Management in the Interactive Glyph Visualization", "year": "2016", "conferenceName": "SciVis", "authors": "Xin Tong;Cheng Li;Han-Wei Shen", "citationCount": "13", "affiliation": "Tong, X (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA. Tong, Xin; Li, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Glyph as a powerful multivariate visualization technique is used to visualize data through its visual channels. To visualize 3D volumetric dataset, glyphs are usually placed on 2D surface, such as the slicing plane or the feature surface, to avoid occluding each other. However, the 3D spatial structure of some features may be missing. On the other hand, placing large number of glyphs over the entire 3D space results in occlusion and visual clutter that make the visualization ineffective. To avoid the occlusion, we propose a view-dependent interactive 3D lens that removes the occluding glyphs by pulling the glyphs aside through the animation. We provide two space deformation models and two lens shape models to displace the glyphs based on their spatial distributions. After the displacement, the glyphs around the user-interested region are still visible as the context information, and their spatial structures are preserved. Besides, we attenuate the brightness of the glyphs inside the lens based on their depths to provide more depth cue. Furthermore, we developed an interactive glyph visualization system to explore different glyph-based visualization applications. In the system, we provide a few lens utilities that allows users to pick a glyph or a feature and look at it from different view directions. We compare different display/interaction techniques to visualize/manipulate our lens and glyphs.", "keywords": "View-dependent visualization;focus + context techniques;manipulation and deformation;glyph-based techniques;human-computer interaction", "link": "http://dx.doi.org/10.1109/TVCG.2016.2599049", "refList": ["10.1109/visual.2003.1250400", "10.1145/989863.989871", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2013.100", "10.1145/988834.988870", "10.1111/j.1467-8659.2006.01000.x", "10.1109/visual.1993.398849", "10.1109/pacificvis.2015.7156349", "10.1109/pacificvis.2009.4906851", "10.1145/2766890", "10.1109/pacificvis.2015.7156385", "10.1145/1239451.1239482", "10.2312/vissym/vissym04/147-154", "10.1109/tvcg.2015.2443804", "10.1109/tvcg.2006.167", "10.1145/166117.166126", "10.1007/bf01897116", "10.1111/1467-8659.t01-3-00700", "10.1111/j.1467-8659.2012.03115.x", "10.3390/s130506380", "10.1109/mcg.1984.275995", "10.1145/237091.237098", "10.1109/tmi.2009.2016561", "10.1145/1618452.1618504", "10.1109/tvcg.2010.199", "10.1109/tvcg.2010.127", "10.1109/tvcg.2013.121", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2007.1059", "10.1109/ismar.2004.36", "10.1111/cgf.12099", "10.1109/tvcg.2010.157", "10.1109/infvis.1996.559215"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2864690", "title": "Interactive obstruction-free lensing for volumetric data visualization", "year": "2018", "conferenceName": "SciVis", "authors": "Michael Traor\u00e9;Christophe Hurter;Alexandru Telea", "citationCount": "1", "affiliation": "Traore, M (Corresponding Author), French Civil Aviat Univ, ENAC, Toulouse, France. Traore, Michael; Hurter, Christophe, French Civil Aviat Univ, ENAC, Toulouse, France. Telea, Alexandru, Univ Groningen, Inst Johan Bernoulli, Groningen, Netherlands.", "countries": "France;Netherlands", "abstract": "Occlusion is an issue in volumetric visualization as it prevents direct visualization of the region of interest. While many techniques such as transfer functions, volume segmentation or view distortion have been developed to address this, there is still room for improvement to better support the understanding of objects' vicinity. However, most existing Focus+Context fail to solve partial occlusion in datasets where the target and the occluder are very similar density-wise. For these reasons, we investigate a new technique which maintains the general structure of the investigated volumetric dataset while addressing occlusion issues. With our technique, the user interactively defines an area of interest where an occluded region or object is partially visible. Then our lens starts pushing at its border occluding objects, thus revealing hidden volumetric data. Next, the lens is modified with an extended field of view (fish-eye deformation) to better see the vicinity of the selected region. Finally, the user can freely explore the surroundings of the area under investigation within the lens. To provide real-time exploration, we implemented our lens using a GPU accelerated ray-casting framework to handle ray deformations, local lighting, and local viewpoint manipulation. We illustrate our technique with five application scenarios in baggage inspection, 3D fluid flow visualization, chest radiology, air traffic planning, and DTI fiber exploration.", "keywords": "Interaction techniques,focus + context,volume visualization,volume rendering,raycasting", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864690", "refList": ["10.1109/visual.1999.809865", "10.1145/1409060.1409107", "10.1109/visual.2003.1250400", "10.1145/2598153.2598200", "10.1007/978-1-4614-7657-3\\_19", "10.1109/tvcg.2008.59", "10.1145/989863.989871", "10.1109/tvcg.2016.2599049", "10.1145/345513.345271", "10.1016/s1470-2045(17)30438-2", "10.1109/pacificvis.2014.61", "10.1111/j.1467-8659.2006.00979.x", "10.1109/tvcg.2010.35", "10.2312/conf/eg2012/stars/075-094", "10.1111/cgf.12927", "10.1111/cgf.12871", "10.1145/2024156.2024165", "10.1007/978-3-540-85412-8\\_16", "10.1057/ivs.2009.32", "10.1109/tvcg.2006.140", "10.1109/tvcg.2007.70565", "10.1109/tvcg.2015.2403323", "10.1109/visual.2004.32", "10.2200/s00688ed1v01y201512vis006", "10.1109/tvcg.2015.2443804", "10.1109/tvcg.2006.144", "10.1109/tvcg.2016.2515611", "10.1109/tvcg.2007.48", "10.1109/tvcg.2010.193", "10.1109/tvcg.2007.1051", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.223", "10.1109/38.595268", "10.1111/j.1467-8659.2012.03079.x", "10.1109/tvcg.2009.145", "10.1109/pacificvis.2017.8031594", "10.1145/2425296.2425325", "10.1109/tvcg.2012.265", "10.1109/mcg.2017.10", "10.1007/s12031-007-0029-0", "10.1109/tvcg.2010.127", "10.1016/j.trc.2014.03.005", "10.1109/tvcg.2009.138", "10.1109/tvcg.2006.124"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934332", "title": "LassoNet: Deep Lasso-Selection of 3D Point Clouds", "year": "2019", "conferenceName": "SciVis", "authors": "Zhutian Chen;Wei Zeng 0004;Zhiguang Yang;Lingyun Yu;Chi-Wing Fu;Huamin Qu", "citationCount": "4", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Zhutian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei; Yang, Zhiguang, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yu, Lingyun, Univ Groningen, Groningen, Netherlands. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China.", "countries": "China;Netherlands", "abstract": "Selection is a fundamental task in exploratory analysis and visualization of 3D point clouds. Prior researches on selection methods were developed mainly based on heuristics such as local point density, thus limiting their applicability in general data. Specific challenges root in the great variabilities implied by point clouds (e.g., dense vs. sparse), viewpoint (e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this work, we introduce LassoNet, a new deep neural network for lasso selection of 3D point clouds, attempting to learn a latent mapping from viewpoint and lasso to point cloud regions. To achieve this, we couple user-target points with viewpoint and lasso information through 3D coordinate transform and naive selection, and improve the method scalability via an intention filtering and farthest point sampling. A hierarchical network is trained using a dataset with over 30K lasso-selection records on two different point cloud data. We conduct a formal user study to compare LassoNet with two state-of-the-art lasso-selection methods. The evaluations confirm that our approach improves the selection effectiveness and efficiency across different combinations of 3D point clouds, viewpoints, and lasso selections. Project Website: https://LassoNet.github.io", "keywords": "Point Clouds,Lasso Selection,Deep Learning", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934332", "refList": ["10.1111/j.1467-8659.2009.01515.x", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2016.2599049", "10.1109/iccv.2015.114", "10.1109/vppc.2018.8604993", "10.1111/cgf.13405", "10.1109/tvcg.2018.2843369", "10.1109/cvpr.2018.00278", "10.1145/3025453.3025957", "10.1109/mc.2013.178", "10.1145/2980179.2980238", "10.1109/msp.2017.2693418", "10.1145/237091.237105", "10.1145/2835487", "10.1109/cvpr.2010.5539838", "10.1016/j.visinf.2017.01.006", "10.1109/tcst.2018.2819965", "10.1109/tvcg.2018.2865138", "10.1109/83.623193", "10.1145/3072959.3073608", "10.1145/1053427.1053445", "10.1016/s0039-9140(96)02179-0", "10.1109/cvpr.2015.7298801", "10.1109/cvpr.2017.693", "10.1007/s12650-014-0206-5", "10.1145/3272127.3275110", "10.1109/cvpr.2015.7298845", "10.1109/iccvw.2015.112", "10.1109/iros.2015.7353481", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2012.217", "10.1109/tridui.2006.1618279", "10.1109/tvcg.2012.292", "10.1109/cvpr.2016.609", "10.1016/j.cag.2012.12.003"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028947", "title": "A Fluid Flow Data Set for Machine Learning and its Application to Neural Flow Map Interpolation", "year": "2020", "conferenceName": "SciVis", "authors": "Jakob Jakob;Markus H. Gross;Tobias G\u00fcnther", "citationCount": "0", "affiliation": "Jakob, J (Corresponding Author), Swiss Fed Inst Technol, Zurich, Switzerland. Jakob, Jakob; Gross, Markus; Guenther, Tobias, Swiss Fed Inst Technol, Zurich, Switzerland.", "countries": "Switzerland", "abstract": "In recent years, deep learning has opened countless research opportunities across many different disciplines. At present, visualization is mainly applied to explore and explain neural networks. Its counterpart-the application of deep learning to visualization problems-requires us to share data more openly in order to enable more scientists to engage in data-driven research. In this paper, we construct a large fluid flow data set and apply it to a deep learning problem in scientific visualization. Parameterized by the Reynolds number, the data set contains a wide spectrum of laminar and turbulent fluid flow regimes. The full data set was simulated on a high-performance compute cluster and contains 8000 time-dependent 2D vector fields, accumulating to more than 16 TB in size. Using our public fluid data set, we trained deep convolutional neural networks in order to set a benchmark for an improved post-hoc Lagrangian fluid flow analysis. In in-situ settings, flow maps are exported and interpolated in order to assess the transport characteristics of time-dependent fluids. Using deep learning, we improve the accuracy of flow map interpolations, allowing a more precise flow analysis at a reduced memory IO footprint.", "keywords": "Scientific visualization,deep learning,flow maps", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028947", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1145/3355089.3356560", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1007/978-3-030-00533-7\\_36", "10.3390/rs11161921", "10.1007/978-3-030-48457-6\\_1", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/mcg.2018.2881523", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1145/3072959.3073643", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/tvcg.2013.128", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1007/978-3-319-46475-6\\_43", "10.1126/science.1127647", "10.1007/978-3-319-46475-6\\_25", "10.1111/cgf.13689"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030346", "title": "V2V: A Deep Learning Approach to Variable-to-Variable Selection and Translation for Multivariate Time-Varying Data", "year": "2020", "conferenceName": "SciVis", "authors": "Jun Han;Hao Zheng 0006;Yunhao Xing;Danny Ziyi Chen;Chaoli Wang", "citationCount": "0", "affiliation": "Han, J (Corresponding Author), Univ Notre Dame, Notre Dame, IN 46556 USA. Han, Jun; Zheng, Hao; Chen, Danny Z.; Wang, Chaoli, Univ Notre Dame, Notre Dame, IN 46556 USA. Xing, Yunhao, Sichuan Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China;USA", "abstract": "We present V2V, a novel deep learning framework, as a general-purpose solution to the variable-to-variable (V2V) selection and translation problem for multivariate time-varying data (MTVD) analysis and visualization. V2V leverages a representation learning algorithm to identify transferable variables and utilizes Kullback-Leibler divergence to determine the source and target variables. It then uses a generative adversarial network (GAN) to learn the mapping from the source variable to the target variable via the adversarial, volumetric, and feature losses. V2V takes the pairs of time steps of the source and target variable as input for training, Once trained, it can infer unseen time steps of the target variable given the corresponding time steps of the source variable. Several multivariate time-varying data sets of different characteristics are used to demonstrate the effectiveness of V2V, both quantitatively and qualitatively. We compare V2V against histogram matching and two other deep learning solutions (Pix2Pix and CycleGAN).", "keywords": "Multivariate time-varying data,variable selection and translation,generative adversarial network,data extrapolation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030346", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/cvpr.2017.19", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1146/annurev", "10.3390/rs11161921", "10.1109/tvcg.2007.70551", "10.1145/3309993", "10.1007/978-3-030-48457-6\\_1", "10.1109/igarss.2018.8518411", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1146/annurev-fluid-010313-141322", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1126/science.1127647"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030374", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data", "year": "2020", "conferenceName": "SciVis", "authors": "Yifan Wang;Guoli Yan;Haikuan Zhu;Sagar Buch;Ying Wang;E. Mark Haacke;Jing Hua;Zichun Zhong", "citationCount": "0", "affiliation": "Wang, YF (Corresponding Author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Wang, Yifan; Yan, Guoli; Zhu, Haikuan; Hua, Jing; Zhong, Zichun, Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Buch, Sagar; Wang, Ying; Haacke, Ewart Mark, Wayne State Univ, Dept Radiol, Detroit, MI 48201 USA.", "countries": "USA", "abstract": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.", "keywords": "Deep neural network,3D cerebrovascular segmentation and visualization,maximum intensity projection (MIP),joint embedding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030374", "refList": ["10.1109/cluster.2018.00036", "10.1109/iccv.2015.123", "10.1109/tvcg.2013.133", "10.1109/cvpr.2017.19", "10.1109/tvcg.2019.2934312", "10.1007/978-3-319-46487-9\\_40", "10.1109/tvcg.2006.175", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2018.2880207", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/tvcg.2018.2816059", "10.1109/pacificvis.2009.4906852", "10.1109/tvcg.2018.2864808", "10.1109/mcg.2018.2881523", "10.1109/tvcg.2015.2467431", "10.1109/tpami.2015.2439281", "10.1109/pacificvis48177.2020.8737", "10.1109/cvpr.2019.00244", "10.1109/tvcg.2006.165", "10.1109/bigdata.2018.8622520", "10.1007/978-3-319-46466-4\\_29", "10.1109/visual.2019.8933759", "10.1145/3197517.3201304", "10.1109/pacificvis.2011.5742378", "10.1109/cvpr.2006.91", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1007/978-3-319-24574-4\\_28", "10.1109/pacificvis.2019.00041", "10.1109/tvcg.2019.2934332", "10.1109/cvpr.2018.00916", "10.1007/978-3-319-46475-6\\_43"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2018.00035", "year": "2018", "title": "An Automatic Deformation Approach for Occlusion Free Egocentric Data Exploration", "conferenceName": "PacificVis", "authors": "Cheng Li;Joachim Moortgat;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Moortgat, Joachim; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Occlusion management is an important task for three dimension data exploration. For egocentric data exploration, the occlusion problems, caused by the camera being too close to opaque data elements, have not been well addressed by previous studies. In this paper, we propose an automatic approach to resolve these problems and provide an occlusion free egocentric data exploration. Our system utilizes a state transition model to monitor both the camera and the data, and manages the initiation, duration, and termination of deformation with animation. Our method can be applied to multiple types of scientific datasets, including volumetric data, polygon mesh data, and particle data. We demonstrate our method with different exploration tasks, including camera navigation, isovalue adjustment, transfer function adjustment, and time varying exploration. We have collaborated with a domain expert and received positive feedback.", "keywords": "Data Deformation; Occlusion Management; Data Exploration; Egocentric Visualization", "link": "https://doi.org/10.1109/PacificVis.2018.00035", "refList": ["10.1145/989863.989871", "10.1109/tvcg.2008.59", "10.1111/j.1467-8659.2008.01332.x", "10.1109/tvcg.2016.2599049", "10.1109/pacificvis.2014.14", "10.1109/tvcg.2007.70433", "10.1007/s10596-015-9501-z", "10.1109/38.610209", "10.1109/tvcg.2003.1207447", "10.1111/j.1467-8659.2008.01181.x", "10.1109/tvcg.2012.42", "10.1109/tvcg.2016.2599217", "10.1002/fld.3764", "10.1109/pacificvis.2013.6596123", "10.1109/tvcg.2006.140", "10.1109/tvcg.2012.143", "10.1109/tvcg.2007.70565", "10.1109/scivis.2015.7429485", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.167", "10.1145/505008.505039", "10.1109/tvcg.2006.144", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/38.595268", "10.1109/tvcg.2016.2518338", "10.1109/tvcg.2014.20", "10.1145/1980462.1980487", "10.1111/cgf.12466", "10.2312/compaesth/compaesth05/209-216", "10.1109/tvcg.2010.127", "10.1016/j.advwatres.2016.01.002", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2009.144", "10.1145/1462055.1462056"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/pacificvis.2017.8031579", "year": "2017", "title": "Virtual retractor: An interactive data exploration system using physically based deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Xin Tong;Han{-}Wei Shen", "citationCount": "2", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Tong, Xin; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Interactive data exploration plays a fundamental role in analyzing three dimensional scientific data. Occlusion management and context preservation are among the key factors to ensure effective identification and extraction of three-dimensional features. In this paper, we present an interactive data exploration system that utilizes a physically based deformation method to investigate hidden structures of data in three dimensional data sets. While non-physically based methods are popular for visual analytic applications due to their lower computational cost, physically based deformation methods can often better preserve features and their context. Our physically based deformation method preserves data features by setting the mesh properties according to interesting data attributes. We design effective and intuitive interfaces by using a metaphor of virtual retractor, which reflects the cutting and splitting of data that our system is simulating. We demonstrate case studies on multiple particle datasets and volume datasets, and present feedback from a domain user.", "keywords": "K.6.1 {[}Management of Computing and Information Systems]: Project and People Management-Life Cycle; K.7.m {[}The Computing Profession]: Miscellaneous-Ethics", "link": "https://doi.org/10.1109/PACIFICVIS.2017.8031579", "refList": ["10.1145/1409060.1409107", "10.1109/visual.2003.1250400", "10.1145/989863.989871", "10.1109/tvcg.2008.59", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1145/988834.988870", "10.1109/pacificvis.2014.61", "10.1111/j.1467-8659.2006.01000.x", "10.1111/j.1467-8659.2006.00979.x", "10.1145/1239451.1239482", "10.1109/tvcg.2006.140", "10.1109/tvcg.2007.70565", "10.1088/1742-6596/125/1/012076", "10.1109/tvcg.2015.2443804", "10.1109/tvcg.2006.144", "10.1111/j.1467-8659.2012.03115.x", "10.3390/s130506380", "10.1145/237091.237098", "10.1109/tvcg.2015.2502583", "10.1016/j.cag.2007.09.006", "10.1109/38.595268", "10.1145/1618452.1618504", "10.1145/1980462.1980487", "10.3171/2012.5.jns112334", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.1109/tvcg.2010.127", "10.1109/tvcg.2013.121", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.1109/ismar.2004.36", "10.1109/tvcg.2010.157"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2018.00035", "year": "2018", "title": "An Automatic Deformation Approach for Occlusion Free Egocentric Data Exploration", "conferenceName": "PacificVis", "authors": "Cheng Li;Joachim Moortgat;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Moortgat, Joachim; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Occlusion management is an important task for three dimension data exploration. For egocentric data exploration, the occlusion problems, caused by the camera being too close to opaque data elements, have not been well addressed by previous studies. In this paper, we propose an automatic approach to resolve these problems and provide an occlusion free egocentric data exploration. Our system utilizes a state transition model to monitor both the camera and the data, and manages the initiation, duration, and termination of deformation with animation. Our method can be applied to multiple types of scientific datasets, including volumetric data, polygon mesh data, and particle data. We demonstrate our method with different exploration tasks, including camera navigation, isovalue adjustment, transfer function adjustment, and time varying exploration. We have collaborated with a domain expert and received positive feedback.", "keywords": "Data Deformation; Occlusion Management; Data Exploration; Egocentric Visualization", "link": "https://doi.org/10.1109/PacificVis.2018.00035", "refList": ["10.1145/989863.989871", "10.1109/tvcg.2008.59", "10.1111/j.1467-8659.2008.01332.x", "10.1109/tvcg.2016.2599049", "10.1109/pacificvis.2014.14", "10.1109/tvcg.2007.70433", "10.1007/s10596-015-9501-z", "10.1109/38.610209", "10.1109/tvcg.2003.1207447", "10.1111/j.1467-8659.2008.01181.x", "10.1109/tvcg.2012.42", "10.1109/tvcg.2016.2599217", "10.1002/fld.3764", "10.1109/pacificvis.2013.6596123", "10.1109/tvcg.2006.140", "10.1109/tvcg.2012.143", "10.1109/tvcg.2007.70565", "10.1109/scivis.2015.7429485", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.167", "10.1145/505008.505039", "10.1109/tvcg.2006.144", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/38.595268", "10.1109/tvcg.2016.2518338", "10.1109/tvcg.2014.20", "10.1145/1980462.1980487", "10.1111/cgf.12466", "10.2312/compaesth/compaesth05/209-216", "10.1109/tvcg.2010.127", "10.1016/j.advwatres.2016.01.002", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2009.144", "10.1145/1462055.1462056"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 7}], "len": 27}, {"doi": "10.1109/tvcg.2016.2599217", "title": "Hybrid Tactile/Tangible Interaction for 3D Data Exploration", "year": "2016", "conferenceName": "SciVis", "authors": "Lonni Besan\u00e7on;Paul Issartel;Mehdi Ammi;Tobias Isenberg", "citationCount": "22", "affiliation": "Besancon, L (Corresponding Author), Inria Saclay, Paris, France. Besancon, L (Corresponding Author), Univ Paris, Paris, France. Besancon, Lonni, Inria Saclay, Paris, France. Besancon, Lonni; Issartel, Paul, Univ Paris, Paris, France. Ammi, Mehdi, CNRS, Limsi, F-75700 Paris, France. Isenberg, Tobias, Inria, Paris, France.", "countries": "France", "abstract": "We present the design and evaluation of an interface that combines tactile and tangible paradigms for 3D visualization. While studies have demonstrated that both tactile and tangible input can be efficient for a subset of 3D manipulation tasks, we reflect here on the possibility to combine the two complementary input types. Based on a field study and follow-up interviews, we present a conceptual framework of the use of these different interaction modalities for visualization both separately and combined-focusing on free exploration as well as precise control. We present a prototypical application of a subset of these combined mappings for fluid dynamics data visualization using a portable, position-aware device which offers both tactile input and tangible sensing. We evaluate our approach with domain experts and report on their qualitative feedback.", "keywords": "3D data visualization;Interaction;tactile input;tangible input", "link": "http://dx.doi.org/10.1109/TVCG.2016.2599217", "refList": ["10.1145/2671015.2671130", "10.1109/mmul.2006.69", "10.1145/1347390.1347436", "10.1145/1731903.1731930", "10.1145/2493102.2493104", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/1vcg2011.224", "10.1109/tvcg.2013.126", "10.1145/1517664.1517705", "10.1145/159544.159566", "10.1145/2207676.2208391", "10.1109/mc.2013.178", "10.1007/978-3-540-70956-5\\_2", "10.1109/tvcg.2015.2440233", "10.1109/icat.2007.41", "10.1145/300523.300542", "10.1057/palgrave.ivs.9500081", "10.1145/1143518.1143521", "10.1109/tvcg.2007.70515", "10.1016/j.ijhcs.2013.04.003", "10.1109/vl.1996.545307", "10.1111/1467-8659.00194", "10.1109/visual.2004.47", "10.1109/tvcg2011.224", "10.1109/38.888006", "10.1145/258549.258715", "10.1145/1731903.1731920", "10.1109/tvcg.2013.124", "10.1145/258519.258715", "10.1145/2207676.2208691", "10.1145/642611.642613", "10.1109/52.73754", "10.1111/j.1467-8659.2012.03115.x", "10.1145/1226969.1226998", "10.1145/765891.766003", "10.1109/38.267473", "10.2312/conf/eg2013/starni65-093", "10.1145/1347390.1347392", "10.1109/3dui.2014.6798839", "10.1109/tvcg.2011.279", "10.1109/tvcg.2011.283", "10.1109/3dvis.2014.7160102", "10.1145/253284.253315", "10.1109/tvcg.2010.164", "10.1016/0020-7373(91)90037-8", "10.1145/2839462.2839464", "10.1109/tvcg.2013.121", "10.1145/1088463.1088507", "10.1145/571985.572001", "10.1145/26283632628374", "10.1147/sj.393.0915", "10.20380/gi2009.16", "10.1109/tvcg.2012.217", "10.1109/tvcg.2010.157", "10.1057/palgrave.ivs.9500065", "10.1145/1978942.1979140", "10.1109/tvcg.2012.292", "10.1016/j.compmedimag.2011.06.003"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865191", "title": "FiberClay: Sculpting Three Dimensional Trajectories to Reveal Structural Insights", "year": "2018", "conferenceName": "InfoVis", "authors": "Christophe Hurter;Nathalie Henry Riche;Steven Mark Drucker;Maxime Cordeil;Richard Alligier;Romain Vuillemot", "citationCount": "11", "affiliation": "Hurter, C (Corresponding Author), French Civil Aviat Univ, ENAC, Toulouse, France. Hurter, C (Corresponding Author), Toulouse Univ, Toulouse, France. Hurter, Christophe ll; Alligier, Richard, French Civil Aviat Univ, ENAC, Toulouse, France. Hurter, Christophe ll; Alligier, Richard, Toulouse Univ, Toulouse, France. Riche, Nathalie Henry; Drucker, Steven M., Microsoft Res, Redmond, WA USA. Cordeil, Maxime, Monash Univ, Clayton, Vic, Australia. Vuillemot, Romain, Univ Lyon, Ecole Cent Lyon, CNRS, UMR 5205,LIRIS, F-69134 Lyon, France.", "countries": "USA;France;Australia", "abstract": "Visualizing 3D trajectories to extract insights about their similarities and spatial configuration is a critical task in several domains. Air traffic controllers for example deal with large quantities of aircrafts routes to optimize safety in airspace and neuroscientists attempt to understand neuronal pathways in the human brain by visualizing bundles of fibers from DTI images. Extracting insights from masses of 3D trajectories is challenging as the multiple three dimensional lines have complex geometries, may overlap, cross or even merge with each other, making it impossible to follow individual ones in dense areas. As trajectories are inherently spatial and three dimensional, we propose FiberClay: a system to display and interact with 3D trajectories in immersive environments. FiberClay renders a large quantity of trajectories in real time using GP-GPU techniques. FiberClay also introduces a new set of interactive techniques for composing complex queries in 3D space leveraging immersive environment controllers and user position. These techniques enable an analyst to select and compare sets of trajectories with specific geometries and data properties. We conclude by discussing insights found using FiberClay with domain experts in air traffic control and neurology.", "keywords": "Immersive Analytics,3D Visualization,Dynamic Queries,Bimanual Interaction,Multidimensional Data", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865191", "refList": ["10.1016/b978-0-08-051574-8.50047-9", "10.1016/j.ijhcs.2013.03.003", "10.1057/palgrave.ivs.9500097", "10.1109/tvcg.2008.153", "10.2307/1269768", "10.1109/tvcg.2013.226", "10.1109/tvcg.2005.59", "10.1016/j.compenvurbsys.2014.01.005", "10.1017/cbo9781139128926.013", "10.1109/pacificvis.2014.61", "10.1145/1279640.1279642", "10.1109/pacificvis.2017.8031577", "10.1109/tvcg.2016.2599217", "10.1109/visual.1991.175794", "10.1007/978-1-4615-1177-9\\_27", "10.1007/978-0-387-35504-714", "10.1109/tvcg.2011.233", "10.3390/informatics4030026", "10.1109/tvcg.2011.192", "10.1109/tvcg.2015.2403323", "10.1145/800186.810616", "10.1145/3013971.3014006", "10.1016/j.ijhcs.2005.02.001", "10.1109/3dvis.2014.7160096", "10.1111/j.1467-8659.2012.03115.x", "10.1109/38.946631", "10.1145/3126594.3126613", "10.1109/pacificvis.2011.5742390", "10.4230/dagrep.6.6.1", "10.1111/j.1467-8659.2012.03079.x", "10.1057/palgrave.ivs.9500061", "10.1109/tvcg.2017.2744338", "10.1109/tvcg.2009.145", "10.1109/tvcg.2012.265", "10.1007/s12031-007-0029-0", "10.1145/3025453.3025566", "10.1109/tvcg.2013.153", "10.1109/tvcg.2016.2520921", "10.1016/j.trc.2014.03.005", "10.1109/3dvis.2014.7160093", "10.1109/ipsn.2014.6846743", "10.1109/tvcg.2015.2467112", "10.1109/tvcg.2017.2744079", "10.1109/tvcg.2011.224", "10.1111/cgf.12804", "10.1111/j.1467-8659.2009.01687.x", "10.1016/s1088-467x(99)00013-x", "10.1109/tvcg.2012.217", "10.1145/2992154.2996365"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934415", "title": "Evaluating an Immersive Space-Time Cube Geovisualization for Intuitive Trajectory Data Exploration", "year": "2019", "conferenceName": "InfoVis", "authors": "Jorge A. Wagner Filho;Wolfgang Stuerzlinger;Luciana Porcher Nedel", "citationCount": "4", "affiliation": "Wagner, JA (Corresponding Author), Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil. Wagner, JA (Corresponding Author), Simon Fraser Univ, Burnaby, BC, Canada. Wagner Filho, Jorge A.; Nedel, Luciana, Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil. Wagner Filho, Jorge A.; Stuerzlinger, Wolfgang, Simon Fraser Univ, Burnaby, BC, Canada.", "countries": "Canada;Brazil", "abstract": "A Space-Time Cube enables analysts to clearly observe spatio-temporal features in movement trajectory datasets in geovisualization. However, its general usability is impacted by a lack of depth cues, a reported steep learning curve, and the requirement for efficient 3D navigation. In this work, we investigate a Space-Time Cube in the Immersive Analytics domain. Based on a review of previous work and selecting an appropriate exploration metaphor, we built a prototype environment where the cube is coupled to a virtual representation of the analyst's real desk, and zooming and panning in space and time are intuitively controlled using mid-air gestures. We compared our immersive environment to a desktop-based implementation in a user study with 20 participants across 7 tasks of varying difficulty, which targeted different user interface features. To investigate how performance is affected in the presence of clutter, we explored two scenarios with different numbers of trajectories. While the quantitative performance was similar for the majority of tasks, large differences appear when we analyze the patterns of interaction and consider subjective metrics. The immersive version of the Space-Time Cube received higher usability scores, much higher user preference, and was rated to have a lower mental workload, without causing participants discomfort in 25-minute-long VR sessions.", "keywords": "Space-time cube,Trajectory visualization,Immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934415", "refList": ["10.1109/mcg.2006.74", "10.1080/13658816.2015.1058386", "10.1145/1456650.1456652", "10.1111/cgf.13430", "10.1145/1944745.1944777", "10.1016/b978-155860819-1/50001-7", "10.1109/bdva.2018.8534024", "10.1109/pacificvis.2017.8031578", "10.1111/j.1467-8306.1994.tb01869.x", "10.2307/3001968", "10.1007/978-3-642-15300-6\\_21", "10.1080/13658816.2010.511223", "10.1007/978-3-030-01388-2", "10.1080/00087041.2018.1495898", "10.1145/1773965.1773970", "10.1002/9780470987643.ch15", "10.1109/glocom.2015.7417476", "10.2312/eurovisstar.20141171", "10.1080/17489725.2015.1074736", "10.1109/bdva.2016.7787050", "10.1109/wevr.2017.7957707", "10.1016/s1045-926x(03)00046-6", "10.1109/iv.2018.00026", "10.1179/1743277413y.0000000061", "10.1109/mcg.2019.2898856", "10.1109/iv.2004.1320137", "10.1111/j.1467-8659.2011.01929.x", "10.1109/tvcg.2008.194", "10.1016/j.jtrangeo.2010.11.002", "10.1177/154193120605000909", "10.1109/bigdata.2015.7364040", "10.1111/cgf.12466", "10.1109/icsens.2015.7370446", "10.1080/13658816.2010.508043", "10.1057/ivs.2009.8", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.14236/ewic/hci2016.22", "10.1145/2655691", "10.1007/bf01936872", "10.1109/infvis.2004.27", "10.1109/tvcg.2014.2329308", "10.1111/j.1435-5597.1970.tb01464.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934332", "title": "LassoNet: Deep Lasso-Selection of 3D Point Clouds", "year": "2019", "conferenceName": "SciVis", "authors": "Zhutian Chen;Wei Zeng 0004;Zhiguang Yang;Lingyun Yu;Chi-Wing Fu;Huamin Qu", "citationCount": "4", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Zhutian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei; Yang, Zhiguang, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yu, Lingyun, Univ Groningen, Groningen, Netherlands. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China.", "countries": "China;Netherlands", "abstract": "Selection is a fundamental task in exploratory analysis and visualization of 3D point clouds. Prior researches on selection methods were developed mainly based on heuristics such as local point density, thus limiting their applicability in general data. Specific challenges root in the great variabilities implied by point clouds (e.g., dense vs. sparse), viewpoint (e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this work, we introduce LassoNet, a new deep neural network for lasso selection of 3D point clouds, attempting to learn a latent mapping from viewpoint and lasso to point cloud regions. To achieve this, we couple user-target points with viewpoint and lasso information through 3D coordinate transform and naive selection, and improve the method scalability via an intention filtering and farthest point sampling. A hierarchical network is trained using a dataset with over 30K lasso-selection records on two different point cloud data. We conduct a formal user study to compare LassoNet with two state-of-the-art lasso-selection methods. The evaluations confirm that our approach improves the selection effectiveness and efficiency across different combinations of 3D point clouds, viewpoints, and lasso selections. Project Website: https://LassoNet.github.io", "keywords": "Point Clouds,Lasso Selection,Deep Learning", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934332", "refList": ["10.1111/j.1467-8659.2009.01515.x", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2016.2599049", "10.1109/iccv.2015.114", "10.1109/vppc.2018.8604993", "10.1111/cgf.13405", "10.1109/tvcg.2018.2843369", "10.1109/cvpr.2018.00278", "10.1145/3025453.3025957", "10.1109/mc.2013.178", "10.1145/2980179.2980238", "10.1109/msp.2017.2693418", "10.1145/237091.237105", "10.1145/2835487", "10.1109/cvpr.2010.5539838", "10.1016/j.visinf.2017.01.006", "10.1109/tcst.2018.2819965", "10.1109/tvcg.2018.2865138", "10.1109/83.623193", "10.1145/3072959.3073608", "10.1145/1053427.1053445", "10.1016/s0039-9140(96)02179-0", "10.1109/cvpr.2015.7298801", "10.1109/cvpr.2017.693", "10.1007/s12650-014-0206-5", "10.1145/3272127.3275110", "10.1109/cvpr.2015.7298845", "10.1109/iccvw.2015.112", "10.1109/iros.2015.7353481", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2012.217", "10.1109/tridui.2006.1618279", "10.1109/tvcg.2012.292", "10.1109/cvpr.2016.609", "10.1016/j.cag.2012.12.003"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028947", "title": "A Fluid Flow Data Set for Machine Learning and its Application to Neural Flow Map Interpolation", "year": "2020", "conferenceName": "SciVis", "authors": "Jakob Jakob;Markus H. Gross;Tobias G\u00fcnther", "citationCount": "0", "affiliation": "Jakob, J (Corresponding Author), Swiss Fed Inst Technol, Zurich, Switzerland. Jakob, Jakob; Gross, Markus; Guenther, Tobias, Swiss Fed Inst Technol, Zurich, Switzerland.", "countries": "Switzerland", "abstract": "In recent years, deep learning has opened countless research opportunities across many different disciplines. At present, visualization is mainly applied to explore and explain neural networks. Its counterpart-the application of deep learning to visualization problems-requires us to share data more openly in order to enable more scientists to engage in data-driven research. In this paper, we construct a large fluid flow data set and apply it to a deep learning problem in scientific visualization. Parameterized by the Reynolds number, the data set contains a wide spectrum of laminar and turbulent fluid flow regimes. The full data set was simulated on a high-performance compute cluster and contains 8000 time-dependent 2D vector fields, accumulating to more than 16 TB in size. Using our public fluid data set, we trained deep convolutional neural networks in order to set a benchmark for an improved post-hoc Lagrangian fluid flow analysis. In in-situ settings, flow maps are exported and interpolated in order to assess the transport characteristics of time-dependent fluids. Using deep learning, we improve the accuracy of flow map interpolations, allowing a more precise flow analysis at a reduced memory IO footprint.", "keywords": "Scientific visualization,deep learning,flow maps", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028947", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1145/3355089.3356560", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1007/978-3-030-00533-7\\_36", "10.3390/rs11161921", "10.1007/978-3-030-48457-6\\_1", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/mcg.2018.2881523", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1145/3072959.3073643", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/tvcg.2013.128", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1007/978-3-319-46475-6\\_43", "10.1126/science.1127647", "10.1007/978-3-319-46475-6\\_25", "10.1111/cgf.13689"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030346", "title": "V2V: A Deep Learning Approach to Variable-to-Variable Selection and Translation for Multivariate Time-Varying Data", "year": "2020", "conferenceName": "SciVis", "authors": "Jun Han;Hao Zheng 0006;Yunhao Xing;Danny Ziyi Chen;Chaoli Wang", "citationCount": "0", "affiliation": "Han, J (Corresponding Author), Univ Notre Dame, Notre Dame, IN 46556 USA. Han, Jun; Zheng, Hao; Chen, Danny Z.; Wang, Chaoli, Univ Notre Dame, Notre Dame, IN 46556 USA. Xing, Yunhao, Sichuan Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China;USA", "abstract": "We present V2V, a novel deep learning framework, as a general-purpose solution to the variable-to-variable (V2V) selection and translation problem for multivariate time-varying data (MTVD) analysis and visualization. V2V leverages a representation learning algorithm to identify transferable variables and utilizes Kullback-Leibler divergence to determine the source and target variables. It then uses a generative adversarial network (GAN) to learn the mapping from the source variable to the target variable via the adversarial, volumetric, and feature losses. V2V takes the pairs of time steps of the source and target variable as input for training, Once trained, it can infer unseen time steps of the target variable given the corresponding time steps of the source variable. Several multivariate time-varying data sets of different characteristics are used to demonstrate the effectiveness of V2V, both quantitatively and qualitatively. We compare V2V against histogram matching and two other deep learning solutions (Pix2Pix and CycleGAN).", "keywords": "Multivariate time-varying data,variable selection and translation,generative adversarial network,data extrapolation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030346", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/cvpr.2017.19", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1146/annurev", "10.3390/rs11161921", "10.1109/tvcg.2007.70551", "10.1145/3309993", "10.1007/978-3-030-48457-6\\_1", "10.1109/igarss.2018.8518411", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1146/annurev-fluid-010313-141322", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1126/science.1127647"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030374", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data", "year": "2020", "conferenceName": "SciVis", "authors": "Yifan Wang;Guoli Yan;Haikuan Zhu;Sagar Buch;Ying Wang;E. Mark Haacke;Jing Hua;Zichun Zhong", "citationCount": "0", "affiliation": "Wang, YF (Corresponding Author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Wang, Yifan; Yan, Guoli; Zhu, Haikuan; Hua, Jing; Zhong, Zichun, Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Buch, Sagar; Wang, Ying; Haacke, Ewart Mark, Wayne State Univ, Dept Radiol, Detroit, MI 48201 USA.", "countries": "USA", "abstract": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.", "keywords": "Deep neural network,3D cerebrovascular segmentation and visualization,maximum intensity projection (MIP),joint embedding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030374", "refList": ["10.1109/cluster.2018.00036", "10.1109/iccv.2015.123", "10.1109/tvcg.2013.133", "10.1109/cvpr.2017.19", "10.1109/tvcg.2019.2934312", "10.1007/978-3-319-46487-9\\_40", "10.1109/tvcg.2006.175", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2018.2880207", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/tvcg.2018.2816059", "10.1109/pacificvis.2009.4906852", "10.1109/tvcg.2018.2864808", "10.1109/mcg.2018.2881523", "10.1109/tvcg.2015.2467431", "10.1109/tpami.2015.2439281", "10.1109/pacificvis48177.2020.8737", "10.1109/cvpr.2019.00244", "10.1109/tvcg.2006.165", "10.1109/bigdata.2018.8622520", "10.1007/978-3-319-46466-4\\_29", "10.1109/visual.2019.8933759", "10.1145/3197517.3201304", "10.1109/pacificvis.2011.5742378", "10.1109/cvpr.2006.91", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1007/978-3-319-24574-4\\_28", "10.1109/pacificvis.2019.00041", "10.1109/tvcg.2019.2934332", "10.1109/cvpr.2018.00916", "10.1007/978-3-319-46475-6\\_43"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934803", "title": "There Is No Spoon: Evaluating Performance, Space Use, and Presence with Expert Domain Users in Immersive Analytics", "year": "2019", "conferenceName": "InfoVis", "authors": "Andrea Batch;Andrew Cunningham;Maxime Cordeil;Niklas Elmqvist;Tim Dwyer;Bruce H. Thomas;Kim Marriott", "citationCount": "6", "affiliation": "Batch, A (Corresponding Author), Univ Maryland, College Pk, MD 20742 USA. Batch, Andrea; Elmqvist, Niklas, Univ Maryland, College Pk, MD 20742 USA. Cunningham, Andrew; Thomas, Bruce H., Univ South Australia Adelaide, Adelaide, SA, Australia. Cordeil, Maxime; Dwyer, Tim; Marriott, Kim, Univ Melbourne, Melbourne, Vic, Australia.", "countries": "USA;Australia", "abstract": "Immersive analytics turns the very space surrounding the user into a canvas for data analysis, supporting human cognitive abilities in myriad ways. We present the results of a design study, contextual inquiry, and longitudinal evaluation involving professional economists using a Virtual Reality (VR) system for multidimensional visualization to explore actual economic data. Results from our preregistered evaluation highlight the varied use of space depending on context (exploration vs. presentation), the organization of space to support work, and the impact of immersion on navigation and orientation in the 3D analysis space.", "keywords": "Design study,evaluation,economic analysis,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934803", "refList": ["10.1145/3139131.3139141", "10.1109/2.60882", "10.1007/978-3-030-01388-2\\_1", "10.1109/38.250911", "10.1145/302979.303166", "10.1073/pnas.1306779110", "10.1109/tvcg.2012.219", "10.1162/105474698565686", "10.1016/0364-0213(94)90007-8", "10.1145/642611.642650", "10.1162/pres.1992.1.4.482", "10.1007/978-3-030-01388-2", "10.1109/2.19829", "10.1111/1467-8284.00096", "10.1007/978-3-030-01388-2\\_7", "10.1109/hicss.2011.339", "10.1109/tvcg.2016.2599107", "10.1145/3173574.3173664", "10.1145/1008653.1008669", "10.1109/tvcg.2017.2743990", "10.1145/642611", "10.1145/3126594.3126613", "10.1007/978-94-007-6833-89", "10.1162/pres\\_a\\_00124", "10.1016/0004-3702(94)00017-u", "10.1109/tvcg.2014.20", "10.1145/1168149.1168158", "10.1109/visual.1995.480800", "10.1109/icsens.2015.7370446", "10.1207/s15516709cog1701\\_1", "10.1162/105474601300343603", "10.1109/tvcg.2016.2518135", "10.1109/tvcg.2012.213", "10.1016/s0097-8493(02)00113-9", "10.1162/pres.1994.3.2.130", "10.1109/tvcg.2018.2865191", "10.1109/visual.1990.146402", "10.1162/105474601300343612", "10.1145/2702123.2702406", "10.1162/pres.1992.1.1.120", "10.1145/3009939.3009955", "10.1109/glocom.2015.7417476", "10.1162/pres\\_a\\_00261"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030450", "title": "Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment", "year": "2020", "conferenceName": "InfoVis", "authors": "Benjamin Lee;Xiaoyun Hu;Maxime Cordeil;Arnaud Prouzeau;Bernhard Jenny;Tim Dwyer", "citationCount": "0", "affiliation": "Lee, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Lee, Benjamin; Hu, Xiaoyun; Cordeil, Maxime; Prouzeau, Arnaud; Jenny, Bernhard; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia.", "countries": "Australia", "abstract": "Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA: the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner's personal workspace.", "keywords": "Immersive analytics,collaboration,virtual reality,qualitative study,multivariate data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030450", "refList": ["10.1109/tvcg.2008.153", "10.1007/s10606-004-5062-8", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2019.2914677", "10.1016/0020-7373(91)90039-a", "10.1109/tvcg.2011.287", "10.1117/12.2005484", "10.1109/mmul.2009.35", "10.1109/tvcg.2019.2934803", "10.1145/3359996.3364242", "10.1109/immersive.2016.7932384", "10.1016/j.future.2008.07.015", "10.1057/palgrave.ivs.9500167", "10.1109/mcg.2019.2898941", "10.1145/3343055.3360746", "10.1007/978-3-319-45853-3\\_8", "10.1145/2576099", "10.1145/2858036.2858039", "10.1109/tvcg.2016.2599107", "10.1145/3173574.3173664", "10.1007/978-3-030-01388-2\\_2", "10.1109/vr.2019.8797978", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1007/978-3-030-01388-2\\_8", "10.1023/a:1021271517844", "10.1109/bigdata.2014.7004282", "10.1109/tvcg.2019.2934395", "10.1109/ismar.2010.5643530", "10.1007/978-3-030-01388-22", "10.1145/2133806.2133821", "10.1109/3dvis.2014.7160093", "10.1145/2556288.2557058", "10.1109/vr.2019.8797845"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030392", "title": "ShuttleSpace: Exploring and Analyzing Movement Trajectory in Immersive Visualization", "year": "2020", "conferenceName": "InfoVis", "authors": "Shuainan Ye;Zhutian Chen;Xiangtong Chu;Yifan Wang;Siwei Fu;Lejun Shen;Kun Zhou;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD CG, Hangzhou, Peoples R China. Ye, Shuainan; Chu, Xiangtong; Wang, Yifan; Zhou, Kun; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD CG, Hangzhou, Peoples R China. Chen, Zhutian, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Fu, Siwei, Zhejiang Lab, Hangzhou, Zhejiang, Peoples R China. Shen, Lejun, Chengdu Sports Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China", "abstract": "We present ShuttleSpace, an immersive analytics system to assist experts in analyzing trajectory data in badminton. Trajectories in sports, such as the movement of players and balls, contain rich information on player behavior and thus have been widely analyzed by coaches and analysts to improve the players' performance. However, existing visual analytics systems often present the trajectories in court diagrams that are abstractions of reality, thereby causing difficulty for the experts to imagine the situation on the court and understand why the player acted in a certain way. With recent developments in immersive technologies, such as virtual reality (VR), experts gradually have the opportunity to see, feel, explore, and understand these 3D trajectories from the player's perspective. Yet, few research has studied how to support immersive analysis of sports data from such a perspective. Specific challenges are rooted in data presentation (e.g., how to seamlessly combine 2D and 3D visualizations) and interaction (e.g., how to naturally interact with data without keyboard and mouse) in VR. To address these challenges, we have worked closely with domain experts who have worked for a top national badminton team to design ShuttleSpace. Our system leverages 1) the peripheral vision to combine the 2D and 3D visualizations and 2) the VR controller to support natural interactions via a stroke metaphor. We demonstrate the effectiveness of ShuttleSpace through three case studies conducted by the experts with useful insights. We further conduct interviews with the experts whose feedback confirms that our first-person immersive analytics system is suitable and useful for analyzing badminton data.", "keywords": "Movement trajectory,badminton analytics,virtual reality", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030392", "refList": ["10.1016/j.eswa.2012.01.204", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1142/s1005386719000178", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/978-981-10-3994-2\\_1", "10.1007/978-3-319-07812-0\\_12", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1016/j.eswa.2017.07.027", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 35}, {"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2018.00035", "year": "2018", "title": "An Automatic Deformation Approach for Occlusion Free Egocentric Data Exploration", "conferenceName": "PacificVis", "authors": "Cheng Li;Joachim Moortgat;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Moortgat, Joachim; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Occlusion management is an important task for three dimension data exploration. For egocentric data exploration, the occlusion problems, caused by the camera being too close to opaque data elements, have not been well addressed by previous studies. In this paper, we propose an automatic approach to resolve these problems and provide an occlusion free egocentric data exploration. Our system utilizes a state transition model to monitor both the camera and the data, and manages the initiation, duration, and termination of deformation with animation. Our method can be applied to multiple types of scientific datasets, including volumetric data, polygon mesh data, and particle data. We demonstrate our method with different exploration tasks, including camera navigation, isovalue adjustment, transfer function adjustment, and time varying exploration. We have collaborated with a domain expert and received positive feedback.", "keywords": "Data Deformation; Occlusion Management; Data Exploration; Egocentric Visualization", "link": "https://doi.org/10.1109/PacificVis.2018.00035", "refList": ["10.1145/989863.989871", "10.1109/tvcg.2008.59", "10.1111/j.1467-8659.2008.01332.x", "10.1109/tvcg.2016.2599049", "10.1109/pacificvis.2014.14", "10.1109/tvcg.2007.70433", "10.1007/s10596-015-9501-z", "10.1109/38.610209", "10.1109/tvcg.2003.1207447", "10.1111/j.1467-8659.2008.01181.x", "10.1109/tvcg.2012.42", "10.1109/tvcg.2016.2599217", "10.1002/fld.3764", "10.1109/pacificvis.2013.6596123", "10.1109/tvcg.2006.140", "10.1109/tvcg.2012.143", "10.1109/tvcg.2007.70565", "10.1109/scivis.2015.7429485", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.167", "10.1145/505008.505039", "10.1109/tvcg.2006.144", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/38.595268", "10.1109/tvcg.2016.2518338", "10.1109/tvcg.2014.20", "10.1145/1980462.1980487", "10.1111/cgf.12466", "10.2312/compaesth/compaesth05/209-216", "10.1109/tvcg.2010.127", "10.1016/j.advwatres.2016.01.002", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2009.144", "10.1145/1462055.1462056"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.13710", "year": "2019", "title": "Hybrid Touch/Tangible Spatial 3D Data Selection", "conferenceName": "EuroVis", "authors": "Lonni Besan{\\c{c}}on;Mickael Sereno;Lingyun Yu;Mehdi Ammi;Tobias Isenberg", "citationCount": "6", "affiliation": "Besancon, L (Corresponding Author), Linkoping Univ, Norrkoping, Sweden.\nBesancon, L (Corresponding Author), Univ Paris Saclay, Paris, France.\nBesancon, Lonni, Linkoping Univ, Norrkoping, Sweden.\nSereno, Mickael; Isenberg, Tobias, Inria, Rocquencourt, France.\nBesancon, Lonni; Sereno, Mickael, Univ Paris Saclay, Paris, France.\nYu, Lingyun, Univ Groningen, Groningen, Netherlands.\nAmmi, Mehdi, Univ Paris 08, Paris, France.", "countries": "Sweden;France;Netherlands", "abstract": "We discuss spatial selection techniques for three-dimensional datasets. Such 3D spatial selection is fundamental to exploratory data analysis. While 2D selection is efficient for datasets with explicit shapes and structures, it is less efficient for data without such properties. We first propose a new taxonomy of 3D selection techniques, focusing on the amount of control the user has to define the selection volume. We then describe the 3D spatial selection technique Tangible Brush, which gives manual control over the final selection volume. It combines 2D touch with 6-DOF 3D tangible input to allow users to perform 3D selections in volumetric data. We use touch input to draw a 2D lasso, extruding it to a 3D selection volume based on the motion of a tangible, spatially-aware tablet. We describe our approach and present its quantitative and qualitative comparison to state-of-the-art structure-dependent selection. Our results show that, in addition to being dataset-independent, Tangible Brush is more accurate than existing dataset-dependent techniques, thus providing a trade-off between precision and effort.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13710", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2005.59", "10.1109/tvcg.2015.2467202", "10.7326/0003-4819-130-12-199906150-00008", "10.1145/1517664.1517705", "10.1145/2992154.2996779", "10.1109/mcg.2010.30", "10.1071/ch9490149", "10.1109/tridui.2006.1618277", "10.1007/978-3-319-45853-3\\_2", "10.1162/105474601750182333", "10.1007/bf00133570", "10.1109/3dui.2011.5759219", "10.1145/3025453.3025890", "10.1002/vis.277", "10.1145/3132272.3134125", "10.1007/978-3-642-18421-5\\_20", "10.1145/2839462.2839464", "10.1007/978-3-319-16940-8\\_3", "10.2312/egve/ipt\\_egve2005/201-209", "10.1145/3025453.3025863", "10.1109/tabletop.2006.33", "10.1109/tridui.2006.1618279", "10.12720/joig.1.4.166-170", "10.1109/tvcg.2012.292", "10.1145/965139.807392", "10.1145/2983310.2985750", "10.1016/j.ijhcs.2013.03.003", "10.1145/2559206.2578881", "10.2312/sbm/sbm06/123-129", "10.1109/tridui.2006.1618264", "10.1016/0097-8493(94)90062-0", "10.1006/jvlc.1998.0112", "10.1007/978-3-030-01388-2\\_4", "10.1109/tvcg.2016.2599217", "10.1016/j.ijhcs.2013.04.003", "10.1145/253284.253303", "10.1145/964696.964721", "10.1348/000712608x377117", "10.1145/3290607.3310432", "10.1007/bf01409796", "10.1145/1851600.1851631", "10.1177/154193120605000909", "10.1109/tvcg.2018.2848906", "10.1007/s12650-014-0206-5", "10.2312/conf/eg2013/stars/065-093", "10.1109/mcg.2009.117", "10.1145/2396636.2396674", "10.1109/tridui.2006.1618271", "10.1007/978-3-319-26633-6\\_13", "10.1145/237091.237102", "10.1109/38.974511", "10.1038/nature.2016.19503", "10.1145/1166253.1166260", "10.1145/2254556.2254641", "10.1177/0956797613504966", "10.1109/tvcg.2015.2440233", "10.1145/2470654.2470688", "10.1109/tvcg.2004.1260759", "10.1016/0031-3203(81)90028-5", "10.1111/1467-8659.00194", "10.1007/978-3-642-03655-2\\_73", "10.1007/978-3-319-45853-3\\_6", "10.2312/egve/jvrc10/017-024", "10.1109/3dui.2007.340783", "10.3389/fnins.2016.00454", "10.1145/1008653.1008689", "10.1038/nmeth.2659", "10.1007/978-3-540-85412-8\\_4", "10.1109/visual.2004.30", "10.1109/tvcg.2013.121", "10.1145/1095034.1095041", "10.1016/j.cag.2012.12.003", "10.1016/j.socec.2004.09.033", "10.1007/s00371-005-0330-2", "10.1016/s0734-189x(85)90153-7", "10.1145/1936652.1936684", "10.1145/2076354.2076390", "10.1145/1450579.1450588", "10.1145/765891.765982", "10.1109/mc.2013.178", "10.1201/b17511", "10.1145/958432.958438", "10.1109/mcg.2004.20", "10.1109/tablet0p.2006.33", "10.1145/1053427.1053445", "10.1145/1226969.1226998", "10.1109/tvcg.2012.217"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13716", "year": "2019", "title": "Augmenting Tactile 3D Data Navigation With Pressure Sensing", "conferenceName": "EuroVis", "authors": "Xiyao Wang;Lonni Besan{\\c{c}}on;Mehdi Ammi;Tobias Isenberg", "citationCount": "1", "affiliation": "Wang, XY (Corresponding Author), INRIA, Rocquencourt, France.\nWang, XY (Corresponding Author), Univ Paris Saclay, St Aubin, France.\nWang, Xiyao; Isenberg, Tobias, INRIA, Rocquencourt, France.\nWang, Xiyao, Univ Paris Saclay, St Aubin, France.\nBesancon, Lonni, Linkoping Univ, Norrkoping, Sweden.\nAmmi, Mehdi, Univ Paris 08, St Denis, France.", "countries": "Sweden;France", "abstract": "We present a pressure-augmented tactile 3D data navigation technique, specifically designed for small devices, motivated by the need to support the interactive visualization beyond traditional workstations. While touch input has been studied extensively on large screens, current techniques do not scale to small and portable devices. We use phone-based pressure sensing with a binary mapping to separate interaction degrees of freedom (DOF) and thus allow users to easily select different manipulation schemes (e. g., users first perform only rotation and then with a simple pressure input to switch to translation). We compare our technique to traditional 3D-RST (rotation, scaling, translation) using a docking task in a controlled experiment. The results show that our technique increases the accuracy of interaction, with limited impact on speed. We discuss the implications for 3D interaction design and verify that our results extend to older devices with pseudo pressure and are valid in realistic phone usage scenarios.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13716", "refList": ["10.1145/2207676.2208589", "10.1145/2788940.2788950", "10.1109/38.974511", "10.1145/1731903.1731930", "10.1038/nature.2016.19503", "10.1145/2559206.2578881", "10.1145/985692.985754", "10.1145/1851600.1851631", "10.1145/1124772.1124801", "10.1145/1643928.1643942", "10.1177/0956797613504966", "10.1109/mc.2013.178", "10.1109/mcg.2010.30", "10.1109/tvcg.2017.2744019", "10.1109/tvcg.2015.2440233", "10.1145/2992154.2996779", "10.1007/978-3-030-01388-2\\_4", "10.1145/3025453.3025565", "10.1109/tvcg.2016.2599217", "10.4108/icst.mobiquitous.2014.257919", "10.1109/tvcg.2004.1260759", "10.1207/s15327051hci0702\\_1", "10.1201/b17511", "10.7717/peerj.3544", "10.1016/b978-0-08-051574-8.50051-0", "10.1145/964696.964721", "10.1007/978-3-319-45853-3\\_6", "10.3390/s150101022", "10.1145/2642918.2647392", "10.1145/2207676.2208691", "10.1109/mcg.2004.20", "10.1145/2037373.2037393", "10.1145/2702123.2702371", "10.1145/3290607.3310432", "10.1145/2702123.2702300", "10.1145/1979742.1979895", "10.1111/j.1467-8659.2012.03197.x", "10.1145/2858036.2858095", "10.1109/3dui.2011.5759220", "10.1145/2493190.2493193", "10.1145/302979.302983", "10.1145/2541016.2541024", "10.1145/1054972.1055055", "10.1162/105474602317343640", "10.1145/2414536.2414572", "10.1145/2468356.2468578", "10.1038/nmeth.2659", "10.1109/tvcg.2011.283", "10.1016/0022-1031(71)90078-3", "10.1145/1868914.1868958", "10.1109/tvcg.2018.2865142", "10.1145/3025453.3025890", "10.1145/1140491.1140502", "10.1177/154193120605000909", "10.1109/tvcg.2018.2848906", "10.1145/2993148.2993152", "10.1007/s00779-018-1147-0", "10.4324/9780203807002", "10.1145/3027063.3053151", "10.1145/2371574.2371582", "10.1145/2396636.2396674", "10.1145/1088463.1088507", "10.1145/300523.300546", "10.1145/3025453.3025863", "10.1109/tvcg.2012.251", "10.1109/tridui.2006.1618267", "10.1109/tvcg.2011.224", "10.1080/01973533.2015.1060240", "10.1007/978-3-319-26633-6\\_13", "10.1145/1889863.1889888", "10.1109/tvcg.2010.157", "10.1109/tvcg.2007.70515", "10.1109/tabletop.2006.26", "10.1145/965139.807392", "10.1145/2983310.2985750", "10.20380/gi2009.23"], "wos": 1, "children": [], "len": 1}], "len": 47}, {"doi": "10.1109/tvcg.2017.2745941", "title": "The Hologram in My Hand: How Effective is Interactive Exploration of 3D Visualizations in Immersive Tangible Augmented Reality?", "year": "2017", "conferenceName": "InfoVis", "authors": "Benjamin Bach;Ronell Sicat;Johanna Beyer;Maxime Cordeil;Hanspeter Pfister", "citationCount": "32", "affiliation": "Bach, B (Corresponding Author), Harvard Univ, Cambridge, MA 02138 USA. Bach, Benjamin; Sicat, Ronell; Beyer, Johanna; Pfister, Hanspeter, Harvard Univ, Cambridge, MA 02138 USA. Cordeil, Maxime, Monash Univ, Clayton, Vic, Australia.", "countries": "USA;Australia", "abstract": "We report on a controlled user study comparing three visualization environments for common 3D exploration. Our environments differ in how they exploit natural human perception and interaction capabilities. We compare an augmented-reality head-mounted display (Microsoft HoloLens), a handheld tablet, and a desktop setup. The novel head-mounted HoloLens display projects stereoscopic images of virtual content into a user's real world and allows for interaction in-situ at the spatial position of the 3D hologram. The tablet is able to interact with 3D content through touch, spatial positioning, and tangible markers, however, 3D content is still presented on a 2D surface. Our hypothesis is that visualization environments that match human perceptual and interaction capabilities better to the task at hand improve understanding of 3D visualizations. To better understand the space of display and interaction modalities in visualization environments, we first propose a classification based on three dimensions: perception, interaction, and the spatial and cognitive proximity of the two. Each technique in our study is located at a different position along these three dimensions. We asked 15 participants to perform four tasks, each task having different levels of difficulty for both spatial perception and degrees of freedom for interaction. Our results show that each of the tested environments is more effective for certain tasks, but that generally the desktop environment is still fastest and most precise in almost all cases.", "keywords": "Augmented Reality,3D Interaction,User Study,Immersive Displays", "link": "http://dx.doi.org/10.1109/TVCG.2017.2745941", "refList": ["10.1109/3dui.2010.5444707", "10.1109/tvcg.2008.153", "10.1109/tvcg.2015.2467202", "10.1109/mcg.2003.1242376", "10.1145/234972.234975", "10.1145/354384.354535", "10.1518/001872001775992534", "10.1109/3dui.2012.6184212", "10.1109/ismar.2014.6948430", "10.1109/mc.2013.178", "10.1109/tvcg.2015.2440233", "10.1145/1279640.1279642", "10.1109/tvcg.2013.134", "10.1109/vrais.1996.490509", "10.1109/vrais.1993.380805", "10.1117/12.342825", "10.1007/s00779-004-0297-4", "10.1145/258549.258715", "10.1109/ismar.2008.4637362", "10.1145/258519.258715", "10.1145/3009939.3009947", "10.1109/tvcg.2016.2599107", "10.1109/iv.2013.51", "10.1109/3dvis.2014.7160096", "10.1109/3dvis.2014.7160098", "10.1109/tvcg.2012.216", "10.1109/3dui.2014.6798839", "10.1109/ismar.2003.1240691", "10.1109/ismar.2010.5643530", "10.1145/1842993.1843023", "10.1109/tvcg.2013.121", "10.1007/bf01936872", "10.1109/3dvis.2014.7160093", "10.1007/s00779-004-0296-5", "10.1145/257089.257242", "10.1145/2702123.2702604", "10.1145/1180495.1180518", "10.1145/2470654.2481359", "10.1111/j.1435-5597.1970.tb01464.x", "10.1109/tvcg.2006.17", "10.1145/2992154.2996365", "10.1109/tvcg.2011.234"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865152", "title": "DXR: A Toolkit for Building Immersive Data Visualizations", "year": "2018", "conferenceName": "InfoVis", "authors": "Ronell Sicat;Jiabao Li;Junyoung Choi;Maxime Cordeil;Won-Ki Jeong;Benjamin Bach;Hanspeter Pfister", "citationCount": "12", "affiliation": "Sicat, R (Corresponding Author), Harvard Visual Comp Grp, Cambridge, MA USA. Sicat, Ronell; Pfister, Hanspeter, Harvard Visual Comp Grp, Cambridge, MA USA. Li, Jiabao, Harvard Grad Sch Design, Cambridge, MA USA. Bach, Benjamin, Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland. Cordeil, Maxime, Monash Univ, Immers Analyt Lab, Clayton, Vic, Australia. Choi, JunYoung; Jeong, Won-Ki, Ulsan Natl Inst Sci \\& Technol, Ulsan, South Korea.", "countries": "Scotland;USA;Korea;Australia", "abstract": "This paper presents DXR, a toolkit for building immersive data visualizations based on the Unity development platform. Over the past years, immersive data visualizations in augmented and virtual reality (AR, VR) have been emerging as a promising medium for data sense-making beyond the desktop. However, creating immersive visualizations remains challenging, and often require complex low-level programming and tedious manual encoding of data attributes to geometric and visual properties. These can hinder the iterative idea-to-prototype process, especially for developers without experience in 3D graphics, AR, and VR programming. With DXR, developers can efficiently specify visualization designs using a concise declarative visualization grammar inspired by Vega-Lite. DXR further provides a GUI for easy and quick edits and previews of visualization designs in-situ, i.e., while immersed in the virtual world. DXR also provides reusable templates and customizable graphical marks, enabling unique and engaging visualizations. We demonstrate the flexibility of DXR through several examples spanning a wide range of applications.", "keywords": "Augmented Reality,Virtual Reality,Immersive Visualization,Immersive Analytics,Visualization Toolkit", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865152", "refList": ["10.1145/2642918.2647369", "10.1109/tvcg.2015.2467449", "10.1109/tvcg.2011.185", "10.1145/2598153.2598175", "10.1109/tvcg.2016.2598609", "10.1145/1980462.1980470", "10.1109/tvcg.2014.2346318", "10.1145/3009939.3009947", "10.2312/conf/eg2013/stars/039-063", "10.1145/3013971.3014006", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2016.2599107", "10.1145/3173574.3173664", "10.1109/tvcg.2017.2745941", "10.1145/2556288.2557010", "10.1145/3126594.3126613", "10.1016/j.visinf.2017.11.002", "10.1111/cgf.12391", "10.1109/tvcg.2015.2467091", "10.1109/bigdata.2014.7004282", "10.1109/bdva.2015.7314302", "10.1109/tvcg.2010.144", "10.1109/tvcg.2009.174", "10.1109/infvis.2004.64", "10.1145/2133416.2146416", "10.1109/tvcg.2014.2346322", "10.1109/tvcg.2017.2744079", "10.1111/cgf.12804", "10.1371/journal.pone.0057990", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934282", "title": "Designing for Mobile and Immersive Visual Analytics in the Field", "year": "2019", "conferenceName": "InfoVis", "authors": "Matt Whitlock;Keke Wu;Danielle Albers Szafir", "citationCount": "0", "affiliation": "Whitlock, M (Corresponding Author), Univ Colorado, Boulder, CO 80309 USA. Whitlock, Matt; Wu, Keke; Szafir, Danielle, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Data collection and analysis in the field is critical for operations in domains such as environmental science and public safety. However, field workers currently face data- and platform-oriented issues in efficient data collection and analysis in the field, such as limited connectivity, screen space, and attentional resources. In this paper, we explore how visual analytics tools might transform field practices by more deeply integrating data into these operations. We use a design probe coupling mobile, cloud, and immersive analytics components to guide interviews with ten experts from five domains to explore how visual analytics could support data collection and analysis needs in the field. The results identify shortcomings of current approaches and target scenarios and design considerations for future field analysis systems. We embody these findings in FieldView, an extensible, open-source prototype designed to support critical use cases for situated field analysis. Our findings suggest the potential for integrating mobile and immersive technologies to enhance data's utility for various field operations and new directions for visual analytics tools to transform fieldwork.", "keywords": "Immersive Analytics,Augmented Reality,Mobile Visualization,Outdoor Visualization,Emergency Response", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934282", "refList": ["10.1007/978-3-319-45853-3\\_15", "10.1145/355324.355329", "10.1186/1472-6947-9-51", "10.1007/978-3-540-69878-4\\_12", "10.1109/tvcg.2018.2868584", "10.1007/978-3-319-45853-315", "10.1007/s10606-015-9235-4", "10.1109/futuretech.2010.5482712", "10.1016/s0001-4575(99)00018-4", "10.1130/ges00503.1", "10.1109/tvcg.2017.2744019", "10.1145/3173574.3173593", "10.1109/mcom.2011.6069707", "10.1145/2750858.2804266", "10.1145/2785830.2785876", "10.1109/tvcg.2018.2865234", "10.1016/s0097-8493(01)00090-5", "10.1109/tvcg.2016.2598608", "10.1145/2851581.2892322", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2017.2745941", "10.1518/155534308x284381", "10.1057/palgrave.ivs.9500168", "10.1144/0016-764905-017", "10.3390/mti1040029", "10.1007/978-3-319-40397-7\\_2", "10.1109/mc.2006.109", "10.1145/2535597.2535608", "10.1109/mcom.2010.5560598", "10.1518/001872095779049543", "10.1145/3025453.3025860", "10.1145/3025453.3025752", "10.1016/b978-0-12-809477-8.00009-1", "10.1126/science.aag2579", "10.1109/mc.2013.147", "10.1145/1050491.1050499", "10.1186/s41039-016-0028-2", "10.1007/978-3-030-01388-22", "10.1002/esp.1417", "10.1177/0193841x02250527", "10.1111/1467-9671.00157", "10.1007/s11069-017-2929-9", "10.1109/tvcg.2018.2864914", "10.1016/j.autcon.2012.09.002"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934395", "title": "The Impact of Immersion on Cluster Identification Tasks", "year": "2019", "conferenceName": "InfoVis", "authors": "Matthias Kraus;Niklas Weiler;Daniela Oelke;Johannes Kehrer;Daniel A. Keim;Johannes Fuchs", "citationCount": "4", "affiliation": "Kraus, M (Corresponding Author), Univ Konstanz, Constance, Germany. Kraus, M.; Weiler, N.; Keim, D. A.; Fuchs, J., Univ Konstanz, Constance, Germany. Oelke, D.; Kehrer, J., Siemens Corp Technol, Munich, Germany.", "countries": "Germany", "abstract": "Recent developments in technology encourage the use of head-mounted displays (HMDs) as a medium to explore visualizations in virtual realities (VRs). VR environments (VREs) enable new, more immersive visualization design spaces compared to traditional computer screens. Previous studies in different domains, such as medicine, psychology, and geology, report a positive effect of immersion, e.g., on learning performance or phobia treatment effectiveness. Our work presented in this paper assesses the applicability of those findings to a common task from the information visualization (InfoVis) domain. We conducted a quantitative user study to investigate the impact of immersion on cluster identification tasks in scatterplot visualizations. The main experiment was carried out with 18 participants in a within-subjects setting using four different visualizations, (1) a 2D scatterplot matrix on a screen, (2) a 3D scatterplot on a screen, (3) a 3D scatterplot miniature in a VRE and (4) a fully immersive 3D scatterplot in a VRE. The four visualization design spaces vary in their level of immersion, as shown in a supplementary study. The results of our main study indicate that task performance differs between the investigated visualization design spaces in terms of accuracy, efficiency, memorability, sense of orientation, and user preference. In particular, the 2D visualization on the screen performed worse compared to the 3D visualizations with regard to the measured variables. The study shows that an increased level of immersion can be a substantial benefit in the context of 3D data and cluster detection.", "keywords": "Virtual reality,evaluation,visual analytics,clustering", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934395", "refList": ["10.1111/j.1467-9671.2010.01194.x", "10.1177/1473871614556393", "10.1109/tvcg.2008.153", "10.1111/cgf.13430", "10.1109/tvcg.2004.17", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2007.70433", "10.1089/109493103322011641", "10.2307/2290001", "10.2307/2289444", "10.1016/j.ijhcs.2008.04.004", "10.1109/tvcg.2012.42", "10.1109/vr.2018.8447558", "10.1162/105474602760204318", "10.2307/2986199", "10.3758/bf03200735", "10.1097/opx.0b013e31825da430", "10.1109/infvis.1999.801851", "10.1109/vast.2008.4677350", "10.1162/105474698565686", "10.1109/iv.2013.51", "10.1109/tvcg.2017.2745941", "10.1109/infvis.1998.729555", "10.1111/j.1467-8659.2012.03125.x", "10.1109/2945.506223", "10.1016/s1045-926x(03)00046-6", "10.1097/00042871-200701010-00099", "10.1109/iv.2004.1320137", "10.1109/visual.2002.1183816", "10.1109/tvcg.2018.2864477", "10.1109/vast.2007.4389000", "10.1016/j.ijms.2006.06.015", "10.1007/pl00022704", "10.1111/cgf.13072", "10.1145/3290605.3300555", "10.1109/bdva.2016.7787042", "10.1109/hicss.2013.197", "10.2312/vissym/vissym04/255-260", "10.1115/imece2007-43781", "10.1007/978-4-431-68057-43", "10.1109/tvcg.2016.2520921", "10.1109/mcg.2004.1255801", "10.1109/tvcg.2013.153", "10.1162/pres.1996.5.3.274", "10.1198/106186004x12425", "10.1162/105474601300343612", "10.1162/pres.1997.6.6.603", "10.1111/cgf.12804", "10.1111/j.1467-8659.2009.01666.x", "10.1109/vr.2004.1310069", "10.1109/vr.1999.756938", "10.1007/978-3-658-02897-8\\_16", "10.1162/pres\\_a\\_00016", "10.2307/2288711"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030450", "title": "Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment", "year": "2020", "conferenceName": "InfoVis", "authors": "Benjamin Lee;Xiaoyun Hu;Maxime Cordeil;Arnaud Prouzeau;Bernhard Jenny;Tim Dwyer", "citationCount": "0", "affiliation": "Lee, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Lee, Benjamin; Hu, Xiaoyun; Cordeil, Maxime; Prouzeau, Arnaud; Jenny, Bernhard; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia.", "countries": "Australia", "abstract": "Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA: the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner's personal workspace.", "keywords": "Immersive analytics,collaboration,virtual reality,qualitative study,multivariate data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030450", "refList": ["10.1109/tvcg.2008.153", "10.1007/s10606-004-5062-8", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2019.2914677", "10.1016/0020-7373(91)90039-a", "10.1109/tvcg.2011.287", "10.1117/12.2005484", "10.1109/mmul.2009.35", "10.1109/tvcg.2019.2934803", "10.1145/3359996.3364242", "10.1109/immersive.2016.7932384", "10.1016/j.future.2008.07.015", "10.1057/palgrave.ivs.9500167", "10.1109/mcg.2019.2898941", "10.1145/3343055.3360746", "10.1007/978-3-319-45853-3\\_8", "10.1145/2576099", "10.1145/2858036.2858039", "10.1109/tvcg.2016.2599107", "10.1145/3173574.3173664", "10.1007/978-3-030-01388-2\\_2", "10.1109/vr.2019.8797978", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1007/978-3-030-01388-2\\_8", "10.1023/a:1021271517844", "10.1109/bigdata.2014.7004282", "10.1109/tvcg.2019.2934395", "10.1109/ismar.2010.5643530", "10.1007/978-3-030-01388-22", "10.1145/2133806.2133821", "10.1109/3dvis.2014.7160093", "10.1145/2556288.2557058", "10.1109/vr.2019.8797845"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/tvcg.2020.3029412", "title": "A Bayesian cognition approach for belief updating of correlation judgement through uncertainty visualizations", "year": "2020", "conferenceName": "InfoVis", "authors": "Alireza Karduni;Douglas Markant;Ryan Wesslen;Wenwen Dou", "citationCount": "0", "affiliation": "Karduni, A (Corresponding Author), Univ North Carolina Charlotte, Charlotte, NC 28223 USA. Karduni, Alireza; Markant, Douglas; Wesslen, Ryan; Dou, Wenwen, Univ North Carolina Charlotte, Charlotte, NC 28223 USA.", "countries": "USA", "abstract": "Understanding correlation judgement is important to designing effective visualizations of bivariate data. Prior work on correlation perception has not considered how factors including prior beliefs and uncertainty representation impact such judgements. The present work focuses on the impact of uncertainty communication when judging bivariate visualizations. Specifically, we model how users update their beliefs about variable relationships after seeing a scatterplot with and without uncertainty representation. To model and evaluate the belief updating, we present three studies. Study 1 focuses on a proposed \u201cLine + Cone\u201d visual elicitation method for capturing users' beliefs in an accurate and intuitive fashion. The findings reveal that our proposed method of belief solicitation reduces complexity and accurately captures the users' uncertainty about a range of bivariate relationships. Study 2 leverages the \u201cLine + Cone\u201d elicitation method to measure belief updating on the relationship between different sets of variables when seeing correlation visualization with and without uncertainty representation. We compare changes in users beliefs to the predictions of Bayesian cognitive models which provide normative benchmarks for how users should update their prior beliefs about a relationship in light of observed data. The findings from Study 2 revealed that one of the visualization conditions with uncertainty communication led to users being slightly more confident about their judgement compared to visualization without uncertainty information. Study 3 builds on findings from Study 2 and explores differences in belief update when the bivariate visualization is congruent or incongruent with users' prior belief. Our results highlight the effects of incorporating uncertainty representation, and the potential of measuring belief updating on correlation judgement with Bayesian cognitive models.", "keywords": "Information visualization,Bayesian modeling,uncertainty visualizations,correlations,belief elicitation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3029412", "refList": ["10.1109/tvcg.2014.2346424", "10.1177/0539018417752089", "10.1145/2702613.2732501", "10.1007/978-3-319-91376-6\\_19", "10.1109/vlsicircuits18222.2020.9162811", "10.1145/2702123.2702431", "10.1371/journal.pbio.1000412", "10.1145/3290605.3300447", "10.1109/tvcg.2009.139", "10.1145/3161187", "10.1016/j.endeavour.2004.07.003", "10.1371/journal.pone.0133753", "10.1109/tfuzz.2019.2919484", "10.1111/j.1365-2362.2009.02234.x", "10.1145/2858036.2858387", "10.1017/s0269889713000082", "10.1109/tvcg.2017.2745941", "10.1080/21548455.2014.941040", "10.1073/pnas.1710351114", "10.1145/1323688.1323689", "10.1109/tvcg.2013.210", "10.1109/tvcg.2012.262", "10.1007/s11165-013-9358-x", "10.1109/mcg.2017.33", "10.3389/fpsyg.2013.00186", "10.1162/posc\\_a\\_00240", "10.1136/bmj.c332", "10.1001/jama.283.15.2008", "10.1145/3173574.3173612", "10.1145/2470654.2470724", "10.1016/j.jhevol.2010.04.005", "10.22323/2.17010401", "10.1002/0470870168", "10.1109/tvcg.2014.2346298", "10.1109/tvcg.2013.234", "10.1109/tvcg.2018.2864889", "10.1136/bmj.330.7485.256-a", "10.1109/tii.2019.2958106", "10.1007/978-3-319-26633-6\\_13", "10.1109/cicc48029.2020.9075900", "10.1016/s0140-6736(09)60329-9"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030433", "title": "Data Comics for Reporting Controlled User Studies in Human-Computer Interaction", "year": "2020", "conferenceName": "InfoVis", "authors": "Zezhong Wang;Jacob Ritchie;Jingtao Zhou;Fanny Chevalier;Benjamin Bach", "citationCount": "0", "affiliation": "Wang, ZZ (Corresponding Author), Univ Edinburgh, Edinburgh, Midlothian, Scotland. Wang, Zezhong; Zhou, Jingtao; Bach, Benjamin, Univ Edinburgh, Edinburgh, Midlothian, Scotland. Ritchie, Jacob, Stanford Univ, Stanford, CA 94305 USA. Zhou, Jingtao, Tianjin Univ, Tianjin, Peoples R China. Chevalier, Fanny, Univ Toronto, Toronto, ON, Canada.", "countries": "Scotland;Canada;China;USA", "abstract": "Inspired by data comics, this paper introduces a novel format for reporting controlled studies in the domain of human-computer interaction (HCI). While many studies in HCI follow similar steps in explaining hypotheses, laying out a study design, and reporting results, many of these decisions are buried in blocks of dense scientific text. We propose leveraging data comics as study reports to provide an open and glanceable view of studies by tightly integrating text and images, illustrating design decisions and key insights visually, resulting in visual narratives that can be compelling to non-scientists and researchers alike. Use cases of data comics study reports range from illustrations for non-scientific audiences to graphical abstracts, study summaries, technical talks, textbooks, teaching, blogs, supplementary submission material, and inclusion in scientific articles. This paper provides examples of data comics study reports alongside a graphical repertoire of examples, embedded in a framework of guidelines for creating comics reports which was iterated upon and evaluated through a series of collaborative design sessions.", "keywords": "Statistical communication,comics,scientific reports", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030433", "refList": ["10.1109/tvcg.2014.2346424", "10.1177/0539018417752089", "10.1145/2702123.2702431", "10.1371/journal.pbio.1000412", "10.1145/3290605.3300447", "10.1109/tvcg.2009.139", "10.1145/3161187", "10.18637/jss.v040.i01", "10.1016/j.endeavour.2004.07.003", "10.1145/989863.989865", "10.1111/j.1365-2362.2009.02234.x", "10.1145/2858036.2858387", "10.1109/tvcg.2017.2745941", "10.1080/21548455.2014.941040", "10.1073/pnas.1710351114", "10.1145/1323688.1323689", "10.1109/tvcg.2013.210", "10.1109/tvcg.2012.262", "10.1007/s00778-008-0098-x", "10.1007/s11165-013-9358-x", "10.1109/mcg.2017.33", "10.1162/posc\\_a\\_00240", "10.1001/jama.283.15.2008", "10.1145/3173574.3173612", "10.17349/jmc117309", "10.1016/j.jhevol.2010.04.005", "10.22323/2.17010401", "10.1002/0470870168", "10.1145/1378773.1378792", "10.1109/tvcg.2014.2346298", "10.1109/tvcg.2013.234", "10.1109/tvcg.2018.2864889", "10.1136/bmj.330.7485.256-a", "10.25165/j.ijabe.20201302.5353", "10.1007/978-3-319-26633-6\\_13", "10.1016/s0140-6736(09)60329-9", "10.1016/j.ces.2018.11.036"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13673", "year": "2019", "title": "Multiple Views: different meanings and collocated words", "conferenceName": "EuroVis", "authors": "Jonathan C. Roberts;Hayder Al{-}Maneea;Peter W. S. Butcher;Robert Lew;Geraint Rees;Nirwan Sharma;Ana Frankenberg{-}Garcia", "citationCount": "1", "affiliation": "Roberts, JC (Corresponding Author), Bangor Univ, Bangor, Gwynedd, Wales.\nRoberts, J. C.; Al-maneea, H.; Butcher, P. W. S.; Sharma, N., Bangor Univ, Bangor, Gwynedd, Wales.\nAl-maneea, H., Basrah Univ, Basrah, Iraq.\nLew, R.; Rees, G., Adam Mickiewicz Univ, Poznan, Poland.\nFrankenberg-Garcia, A., Univ Surrey, Guildford, Surrey, England.", "countries": "Poland;Wales;England;Iraq", "abstract": "We report on an in-depth corpus linguistic study on multiple views' terminology and word collocation. We take a broad interpretation of these terms, and explore the meaning and diversity of their use in visualisation literature. First we explore senses of the term multiple views' (e.g., multiple views' can mean juxtaposition, many viewport projections or several alternative opinions). Second, we investigate term popularity and frequency of occurrences, investigating usage of multiple' and view' (e.g., multiple views, multiple visualisations, multiple sets). Third, we investigate word collocations and terms that have a similar sense (e.g., multiple views, side-by-side, small multiples). We built and used several corpora, including a 6-million-word corpus of all IEEE Visualisation conference articles published in IEEE Transactions on Visualisation and Computer Graphics 2012 to 2017. We draw on our substantial experience from early work in coordinated and multiple views, and with collocation analysis develop several lists of terms. This research provides insight into term use, a reference for novice and expert authors in visualisation, and contributes a taxonomy of multiple view' terms.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13673", "refList": ["10.1109/tvcg.2017.2744359", "10.1109/38.31462", "10.1109/cmv.2003.1215002", "10.2307/2289444", "10.1016/j.ijhcs.2011.02.007", "10.1017/s0022112091210654", "10.1016/b978-008044531-1/50426-7", "10.1109/cmv.2003.1215001", "10.1109/tvcg.2013.134", "10.1117/12.378894", "10.1037/0096-1523.21.6.1494", "10.1057/palgrave.ivs.9500086", "10.1109/iv.2011.42", "10.1109/tvcg.2017.2744199", "10.4324/9780203810088", "10.1145/1321440.1321580", "10.1109/tvcg.2015.2467271", "10.1016/j.learninstruc.2006.03.001", "10.1145/345513.345282", "10.1007/s40607-014-0009-9", "10.1109/tvcg.2017.2743859", "10.1109/infvis.1997.636761", "10.1109/tvcg.2013.219", "10.1007/978-3-319-55627-7", "10.1109/tvcg.2009.111", "10.1109/iv.2008.21", "10.1109/infvis.2002.1173157", "10.1109/mcg.2014.82", "10.1145/3143699.3143717", "10.1109/tvcg.2012.226", "10.1145/345513.345271", "10.2478/jazcas-2018-0006", "10.1145/2556288.2556969", "10.1109/visual.1998.745282", "10.1145/1276377.1276427", "10.1109/infvis.2001.963283", "10.1007/978-1-4471-6497-5\\_1", "10.1109/visual.1994.346302", "10.1057/palgrave.ivs.9500068", "10.1109/tvcg.2006.160", "10.1109/visual.1990.146374", "10.1109/tvcg.2016.2614803", "10.1177/1473871611416549", "10.1109/tvcg.2017.2745878", "10.1109/tvcg.2006.69", "10.1109/tpami.1983.4767367", "10.1109/visual.1991.175794", "10.1109/tvcg.2017.2744159", "10.1109/tvcg.2017.2744198", "10.1109/32.328995", "10.1016/j.jeap.2018.07.003", "10.1016/j.geomorph.2012.08.021", "10.1109/tvcg.2014.2346920", "10.1109/2.917550", "10.1017/s0958344018000150", "10.1109/tvcg.2009.94", "10.1179/2051819613z.0000000003", "10.1109/vast.2008.4677370", "10.1117/12.309533", "10.1109/tvcg.2014.2346747", "10.1075/ijcl.13.4.06ray", "10.1007/s12650-015-0323-9", "10.1109/tvcg.2006.178", "10.1109/tvcg.2016.2615308", "10.2307/1269768", "10.2307/4132312", "10.1109/tvcg.2018.2864903", "10.1109/mis.2006.100", "10.1109/iv.1998.694193", "10.1007/s11192-015-1830-0", "10.1109/tvcg.2017.2744080", "10.1111/j.1467-9280.1997.tb00442.x", "10.1109/cmv.2007.20", "10.1109/infvis.2003.1249006", "10.1109/tvcg.2017.2745941", "10.1016/s1364-6613(99)01332-7", "10.1145/989863.989893", "10.1109/tse.1985.232211", "10.1109/tvcg.2017.2744184", "10.1109/infvis.2004.12", "10.1075/ijcl.17.3.04har", "10.1109/tvcg.2010.179", "10.1016/j.jss.2006.05.024", "10.1109/cmv.2003.1215005", "10.1109/tvcg.2017.2744358", "10.1145/2992154.2996365", "10.1109/tvcg.2016.2598827"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030338", "title": "Composition and Configuration Patterns in Multiple-View Visualizations", "year": "2020", "conferenceName": "InfoVis", "authors": "Xi Chen;Wei Zeng 0004;Yanna Lin;Hayder Al-Maneea;Jonathan Roberts 0002;Remco Chang", "citationCount": "0", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Xi; Zeng, Wei; Lin, Yanna, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. AI-maneea, Hayder Mahdi; Roberts, Jonathan, Bangor Univ, Bangor, Gwynedd, Wales. Chang, Remco, Tufts Univ, Medford, MA 02155 USA.", "countries": "USA;Wales;China", "abstract": "Multiple-view visualization (MV) is a layout design technique often employed to help users see a large number of data attributes and values in a single cohesive representation. Because of its generalizability, the MV design has been widely adopted by the visualization community to help users examine and interact with large, complex, and high-dimensional data. However, although ubiquitous, there has been little work to categorize and analyze MVs in order to better understand its design space. As a result, there has been little to no guideline in how to use the MV design effectively. In this paper, we present an in-depth study of how MVs are designed in practice. We focus on two fundamental measures of multiple-view patterns: composition, which quantifies what view types and how many are there; and configuration, which characterizes spatial arrangement of view layouts in the display space. We build a new dataset containing 360 images of MVs collected from IEEE VIS, EuroVis, and PacificVis publications 2011 to 2019, and make fine-grained annotations of view types and layouts for these visualization images. From this data we conduct composition and configuration analyses using quantitative metrics of term frequency and layout topology. We identify common practices around MVs, including relationship of view types, popular view layouts, and correlation between view types and layouts. We combine the findings into a MV recommendation system, providing interactive tools to explore the design space, and support example-based design.", "keywords": "Multiple views,design pattern,quantitative analysis,example-based design", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030338", "refList": ["10.1109/tvcg.2018.2865235", "10.1109/tvcg.2016.2615308", "10.1145/2642918.2647398", "10.1177/1473871611416549", "10.1109/tvcg.2019.2934810", "10.1109/tvcg.2014.48", "10.1109/tvcg.2018.2864903", "10.1145/345513.345271", "10.1109/iv.1998.694193", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2017.2744019", "10.1109/tvcg.2011.185", "10.1109/tvcg.2015.2467194", "10.1109/visual.1991.175815", "10.14778/2831360.2831371", "10.1109/mcg.2019.2924636", "10.1111/cgf.13673", "10.1111/cgf.12131", "10.1109/tvcg.2017.2744198", "10.1145/108360.108361", "10.1109/vl.1996.545307", "10.1109/tvcg.2017.2745140", "10.1109/cmv.2007.20", "10.1145/198366.198376", "10.1145/2508363.2508405", "10.1111/cgf.12114", "10.1109/icde.2018.00019", "10.1109/tvcg.2017.2787113", "10.1109/tvcg.2018.2865240", "10.1111/cgf.13380", "10.1111/cgf.12902", "10.1109/vast.2015.7347628", "10.2307/2288400", "10.1109/infvis.2004.12", "10.1145/102377.115768", "10.1109/tvcg.2009.179", "10.1109/2945.981851", "10.1109/tvcg.2007.70521", "10.1109/pacificvis.2012.6183556", "10.1145/2213836", "10.1109/tvcg.2013.234", "10.1109/iv.2008.87"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 23}, {"doi": "10.1109/tvcg.2019.2934332", "title": "LassoNet: Deep Lasso-Selection of 3D Point Clouds", "year": "2019", "conferenceName": "SciVis", "authors": "Zhutian Chen;Wei Zeng 0004;Zhiguang Yang;Lingyun Yu;Chi-Wing Fu;Huamin Qu", "citationCount": "4", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Zhutian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei; Yang, Zhiguang, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yu, Lingyun, Univ Groningen, Groningen, Netherlands. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China.", "countries": "China;Netherlands", "abstract": "Selection is a fundamental task in exploratory analysis and visualization of 3D point clouds. Prior researches on selection methods were developed mainly based on heuristics such as local point density, thus limiting their applicability in general data. Specific challenges root in the great variabilities implied by point clouds (e.g., dense vs. sparse), viewpoint (e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this work, we introduce LassoNet, a new deep neural network for lasso selection of 3D point clouds, attempting to learn a latent mapping from viewpoint and lasso to point cloud regions. To achieve this, we couple user-target points with viewpoint and lasso information through 3D coordinate transform and naive selection, and improve the method scalability via an intention filtering and farthest point sampling. A hierarchical network is trained using a dataset with over 30K lasso-selection records on two different point cloud data. We conduct a formal user study to compare LassoNet with two state-of-the-art lasso-selection methods. The evaluations confirm that our approach improves the selection effectiveness and efficiency across different combinations of 3D point clouds, viewpoints, and lasso selections. Project Website: https://LassoNet.github.io", "keywords": "Point Clouds,Lasso Selection,Deep Learning", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934332", "refList": ["10.1111/j.1467-8659.2009.01515.x", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2016.2599049", "10.1109/iccv.2015.114", "10.1109/vppc.2018.8604993", "10.1111/cgf.13405", "10.1109/tvcg.2018.2843369", "10.1109/cvpr.2018.00278", "10.1145/3025453.3025957", "10.1109/mc.2013.178", "10.1145/2980179.2980238", "10.1109/msp.2017.2693418", "10.1145/237091.237105", "10.1145/2835487", "10.1109/cvpr.2010.5539838", "10.1016/j.visinf.2017.01.006", "10.1109/tcst.2018.2819965", "10.1109/tvcg.2018.2865138", "10.1109/83.623193", "10.1145/3072959.3073608", "10.1145/1053427.1053445", "10.1016/s0039-9140(96)02179-0", "10.1109/cvpr.2015.7298801", "10.1109/cvpr.2017.693", "10.1007/s12650-014-0206-5", "10.1145/3272127.3275110", "10.1109/cvpr.2015.7298845", "10.1109/iccvw.2015.112", "10.1109/iros.2015.7353481", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2012.217", "10.1109/tridui.2006.1618279", "10.1109/tvcg.2012.292", "10.1109/cvpr.2016.609", "10.1016/j.cag.2012.12.003"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028947", "title": "A Fluid Flow Data Set for Machine Learning and its Application to Neural Flow Map Interpolation", "year": "2020", "conferenceName": "SciVis", "authors": "Jakob Jakob;Markus H. Gross;Tobias G\u00fcnther", "citationCount": "0", "affiliation": "Jakob, J (Corresponding Author), Swiss Fed Inst Technol, Zurich, Switzerland. Jakob, Jakob; Gross, Markus; Guenther, Tobias, Swiss Fed Inst Technol, Zurich, Switzerland.", "countries": "Switzerland", "abstract": "In recent years, deep learning has opened countless research opportunities across many different disciplines. At present, visualization is mainly applied to explore and explain neural networks. Its counterpart-the application of deep learning to visualization problems-requires us to share data more openly in order to enable more scientists to engage in data-driven research. In this paper, we construct a large fluid flow data set and apply it to a deep learning problem in scientific visualization. Parameterized by the Reynolds number, the data set contains a wide spectrum of laminar and turbulent fluid flow regimes. The full data set was simulated on a high-performance compute cluster and contains 8000 time-dependent 2D vector fields, accumulating to more than 16 TB in size. Using our public fluid data set, we trained deep convolutional neural networks in order to set a benchmark for an improved post-hoc Lagrangian fluid flow analysis. In in-situ settings, flow maps are exported and interpolated in order to assess the transport characteristics of time-dependent fluids. Using deep learning, we improve the accuracy of flow map interpolations, allowing a more precise flow analysis at a reduced memory IO footprint.", "keywords": "Scientific visualization,deep learning,flow maps", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028947", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1145/3355089.3356560", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1007/978-3-030-00533-7\\_36", "10.3390/rs11161921", "10.1007/978-3-030-48457-6\\_1", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/mcg.2018.2881523", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1145/3072959.3073643", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/tvcg.2013.128", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1007/978-3-319-46475-6\\_43", "10.1126/science.1127647", "10.1007/978-3-319-46475-6\\_25", "10.1111/cgf.13689"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030346", "title": "V2V: A Deep Learning Approach to Variable-to-Variable Selection and Translation for Multivariate Time-Varying Data", "year": "2020", "conferenceName": "SciVis", "authors": "Jun Han;Hao Zheng 0006;Yunhao Xing;Danny Ziyi Chen;Chaoli Wang", "citationCount": "0", "affiliation": "Han, J (Corresponding Author), Univ Notre Dame, Notre Dame, IN 46556 USA. Han, Jun; Zheng, Hao; Chen, Danny Z.; Wang, Chaoli, Univ Notre Dame, Notre Dame, IN 46556 USA. Xing, Yunhao, Sichuan Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China;USA", "abstract": "We present V2V, a novel deep learning framework, as a general-purpose solution to the variable-to-variable (V2V) selection and translation problem for multivariate time-varying data (MTVD) analysis and visualization. V2V leverages a representation learning algorithm to identify transferable variables and utilizes Kullback-Leibler divergence to determine the source and target variables. It then uses a generative adversarial network (GAN) to learn the mapping from the source variable to the target variable via the adversarial, volumetric, and feature losses. V2V takes the pairs of time steps of the source and target variable as input for training, Once trained, it can infer unseen time steps of the target variable given the corresponding time steps of the source variable. Several multivariate time-varying data sets of different characteristics are used to demonstrate the effectiveness of V2V, both quantitatively and qualitatively. We compare V2V against histogram matching and two other deep learning solutions (Pix2Pix and CycleGAN).", "keywords": "Multivariate time-varying data,variable selection and translation,generative adversarial network,data extrapolation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030346", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/cvpr.2017.19", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1146/annurev", "10.3390/rs11161921", "10.1109/tvcg.2007.70551", "10.1145/3309993", "10.1007/978-3-030-48457-6\\_1", "10.1109/igarss.2018.8518411", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1146/annurev-fluid-010313-141322", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1126/science.1127647"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030374", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data", "year": "2020", "conferenceName": "SciVis", "authors": "Yifan Wang;Guoli Yan;Haikuan Zhu;Sagar Buch;Ying Wang;E. Mark Haacke;Jing Hua;Zichun Zhong", "citationCount": "0", "affiliation": "Wang, YF (Corresponding Author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Wang, Yifan; Yan, Guoli; Zhu, Haikuan; Hua, Jing; Zhong, Zichun, Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Buch, Sagar; Wang, Ying; Haacke, Ewart Mark, Wayne State Univ, Dept Radiol, Detroit, MI 48201 USA.", "countries": "USA", "abstract": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.", "keywords": "Deep neural network,3D cerebrovascular segmentation and visualization,maximum intensity projection (MIP),joint embedding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030374", "refList": ["10.1109/cluster.2018.00036", "10.1109/iccv.2015.123", "10.1109/tvcg.2013.133", "10.1109/cvpr.2017.19", "10.1109/tvcg.2019.2934312", "10.1007/978-3-319-46487-9\\_40", "10.1109/tvcg.2006.175", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2018.2880207", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/tvcg.2018.2816059", "10.1109/pacificvis.2009.4906852", "10.1109/tvcg.2018.2864808", "10.1109/mcg.2018.2881523", "10.1109/tvcg.2015.2467431", "10.1109/tpami.2015.2439281", "10.1109/pacificvis48177.2020.8737", "10.1109/cvpr.2019.00244", "10.1109/tvcg.2006.165", "10.1109/bigdata.2018.8622520", "10.1007/978-3-319-46466-4\\_29", "10.1109/visual.2019.8933759", "10.1145/3197517.3201304", "10.1109/pacificvis.2011.5742378", "10.1109/cvpr.2006.91", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1007/978-3-319-24574-4\\_28", "10.1109/pacificvis.2019.00041", "10.1109/tvcg.2019.2934332", "10.1109/cvpr.2018.00916", "10.1007/978-3-319-46475-6\\_43"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934395", "title": "The Impact of Immersion on Cluster Identification Tasks", "year": "2019", "conferenceName": "InfoVis", "authors": "Matthias Kraus;Niklas Weiler;Daniela Oelke;Johannes Kehrer;Daniel A. Keim;Johannes Fuchs", "citationCount": "4", "affiliation": "Kraus, M (Corresponding Author), Univ Konstanz, Constance, Germany. Kraus, M.; Weiler, N.; Keim, D. A.; Fuchs, J., Univ Konstanz, Constance, Germany. Oelke, D.; Kehrer, J., Siemens Corp Technol, Munich, Germany.", "countries": "Germany", "abstract": "Recent developments in technology encourage the use of head-mounted displays (HMDs) as a medium to explore visualizations in virtual realities (VRs). VR environments (VREs) enable new, more immersive visualization design spaces compared to traditional computer screens. Previous studies in different domains, such as medicine, psychology, and geology, report a positive effect of immersion, e.g., on learning performance or phobia treatment effectiveness. Our work presented in this paper assesses the applicability of those findings to a common task from the information visualization (InfoVis) domain. We conducted a quantitative user study to investigate the impact of immersion on cluster identification tasks in scatterplot visualizations. The main experiment was carried out with 18 participants in a within-subjects setting using four different visualizations, (1) a 2D scatterplot matrix on a screen, (2) a 3D scatterplot on a screen, (3) a 3D scatterplot miniature in a VRE and (4) a fully immersive 3D scatterplot in a VRE. The four visualization design spaces vary in their level of immersion, as shown in a supplementary study. The results of our main study indicate that task performance differs between the investigated visualization design spaces in terms of accuracy, efficiency, memorability, sense of orientation, and user preference. In particular, the 2D visualization on the screen performed worse compared to the 3D visualizations with regard to the measured variables. The study shows that an increased level of immersion can be a substantial benefit in the context of 3D data and cluster detection.", "keywords": "Virtual reality,evaluation,visual analytics,clustering", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934395", "refList": ["10.1111/j.1467-9671.2010.01194.x", "10.1177/1473871614556393", "10.1109/tvcg.2008.153", "10.1111/cgf.13430", "10.1109/tvcg.2004.17", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2007.70433", "10.1089/109493103322011641", "10.2307/2290001", "10.2307/2289444", "10.1016/j.ijhcs.2008.04.004", "10.1109/tvcg.2012.42", "10.1109/vr.2018.8447558", "10.1162/105474602760204318", "10.2307/2986199", "10.3758/bf03200735", "10.1097/opx.0b013e31825da430", "10.1109/infvis.1999.801851", "10.1109/vast.2008.4677350", "10.1162/105474698565686", "10.1109/iv.2013.51", "10.1109/tvcg.2017.2745941", "10.1109/infvis.1998.729555", "10.1111/j.1467-8659.2012.03125.x", "10.1109/2945.506223", "10.1016/s1045-926x(03)00046-6", "10.1097/00042871-200701010-00099", "10.1109/iv.2004.1320137", "10.1109/visual.2002.1183816", "10.1109/tvcg.2018.2864477", "10.1109/vast.2007.4389000", "10.1016/j.ijms.2006.06.015", "10.1007/pl00022704", "10.1111/cgf.13072", "10.1145/3290605.3300555", "10.1109/bdva.2016.7787042", "10.1109/hicss.2013.197", "10.2312/vissym/vissym04/255-260", "10.1115/imece2007-43781", "10.1007/978-4-431-68057-43", "10.1109/tvcg.2016.2520921", "10.1109/mcg.2004.1255801", "10.1109/tvcg.2013.153", "10.1162/pres.1996.5.3.274", "10.1198/106186004x12425", "10.1162/105474601300343612", "10.1162/pres.1997.6.6.603", "10.1111/cgf.12804", "10.1111/j.1467-8659.2009.01666.x", "10.1109/vr.2004.1310069", "10.1109/vr.1999.756938", "10.1007/978-3-658-02897-8\\_16", "10.1162/pres\\_a\\_00016", "10.2307/2288711"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030450", "title": "Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment", "year": "2020", "conferenceName": "InfoVis", "authors": "Benjamin Lee;Xiaoyun Hu;Maxime Cordeil;Arnaud Prouzeau;Bernhard Jenny;Tim Dwyer", "citationCount": "0", "affiliation": "Lee, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Lee, Benjamin; Hu, Xiaoyun; Cordeil, Maxime; Prouzeau, Arnaud; Jenny, Bernhard; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia.", "countries": "Australia", "abstract": "Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA: the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner's personal workspace.", "keywords": "Immersive analytics,collaboration,virtual reality,qualitative study,multivariate data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030450", "refList": ["10.1109/tvcg.2008.153", "10.1007/s10606-004-5062-8", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2019.2914677", "10.1016/0020-7373(91)90039-a", "10.1109/tvcg.2011.287", "10.1117/12.2005484", "10.1109/mmul.2009.35", "10.1109/tvcg.2019.2934803", "10.1145/3359996.3364242", "10.1109/immersive.2016.7932384", "10.1016/j.future.2008.07.015", "10.1057/palgrave.ivs.9500167", "10.1109/mcg.2019.2898941", "10.1145/3343055.3360746", "10.1007/978-3-319-45853-3\\_8", "10.1145/2576099", "10.1145/2858036.2858039", "10.1109/tvcg.2016.2599107", "10.1145/3173574.3173664", "10.1007/978-3-030-01388-2\\_2", "10.1109/vr.2019.8797978", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1007/978-3-030-01388-2\\_8", "10.1023/a:1021271517844", "10.1109/bigdata.2014.7004282", "10.1109/tvcg.2019.2934395", "10.1109/ismar.2010.5643530", "10.1007/978-3-030-01388-22", "10.1145/2133806.2133821", "10.1109/3dvis.2014.7160093", "10.1145/2556288.2557058", "10.1109/vr.2019.8797845"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2018.00014", "year": "2018", "title": "Smart Surrogate Widgets for Direct Volume Manipulation", "conferenceName": "PacificVis", "authors": "Sergej Stoppel;Stefan Bruckner", "citationCount": "1", "affiliation": "Stoppel, S (Corresponding Author), Univ Bergen, Bergen, Norway.\nStoppel, Sergej; Bruckner, Stefan, Univ Bergen, Bergen, Norway.", "countries": "Norway", "abstract": "Interaction is an essential aspect in volume visualization, yet common manipulation tools such as bounding boxes or clipping plane widgets provide rather crude tools as they neglect the complex structure of the underlying data. In this paper, we introduce a novel volume interaction approach based on smart widgets that are automatically placed directly into the data in a visibility-driven manner. By adapting to what the user actually sees, they act as proxies that allow for goal-oriented modifications while still providing an intuitive set of simple operations that is easy to control. In particular, our method is well-suited for direct manipulation scenarios such as touch screens, where traditional user interface elements commonly exhibit limited utility. To evaluate out approach we conducted a qualitative user study with nine participants with various backgrounds.", "keywords": "{[}Human-centered computing]: Interaction design process and methods; User interface design", "link": "https://doi.org/10.1109/PacificVis.2018.00014", "refList": ["10.2312/vg/vg-pbg08/041-048", "10.2312/eurovisshort.20141164", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1109/visual.2001.964519", "10.1109/tvcg.2015.2467961", "10.1145/2492684", "10.1109/svv.1998.729584", "10.1109/tvcg.2005.62", "10.1109/2945.942692", "10.1145/1239451.1239482", "10.1109/tvcg.2006.140", "10.1111/cgf.12892", "10.1145/1053427.1053445", "10.1111/j.1467-8659.2012.03083.x", "10.1109/tvcg.2003.1207438", "10.2312/compaesth/compaesth05/209-216", "10.1109/tvcg.2006.152", "10.1016/j.cag.2011.10.006", "10.1109/tvcg.2010.157", "10.1109/tvcg.2006.96", "10.1109/tvcg.2012.292", "10.1145/358669.358692", "10.1109/tvcg.2011.261"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2017.8031578", "year": "2017", "title": "Design space for spatio-data coordination: Tangible interaction devices for immersive information visualisation", "conferenceName": "PacificVis", "authors": "Maxime Cordeil;Benjamin Bach;Yongchao Li;Elliot Wilson;Tim Dwyer", "citationCount": "7", "affiliation": "Cordeil, M (Corresponding Author), Monash Univ, Clayton, Vic, Australia.\nCordeil, Maxime; Bach, Benjamin; Li, Yongchao; Wilson, Elliott; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia.", "countries": "Australia", "abstract": "We introduce the concept of ``spatio-data coordination{''} (SD coordination) which defines the mapping of user actions in physical space into the space of data in a visualisation. SD coordination is intended to lower the user's cognitive load when exploring complex multidimensional data such as biomedical data, multiple data attributes vs time in a space-time-cube visualisation, or three-dimensional projections of three-or-higher-dimensional data sets. To inform the design of interaction devices to allow for SD coordination, we define a design space and demonstrate it with sketches and early prototypes of three exemplar devices for SD coordinated interaction.", "keywords": "", "link": "https://doi.org/10.1109/PACIFICVIS.2017.8031578", "refList": ["10.1109/tvcg.2015.2467202", "10.1145/9999997.9999999", "10.1145/1450579.1450593", "10.1109/tvcg.2015.2440233", "10.1109/tvcg.2013.134", "10.1145/2559206.2581340", "10.1109/infvis.2005.1532136", "10.1111/j.1467-8659.2012.03115.x", "10.1145/2897824.2925953", "10.1016/0020-7373(89)90014-x", "10.2307/2288400", "10.1145/2559206.2581275", "10.1145/958432.958472", "10.1145/2133416.2146416", "10.1016/j.cag.2012.12.006", "10.1109/3dvis.2014.7160093", "10.1007/s10055-009-0126-1", "10.1109/iv.2016.70", "10.1145/257089.257242", "10.1007/978-3-319-16766-4\\_20", "10.1145/2702123.2702604", "10.1109/tvcg.2007.70515", "10.1145/2470654.2481359", "10.1016/j.displa.2013.10.004"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934415", "title": "Evaluating an Immersive Space-Time Cube Geovisualization for Intuitive Trajectory Data Exploration", "year": "2019", "conferenceName": "InfoVis", "authors": "Jorge A. Wagner Filho;Wolfgang Stuerzlinger;Luciana Porcher Nedel", "citationCount": "4", "affiliation": "Wagner, JA (Corresponding Author), Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil. Wagner, JA (Corresponding Author), Simon Fraser Univ, Burnaby, BC, Canada. Wagner Filho, Jorge A.; Nedel, Luciana, Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil. Wagner Filho, Jorge A.; Stuerzlinger, Wolfgang, Simon Fraser Univ, Burnaby, BC, Canada.", "countries": "Canada;Brazil", "abstract": "A Space-Time Cube enables analysts to clearly observe spatio-temporal features in movement trajectory datasets in geovisualization. However, its general usability is impacted by a lack of depth cues, a reported steep learning curve, and the requirement for efficient 3D navigation. In this work, we investigate a Space-Time Cube in the Immersive Analytics domain. Based on a review of previous work and selecting an appropriate exploration metaphor, we built a prototype environment where the cube is coupled to a virtual representation of the analyst's real desk, and zooming and panning in space and time are intuitively controlled using mid-air gestures. We compared our immersive environment to a desktop-based implementation in a user study with 20 participants across 7 tasks of varying difficulty, which targeted different user interface features. To investigate how performance is affected in the presence of clutter, we explored two scenarios with different numbers of trajectories. While the quantitative performance was similar for the majority of tasks, large differences appear when we analyze the patterns of interaction and consider subjective metrics. The immersive version of the Space-Time Cube received higher usability scores, much higher user preference, and was rated to have a lower mental workload, without causing participants discomfort in 25-minute-long VR sessions.", "keywords": "Space-time cube,Trajectory visualization,Immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934415", "refList": ["10.1109/mcg.2006.74", "10.1080/13658816.2015.1058386", "10.1145/1456650.1456652", "10.1111/cgf.13430", "10.1145/1944745.1944777", "10.1016/b978-155860819-1/50001-7", "10.1109/bdva.2018.8534024", "10.1109/pacificvis.2017.8031578", "10.1111/j.1467-8306.1994.tb01869.x", "10.2307/3001968", "10.1007/978-3-642-15300-6\\_21", "10.1080/13658816.2010.511223", "10.1007/978-3-030-01388-2", "10.1080/00087041.2018.1495898", "10.1145/1773965.1773970", "10.1002/9780470987643.ch15", "10.1109/glocom.2015.7417476", "10.2312/eurovisstar.20141171", "10.1080/17489725.2015.1074736", "10.1109/bdva.2016.7787050", "10.1109/wevr.2017.7957707", "10.1016/s1045-926x(03)00046-6", "10.1109/iv.2018.00026", "10.1179/1743277413y.0000000061", "10.1109/mcg.2019.2898856", "10.1109/iv.2004.1320137", "10.1111/j.1467-8659.2011.01929.x", "10.1109/tvcg.2008.194", "10.1016/j.jtrangeo.2010.11.002", "10.1177/154193120605000909", "10.1109/bigdata.2015.7364040", "10.1111/cgf.12466", "10.1109/icsens.2015.7370446", "10.1080/13658816.2010.508043", "10.1057/ivs.2009.8", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.14236/ewic/hci2016.22", "10.1145/2655691", "10.1007/bf01936872", "10.1109/infvis.2004.27", "10.1109/tvcg.2014.2329308", "10.1111/j.1435-5597.1970.tb01464.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}], "len": 7}], "len": 9}, {"doi": "10.1111/cgf.13710", "year": "2019", "title": "Hybrid Touch/Tangible Spatial 3D Data Selection", "conferenceName": "EuroVis", "authors": "Lonni Besan{\\c{c}}on;Mickael Sereno;Lingyun Yu;Mehdi Ammi;Tobias Isenberg", "citationCount": "6", "affiliation": "Besancon, L (Corresponding Author), Linkoping Univ, Norrkoping, Sweden.\nBesancon, L (Corresponding Author), Univ Paris Saclay, Paris, France.\nBesancon, Lonni, Linkoping Univ, Norrkoping, Sweden.\nSereno, Mickael; Isenberg, Tobias, Inria, Rocquencourt, France.\nBesancon, Lonni; Sereno, Mickael, Univ Paris Saclay, Paris, France.\nYu, Lingyun, Univ Groningen, Groningen, Netherlands.\nAmmi, Mehdi, Univ Paris 08, Paris, France.", "countries": "Sweden;France;Netherlands", "abstract": "We discuss spatial selection techniques for three-dimensional datasets. Such 3D spatial selection is fundamental to exploratory data analysis. While 2D selection is efficient for datasets with explicit shapes and structures, it is less efficient for data without such properties. We first propose a new taxonomy of 3D selection techniques, focusing on the amount of control the user has to define the selection volume. We then describe the 3D spatial selection technique Tangible Brush, which gives manual control over the final selection volume. It combines 2D touch with 6-DOF 3D tangible input to allow users to perform 3D selections in volumetric data. We use touch input to draw a 2D lasso, extruding it to a 3D selection volume based on the motion of a tangible, spatially-aware tablet. We describe our approach and present its quantitative and qualitative comparison to state-of-the-art structure-dependent selection. Our results show that, in addition to being dataset-independent, Tangible Brush is more accurate than existing dataset-dependent techniques, thus providing a trade-off between precision and effort.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13710", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2005.59", "10.1109/tvcg.2015.2467202", "10.7326/0003-4819-130-12-199906150-00008", "10.1145/1517664.1517705", "10.1145/2992154.2996779", "10.1109/mcg.2010.30", "10.1071/ch9490149", "10.1109/tridui.2006.1618277", "10.1007/978-3-319-45853-3\\_2", "10.1162/105474601750182333", "10.1007/bf00133570", "10.1109/3dui.2011.5759219", "10.1145/3025453.3025890", "10.1002/vis.277", "10.1145/3132272.3134125", "10.1007/978-3-642-18421-5\\_20", "10.1145/2839462.2839464", "10.1007/978-3-319-16940-8\\_3", "10.2312/egve/ipt\\_egve2005/201-209", "10.1145/3025453.3025863", "10.1109/tabletop.2006.33", "10.1109/tridui.2006.1618279", "10.12720/joig.1.4.166-170", "10.1109/tvcg.2012.292", "10.1145/965139.807392", "10.1145/2983310.2985750", "10.1016/j.ijhcs.2013.03.003", "10.1145/2559206.2578881", "10.2312/sbm/sbm06/123-129", "10.1109/tridui.2006.1618264", "10.1016/0097-8493(94)90062-0", "10.1006/jvlc.1998.0112", "10.1007/978-3-030-01388-2\\_4", "10.1109/tvcg.2016.2599217", "10.1016/j.ijhcs.2013.04.003", "10.1145/253284.253303", "10.1145/964696.964721", "10.1348/000712608x377117", "10.1145/3290607.3310432", "10.1007/bf01409796", "10.1145/1851600.1851631", "10.1177/154193120605000909", "10.1109/tvcg.2018.2848906", "10.1007/s12650-014-0206-5", "10.2312/conf/eg2013/stars/065-093", "10.1109/mcg.2009.117", "10.1145/2396636.2396674", "10.1109/tridui.2006.1618271", "10.1007/978-3-319-26633-6\\_13", "10.1145/237091.237102", "10.1109/38.974511", "10.1038/nature.2016.19503", "10.1145/1166253.1166260", "10.1145/2254556.2254641", "10.1177/0956797613504966", "10.1109/tvcg.2015.2440233", "10.1145/2470654.2470688", "10.1109/tvcg.2004.1260759", "10.1016/0031-3203(81)90028-5", "10.1111/1467-8659.00194", "10.1007/978-3-642-03655-2\\_73", "10.1007/978-3-319-45853-3\\_6", "10.2312/egve/jvrc10/017-024", "10.1109/3dui.2007.340783", "10.3389/fnins.2016.00454", "10.1145/1008653.1008689", "10.1038/nmeth.2659", "10.1007/978-3-540-85412-8\\_4", "10.1109/visual.2004.30", "10.1109/tvcg.2013.121", "10.1145/1095034.1095041", "10.1016/j.cag.2012.12.003", "10.1016/j.socec.2004.09.033", "10.1007/s00371-005-0330-2", "10.1016/s0734-189x(85)90153-7", "10.1145/1936652.1936684", "10.1145/2076354.2076390", "10.1145/1450579.1450588", "10.1145/765891.765982", "10.1109/mc.2013.178", "10.1201/b17511", "10.1145/958432.958438", "10.1109/mcg.2004.20", "10.1109/tablet0p.2006.33", "10.1145/1053427.1053445", "10.1145/1226969.1226998", "10.1109/tvcg.2012.217"], "wos": 1, "children": [], "len": 1}], "len": 133}, {"doi": "10.1109/tvcg.2018.2864690", "title": "Interactive obstruction-free lensing for volumetric data visualization", "year": "2018", "conferenceName": "SciVis", "authors": "Michael Traor\u00e9;Christophe Hurter;Alexandru Telea", "citationCount": "1", "affiliation": "Traore, M (Corresponding Author), French Civil Aviat Univ, ENAC, Toulouse, France. Traore, Michael; Hurter, Christophe, French Civil Aviat Univ, ENAC, Toulouse, France. Telea, Alexandru, Univ Groningen, Inst Johan Bernoulli, Groningen, Netherlands.", "countries": "France;Netherlands", "abstract": "Occlusion is an issue in volumetric visualization as it prevents direct visualization of the region of interest. While many techniques such as transfer functions, volume segmentation or view distortion have been developed to address this, there is still room for improvement to better support the understanding of objects' vicinity. However, most existing Focus+Context fail to solve partial occlusion in datasets where the target and the occluder are very similar density-wise. For these reasons, we investigate a new technique which maintains the general structure of the investigated volumetric dataset while addressing occlusion issues. With our technique, the user interactively defines an area of interest where an occluded region or object is partially visible. Then our lens starts pushing at its border occluding objects, thus revealing hidden volumetric data. Next, the lens is modified with an extended field of view (fish-eye deformation) to better see the vicinity of the selected region. Finally, the user can freely explore the surroundings of the area under investigation within the lens. To provide real-time exploration, we implemented our lens using a GPU accelerated ray-casting framework to handle ray deformations, local lighting, and local viewpoint manipulation. We illustrate our technique with five application scenarios in baggage inspection, 3D fluid flow visualization, chest radiology, air traffic planning, and DTI fiber exploration.", "keywords": "Interaction techniques,focus + context,volume visualization,volume rendering,raycasting", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864690", "refList": ["10.1109/visual.1999.809865", "10.1145/1409060.1409107", "10.1109/visual.2003.1250400", "10.1145/2598153.2598200", "10.1007/978-1-4614-7657-3\\_19", "10.1109/tvcg.2008.59", "10.1145/989863.989871", "10.1109/tvcg.2016.2599049", "10.1145/345513.345271", "10.1016/s1470-2045(17)30438-2", "10.1109/pacificvis.2014.61", "10.1111/j.1467-8659.2006.00979.x", "10.1109/tvcg.2010.35", "10.2312/conf/eg2012/stars/075-094", "10.1111/cgf.12927", "10.1111/cgf.12871", "10.1145/2024156.2024165", "10.1007/978-3-540-85412-8\\_16", "10.1057/ivs.2009.32", "10.1109/tvcg.2006.140", "10.1109/tvcg.2007.70565", "10.1109/tvcg.2015.2403323", "10.1109/visual.2004.32", "10.2200/s00688ed1v01y201512vis006", "10.1109/tvcg.2015.2443804", "10.1109/tvcg.2006.144", "10.1109/tvcg.2016.2515611", "10.1109/tvcg.2007.48", "10.1109/tvcg.2010.193", "10.1109/tvcg.2007.1051", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.223", "10.1109/38.595268", "10.1111/j.1467-8659.2012.03079.x", "10.1109/tvcg.2009.145", "10.1109/pacificvis.2017.8031594", "10.1145/2425296.2425325", "10.1109/tvcg.2012.265", "10.1109/mcg.2017.10", "10.1007/s12031-007-0029-0", "10.1109/tvcg.2010.127", "10.1016/j.trc.2014.03.005", "10.1109/tvcg.2009.138", "10.1109/tvcg.2006.124"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2018.2865191", "title": "FiberClay: Sculpting Three Dimensional Trajectories to Reveal Structural Insights", "year": "2018", "conferenceName": "InfoVis", "authors": "Christophe Hurter;Nathalie Henry Riche;Steven Mark Drucker;Maxime Cordeil;Richard Alligier;Romain Vuillemot", "citationCount": "11", "affiliation": "Hurter, C (Corresponding Author), French Civil Aviat Univ, ENAC, Toulouse, France. Hurter, C (Corresponding Author), Toulouse Univ, Toulouse, France. Hurter, Christophe ll; Alligier, Richard, French Civil Aviat Univ, ENAC, Toulouse, France. Hurter, Christophe ll; Alligier, Richard, Toulouse Univ, Toulouse, France. Riche, Nathalie Henry; Drucker, Steven M., Microsoft Res, Redmond, WA USA. Cordeil, Maxime, Monash Univ, Clayton, Vic, Australia. Vuillemot, Romain, Univ Lyon, Ecole Cent Lyon, CNRS, UMR 5205,LIRIS, F-69134 Lyon, France.", "countries": "USA;France;Australia", "abstract": "Visualizing 3D trajectories to extract insights about their similarities and spatial configuration is a critical task in several domains. Air traffic controllers for example deal with large quantities of aircrafts routes to optimize safety in airspace and neuroscientists attempt to understand neuronal pathways in the human brain by visualizing bundles of fibers from DTI images. Extracting insights from masses of 3D trajectories is challenging as the multiple three dimensional lines have complex geometries, may overlap, cross or even merge with each other, making it impossible to follow individual ones in dense areas. As trajectories are inherently spatial and three dimensional, we propose FiberClay: a system to display and interact with 3D trajectories in immersive environments. FiberClay renders a large quantity of trajectories in real time using GP-GPU techniques. FiberClay also introduces a new set of interactive techniques for composing complex queries in 3D space leveraging immersive environment controllers and user position. These techniques enable an analyst to select and compare sets of trajectories with specific geometries and data properties. We conclude by discussing insights found using FiberClay with domain experts in air traffic control and neurology.", "keywords": "Immersive Analytics,3D Visualization,Dynamic Queries,Bimanual Interaction,Multidimensional Data", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865191", "refList": ["10.1016/b978-0-08-051574-8.50047-9", "10.1016/j.ijhcs.2013.03.003", "10.1057/palgrave.ivs.9500097", "10.1109/tvcg.2008.153", "10.2307/1269768", "10.1109/tvcg.2013.226", "10.1109/tvcg.2005.59", "10.1016/j.compenvurbsys.2014.01.005", "10.1017/cbo9781139128926.013", "10.1109/pacificvis.2014.61", "10.1145/1279640.1279642", "10.1109/pacificvis.2017.8031577", "10.1109/tvcg.2016.2599217", "10.1109/visual.1991.175794", "10.1007/978-1-4615-1177-9\\_27", "10.1007/978-0-387-35504-714", "10.1109/tvcg.2011.233", "10.3390/informatics4030026", "10.1109/tvcg.2011.192", "10.1109/tvcg.2015.2403323", "10.1145/800186.810616", "10.1145/3013971.3014006", "10.1016/j.ijhcs.2005.02.001", "10.1109/3dvis.2014.7160096", "10.1111/j.1467-8659.2012.03115.x", "10.1109/38.946631", "10.1145/3126594.3126613", "10.1109/pacificvis.2011.5742390", "10.4230/dagrep.6.6.1", "10.1111/j.1467-8659.2012.03079.x", "10.1057/palgrave.ivs.9500061", "10.1109/tvcg.2017.2744338", "10.1109/tvcg.2009.145", "10.1109/tvcg.2012.265", "10.1007/s12031-007-0029-0", "10.1145/3025453.3025566", "10.1109/tvcg.2013.153", "10.1109/tvcg.2016.2520921", "10.1016/j.trc.2014.03.005", "10.1109/3dvis.2014.7160093", "10.1109/ipsn.2014.6846743", "10.1109/tvcg.2015.2467112", "10.1109/tvcg.2017.2744079", "10.1109/tvcg.2011.224", "10.1111/cgf.12804", "10.1111/j.1467-8659.2009.01687.x", "10.1016/s1088-467x(99)00013-x", "10.1109/tvcg.2012.217", "10.1145/2992154.2996365"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934415", "title": "Evaluating an Immersive Space-Time Cube Geovisualization for Intuitive Trajectory Data Exploration", "year": "2019", "conferenceName": "InfoVis", "authors": "Jorge A. Wagner Filho;Wolfgang Stuerzlinger;Luciana Porcher Nedel", "citationCount": "4", "affiliation": "Wagner, JA (Corresponding Author), Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil. Wagner, JA (Corresponding Author), Simon Fraser Univ, Burnaby, BC, Canada. Wagner Filho, Jorge A.; Nedel, Luciana, Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil. Wagner Filho, Jorge A.; Stuerzlinger, Wolfgang, Simon Fraser Univ, Burnaby, BC, Canada.", "countries": "Canada;Brazil", "abstract": "A Space-Time Cube enables analysts to clearly observe spatio-temporal features in movement trajectory datasets in geovisualization. However, its general usability is impacted by a lack of depth cues, a reported steep learning curve, and the requirement for efficient 3D navigation. In this work, we investigate a Space-Time Cube in the Immersive Analytics domain. Based on a review of previous work and selecting an appropriate exploration metaphor, we built a prototype environment where the cube is coupled to a virtual representation of the analyst's real desk, and zooming and panning in space and time are intuitively controlled using mid-air gestures. We compared our immersive environment to a desktop-based implementation in a user study with 20 participants across 7 tasks of varying difficulty, which targeted different user interface features. To investigate how performance is affected in the presence of clutter, we explored two scenarios with different numbers of trajectories. While the quantitative performance was similar for the majority of tasks, large differences appear when we analyze the patterns of interaction and consider subjective metrics. The immersive version of the Space-Time Cube received higher usability scores, much higher user preference, and was rated to have a lower mental workload, without causing participants discomfort in 25-minute-long VR sessions.", "keywords": "Space-time cube,Trajectory visualization,Immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934415", "refList": ["10.1109/mcg.2006.74", "10.1080/13658816.2015.1058386", "10.1145/1456650.1456652", "10.1111/cgf.13430", "10.1145/1944745.1944777", "10.1016/b978-155860819-1/50001-7", "10.1109/bdva.2018.8534024", "10.1109/pacificvis.2017.8031578", "10.1111/j.1467-8306.1994.tb01869.x", "10.2307/3001968", "10.1007/978-3-642-15300-6\\_21", "10.1080/13658816.2010.511223", "10.1007/978-3-030-01388-2", "10.1080/00087041.2018.1495898", "10.1145/1773965.1773970", "10.1002/9780470987643.ch15", "10.1109/glocom.2015.7417476", "10.2312/eurovisstar.20141171", "10.1080/17489725.2015.1074736", "10.1109/bdva.2016.7787050", "10.1109/wevr.2017.7957707", "10.1016/s1045-926x(03)00046-6", "10.1109/iv.2018.00026", "10.1179/1743277413y.0000000061", "10.1109/mcg.2019.2898856", "10.1109/iv.2004.1320137", "10.1111/j.1467-8659.2011.01929.x", "10.1109/tvcg.2008.194", "10.1016/j.jtrangeo.2010.11.002", "10.1177/154193120605000909", "10.1109/bigdata.2015.7364040", "10.1111/cgf.12466", "10.1109/icsens.2015.7370446", "10.1080/13658816.2010.508043", "10.1057/ivs.2009.8", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.14236/ewic/hci2016.22", "10.1145/2655691", "10.1007/bf01936872", "10.1109/infvis.2004.27", "10.1109/tvcg.2014.2329308", "10.1111/j.1435-5597.1970.tb01464.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934332", "title": "LassoNet: Deep Lasso-Selection of 3D Point Clouds", "year": "2019", "conferenceName": "SciVis", "authors": "Zhutian Chen;Wei Zeng 0004;Zhiguang Yang;Lingyun Yu;Chi-Wing Fu;Huamin Qu", "citationCount": "4", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Zhutian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei; Yang, Zhiguang, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yu, Lingyun, Univ Groningen, Groningen, Netherlands. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China.", "countries": "China;Netherlands", "abstract": "Selection is a fundamental task in exploratory analysis and visualization of 3D point clouds. Prior researches on selection methods were developed mainly based on heuristics such as local point density, thus limiting their applicability in general data. Specific challenges root in the great variabilities implied by point clouds (e.g., dense vs. sparse), viewpoint (e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this work, we introduce LassoNet, a new deep neural network for lasso selection of 3D point clouds, attempting to learn a latent mapping from viewpoint and lasso to point cloud regions. To achieve this, we couple user-target points with viewpoint and lasso information through 3D coordinate transform and naive selection, and improve the method scalability via an intention filtering and farthest point sampling. A hierarchical network is trained using a dataset with over 30K lasso-selection records on two different point cloud data. We conduct a formal user study to compare LassoNet with two state-of-the-art lasso-selection methods. The evaluations confirm that our approach improves the selection effectiveness and efficiency across different combinations of 3D point clouds, viewpoints, and lasso selections. Project Website: https://LassoNet.github.io", "keywords": "Point Clouds,Lasso Selection,Deep Learning", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934332", "refList": ["10.1111/j.1467-8659.2009.01515.x", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2016.2599049", "10.1109/iccv.2015.114", "10.1109/vppc.2018.8604993", "10.1111/cgf.13405", "10.1109/tvcg.2018.2843369", "10.1109/cvpr.2018.00278", "10.1145/3025453.3025957", "10.1109/mc.2013.178", "10.1145/2980179.2980238", "10.1109/msp.2017.2693418", "10.1145/237091.237105", "10.1145/2835487", "10.1109/cvpr.2010.5539838", "10.1016/j.visinf.2017.01.006", "10.1109/tcst.2018.2819965", "10.1109/tvcg.2018.2865138", "10.1109/83.623193", "10.1145/3072959.3073608", "10.1145/1053427.1053445", "10.1016/s0039-9140(96)02179-0", "10.1109/cvpr.2015.7298801", "10.1109/cvpr.2017.693", "10.1007/s12650-014-0206-5", "10.1145/3272127.3275110", "10.1109/cvpr.2015.7298845", "10.1109/iccvw.2015.112", "10.1109/iros.2015.7353481", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2012.217", "10.1109/tridui.2006.1618279", "10.1109/tvcg.2012.292", "10.1109/cvpr.2016.609", "10.1016/j.cag.2012.12.003"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028947", "title": "A Fluid Flow Data Set for Machine Learning and its Application to Neural Flow Map Interpolation", "year": "2020", "conferenceName": "SciVis", "authors": "Jakob Jakob;Markus H. Gross;Tobias G\u00fcnther", "citationCount": "0", "affiliation": "Jakob, J (Corresponding Author), Swiss Fed Inst Technol, Zurich, Switzerland. Jakob, Jakob; Gross, Markus; Guenther, Tobias, Swiss Fed Inst Technol, Zurich, Switzerland.", "countries": "Switzerland", "abstract": "In recent years, deep learning has opened countless research opportunities across many different disciplines. At present, visualization is mainly applied to explore and explain neural networks. Its counterpart-the application of deep learning to visualization problems-requires us to share data more openly in order to enable more scientists to engage in data-driven research. In this paper, we construct a large fluid flow data set and apply it to a deep learning problem in scientific visualization. Parameterized by the Reynolds number, the data set contains a wide spectrum of laminar and turbulent fluid flow regimes. The full data set was simulated on a high-performance compute cluster and contains 8000 time-dependent 2D vector fields, accumulating to more than 16 TB in size. Using our public fluid data set, we trained deep convolutional neural networks in order to set a benchmark for an improved post-hoc Lagrangian fluid flow analysis. In in-situ settings, flow maps are exported and interpolated in order to assess the transport characteristics of time-dependent fluids. Using deep learning, we improve the accuracy of flow map interpolations, allowing a more precise flow analysis at a reduced memory IO footprint.", "keywords": "Scientific visualization,deep learning,flow maps", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028947", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1145/3355089.3356560", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1007/978-3-030-00533-7\\_36", "10.3390/rs11161921", "10.1007/978-3-030-48457-6\\_1", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/mcg.2018.2881523", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1145/3072959.3073643", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/tvcg.2013.128", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1007/978-3-319-46475-6\\_43", "10.1126/science.1127647", "10.1007/978-3-319-46475-6\\_25", "10.1111/cgf.13689"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030346", "title": "V2V: A Deep Learning Approach to Variable-to-Variable Selection and Translation for Multivariate Time-Varying Data", "year": "2020", "conferenceName": "SciVis", "authors": "Jun Han;Hao Zheng 0006;Yunhao Xing;Danny Ziyi Chen;Chaoli Wang", "citationCount": "0", "affiliation": "Han, J (Corresponding Author), Univ Notre Dame, Notre Dame, IN 46556 USA. Han, Jun; Zheng, Hao; Chen, Danny Z.; Wang, Chaoli, Univ Notre Dame, Notre Dame, IN 46556 USA. Xing, Yunhao, Sichuan Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China;USA", "abstract": "We present V2V, a novel deep learning framework, as a general-purpose solution to the variable-to-variable (V2V) selection and translation problem for multivariate time-varying data (MTVD) analysis and visualization. V2V leverages a representation learning algorithm to identify transferable variables and utilizes Kullback-Leibler divergence to determine the source and target variables. It then uses a generative adversarial network (GAN) to learn the mapping from the source variable to the target variable via the adversarial, volumetric, and feature losses. V2V takes the pairs of time steps of the source and target variable as input for training, Once trained, it can infer unseen time steps of the target variable given the corresponding time steps of the source variable. Several multivariate time-varying data sets of different characteristics are used to demonstrate the effectiveness of V2V, both quantitatively and qualitatively. We compare V2V against histogram matching and two other deep learning solutions (Pix2Pix and CycleGAN).", "keywords": "Multivariate time-varying data,variable selection and translation,generative adversarial network,data extrapolation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030346", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/cvpr.2017.19", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1146/annurev", "10.3390/rs11161921", "10.1109/tvcg.2007.70551", "10.1145/3309993", "10.1007/978-3-030-48457-6\\_1", "10.1109/igarss.2018.8518411", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1146/annurev-fluid-010313-141322", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1126/science.1127647"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030374", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data", "year": "2020", "conferenceName": "SciVis", "authors": "Yifan Wang;Guoli Yan;Haikuan Zhu;Sagar Buch;Ying Wang;E. Mark Haacke;Jing Hua;Zichun Zhong", "citationCount": "0", "affiliation": "Wang, YF (Corresponding Author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Wang, Yifan; Yan, Guoli; Zhu, Haikuan; Hua, Jing; Zhong, Zichun, Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Buch, Sagar; Wang, Ying; Haacke, Ewart Mark, Wayne State Univ, Dept Radiol, Detroit, MI 48201 USA.", "countries": "USA", "abstract": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.", "keywords": "Deep neural network,3D cerebrovascular segmentation and visualization,maximum intensity projection (MIP),joint embedding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030374", "refList": ["10.1109/cluster.2018.00036", "10.1109/iccv.2015.123", "10.1109/tvcg.2013.133", "10.1109/cvpr.2017.19", "10.1109/tvcg.2019.2934312", "10.1007/978-3-319-46487-9\\_40", "10.1109/tvcg.2006.175", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2018.2880207", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/tvcg.2018.2816059", "10.1109/pacificvis.2009.4906852", "10.1109/tvcg.2018.2864808", "10.1109/mcg.2018.2881523", "10.1109/tvcg.2015.2467431", "10.1109/tpami.2015.2439281", "10.1109/pacificvis48177.2020.8737", "10.1109/cvpr.2019.00244", "10.1109/tvcg.2006.165", "10.1109/bigdata.2018.8622520", "10.1007/978-3-319-46466-4\\_29", "10.1109/visual.2019.8933759", "10.1145/3197517.3201304", "10.1109/pacificvis.2011.5742378", "10.1109/cvpr.2006.91", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1007/978-3-319-24574-4\\_28", "10.1109/pacificvis.2019.00041", "10.1109/tvcg.2019.2934332", "10.1109/cvpr.2018.00916", "10.1007/978-3-319-46475-6\\_43"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934803", "title": "There Is No Spoon: Evaluating Performance, Space Use, and Presence with Expert Domain Users in Immersive Analytics", "year": "2019", "conferenceName": "InfoVis", "authors": "Andrea Batch;Andrew Cunningham;Maxime Cordeil;Niklas Elmqvist;Tim Dwyer;Bruce H. Thomas;Kim Marriott", "citationCount": "6", "affiliation": "Batch, A (Corresponding Author), Univ Maryland, College Pk, MD 20742 USA. Batch, Andrea; Elmqvist, Niklas, Univ Maryland, College Pk, MD 20742 USA. Cunningham, Andrew; Thomas, Bruce H., Univ South Australia Adelaide, Adelaide, SA, Australia. Cordeil, Maxime; Dwyer, Tim; Marriott, Kim, Univ Melbourne, Melbourne, Vic, Australia.", "countries": "USA;Australia", "abstract": "Immersive analytics turns the very space surrounding the user into a canvas for data analysis, supporting human cognitive abilities in myriad ways. We present the results of a design study, contextual inquiry, and longitudinal evaluation involving professional economists using a Virtual Reality (VR) system for multidimensional visualization to explore actual economic data. Results from our preregistered evaluation highlight the varied use of space depending on context (exploration vs. presentation), the organization of space to support work, and the impact of immersion on navigation and orientation in the 3D analysis space.", "keywords": "Design study,evaluation,economic analysis,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934803", "refList": ["10.1145/3139131.3139141", "10.1109/2.60882", "10.1007/978-3-030-01388-2\\_1", "10.1109/38.250911", "10.1145/302979.303166", "10.1073/pnas.1306779110", "10.1109/tvcg.2012.219", "10.1162/105474698565686", "10.1016/0364-0213(94)90007-8", "10.1145/642611.642650", "10.1162/pres.1992.1.4.482", "10.1007/978-3-030-01388-2", "10.1109/2.19829", "10.1111/1467-8284.00096", "10.1007/978-3-030-01388-2\\_7", "10.1109/hicss.2011.339", "10.1109/tvcg.2016.2599107", "10.1145/3173574.3173664", "10.1145/1008653.1008669", "10.1109/tvcg.2017.2743990", "10.1145/642611", "10.1145/3126594.3126613", "10.1007/978-94-007-6833-89", "10.1162/pres\\_a\\_00124", "10.1016/0004-3702(94)00017-u", "10.1109/tvcg.2014.20", "10.1145/1168149.1168158", "10.1109/visual.1995.480800", "10.1109/icsens.2015.7370446", "10.1207/s15516709cog1701\\_1", "10.1162/105474601300343603", "10.1109/tvcg.2016.2518135", "10.1109/tvcg.2012.213", "10.1016/s0097-8493(02)00113-9", "10.1162/pres.1994.3.2.130", "10.1109/tvcg.2018.2865191", "10.1109/visual.1990.146402", "10.1162/105474601300343612", "10.1145/2702123.2702406", "10.1162/pres.1992.1.1.120", "10.1145/3009939.3009955", "10.1109/glocom.2015.7417476", "10.1162/pres\\_a\\_00261"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030450", "title": "Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment", "year": "2020", "conferenceName": "InfoVis", "authors": "Benjamin Lee;Xiaoyun Hu;Maxime Cordeil;Arnaud Prouzeau;Bernhard Jenny;Tim Dwyer", "citationCount": "0", "affiliation": "Lee, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Lee, Benjamin; Hu, Xiaoyun; Cordeil, Maxime; Prouzeau, Arnaud; Jenny, Bernhard; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia.", "countries": "Australia", "abstract": "Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA: the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner's personal workspace.", "keywords": "Immersive analytics,collaboration,virtual reality,qualitative study,multivariate data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030450", "refList": ["10.1109/tvcg.2008.153", "10.1007/s10606-004-5062-8", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2019.2914677", "10.1016/0020-7373(91)90039-a", "10.1109/tvcg.2011.287", "10.1117/12.2005484", "10.1109/mmul.2009.35", "10.1109/tvcg.2019.2934803", "10.1145/3359996.3364242", "10.1109/immersive.2016.7932384", "10.1016/j.future.2008.07.015", "10.1057/palgrave.ivs.9500167", "10.1109/mcg.2019.2898941", "10.1145/3343055.3360746", "10.1007/978-3-319-45853-3\\_8", "10.1145/2576099", "10.1145/2858036.2858039", "10.1109/tvcg.2016.2599107", "10.1145/3173574.3173664", "10.1007/978-3-030-01388-2\\_2", "10.1109/vr.2019.8797978", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1007/978-3-030-01388-2\\_8", "10.1023/a:1021271517844", "10.1109/bigdata.2014.7004282", "10.1109/tvcg.2019.2934395", "10.1109/ismar.2010.5643530", "10.1007/978-3-030-01388-22", "10.1145/2133806.2133821", "10.1109/3dvis.2014.7160093", "10.1145/2556288.2557058", "10.1109/vr.2019.8797845"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030392", "title": "ShuttleSpace: Exploring and Analyzing Movement Trajectory in Immersive Visualization", "year": "2020", "conferenceName": "InfoVis", "authors": "Shuainan Ye;Zhutian Chen;Xiangtong Chu;Yifan Wang;Siwei Fu;Lejun Shen;Kun Zhou;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD CG, Hangzhou, Peoples R China. Ye, Shuainan; Chu, Xiangtong; Wang, Yifan; Zhou, Kun; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD CG, Hangzhou, Peoples R China. Chen, Zhutian, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Fu, Siwei, Zhejiang Lab, Hangzhou, Zhejiang, Peoples R China. Shen, Lejun, Chengdu Sports Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China", "abstract": "We present ShuttleSpace, an immersive analytics system to assist experts in analyzing trajectory data in badminton. Trajectories in sports, such as the movement of players and balls, contain rich information on player behavior and thus have been widely analyzed by coaches and analysts to improve the players' performance. However, existing visual analytics systems often present the trajectories in court diagrams that are abstractions of reality, thereby causing difficulty for the experts to imagine the situation on the court and understand why the player acted in a certain way. With recent developments in immersive technologies, such as virtual reality (VR), experts gradually have the opportunity to see, feel, explore, and understand these 3D trajectories from the player's perspective. Yet, few research has studied how to support immersive analysis of sports data from such a perspective. Specific challenges are rooted in data presentation (e.g., how to seamlessly combine 2D and 3D visualizations) and interaction (e.g., how to naturally interact with data without keyboard and mouse) in VR. To address these challenges, we have worked closely with domain experts who have worked for a top national badminton team to design ShuttleSpace. Our system leverages 1) the peripheral vision to combine the 2D and 3D visualizations and 2) the VR controller to support natural interactions via a stroke metaphor. We demonstrate the effectiveness of ShuttleSpace through three case studies conducted by the experts with useful insights. We further conduct interviews with the experts whose feedback confirms that our first-person immersive analytics system is suitable and useful for analyzing badminton data.", "keywords": "Movement trajectory,badminton analytics,virtual reality", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030392", "refList": ["10.1016/j.eswa.2012.01.204", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1142/s1005386719000178", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/978-981-10-3994-2\\_1", "10.1007/978-3-319-07812-0\\_12", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1016/j.eswa.2017.07.027", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 35}, {"doi": "10.1109/pacificvis.2017.8031579", "year": "2017", "title": "Virtual retractor: An interactive data exploration system using physically based deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Xin Tong;Han{-}Wei Shen", "citationCount": "2", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Tong, Xin; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Interactive data exploration plays a fundamental role in analyzing three dimensional scientific data. Occlusion management and context preservation are among the key factors to ensure effective identification and extraction of three-dimensional features. In this paper, we present an interactive data exploration system that utilizes a physically based deformation method to investigate hidden structures of data in three dimensional data sets. While non-physically based methods are popular for visual analytic applications due to their lower computational cost, physically based deformation methods can often better preserve features and their context. Our physically based deformation method preserves data features by setting the mesh properties according to interesting data attributes. We design effective and intuitive interfaces by using a metaphor of virtual retractor, which reflects the cutting and splitting of data that our system is simulating. We demonstrate case studies on multiple particle datasets and volume datasets, and present feedback from a domain user.", "keywords": "K.6.1 {[}Management of Computing and Information Systems]: Project and People Management-Life Cycle; K.7.m {[}The Computing Profession]: Miscellaneous-Ethics", "link": "https://doi.org/10.1109/PACIFICVIS.2017.8031579", "refList": ["10.1145/1409060.1409107", "10.1109/visual.2003.1250400", "10.1145/989863.989871", "10.1109/tvcg.2008.59", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1145/988834.988870", "10.1109/pacificvis.2014.61", "10.1111/j.1467-8659.2006.01000.x", "10.1111/j.1467-8659.2006.00979.x", "10.1145/1239451.1239482", "10.1109/tvcg.2006.140", "10.1109/tvcg.2007.70565", "10.1088/1742-6596/125/1/012076", "10.1109/tvcg.2015.2443804", "10.1109/tvcg.2006.144", "10.1111/j.1467-8659.2012.03115.x", "10.3390/s130506380", "10.1145/237091.237098", "10.1109/tvcg.2015.2502583", "10.1016/j.cag.2007.09.006", "10.1109/38.595268", "10.1145/1618452.1618504", "10.1145/1980462.1980487", "10.3171/2012.5.jns112334", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.1109/tvcg.2010.127", "10.1109/tvcg.2013.121", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.1109/ismar.2004.36", "10.1109/tvcg.2010.157"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2018.00035", "year": "2018", "title": "An Automatic Deformation Approach for Occlusion Free Egocentric Data Exploration", "conferenceName": "PacificVis", "authors": "Cheng Li;Joachim Moortgat;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Moortgat, Joachim; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Occlusion management is an important task for three dimension data exploration. For egocentric data exploration, the occlusion problems, caused by the camera being too close to opaque data elements, have not been well addressed by previous studies. In this paper, we propose an automatic approach to resolve these problems and provide an occlusion free egocentric data exploration. Our system utilizes a state transition model to monitor both the camera and the data, and manages the initiation, duration, and termination of deformation with animation. Our method can be applied to multiple types of scientific datasets, including volumetric data, polygon mesh data, and particle data. We demonstrate our method with different exploration tasks, including camera navigation, isovalue adjustment, transfer function adjustment, and time varying exploration. We have collaborated with a domain expert and received positive feedback.", "keywords": "Data Deformation; Occlusion Management; Data Exploration; Egocentric Visualization", "link": "https://doi.org/10.1109/PacificVis.2018.00035", "refList": ["10.1145/989863.989871", "10.1109/tvcg.2008.59", "10.1111/j.1467-8659.2008.01332.x", "10.1109/tvcg.2016.2599049", "10.1109/pacificvis.2014.14", "10.1109/tvcg.2007.70433", "10.1007/s10596-015-9501-z", "10.1109/38.610209", "10.1109/tvcg.2003.1207447", "10.1111/j.1467-8659.2008.01181.x", "10.1109/tvcg.2012.42", "10.1109/tvcg.2016.2599217", "10.1002/fld.3764", "10.1109/pacificvis.2013.6596123", "10.1109/tvcg.2006.140", "10.1109/tvcg.2012.143", "10.1109/tvcg.2007.70565", "10.1109/scivis.2015.7429485", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.167", "10.1145/505008.505039", "10.1109/tvcg.2006.144", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/38.595268", "10.1109/tvcg.2016.2518338", "10.1109/tvcg.2014.20", "10.1145/1980462.1980487", "10.1111/cgf.12466", "10.2312/compaesth/compaesth05/209-216", "10.1109/tvcg.2010.127", "10.1016/j.advwatres.2016.01.002", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2009.144", "10.1145/1462055.1462056"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 7}, {"doi": "10.1109/pacificvis.2015.7156349", "year": "2015", "title": "Interactive streamline exploration and manipulation using deformation", "conferenceName": "PacificVis", "authors": "Xin Tong;Chun{-}Ming Chen;Han{-}Wei Shen;Pak Chung Wong", "citationCount": "9", "affiliation": "Tong, X (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nTong, Xin; Chen, Chun-Ming; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.\nWong, Pak Chung, Pacific Northwest Natl Lab, Richland, WA 99352 USA.", "countries": "USA", "abstract": "Occlusion presents a major challenge in visualizing 3D flow fields using streamlines. Displaying too many streamlines creates a dense visualization filled with occluded structures, but displaying too few streams risks losing important features. A more ideal streamline exploration approach is to visually manipulate the cluttered streamlines by pulling visible layers apart and revealing the hidden structures underneath. This paper presents a customized deformation algorithm and an interactive visualization tool to minimize visual cluttering. The algorithm is able to maintain the overall integrity of the flow field and expose the previously hidden structures. Our system supports both mouse and direct-touch interactions to manipulate the viewing perspectives and visualize the streamlines in depth. By using a lens metaphor of different shapes to select the transition zone of the targeted area interactively, the users can move their focus and examine the flow field freely.", "keywords": "", "link": "https://doi.org/10.1109/PACIFICVIS.2015.7156349", "refList": ["10.1109/visual.2003.1250400", "10.1109/tvcg.2013.100", "10.1109/pacificvis.2014.61", "10.1109/tvcg.2010.212", "10.1109/tvcg.2011.243", "10.1145/1239451.1239482", "10.1145/22339.22342", "10.1109/infvis.2003.1249008", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/tvcg.2007.1009", "10.1007/978-3-642-33786-4\\_2", "10.1109/visual.1998.745317", "10.1111/j.1467-8659.2012.03115.x", "10.1109/pacificvis.2011.5742376", "10.1145/2461912.2461930", "10.1109/tvcg.2005.66", "10.1109/tvcg.2010.131", "10.1002/spe.4380211102", "10.1109/tvcg.2012.70", "10.1145/984952.984987", "10.1109/tvcg.2006.124"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2016.2599049", "title": "GlyphLens: View-Dependent Occlusion Management in the Interactive Glyph Visualization", "year": "2016", "conferenceName": "SciVis", "authors": "Xin Tong;Cheng Li;Han-Wei Shen", "citationCount": "13", "affiliation": "Tong, X (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA. Tong, Xin; Li, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Glyph as a powerful multivariate visualization technique is used to visualize data through its visual channels. To visualize 3D volumetric dataset, glyphs are usually placed on 2D surface, such as the slicing plane or the feature surface, to avoid occluding each other. However, the 3D spatial structure of some features may be missing. On the other hand, placing large number of glyphs over the entire 3D space results in occlusion and visual clutter that make the visualization ineffective. To avoid the occlusion, we propose a view-dependent interactive 3D lens that removes the occluding glyphs by pulling the glyphs aside through the animation. We provide two space deformation models and two lens shape models to displace the glyphs based on their spatial distributions. After the displacement, the glyphs around the user-interested region are still visible as the context information, and their spatial structures are preserved. Besides, we attenuate the brightness of the glyphs inside the lens based on their depths to provide more depth cue. Furthermore, we developed an interactive glyph visualization system to explore different glyph-based visualization applications. In the system, we provide a few lens utilities that allows users to pick a glyph or a feature and look at it from different view directions. We compare different display/interaction techniques to visualize/manipulate our lens and glyphs.", "keywords": "View-dependent visualization;focus + context techniques;manipulation and deformation;glyph-based techniques;human-computer interaction", "link": "http://dx.doi.org/10.1109/TVCG.2016.2599049", "refList": ["10.1109/visual.2003.1250400", "10.1145/989863.989871", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2013.100", "10.1145/988834.988870", "10.1111/j.1467-8659.2006.01000.x", "10.1109/visual.1993.398849", "10.1109/pacificvis.2015.7156349", "10.1109/pacificvis.2009.4906851", "10.1145/2766890", "10.1109/pacificvis.2015.7156385", "10.1145/1239451.1239482", "10.2312/vissym/vissym04/147-154", "10.1109/tvcg.2015.2443804", "10.1109/tvcg.2006.167", "10.1145/166117.166126", "10.1007/bf01897116", "10.1111/1467-8659.t01-3-00700", "10.1111/j.1467-8659.2012.03115.x", "10.3390/s130506380", "10.1109/mcg.1984.275995", "10.1145/237091.237098", "10.1109/tmi.2009.2016561", "10.1145/1618452.1618504", "10.1109/tvcg.2010.199", "10.1109/tvcg.2010.127", "10.1109/tvcg.2013.121", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2007.1059", "10.1109/ismar.2004.36", "10.1111/cgf.12099", "10.1109/tvcg.2010.157", "10.1109/infvis.1996.559215"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2864690", "title": "Interactive obstruction-free lensing for volumetric data visualization", "year": "2018", "conferenceName": "SciVis", "authors": "Michael Traor\u00e9;Christophe Hurter;Alexandru Telea", "citationCount": "1", "affiliation": "Traore, M (Corresponding Author), French Civil Aviat Univ, ENAC, Toulouse, France. Traore, Michael; Hurter, Christophe, French Civil Aviat Univ, ENAC, Toulouse, France. Telea, Alexandru, Univ Groningen, Inst Johan Bernoulli, Groningen, Netherlands.", "countries": "France;Netherlands", "abstract": "Occlusion is an issue in volumetric visualization as it prevents direct visualization of the region of interest. While many techniques such as transfer functions, volume segmentation or view distortion have been developed to address this, there is still room for improvement to better support the understanding of objects' vicinity. However, most existing Focus+Context fail to solve partial occlusion in datasets where the target and the occluder are very similar density-wise. For these reasons, we investigate a new technique which maintains the general structure of the investigated volumetric dataset while addressing occlusion issues. With our technique, the user interactively defines an area of interest where an occluded region or object is partially visible. Then our lens starts pushing at its border occluding objects, thus revealing hidden volumetric data. Next, the lens is modified with an extended field of view (fish-eye deformation) to better see the vicinity of the selected region. Finally, the user can freely explore the surroundings of the area under investigation within the lens. To provide real-time exploration, we implemented our lens using a GPU accelerated ray-casting framework to handle ray deformations, local lighting, and local viewpoint manipulation. We illustrate our technique with five application scenarios in baggage inspection, 3D fluid flow visualization, chest radiology, air traffic planning, and DTI fiber exploration.", "keywords": "Interaction techniques,focus + context,volume visualization,volume rendering,raycasting", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864690", "refList": ["10.1109/visual.1999.809865", "10.1145/1409060.1409107", "10.1109/visual.2003.1250400", "10.1145/2598153.2598200", "10.1007/978-1-4614-7657-3\\_19", "10.1109/tvcg.2008.59", "10.1145/989863.989871", "10.1109/tvcg.2016.2599049", "10.1145/345513.345271", "10.1016/s1470-2045(17)30438-2", "10.1109/pacificvis.2014.61", "10.1111/j.1467-8659.2006.00979.x", "10.1109/tvcg.2010.35", "10.2312/conf/eg2012/stars/075-094", "10.1111/cgf.12927", "10.1111/cgf.12871", "10.1145/2024156.2024165", "10.1007/978-3-540-85412-8\\_16", "10.1057/ivs.2009.32", "10.1109/tvcg.2006.140", "10.1109/tvcg.2007.70565", "10.1109/tvcg.2015.2403323", "10.1109/visual.2004.32", "10.2200/s00688ed1v01y201512vis006", "10.1109/tvcg.2015.2443804", "10.1109/tvcg.2006.144", "10.1109/tvcg.2016.2515611", "10.1109/tvcg.2007.48", "10.1109/tvcg.2010.193", "10.1109/tvcg.2007.1051", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.223", "10.1109/38.595268", "10.1111/j.1467-8659.2012.03079.x", "10.1109/tvcg.2009.145", "10.1109/pacificvis.2017.8031594", "10.1145/2425296.2425325", "10.1109/tvcg.2012.265", "10.1109/mcg.2017.10", "10.1007/s12031-007-0029-0", "10.1109/tvcg.2010.127", "10.1016/j.trc.2014.03.005", "10.1109/tvcg.2009.138", "10.1109/tvcg.2006.124"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934332", "title": "LassoNet: Deep Lasso-Selection of 3D Point Clouds", "year": "2019", "conferenceName": "SciVis", "authors": "Zhutian Chen;Wei Zeng 0004;Zhiguang Yang;Lingyun Yu;Chi-Wing Fu;Huamin Qu", "citationCount": "4", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Zhutian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei; Yang, Zhiguang, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yu, Lingyun, Univ Groningen, Groningen, Netherlands. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China.", "countries": "China;Netherlands", "abstract": "Selection is a fundamental task in exploratory analysis and visualization of 3D point clouds. Prior researches on selection methods were developed mainly based on heuristics such as local point density, thus limiting their applicability in general data. Specific challenges root in the great variabilities implied by point clouds (e.g., dense vs. sparse), viewpoint (e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this work, we introduce LassoNet, a new deep neural network for lasso selection of 3D point clouds, attempting to learn a latent mapping from viewpoint and lasso to point cloud regions. To achieve this, we couple user-target points with viewpoint and lasso information through 3D coordinate transform and naive selection, and improve the method scalability via an intention filtering and farthest point sampling. A hierarchical network is trained using a dataset with over 30K lasso-selection records on two different point cloud data. We conduct a formal user study to compare LassoNet with two state-of-the-art lasso-selection methods. The evaluations confirm that our approach improves the selection effectiveness and efficiency across different combinations of 3D point clouds, viewpoints, and lasso selections. Project Website: https://LassoNet.github.io", "keywords": "Point Clouds,Lasso Selection,Deep Learning", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934332", "refList": ["10.1111/j.1467-8659.2009.01515.x", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2016.2599049", "10.1109/iccv.2015.114", "10.1109/vppc.2018.8604993", "10.1111/cgf.13405", "10.1109/tvcg.2018.2843369", "10.1109/cvpr.2018.00278", "10.1145/3025453.3025957", "10.1109/mc.2013.178", "10.1145/2980179.2980238", "10.1109/msp.2017.2693418", "10.1145/237091.237105", "10.1145/2835487", "10.1109/cvpr.2010.5539838", "10.1016/j.visinf.2017.01.006", "10.1109/tcst.2018.2819965", "10.1109/tvcg.2018.2865138", "10.1109/83.623193", "10.1145/3072959.3073608", "10.1145/1053427.1053445", "10.1016/s0039-9140(96)02179-0", "10.1109/cvpr.2015.7298801", "10.1109/cvpr.2017.693", "10.1007/s12650-014-0206-5", "10.1145/3272127.3275110", "10.1109/cvpr.2015.7298845", "10.1109/iccvw.2015.112", "10.1109/iros.2015.7353481", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2012.217", "10.1109/tridui.2006.1618279", "10.1109/tvcg.2012.292", "10.1109/cvpr.2016.609", "10.1016/j.cag.2012.12.003"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028947", "title": "A Fluid Flow Data Set for Machine Learning and its Application to Neural Flow Map Interpolation", "year": "2020", "conferenceName": "SciVis", "authors": "Jakob Jakob;Markus H. Gross;Tobias G\u00fcnther", "citationCount": "0", "affiliation": "Jakob, J (Corresponding Author), Swiss Fed Inst Technol, Zurich, Switzerland. Jakob, Jakob; Gross, Markus; Guenther, Tobias, Swiss Fed Inst Technol, Zurich, Switzerland.", "countries": "Switzerland", "abstract": "In recent years, deep learning has opened countless research opportunities across many different disciplines. At present, visualization is mainly applied to explore and explain neural networks. Its counterpart-the application of deep learning to visualization problems-requires us to share data more openly in order to enable more scientists to engage in data-driven research. In this paper, we construct a large fluid flow data set and apply it to a deep learning problem in scientific visualization. Parameterized by the Reynolds number, the data set contains a wide spectrum of laminar and turbulent fluid flow regimes. The full data set was simulated on a high-performance compute cluster and contains 8000 time-dependent 2D vector fields, accumulating to more than 16 TB in size. Using our public fluid data set, we trained deep convolutional neural networks in order to set a benchmark for an improved post-hoc Lagrangian fluid flow analysis. In in-situ settings, flow maps are exported and interpolated in order to assess the transport characteristics of time-dependent fluids. Using deep learning, we improve the accuracy of flow map interpolations, allowing a more precise flow analysis at a reduced memory IO footprint.", "keywords": "Scientific visualization,deep learning,flow maps", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028947", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1145/3355089.3356560", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1007/978-3-030-00533-7\\_36", "10.3390/rs11161921", "10.1007/978-3-030-48457-6\\_1", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/mcg.2018.2881523", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1145/3072959.3073643", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/tvcg.2013.128", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1007/978-3-319-46475-6\\_43", "10.1126/science.1127647", "10.1007/978-3-319-46475-6\\_25", "10.1111/cgf.13689"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030346", "title": "V2V: A Deep Learning Approach to Variable-to-Variable Selection and Translation for Multivariate Time-Varying Data", "year": "2020", "conferenceName": "SciVis", "authors": "Jun Han;Hao Zheng 0006;Yunhao Xing;Danny Ziyi Chen;Chaoli Wang", "citationCount": "0", "affiliation": "Han, J (Corresponding Author), Univ Notre Dame, Notre Dame, IN 46556 USA. Han, Jun; Zheng, Hao; Chen, Danny Z.; Wang, Chaoli, Univ Notre Dame, Notre Dame, IN 46556 USA. Xing, Yunhao, Sichuan Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China;USA", "abstract": "We present V2V, a novel deep learning framework, as a general-purpose solution to the variable-to-variable (V2V) selection and translation problem for multivariate time-varying data (MTVD) analysis and visualization. V2V leverages a representation learning algorithm to identify transferable variables and utilizes Kullback-Leibler divergence to determine the source and target variables. It then uses a generative adversarial network (GAN) to learn the mapping from the source variable to the target variable via the adversarial, volumetric, and feature losses. V2V takes the pairs of time steps of the source and target variable as input for training, Once trained, it can infer unseen time steps of the target variable given the corresponding time steps of the source variable. Several multivariate time-varying data sets of different characteristics are used to demonstrate the effectiveness of V2V, both quantitatively and qualitatively. We compare V2V against histogram matching and two other deep learning solutions (Pix2Pix and CycleGAN).", "keywords": "Multivariate time-varying data,variable selection and translation,generative adversarial network,data extrapolation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030346", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/cvpr.2017.19", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1146/annurev", "10.3390/rs11161921", "10.1109/tvcg.2007.70551", "10.1145/3309993", "10.1007/978-3-030-48457-6\\_1", "10.1109/igarss.2018.8518411", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1146/annurev-fluid-010313-141322", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1126/science.1127647"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030374", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data", "year": "2020", "conferenceName": "SciVis", "authors": "Yifan Wang;Guoli Yan;Haikuan Zhu;Sagar Buch;Ying Wang;E. Mark Haacke;Jing Hua;Zichun Zhong", "citationCount": "0", "affiliation": "Wang, YF (Corresponding Author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Wang, Yifan; Yan, Guoli; Zhu, Haikuan; Hua, Jing; Zhong, Zichun, Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Buch, Sagar; Wang, Ying; Haacke, Ewart Mark, Wayne State Univ, Dept Radiol, Detroit, MI 48201 USA.", "countries": "USA", "abstract": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.", "keywords": "Deep neural network,3D cerebrovascular segmentation and visualization,maximum intensity projection (MIP),joint embedding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030374", "refList": ["10.1109/cluster.2018.00036", "10.1109/iccv.2015.123", "10.1109/tvcg.2013.133", "10.1109/cvpr.2017.19", "10.1109/tvcg.2019.2934312", "10.1007/978-3-319-46487-9\\_40", "10.1109/tvcg.2006.175", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2018.2880207", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/tvcg.2018.2816059", "10.1109/pacificvis.2009.4906852", "10.1109/tvcg.2018.2864808", "10.1109/mcg.2018.2881523", "10.1109/tvcg.2015.2467431", "10.1109/tpami.2015.2439281", "10.1109/pacificvis48177.2020.8737", "10.1109/cvpr.2019.00244", "10.1109/tvcg.2006.165", "10.1109/bigdata.2018.8622520", "10.1007/978-3-319-46466-4\\_29", "10.1109/visual.2019.8933759", "10.1145/3197517.3201304", "10.1109/pacificvis.2011.5742378", "10.1109/cvpr.2006.91", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1007/978-3-319-24574-4\\_28", "10.1109/pacificvis.2019.00041", "10.1109/tvcg.2019.2934332", "10.1109/cvpr.2018.00916", "10.1007/978-3-319-46475-6\\_43"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2018.00035", "year": "2018", "title": "An Automatic Deformation Approach for Occlusion Free Egocentric Data Exploration", "conferenceName": "PacificVis", "authors": "Cheng Li;Joachim Moortgat;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Moortgat, Joachim; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Occlusion management is an important task for three dimension data exploration. For egocentric data exploration, the occlusion problems, caused by the camera being too close to opaque data elements, have not been well addressed by previous studies. In this paper, we propose an automatic approach to resolve these problems and provide an occlusion free egocentric data exploration. Our system utilizes a state transition model to monitor both the camera and the data, and manages the initiation, duration, and termination of deformation with animation. Our method can be applied to multiple types of scientific datasets, including volumetric data, polygon mesh data, and particle data. We demonstrate our method with different exploration tasks, including camera navigation, isovalue adjustment, transfer function adjustment, and time varying exploration. We have collaborated with a domain expert and received positive feedback.", "keywords": "Data Deformation; Occlusion Management; Data Exploration; Egocentric Visualization", "link": "https://doi.org/10.1109/PacificVis.2018.00035", "refList": ["10.1145/989863.989871", "10.1109/tvcg.2008.59", "10.1111/j.1467-8659.2008.01332.x", "10.1109/tvcg.2016.2599049", "10.1109/pacificvis.2014.14", "10.1109/tvcg.2007.70433", "10.1007/s10596-015-9501-z", "10.1109/38.610209", "10.1109/tvcg.2003.1207447", "10.1111/j.1467-8659.2008.01181.x", "10.1109/tvcg.2012.42", "10.1109/tvcg.2016.2599217", "10.1002/fld.3764", "10.1109/pacificvis.2013.6596123", "10.1109/tvcg.2006.140", "10.1109/tvcg.2012.143", "10.1109/tvcg.2007.70565", "10.1109/scivis.2015.7429485", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.167", "10.1145/505008.505039", "10.1109/tvcg.2006.144", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/38.595268", "10.1109/tvcg.2016.2518338", "10.1109/tvcg.2014.20", "10.1145/1980462.1980487", "10.1111/cgf.12466", "10.2312/compaesth/compaesth05/209-216", "10.1109/tvcg.2010.127", "10.1016/j.advwatres.2016.01.002", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2009.144", "10.1145/1462055.1462056"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/pacificvis.2017.8031579", "year": "2017", "title": "Virtual retractor: An interactive data exploration system using physically based deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Xin Tong;Han{-}Wei Shen", "citationCount": "2", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Tong, Xin; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Interactive data exploration plays a fundamental role in analyzing three dimensional scientific data. Occlusion management and context preservation are among the key factors to ensure effective identification and extraction of three-dimensional features. In this paper, we present an interactive data exploration system that utilizes a physically based deformation method to investigate hidden structures of data in three dimensional data sets. While non-physically based methods are popular for visual analytic applications due to their lower computational cost, physically based deformation methods can often better preserve features and their context. Our physically based deformation method preserves data features by setting the mesh properties according to interesting data attributes. We design effective and intuitive interfaces by using a metaphor of virtual retractor, which reflects the cutting and splitting of data that our system is simulating. We demonstrate case studies on multiple particle datasets and volume datasets, and present feedback from a domain user.", "keywords": "K.6.1 {[}Management of Computing and Information Systems]: Project and People Management-Life Cycle; K.7.m {[}The Computing Profession]: Miscellaneous-Ethics", "link": "https://doi.org/10.1109/PACIFICVIS.2017.8031579", "refList": ["10.1145/1409060.1409107", "10.1109/visual.2003.1250400", "10.1145/989863.989871", "10.1109/tvcg.2008.59", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1145/988834.988870", "10.1109/pacificvis.2014.61", "10.1111/j.1467-8659.2006.01000.x", "10.1111/j.1467-8659.2006.00979.x", "10.1145/1239451.1239482", "10.1109/tvcg.2006.140", "10.1109/tvcg.2007.70565", "10.1088/1742-6596/125/1/012076", "10.1109/tvcg.2015.2443804", "10.1109/tvcg.2006.144", "10.1111/j.1467-8659.2012.03115.x", "10.3390/s130506380", "10.1145/237091.237098", "10.1109/tvcg.2015.2502583", "10.1016/j.cag.2007.09.006", "10.1109/38.595268", "10.1145/1618452.1618504", "10.1145/1980462.1980487", "10.3171/2012.5.jns112334", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.1109/tvcg.2010.127", "10.1109/tvcg.2013.121", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.1109/ismar.2004.36", "10.1109/tvcg.2010.157"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2018.00035", "year": "2018", "title": "An Automatic Deformation Approach for Occlusion Free Egocentric Data Exploration", "conferenceName": "PacificVis", "authors": "Cheng Li;Joachim Moortgat;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Moortgat, Joachim; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Occlusion management is an important task for three dimension data exploration. For egocentric data exploration, the occlusion problems, caused by the camera being too close to opaque data elements, have not been well addressed by previous studies. In this paper, we propose an automatic approach to resolve these problems and provide an occlusion free egocentric data exploration. Our system utilizes a state transition model to monitor both the camera and the data, and manages the initiation, duration, and termination of deformation with animation. Our method can be applied to multiple types of scientific datasets, including volumetric data, polygon mesh data, and particle data. We demonstrate our method with different exploration tasks, including camera navigation, isovalue adjustment, transfer function adjustment, and time varying exploration. We have collaborated with a domain expert and received positive feedback.", "keywords": "Data Deformation; Occlusion Management; Data Exploration; Egocentric Visualization", "link": "https://doi.org/10.1109/PacificVis.2018.00035", "refList": ["10.1145/989863.989871", "10.1109/tvcg.2008.59", "10.1111/j.1467-8659.2008.01332.x", "10.1109/tvcg.2016.2599049", "10.1109/pacificvis.2014.14", "10.1109/tvcg.2007.70433", "10.1007/s10596-015-9501-z", "10.1109/38.610209", "10.1109/tvcg.2003.1207447", "10.1111/j.1467-8659.2008.01181.x", "10.1109/tvcg.2012.42", "10.1109/tvcg.2016.2599217", "10.1002/fld.3764", "10.1109/pacificvis.2013.6596123", "10.1109/tvcg.2006.140", "10.1109/tvcg.2012.143", "10.1109/tvcg.2007.70565", "10.1109/scivis.2015.7429485", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.167", "10.1145/505008.505039", "10.1109/tvcg.2006.144", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/38.595268", "10.1109/tvcg.2016.2518338", "10.1109/tvcg.2014.20", "10.1145/1980462.1980487", "10.1111/cgf.12466", "10.2312/compaesth/compaesth05/209-216", "10.1109/tvcg.2010.127", "10.1016/j.advwatres.2016.01.002", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2009.144", "10.1145/1462055.1462056"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 7}], "len": 27}], "len": 29}, {"doi": "10.1111/cgf.13213", "year": "2017", "title": "State of the Art in Edge and Trail Bundling Techniques", "conferenceName": "EuroVis", "authors": "Antoine Lhuillier;Christophe Hurter;Alexandru Telea", "citationCount": "24", "affiliation": "Lhuillier, A (Corresponding Author), Univ Toulouse, ENAC, Toulouse, France.\nLhuillier, A.; Hurter, C., Univ Toulouse, ENAC, Toulouse, France.\nTelea, A., Univ Groningen, Inst Johann Bernoulli, Groningen, Netherlands.", "countries": "France;Netherlands", "abstract": "Bundling techniques provide a visual simplification of a graph drawing or trail set, by spatially grouping similar graph edges or trails. This way, the structure of the visualization becomes simpler and thereby easier to comprehend in terms of assessing relations that are encoded by such paths, such as finding groups of strongly interrelated nodes in a graph, finding connections between spatial regions on a map linked by a number of vehicle trails, or discerning the motion structure of a set of objects by analyzing their paths. In this state of the art report, we aim to improve the understanding of graph and trail bundling via the following main contributions. First, we propose a data-based taxonomy that organizes bundling methods on the type of data they work on (graphs vs trails, which we refer to as paths). Based on a formal definition of path bundling, we propose a generic framework that describes the typical steps of all bundling algorithms in terms of high-level operations and show how existing method classes implement these steps. Next, we propose a description of tasks that bundling aims to address. Finally, we provide a wide set of example applications of bundling techniques and relate these to the above-mentioned taxonomies. Through these contributions, we aim to help both researchers and users to understand the bundling landscape as well as its technicalities.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13213", "refList": ["10.1109/tvcg.2008.135", "10.1109/tvcg.2014.2346578", "10.1145/2245276.2245338", "10.1111/j.1467-8659.2008.01214.x", "10.1109/pacificvis.2014.61", "10.1080/03098269708725407", "10.1111/j.1467-8659.2009.01700.x", "10.1002/1097-024x(200009)30:11", "10.1109/infvis.2003.1249008", "10.1109/tvcg.2011.233", "10.1109/tvcg.2011.181", "10.1109/tvcg.2006.147", "10.1109/pacificvis.2011.5742390", "10.1145/2470654.2466444", "10.1145/2049662.2049663", "10.1007/s12031-007-0029-0", "10.1109/tsmc.1981.4308636", "10.1109/tvcg.2015.2467112", "10.1109/dsnw.2011.5958797", "10.1109/iv.2010.53", "10.1016/j.scico.2012.05.002", "10.1109/tvcg.2011.202", "10.1016/j.cag.2014.01.006", "10.1109/visual.1999.809865", "10.1145/2833165.2833168", "10.1109/pacificvis.2016.7465267", "10.1109/tvcg.2016.2598838", "10.1109/pacificvis.2011.5742387", "10.1145/2254556.2254652", "10.1145/331499.331504", "10.1109/tse.2009.28", "10.1109/infvis.2004.66", "10.1111/j.1467-8659.2008.01232.x", "10.1109/tvcg.2015.2403323", "10.1109/pacificvis.2014.46", "10.1109/tvcg.2016.2515611", "10.1109/iv.2003.1217950", "10.1145/1322432.1322434", "10.1109/tvcg.2008.34", "10.1109/tvcg.2011.223", "10.1057/palgrave.ivs.95000/3", "10.1109/pacificvis.2011.5742384", "10.1109/tvcg.2013.114", "10.1002/smr.220", "10.1111/j.1467-8659.2008.01239.x", "10.1109/tst.2013.6509098", "10.1016/j.nurt.2007.05.011", "10.1111/j.1467-8659.2008.01241.x", "10.1006/jvlc.1998.0094", "10.7155/jgaa.00099", "10.1016/j.entcs.2008.06.039", "10.1111/j.1467-8659.2008.01205.x", "10.1109/tvcg.2013.246", "10.1109/vl.1996.545307", "10.1109/tvcg.2013.151", "10.1109/tvcg.2013.124", "10.1007/s00450-007-0036-y", "10.1111/j.1467-8659.2009.01680.x", "10.1109/tvcg.2003.1196007", "10.1007/978-3-642-25591-5\\_27", "10.1109/infvis.2005.1532150", "10.1371/journal.pcbi.1000108", "10.1007/978-3-642-41939-3\\_38", "10.1007/978-3-319-03841-4\\_34", "10.1109/tvcg.2009.145", "10.1179/000870410x12658023467367", "10.5194/npg-22-545-2015", "10.1002/smr.270", "10.1109/pacificvis.2011.5742389", "10.1111/j.1467-8659.2009.01683.x", "10.1068/i0382", "10.1109/infvis.1999.801860", "10.1111/j.0033-0124.1981.00419.x", "10.1007/978-3-319-27261-0\\_41", "10.1111/j.1467-8659.2009.01666.x", "10.1109/tvcg.2007.70582", "10.1109/tvcg.2006.120", "10.1111/j.1467-8659.2011.02037.x", "10.1109/tvcg.2009.108", "10.1111/j.1467-8659.2011.01951.x", "10.1111/j.1467-8659.2011.01898.x", "10.1117/12.704612", "10.1109/tvcg.2011.155", "10.1109/iv.2010.78", "10.1109/vast.2008.4677380", "10.1111/cgf.12881", "10.1109/2945.841119", "10.1016/0020-0190(89)90102-6", "10.1093/bioinformatics/bth078", "10.1109/34.1000236", "10.1109/tvcg.2007.70535", "10.1016/j.jss.2008.02.068", "10.1007/978-3-319-06793-3\\_2", "10.1007/978-3-642-18469-7\\_30", "10.1145/1168149.1168168", "10.1145/1773965.1773969", "10.1111/j.1467-8659.2012.03079.x", "10.1016/s0364-0213(87)80026-5", "10.1109/tvcg.2007.70521", "10.1145/2049662.2049670", "10.1109/tvcg.2011.104", "10.1007/bf00336961", "10.1016/j.trc.2014.03.005", "10.1109/pacificvis.2015.7156354", "10.1080/13658816.2010.511718", "10.2307/2685881", "10.1111/j.1467-8659.2003.00723.x", "10.1109/mcg.2009.87", "10.1109/tvcg.2016.2598885", "10.1016/j.cosrev.2007.05.001", "10.1111/j.1467-8659.2009.01450.x", "10.1145/2254556.2254670"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2017.2744338", "title": "Functional Decomposition for Bundled Simplification of Trail Sets", "year": "2017", "conferenceName": "InfoVis", "authors": "Christophe Hurter;St\u00e9phane Puechmorel;Florence Nicol;Alexandru Telea", "citationCount": "4", "affiliation": "Hurter, C (Corresponding Author), ENAC, Toulouse, France. Hurter, Christophe; Puechmorel, Stephane; Nicol, Florence, ENAC, Toulouse, France. Telea, Alexandru, Univ Groningen, Groningen, Netherlands.", "countries": "France;Netherlands", "abstract": "Bundling visually aggregates curves to reduce clutter and help finding important patterns in trail-sets or graph drawings. We propose a new approach to bundling based on functional decomposition of the underling dataset. We recover the functional nature of the curves by representing them as linear combinations of piecewise-polynomial basis functions with associated expansion coefficients. Next, we express all curves in a given cluster in terms of a centroid curve and a complementary term, via a set of so-called principal component functions. Based on the above, we propose a two-fold contribution: First, we use cluster centroids to design a new bundling method for 2D and 3D curve-sets. Secondly, we deform the cluster centroids and generate new curves along them, which enables us to modify the underlying data in a statistically-controlled way via its simplified (bundled) view. We demonstrate our method by applications on real-world 2D and 3D datasets for graph bundling, trajectory analysis, and vector field and tensor field visualization.", "keywords": "path visualization,trajectory visualization,edge bundles,functional decomposition,path generation,streamlines", "link": "http://dx.doi.org/10.1109/TVCG.2017.2744338", "refList": ["10.1109/tvcg.2008.135", "10.1111/cgf.13213", "10.1111/j.1751-5823.2011.001496.x", "10.1080/01441640110074773", "10.1007/s11634-013-0158-y", "10.1145/2254556.2254652", "10.1109/tst.2013.6509098", "10.1007/978-3-7091-6876-9\\_5", "10.1145/331499.331504", "10.1117/12.704612", "10.1145/2501988.2502046", "10.1016/s1361-8415(02)00053-1", "10.1109/tvcg.2011.155", "10.1109/tvcg.2013.246", "10.1109/iv.2010.78", "10.1109/vast.2008.4677380", "10.1111/j.1751-5823.2011.00149\\_6.x", "10.2307/2291115", "10.1111/j.1467-8659.2009.01700.x", "10.1109/tvcg.2011.233", "10.1109/tvcg.2015.2403323", "10.1214/09-aos741", "10.1109/tvcg.2013.124", "10.1109/visual.2004.32", "10.1093/bioinformatics/bth078", "10.2200/s00688ed1v01y201512vis006", "10.1007/978-3-642-36763-2\\_36", "10.1111/j.1467-8659.2009.01680.x", "10.1109/34.1000236", "10.1111/j.1467-8659.2008.01244.x", "10.1109/tvcg.2016.2515611", "10.1109/infvis.2005.1532150", "10.1109/iv.2003.1217950", "10.1214/009053605000000660", "10.1109/tvcg.2011.181", "10.1109/vissof.2011.6069451", "10.1109/tvcg.2006.147", "10.1016/j.jss.2008.02.068", "10.1007/978-3-642-18469-7\\_30", "10.1145/293347.293348", "10.1007/978-3-642-97385-7", "10.1109/tvcg.2011.223", "10.1111/j.1467-8659.2012.03079.x", "10.1093/comjnl/16.1.30", "10.1007/s10816-016-9307-x", "10.1090/s0002-9947-1950-0051437-7", "10.2307/2289936", "10.1109/tvcg.2011.104", "10.1007/s00453-013-9867-z", "10.1002/cem.1129", "10.1007/s12031-007-0029-0", "10.1109/tvcg.2013.114", "10.1093/comjnl/20.4.364", "10.1109/pacificvis.2013.6596126", "10.1109/tvcg.2011.190", "10.1109/cvpr.2007.383096", "10.1007/978-3-540-68721-4", "10.1109/pacificvis.2015.7156354", "10.5555/uri:pii:001650859291831n", "10.1109/tvcg.2009.138", "10.1111/j.1467-8659.2009.01450.x", "10.1137/1022106", "10.7155/jgaa.00028", "10.1109/tvcg.2011.202", "10.1007/b98888"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865191", "title": "FiberClay: Sculpting Three Dimensional Trajectories to Reveal Structural Insights", "year": "2018", "conferenceName": "InfoVis", "authors": "Christophe Hurter;Nathalie Henry Riche;Steven Mark Drucker;Maxime Cordeil;Richard Alligier;Romain Vuillemot", "citationCount": "11", "affiliation": "Hurter, C (Corresponding Author), French Civil Aviat Univ, ENAC, Toulouse, France. Hurter, C (Corresponding Author), Toulouse Univ, Toulouse, France. Hurter, Christophe ll; Alligier, Richard, French Civil Aviat Univ, ENAC, Toulouse, France. Hurter, Christophe ll; Alligier, Richard, Toulouse Univ, Toulouse, France. Riche, Nathalie Henry; Drucker, Steven M., Microsoft Res, Redmond, WA USA. Cordeil, Maxime, Monash Univ, Clayton, Vic, Australia. Vuillemot, Romain, Univ Lyon, Ecole Cent Lyon, CNRS, UMR 5205,LIRIS, F-69134 Lyon, France.", "countries": "USA;France;Australia", "abstract": "Visualizing 3D trajectories to extract insights about their similarities and spatial configuration is a critical task in several domains. Air traffic controllers for example deal with large quantities of aircrafts routes to optimize safety in airspace and neuroscientists attempt to understand neuronal pathways in the human brain by visualizing bundles of fibers from DTI images. Extracting insights from masses of 3D trajectories is challenging as the multiple three dimensional lines have complex geometries, may overlap, cross or even merge with each other, making it impossible to follow individual ones in dense areas. As trajectories are inherently spatial and three dimensional, we propose FiberClay: a system to display and interact with 3D trajectories in immersive environments. FiberClay renders a large quantity of trajectories in real time using GP-GPU techniques. FiberClay also introduces a new set of interactive techniques for composing complex queries in 3D space leveraging immersive environment controllers and user position. These techniques enable an analyst to select and compare sets of trajectories with specific geometries and data properties. We conclude by discussing insights found using FiberClay with domain experts in air traffic control and neurology.", "keywords": "Immersive Analytics,3D Visualization,Dynamic Queries,Bimanual Interaction,Multidimensional Data", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865191", "refList": ["10.1016/b978-0-08-051574-8.50047-9", "10.1016/j.ijhcs.2013.03.003", "10.1057/palgrave.ivs.9500097", "10.1109/tvcg.2008.153", "10.2307/1269768", "10.1109/tvcg.2013.226", "10.1109/tvcg.2005.59", "10.1016/j.compenvurbsys.2014.01.005", "10.1017/cbo9781139128926.013", "10.1109/pacificvis.2014.61", "10.1145/1279640.1279642", "10.1109/pacificvis.2017.8031577", "10.1109/tvcg.2016.2599217", "10.1109/visual.1991.175794", "10.1007/978-1-4615-1177-9\\_27", "10.1007/978-0-387-35504-714", "10.1109/tvcg.2011.233", "10.3390/informatics4030026", "10.1109/tvcg.2011.192", "10.1109/tvcg.2015.2403323", "10.1145/800186.810616", "10.1145/3013971.3014006", "10.1016/j.ijhcs.2005.02.001", "10.1109/3dvis.2014.7160096", "10.1111/j.1467-8659.2012.03115.x", "10.1109/38.946631", "10.1145/3126594.3126613", "10.1109/pacificvis.2011.5742390", "10.4230/dagrep.6.6.1", "10.1111/j.1467-8659.2012.03079.x", "10.1057/palgrave.ivs.9500061", "10.1109/tvcg.2017.2744338", "10.1109/tvcg.2009.145", "10.1109/tvcg.2012.265", "10.1007/s12031-007-0029-0", "10.1145/3025453.3025566", "10.1109/tvcg.2013.153", "10.1109/tvcg.2016.2520921", "10.1016/j.trc.2014.03.005", "10.1109/3dvis.2014.7160093", "10.1109/ipsn.2014.6846743", "10.1109/tvcg.2015.2467112", "10.1109/tvcg.2017.2744079", "10.1109/tvcg.2011.224", "10.1111/cgf.12804", "10.1111/j.1467-8659.2009.01687.x", "10.1016/s1088-467x(99)00013-x", "10.1109/tvcg.2012.217", "10.1145/2992154.2996365"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934415", "title": "Evaluating an Immersive Space-Time Cube Geovisualization for Intuitive Trajectory Data Exploration", "year": "2019", "conferenceName": "InfoVis", "authors": "Jorge A. Wagner Filho;Wolfgang Stuerzlinger;Luciana Porcher Nedel", "citationCount": "4", "affiliation": "Wagner, JA (Corresponding Author), Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil. Wagner, JA (Corresponding Author), Simon Fraser Univ, Burnaby, BC, Canada. Wagner Filho, Jorge A.; Nedel, Luciana, Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil. Wagner Filho, Jorge A.; Stuerzlinger, Wolfgang, Simon Fraser Univ, Burnaby, BC, Canada.", "countries": "Canada;Brazil", "abstract": "A Space-Time Cube enables analysts to clearly observe spatio-temporal features in movement trajectory datasets in geovisualization. However, its general usability is impacted by a lack of depth cues, a reported steep learning curve, and the requirement for efficient 3D navigation. In this work, we investigate a Space-Time Cube in the Immersive Analytics domain. Based on a review of previous work and selecting an appropriate exploration metaphor, we built a prototype environment where the cube is coupled to a virtual representation of the analyst's real desk, and zooming and panning in space and time are intuitively controlled using mid-air gestures. We compared our immersive environment to a desktop-based implementation in a user study with 20 participants across 7 tasks of varying difficulty, which targeted different user interface features. To investigate how performance is affected in the presence of clutter, we explored two scenarios with different numbers of trajectories. While the quantitative performance was similar for the majority of tasks, large differences appear when we analyze the patterns of interaction and consider subjective metrics. The immersive version of the Space-Time Cube received higher usability scores, much higher user preference, and was rated to have a lower mental workload, without causing participants discomfort in 25-minute-long VR sessions.", "keywords": "Space-time cube,Trajectory visualization,Immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934415", "refList": ["10.1109/mcg.2006.74", "10.1080/13658816.2015.1058386", "10.1145/1456650.1456652", "10.1111/cgf.13430", "10.1145/1944745.1944777", "10.1016/b978-155860819-1/50001-7", "10.1109/bdva.2018.8534024", "10.1109/pacificvis.2017.8031578", "10.1111/j.1467-8306.1994.tb01869.x", "10.2307/3001968", "10.1007/978-3-642-15300-6\\_21", "10.1080/13658816.2010.511223", "10.1007/978-3-030-01388-2", "10.1080/00087041.2018.1495898", "10.1145/1773965.1773970", "10.1002/9780470987643.ch15", "10.1109/glocom.2015.7417476", "10.2312/eurovisstar.20141171", "10.1080/17489725.2015.1074736", "10.1109/bdva.2016.7787050", "10.1109/wevr.2017.7957707", "10.1016/s1045-926x(03)00046-6", "10.1109/iv.2018.00026", "10.1179/1743277413y.0000000061", "10.1109/mcg.2019.2898856", "10.1109/iv.2004.1320137", "10.1111/j.1467-8659.2011.01929.x", "10.1109/tvcg.2008.194", "10.1016/j.jtrangeo.2010.11.002", "10.1177/154193120605000909", "10.1109/bigdata.2015.7364040", "10.1111/cgf.12466", "10.1109/icsens.2015.7370446", "10.1080/13658816.2010.508043", "10.1057/ivs.2009.8", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.14236/ewic/hci2016.22", "10.1145/2655691", "10.1007/bf01936872", "10.1109/infvis.2004.27", "10.1109/tvcg.2014.2329308", "10.1111/j.1435-5597.1970.tb01464.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934332", "title": "LassoNet: Deep Lasso-Selection of 3D Point Clouds", "year": "2019", "conferenceName": "SciVis", "authors": "Zhutian Chen;Wei Zeng 0004;Zhiguang Yang;Lingyun Yu;Chi-Wing Fu;Huamin Qu", "citationCount": "4", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Zhutian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei; Yang, Zhiguang, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yu, Lingyun, Univ Groningen, Groningen, Netherlands. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China.", "countries": "China;Netherlands", "abstract": "Selection is a fundamental task in exploratory analysis and visualization of 3D point clouds. Prior researches on selection methods were developed mainly based on heuristics such as local point density, thus limiting their applicability in general data. Specific challenges root in the great variabilities implied by point clouds (e.g., dense vs. sparse), viewpoint (e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this work, we introduce LassoNet, a new deep neural network for lasso selection of 3D point clouds, attempting to learn a latent mapping from viewpoint and lasso to point cloud regions. To achieve this, we couple user-target points with viewpoint and lasso information through 3D coordinate transform and naive selection, and improve the method scalability via an intention filtering and farthest point sampling. A hierarchical network is trained using a dataset with over 30K lasso-selection records on two different point cloud data. We conduct a formal user study to compare LassoNet with two state-of-the-art lasso-selection methods. The evaluations confirm that our approach improves the selection effectiveness and efficiency across different combinations of 3D point clouds, viewpoints, and lasso selections. Project Website: https://LassoNet.github.io", "keywords": "Point Clouds,Lasso Selection,Deep Learning", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934332", "refList": ["10.1111/j.1467-8659.2009.01515.x", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2016.2599049", "10.1109/iccv.2015.114", "10.1109/vppc.2018.8604993", "10.1111/cgf.13405", "10.1109/tvcg.2018.2843369", "10.1109/cvpr.2018.00278", "10.1145/3025453.3025957", "10.1109/mc.2013.178", "10.1145/2980179.2980238", "10.1109/msp.2017.2693418", "10.1145/237091.237105", "10.1145/2835487", "10.1109/cvpr.2010.5539838", "10.1016/j.visinf.2017.01.006", "10.1109/tcst.2018.2819965", "10.1109/tvcg.2018.2865138", "10.1109/83.623193", "10.1145/3072959.3073608", "10.1145/1053427.1053445", "10.1016/s0039-9140(96)02179-0", "10.1109/cvpr.2015.7298801", "10.1109/cvpr.2017.693", "10.1007/s12650-014-0206-5", "10.1145/3272127.3275110", "10.1109/cvpr.2015.7298845", "10.1109/iccvw.2015.112", "10.1109/iros.2015.7353481", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2012.217", "10.1109/tridui.2006.1618279", "10.1109/tvcg.2012.292", "10.1109/cvpr.2016.609", "10.1016/j.cag.2012.12.003"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028947", "title": "A Fluid Flow Data Set for Machine Learning and its Application to Neural Flow Map Interpolation", "year": "2020", "conferenceName": "SciVis", "authors": "Jakob Jakob;Markus H. Gross;Tobias G\u00fcnther", "citationCount": "0", "affiliation": "Jakob, J (Corresponding Author), Swiss Fed Inst Technol, Zurich, Switzerland. Jakob, Jakob; Gross, Markus; Guenther, Tobias, Swiss Fed Inst Technol, Zurich, Switzerland.", "countries": "Switzerland", "abstract": "In recent years, deep learning has opened countless research opportunities across many different disciplines. At present, visualization is mainly applied to explore and explain neural networks. Its counterpart-the application of deep learning to visualization problems-requires us to share data more openly in order to enable more scientists to engage in data-driven research. In this paper, we construct a large fluid flow data set and apply it to a deep learning problem in scientific visualization. Parameterized by the Reynolds number, the data set contains a wide spectrum of laminar and turbulent fluid flow regimes. The full data set was simulated on a high-performance compute cluster and contains 8000 time-dependent 2D vector fields, accumulating to more than 16 TB in size. Using our public fluid data set, we trained deep convolutional neural networks in order to set a benchmark for an improved post-hoc Lagrangian fluid flow analysis. In in-situ settings, flow maps are exported and interpolated in order to assess the transport characteristics of time-dependent fluids. Using deep learning, we improve the accuracy of flow map interpolations, allowing a more precise flow analysis at a reduced memory IO footprint.", "keywords": "Scientific visualization,deep learning,flow maps", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028947", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1145/3355089.3356560", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1007/978-3-030-00533-7\\_36", "10.3390/rs11161921", "10.1007/978-3-030-48457-6\\_1", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/mcg.2018.2881523", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1145/3072959.3073643", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/tvcg.2013.128", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1007/978-3-319-46475-6\\_43", "10.1126/science.1127647", "10.1007/978-3-319-46475-6\\_25", "10.1111/cgf.13689"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030346", "title": "V2V: A Deep Learning Approach to Variable-to-Variable Selection and Translation for Multivariate Time-Varying Data", "year": "2020", "conferenceName": "SciVis", "authors": "Jun Han;Hao Zheng 0006;Yunhao Xing;Danny Ziyi Chen;Chaoli Wang", "citationCount": "0", "affiliation": "Han, J (Corresponding Author), Univ Notre Dame, Notre Dame, IN 46556 USA. Han, Jun; Zheng, Hao; Chen, Danny Z.; Wang, Chaoli, Univ Notre Dame, Notre Dame, IN 46556 USA. Xing, Yunhao, Sichuan Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China;USA", "abstract": "We present V2V, a novel deep learning framework, as a general-purpose solution to the variable-to-variable (V2V) selection and translation problem for multivariate time-varying data (MTVD) analysis and visualization. V2V leverages a representation learning algorithm to identify transferable variables and utilizes Kullback-Leibler divergence to determine the source and target variables. It then uses a generative adversarial network (GAN) to learn the mapping from the source variable to the target variable via the adversarial, volumetric, and feature losses. V2V takes the pairs of time steps of the source and target variable as input for training, Once trained, it can infer unseen time steps of the target variable given the corresponding time steps of the source variable. Several multivariate time-varying data sets of different characteristics are used to demonstrate the effectiveness of V2V, both quantitatively and qualitatively. We compare V2V against histogram matching and two other deep learning solutions (Pix2Pix and CycleGAN).", "keywords": "Multivariate time-varying data,variable selection and translation,generative adversarial network,data extrapolation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030346", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/cvpr.2017.19", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1146/annurev", "10.3390/rs11161921", "10.1109/tvcg.2007.70551", "10.1145/3309993", "10.1007/978-3-030-48457-6\\_1", "10.1109/igarss.2018.8518411", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1146/annurev-fluid-010313-141322", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1126/science.1127647"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030374", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data", "year": "2020", "conferenceName": "SciVis", "authors": "Yifan Wang;Guoli Yan;Haikuan Zhu;Sagar Buch;Ying Wang;E. Mark Haacke;Jing Hua;Zichun Zhong", "citationCount": "0", "affiliation": "Wang, YF (Corresponding Author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Wang, Yifan; Yan, Guoli; Zhu, Haikuan; Hua, Jing; Zhong, Zichun, Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Buch, Sagar; Wang, Ying; Haacke, Ewart Mark, Wayne State Univ, Dept Radiol, Detroit, MI 48201 USA.", "countries": "USA", "abstract": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.", "keywords": "Deep neural network,3D cerebrovascular segmentation and visualization,maximum intensity projection (MIP),joint embedding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030374", "refList": ["10.1109/cluster.2018.00036", "10.1109/iccv.2015.123", "10.1109/tvcg.2013.133", "10.1109/cvpr.2017.19", "10.1109/tvcg.2019.2934312", "10.1007/978-3-319-46487-9\\_40", "10.1109/tvcg.2006.175", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2018.2880207", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/tvcg.2018.2816059", "10.1109/pacificvis.2009.4906852", "10.1109/tvcg.2018.2864808", "10.1109/mcg.2018.2881523", "10.1109/tvcg.2015.2467431", "10.1109/tpami.2015.2439281", "10.1109/pacificvis48177.2020.8737", "10.1109/cvpr.2019.00244", "10.1109/tvcg.2006.165", "10.1109/bigdata.2018.8622520", "10.1007/978-3-319-46466-4\\_29", "10.1109/visual.2019.8933759", "10.1145/3197517.3201304", "10.1109/pacificvis.2011.5742378", "10.1109/cvpr.2006.91", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1007/978-3-319-24574-4\\_28", "10.1109/pacificvis.2019.00041", "10.1109/tvcg.2019.2934332", "10.1109/cvpr.2018.00916", "10.1007/978-3-319-46475-6\\_43"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934803", "title": "There Is No Spoon: Evaluating Performance, Space Use, and Presence with Expert Domain Users in Immersive Analytics", "year": "2019", "conferenceName": "InfoVis", "authors": "Andrea Batch;Andrew Cunningham;Maxime Cordeil;Niklas Elmqvist;Tim Dwyer;Bruce H. Thomas;Kim Marriott", "citationCount": "6", "affiliation": "Batch, A (Corresponding Author), Univ Maryland, College Pk, MD 20742 USA. Batch, Andrea; Elmqvist, Niklas, Univ Maryland, College Pk, MD 20742 USA. Cunningham, Andrew; Thomas, Bruce H., Univ South Australia Adelaide, Adelaide, SA, Australia. Cordeil, Maxime; Dwyer, Tim; Marriott, Kim, Univ Melbourne, Melbourne, Vic, Australia.", "countries": "USA;Australia", "abstract": "Immersive analytics turns the very space surrounding the user into a canvas for data analysis, supporting human cognitive abilities in myriad ways. We present the results of a design study, contextual inquiry, and longitudinal evaluation involving professional economists using a Virtual Reality (VR) system for multidimensional visualization to explore actual economic data. Results from our preregistered evaluation highlight the varied use of space depending on context (exploration vs. presentation), the organization of space to support work, and the impact of immersion on navigation and orientation in the 3D analysis space.", "keywords": "Design study,evaluation,economic analysis,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934803", "refList": ["10.1145/3139131.3139141", "10.1109/2.60882", "10.1007/978-3-030-01388-2\\_1", "10.1109/38.250911", "10.1145/302979.303166", "10.1073/pnas.1306779110", "10.1109/tvcg.2012.219", "10.1162/105474698565686", "10.1016/0364-0213(94)90007-8", "10.1145/642611.642650", "10.1162/pres.1992.1.4.482", "10.1007/978-3-030-01388-2", "10.1109/2.19829", "10.1111/1467-8284.00096", "10.1007/978-3-030-01388-2\\_7", "10.1109/hicss.2011.339", "10.1109/tvcg.2016.2599107", "10.1145/3173574.3173664", "10.1145/1008653.1008669", "10.1109/tvcg.2017.2743990", "10.1145/642611", "10.1145/3126594.3126613", "10.1007/978-94-007-6833-89", "10.1162/pres\\_a\\_00124", "10.1016/0004-3702(94)00017-u", "10.1109/tvcg.2014.20", "10.1145/1168149.1168158", "10.1109/visual.1995.480800", "10.1109/icsens.2015.7370446", "10.1207/s15516709cog1701\\_1", "10.1162/105474601300343603", "10.1109/tvcg.2016.2518135", "10.1109/tvcg.2012.213", "10.1016/s0097-8493(02)00113-9", "10.1162/pres.1994.3.2.130", "10.1109/tvcg.2018.2865191", "10.1109/visual.1990.146402", "10.1162/105474601300343612", "10.1145/2702123.2702406", "10.1162/pres.1992.1.1.120", "10.1145/3009939.3009955", "10.1109/glocom.2015.7417476", "10.1162/pres\\_a\\_00261"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030450", "title": "Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment", "year": "2020", "conferenceName": "InfoVis", "authors": "Benjamin Lee;Xiaoyun Hu;Maxime Cordeil;Arnaud Prouzeau;Bernhard Jenny;Tim Dwyer", "citationCount": "0", "affiliation": "Lee, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Lee, Benjamin; Hu, Xiaoyun; Cordeil, Maxime; Prouzeau, Arnaud; Jenny, Bernhard; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia.", "countries": "Australia", "abstract": "Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA: the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner's personal workspace.", "keywords": "Immersive analytics,collaboration,virtual reality,qualitative study,multivariate data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030450", "refList": ["10.1109/tvcg.2008.153", "10.1007/s10606-004-5062-8", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2019.2914677", "10.1016/0020-7373(91)90039-a", "10.1109/tvcg.2011.287", "10.1117/12.2005484", "10.1109/mmul.2009.35", "10.1109/tvcg.2019.2934803", "10.1145/3359996.3364242", "10.1109/immersive.2016.7932384", "10.1016/j.future.2008.07.015", "10.1057/palgrave.ivs.9500167", "10.1109/mcg.2019.2898941", "10.1145/3343055.3360746", "10.1007/978-3-319-45853-3\\_8", "10.1145/2576099", "10.1145/2858036.2858039", "10.1109/tvcg.2016.2599107", "10.1145/3173574.3173664", "10.1007/978-3-030-01388-2\\_2", "10.1109/vr.2019.8797978", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1007/978-3-030-01388-2\\_8", "10.1023/a:1021271517844", "10.1109/bigdata.2014.7004282", "10.1109/tvcg.2019.2934395", "10.1109/ismar.2010.5643530", "10.1007/978-3-030-01388-22", "10.1145/2133806.2133821", "10.1109/3dvis.2014.7160093", "10.1145/2556288.2557058", "10.1109/vr.2019.8797845"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030392", "title": "ShuttleSpace: Exploring and Analyzing Movement Trajectory in Immersive Visualization", "year": "2020", "conferenceName": "InfoVis", "authors": "Shuainan Ye;Zhutian Chen;Xiangtong Chu;Yifan Wang;Siwei Fu;Lejun Shen;Kun Zhou;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD CG, Hangzhou, Peoples R China. Ye, Shuainan; Chu, Xiangtong; Wang, Yifan; Zhou, Kun; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD CG, Hangzhou, Peoples R China. Chen, Zhutian, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Fu, Siwei, Zhejiang Lab, Hangzhou, Zhejiang, Peoples R China. Shen, Lejun, Chengdu Sports Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China", "abstract": "We present ShuttleSpace, an immersive analytics system to assist experts in analyzing trajectory data in badminton. Trajectories in sports, such as the movement of players and balls, contain rich information on player behavior and thus have been widely analyzed by coaches and analysts to improve the players' performance. However, existing visual analytics systems often present the trajectories in court diagrams that are abstractions of reality, thereby causing difficulty for the experts to imagine the situation on the court and understand why the player acted in a certain way. With recent developments in immersive technologies, such as virtual reality (VR), experts gradually have the opportunity to see, feel, explore, and understand these 3D trajectories from the player's perspective. Yet, few research has studied how to support immersive analysis of sports data from such a perspective. Specific challenges are rooted in data presentation (e.g., how to seamlessly combine 2D and 3D visualizations) and interaction (e.g., how to naturally interact with data without keyboard and mouse) in VR. To address these challenges, we have worked closely with domain experts who have worked for a top national badminton team to design ShuttleSpace. Our system leverages 1) the peripheral vision to combine the 2D and 3D visualizations and 2) the VR controller to support natural interactions via a stroke metaphor. We demonstrate the effectiveness of ShuttleSpace through three case studies conducted by the experts with useful insights. We further conduct interviews with the experts whose feedback confirms that our first-person immersive analytics system is suitable and useful for analyzing badminton data.", "keywords": "Movement trajectory,badminton analytics,virtual reality", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030392", "refList": ["10.1016/j.eswa.2012.01.204", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1142/s1005386719000178", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/978-981-10-3994-2\\_1", "10.1007/978-3-319-07812-0\\_12", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1016/j.eswa.2017.07.027", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 35}, {"doi": "10.1109/tvcg.2019.2934805", "title": "Interactive Structure-aware Blending of Diverse Edge Bundling Visualizations", "year": "2019", "conferenceName": "InfoVis", "authors": "Yunhai Wang;Mingliang Xue;Yanyan Wang;Xinyuan Yan;Baoquan Chen;Chi-Wing Fu;Christophe Hurter", "citationCount": "1", "affiliation": "Fu, CW (Corresponding Author), Chinese Univ Hong Kong, Hong Kong, Peoples R China. Fu, CW (Corresponding Author), SIAT, Guangdong Prov Key Lab CV \\& VR Tech, Guangzhou, Peoples R China. Wang, Yunhai; Xue, Mingliang; Wang, Yanyan; Yan, Xinyuan, Shandong Univ, Jinan, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China. Fu, Chi-Wing, SIAT, Guangdong Prov Key Lab CV \\& VR Tech, Guangzhou, Peoples R China. Hurter, Christophe, ENAC, Toulouse, France.", "countries": "China;France", "abstract": "Many edge bundling techniques (i.e., data simplification as a support for data visualization and decision making) exist but they are not directly applicable to any kind of dataset and their parameters are often too abstract and difficult to set up. As a result, this hinders the user ability to create efficient aggregated visualizations. To address these issues, we investigated a novel way of handling visual aggregation with a task-driven and user-centered approach. Given a graph, our approach produces a decluttered view as follows: first, the user investigates different edge bundling results and specifies areas, where certain edge bundling techniques would provide user-desired results. Second, our system then computes a smooth and structural preserving transition between these specified areas. Lastly, the user can further fine-tune the global visualization with a direct manipulation technique to remove the local ambiguity and to apply different visual deformations. In this paper, we provide details for our design rationale and implementation. Also, we show how our algorithm gives more suitable results compared to current edge bundling techniques, and in the end, we provide concrete instances of usages, where the algorithm combines various edge bundling results to support diverse data exploration and visualizations.", "keywords": "path visualization,trajectory visualization,edge bundles", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934805", "refList": ["10.1111/cgf.13213", "10.1145/2254556.2254652", "10.1111/cgf.12791", "10.1111/j.1467-8659.2011.01898.x", "10.1111/cgf.12871", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1177/1473871617738122", "10.1111/j.1467-8659.2009.01700.x", "10.2312/eurovisstar.20141172", "10.1109/tvcg.2011.233", "10.1016/0020-0190(89)90102-6", "10.2200/s00688ed1v01y201512vis006", "10.3390/informatics4030026", "10.1109/tvcg.2017.2745919", "10.1111/j.1467-8659.2009.01680.x", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2016.2515611", "10.1111/cgf.13176", "10.1109/tvcg.2006.147", "10.1145/2992154.2992168", "10.1109/tvcg.2016.2598958", "10.1109/tvcg.2010.269", "10.1002/spe.4380211102", "10.1109/pacificvis.2017.8031596", "10.1109/tvcg.2011.223", "10.1145/1168149.1168168", "10.1006/s1045-926x(02)00016-2", "10.2312/vissym/vissym04/221-230", "10.1111/j.1467-8659.2012.03079.x", "10.1016/j.jvlc.2017.09.004", "10.3390/e20090625", "10.1109/tvcg.2014.2346441", "10.1109/tvcg.2017.2744338", "10.1109/pacificvis.2017.8031594", "10.1109/tvcg.2011.104", "10.1007/978-3-319-06793-310", "10.1109/pacificvis.2011.5742389", "10.1111/j.1467-8659.2009.01449.x", "10.1371/journal.pone.0098679", "10.1109/pacificvis.2015.7156354", "10.7155/jgaa.00370", "10.1109/pacificvis.2013.6596147", "10.1109/tvcg.2006.156", "10.1111/j.1467-8659.2009.01450.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934657", "title": "OD Morphing: Balancing Simplicity with Faithfulness for OD Bundling", "year": "2019", "conferenceName": "VAST", "authors": "Yan Lyu;Xu Liu;Hanyi Chen;Arpan Mangal;Kai Liu;Chao Chen 0004;Brian Y. Lim", "citationCount": "0", "affiliation": "Lyu, Y (Corresponding Author), Natl Univ Singapore, Singapore, Singapore. Lyu, Yan; Lim, Brian, Natl Univ Singapore, Singapore, Singapore. Liu, Xu, Southeast Univ, Nanjing, Peoples R China. Chen, Hanyi, Zhejiang Univ, Hangzhou, Peoples R China. Mangal, Arpan, Indian Inst Technol, Delhi, India. Liu, Kai; Chen, Chao, Chongqing Univ, Chongqing, Peoples R China.", "countries": "India;China;Singapore", "abstract": "OD bundling is a promising method to identify key origin-destination (OD) patterns, but the bundling can mislead the interpretation of actual trajectories traveled. We present OD Morphing, an interactive OD bundling technique that improves geographical faithfulness to actual trajectories while preserving visual simplicity for OD patterns. OD Morphing iteratively identifies critical waypoints from the actual trajectory network with a min-cut algorithm and transitions OD bundles to pass through the identified waypoints with a smooth morphing method. Furthermore, we extend OD Morphing to support bundling at interaction speeds to enable users to interactively transition between degrees of faithfulness to aid sensemaking. We introduce metrics for faithfulness and simplicity to evaluate their trade-off achieved by OD morphed bundling. We demonstrate OD Morphing on real-world city-scale taxi trajectory and USA domestic planned flight datasets.", "keywords": "OD Visualization,Edge Bundling,Trajectory", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934657", "refList": ["10.1109/tvcg.2008.135", "10.1145/2833165.2833168", "10.1111/cgf.13213", "10.1109/tvcg.2018.2864503", "10.1109/tvcg.2010.44", "10.1145/1653771.1653820", "10.1016/j.comgeo.2015.10.005", "10.1109/tvcg.2014.2337333", "10.1016/j.sbspro.2014.12.218", "10.1109/tvcg.2016.2535234", "10.1109/pacificvis.2011.5742386", "10.1145/321694.321699", "10.1109/tvcg.2013.246", "10.1111/cgf.12881", "10.1111/j.1467-8659.2009.01700.x", "10.1109/tvcg.2015.2467771", "10.1109/tvcg.2011.233", "10.1109/infvis.2003.1249008", "10.1109/itsc.2011.6082850", "10.1111/j.1467-8659.2009.01680.x", "10.1109/tvcg.2014.2346271", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2016.2515611", "10.1016/j.physa.2006.12.003", "10.1147/sj.41.0025", "10.1007/978-3-642-00304-2\\_1", "10.1109/tvcg.2006.147", "10.1109/tvcg.2016.2598958", "10.1109/tpami.2004.60", "10.1109/tvcg.2011.223", "10.1109/tits.2018.2817282", "10.1111/j.1467-8659.2012.03079.x", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2017.2744338", "10.1109/pacificvis.2017.8031594", "10.1179/000870410x12658023467367", "10.1145/2030112.2030126", "10.1109/tvcg.2011.104", "10.1145/2133416.2146416", "10.1109/tvcg.2011.190", "10.1142/s0218195995000064", "10.1002/cnm.1630040603", "10.1109/pacificvis.2011.5742389", "10.1109/pacificvis.2015.7156354", "10.1111/cgf.12778", "10.1109/dsnw.2011.5958797", "10.1109/cvprw.2008.4563095", "10.1109/tvcg.2016.2598885", "10.1109/tvcg.2017.2744322", "10.1111/j.1467-8659.2009.01450.x", "10.1109/tvcg.2016.2598416", "10.1109/cce.2012.6315867"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13712", "year": "2019", "title": "Route-Aware Edge Bundling for Visualizing Origin-Destination Trails in Urban Traffic", "conferenceName": "EuroVis", "authors": "Wei Zeng;Q. Shen;Y. Jiang;A. Telea", "citationCount": "1", "affiliation": "Shen, Q (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.\nZeng, W., Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China.\nShen, Q.; Jiang, Y., Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.\nTelea, A., Univ Groningen, Groningen, Netherlands.", "countries": "China;Netherlands", "abstract": "Origin-destination (OD) trails describe movements across space. Typical visualizations thereof use either straight lines or plot the actual trajectories. To reduce clutter inherent to visualizing large OD datasets, bundling methods can be used. Yet, bundling OD trails in urban traffic data remains challenging. Two specific reasons hereof are the constraints implied by the underlying road network and the difficulty of finding good bundling settings. To cope with these issues, we propose a new approach called Route Aware Edge Bundling (RAEB). To handle road constraints, we first generate a hierarchical model of the road-and-trajectory data. Next, we derive optimal bundling parameters, including kernel size and number of iterations, for a user-selected level of detail of this model, thereby allowing users to explicitly trade off simplification vs accuracy. We demonstrate the added value of RAEB compared to state-of-the-art trail bundling methods on both synthetic and real-world traffic data for tasks that include the preservation of road network topology and the support of multiscale exploration.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13712", "refList": ["10.1109/tvcg.2008.135", "10.1145/2833165.2833168", "10.1111/cgf.13213", "10.1109/mcg.2011.88", "10.1109/tvcg.2013.226", "10.1109/tvcg.2010.44", "10.1145/1653771.1653820", "10.1109/tvcg.2015.2468111", "10.1109/tst.2013.6509098", "10.1111/j.1467-8659.2009.01440.x", "10.1111/cgf.12922", "10.1109/tvcg.2018.2816219", "10.1109/vast.2014.7042486", "10.1198/tas.2009.0033", "10.1111/cgf.12132", "10.1111/j.1467-8659.2009.01700.x", "10.1016/j.trc.2007.05.002", "10.1007/978-3-540-72680-7\\_22", "10.1109/tvcg.2011.233", "10.1109/tvcg.2014.2346271", "10.1109/tvcg.2016.2515611", "10.1109/infvis.2005.1532150", "10.1109/tvcg.2016.2598472", "10.1109/tvcg.2006.147", "10.1109/tvcg.2016.2598958", "10.1109/tits.2017.2683539", "10.1109/tits.2015.2436897", "10.1111/j.1467-8659.2012.03079.x", "10.1109/tvcg.2017.2744338", "10.1109/pacificvis.2017.8031594", "10.1179/000870410x12658023467367", "10.1057/palgrave.ivs.9500182", "10.1109/tvcg.2011.104", "10.1109/tvcg.2011.190", "10.1109/tvcg.2013.114", "10.1111/cgf.12778", "10.1109/tvcg.2015.2467112", "10.1111/cgf.12107", "10.1109/iv.2010.53", "10.1109/42.563664", "10.1109/tvcg.2017.2666146", "10.1145/2530531", "10.1111/j.1467-8659.2009.01450.x", "10.4028/www.scientific.net/kem.342-343.593", "10.1109/tvcg.2011.202", "10.1038/srep00612"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030410", "title": "Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Wei Zeng 0002;Chengqiao Lin;Juncong Lin;Jincheng Jiang;Jiazhi Xia;Cagatay Turkay;Wei Chen", "citationCount": "0", "affiliation": "Lin, JC (Corresponding Author), Xiamen Univ, Xiamen, Peoples R China. Zeng, Wei; Jiang, Jincheng, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Lin, Chengqiao; Lin, Juncong, Xiamen Univ, Xiamen, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Turkay, Cagatay, Univ Warwick, Coventry, W Midlands, England. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China.", "countries": "China;England", "abstract": "Deep learning methods are being increasingly used for urban traffic prediction where spatiotemporal traffic data is aggregated into sequentially organized matrices that are then fed into convolution-based residual neural networks. However, the widely known modifiable areal unit problem within such aggregation processes can lead to perturbations in the network inputs. This issue can significantly destabilize the feature embeddings and the predictions - rendering deep networks much less useful for the experts. This paper approaches this challenge by leveraging unit visualization techniques that enable the investigation of many-to-many relationships between dynamically varied multi-scalar aggregations of urban traffic data and neural network predictions. Through regular exchanges with a domain expert, we design and develop a visual analytics solution that integrates 1) a Bivariate Map equipped with an advanced bivariate colormap to simultaneously depict input traffic and prediction errors across space, 2) a Moran's I Scatterplot that provides local indicators of spatial association analysis, and 3) a Multi-scale Attribution View that arranges non-linear dot plots in a tree layout to promote model analysis and comparison across scales. We evaluate our approach through a series of case studies involving a real-world dataset of Shenzhen taxi trips, and through interviews with domain experts. We observe that geographical scale variations have important impact on prediction performances, and interactive visual exploration of dynamically varying inputs and outputs benefit experts in the development of deep traffic prediction models.", "keywords": "MAUP,traffic prediction,deep learning,model diagnostic,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030410", "refList": ["10.1038/srep26377", "10.1109/mcg.2011.88", "10.1080/13658816.2015.1119279", "10.1109/tvcg.2013.226", "10.1109/pacificvis.2011.5742387", "10.1038/s41467-017-01882-w", "10.1109/tvcg.2019.2934670", "10.1111/j.1467-8659.2009.01440.x", "10.1111/cgf.13712", "10.1016/j.compenvurbsys.2008.09.006", "10.1109/pacificvis.2014.50", "10.1109/tvcg.2018.2816219", "10.1109/tvcg.2016.2535234", "10.1109/tvcg.2014.2346893", "10.3390/ijgi8080344", "10.1109/tvcg.2013.246", "10.1007/s10940-005-9003-6", "10.1016/j.compenvurbsys.2008.05.001", "10.1007/s10661-019-7831-3", "10.1111/j.1538-4632.2007.00699.x", "10.1016/j.aap.2016.08.015", "10.1080/13658816.2018.1541177", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2011.181", "10.1137/090759069", "10.1109/pacificvis.2011.5742390", "10.1214/10-aos799", "10.1109/tits.2017.2683539", "10.1109/tits.2015.2436897", "10.3390/ijerph16071150", "10.1109/tvcg.2009.145", "10.1109/tvcg.2012.265", "10.1080/10106049.2017.1404140", "10.3390/ijgi8020063", "10.3390/info6020134", "10.1080/13658816.2014.955027", "10.1109/tits.2016.2639320", "10.2307/143141", "10.1109/tvcg.2016.2598432"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030469", "title": "Topology Density Map for Urban Data Visualization and Analysis", "year": "2020", "conferenceName": "VAST", "authors": "Zezheng Feng;Haotian Li;Wei Zeng 0004;Shuang-Hua Yang;Huamin Qu", "citationCount": "0", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Feng, Zezheng; Li, Haotian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yang, Shuang-Hua, Southern Univ Sci \\& Technol, Shenzhen, Peoples R China.", "countries": "China", "abstract": "Density map is an effective visualization technique for depicting the scalar field distribution in 2D space. Conventional methods for constructing density maps are mainly based on Euclidean distance, limiting their applicability in urban analysis that shall consider road network and urban traffic. In this work, we propose a new method named Topology Density Map, targeting for accurate and intuitive density maps in the context of urban environment. Based on the various constraints of road connections and traffic conditions, the method first constructs a directed acyclic graph (DAG) that propagates nonlinear scalar fields along 1D road networks. Next, the method extends the scalar fields to a 2D space by identifying key intersecting points in the DAG and calculating the scalar fields for every point, yielding a weighted Voronoi diagram like effect of space division. Two case studies demonstrate that the Topology Density Map supplies accurate information to users and provides an intuitive visualization for decision making. An interview with domain experts demonstrates the feasibility, usability, and effectiveness of our method.", "keywords": "Density map,network topology,urban data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030469", "refList": ["10.1109/vast.2009.5332584", "10.1109/tvcg.2013.193", "10.1080/03081060.2013.844903", "10.1109/tvcg.2018.2864503", "10.1145/2702123.2702419", "10.1109/tvcg.2019.2934670", "10.1109/tits.2015.2496783", "10.1177/1473871615581216", "10.3141/1617-02", "10.1145/2024156.2024169", "10.1111/cgf.13712", "10.1016/j.ejor.2007.02.005", "10.1109/tvcg.2014.2346893", "10.1007/11871842\\_29", "10.1109/vast.2010.5652478", "10.1016/j.visinf.2019.10.002", "10.1109/tvcg.2016.2616404", "10.1109/vl.1996.545307", "10.1145/2629592", "10.1155/2018/2696037", "10.1061/(asce)0733-947x(1998)124:4(368", "10.3141/1899-21", "10.1023/a:1026123329433", "10.1109/mcg.2010.79", "10.1057/palgrave.ivs.9500174", "10.1109/tcyb.2019.2963681", "10.1109/tvcg.2015.2467554", "10.1111/cgf.12114", "10.1145/2814575", "10.1016/j.jcps.2014.08.002", "10.1109/2945.981847", "10.1080/03052150210909", "10.1109/tciaig.2012.2186810", "10.1109/tits.2017.2683539", "10.1109/iv.2004.1320137", "10.1016/0377-2217(80)90126-5", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2015.2467196", "10.1145/3097983.3098056", "10.1007/s11432-018-9801-4", "10.1109/vast.2014.7042490", "10.1061/(asce)0733-947x(2006)132:2(122", "10.1016/j.tra.2008.03.011", "10.1109/tits.2014.2298892", "10.1016/j.trb.2005.12.003", "10.1007/bf01840357", "10.1109/vast.2011.6102454", "10.1109/tvcg.2013.145", "10.1007/bf02289588", "10.1109/pacificvis.2014.56", "10.1109/mcg.2018.053491730", "10.1109/tvcg.2009.111", "10.1057/palgrave.ivs.9500184", "10.1109/tvcg.2013.173", "10.1109/tvcg.2016.2598432", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 49}, {"doi": "10.1109/tvcg.2019.2934805", "title": "Interactive Structure-aware Blending of Diverse Edge Bundling Visualizations", "year": "2019", "conferenceName": "InfoVis", "authors": "Yunhai Wang;Mingliang Xue;Yanyan Wang;Xinyuan Yan;Baoquan Chen;Chi-Wing Fu;Christophe Hurter", "citationCount": "1", "affiliation": "Fu, CW (Corresponding Author), Chinese Univ Hong Kong, Hong Kong, Peoples R China. Fu, CW (Corresponding Author), SIAT, Guangdong Prov Key Lab CV \\& VR Tech, Guangzhou, Peoples R China. Wang, Yunhai; Xue, Mingliang; Wang, Yanyan; Yan, Xinyuan, Shandong Univ, Jinan, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China. Fu, Chi-Wing, SIAT, Guangdong Prov Key Lab CV \\& VR Tech, Guangzhou, Peoples R China. Hurter, Christophe, ENAC, Toulouse, France.", "countries": "China;France", "abstract": "Many edge bundling techniques (i.e., data simplification as a support for data visualization and decision making) exist but they are not directly applicable to any kind of dataset and their parameters are often too abstract and difficult to set up. As a result, this hinders the user ability to create efficient aggregated visualizations. To address these issues, we investigated a novel way of handling visual aggregation with a task-driven and user-centered approach. Given a graph, our approach produces a decluttered view as follows: first, the user investigates different edge bundling results and specifies areas, where certain edge bundling techniques would provide user-desired results. Second, our system then computes a smooth and structural preserving transition between these specified areas. Lastly, the user can further fine-tune the global visualization with a direct manipulation technique to remove the local ambiguity and to apply different visual deformations. In this paper, we provide details for our design rationale and implementation. Also, we show how our algorithm gives more suitable results compared to current edge bundling techniques, and in the end, we provide concrete instances of usages, where the algorithm combines various edge bundling results to support diverse data exploration and visualizations.", "keywords": "path visualization,trajectory visualization,edge bundles", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934805", "refList": ["10.1111/cgf.13213", "10.1145/2254556.2254652", "10.1111/cgf.12791", "10.1111/j.1467-8659.2011.01898.x", "10.1111/cgf.12871", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1177/1473871617738122", "10.1111/j.1467-8659.2009.01700.x", "10.2312/eurovisstar.20141172", "10.1109/tvcg.2011.233", "10.1016/0020-0190(89)90102-6", "10.2200/s00688ed1v01y201512vis006", "10.3390/informatics4030026", "10.1109/tvcg.2017.2745919", "10.1111/j.1467-8659.2009.01680.x", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2016.2515611", "10.1111/cgf.13176", "10.1109/tvcg.2006.147", "10.1145/2992154.2992168", "10.1109/tvcg.2016.2598958", "10.1109/tvcg.2010.269", "10.1002/spe.4380211102", "10.1109/pacificvis.2017.8031596", "10.1109/tvcg.2011.223", "10.1145/1168149.1168168", "10.1006/s1045-926x(02)00016-2", "10.2312/vissym/vissym04/221-230", "10.1111/j.1467-8659.2012.03079.x", "10.1016/j.jvlc.2017.09.004", "10.3390/e20090625", "10.1109/tvcg.2014.2346441", "10.1109/tvcg.2017.2744338", "10.1109/pacificvis.2017.8031594", "10.1109/tvcg.2011.104", "10.1007/978-3-319-06793-310", "10.1109/pacificvis.2011.5742389", "10.1111/j.1467-8659.2009.01449.x", "10.1371/journal.pone.0098679", "10.1109/pacificvis.2015.7156354", "10.7155/jgaa.00370", "10.1109/pacificvis.2013.6596147", "10.1109/tvcg.2006.156", "10.1111/j.1467-8659.2009.01450.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934657", "title": "OD Morphing: Balancing Simplicity with Faithfulness for OD Bundling", "year": "2019", "conferenceName": "VAST", "authors": "Yan Lyu;Xu Liu;Hanyi Chen;Arpan Mangal;Kai Liu;Chao Chen 0004;Brian Y. Lim", "citationCount": "0", "affiliation": "Lyu, Y (Corresponding Author), Natl Univ Singapore, Singapore, Singapore. Lyu, Yan; Lim, Brian, Natl Univ Singapore, Singapore, Singapore. Liu, Xu, Southeast Univ, Nanjing, Peoples R China. Chen, Hanyi, Zhejiang Univ, Hangzhou, Peoples R China. Mangal, Arpan, Indian Inst Technol, Delhi, India. Liu, Kai; Chen, Chao, Chongqing Univ, Chongqing, Peoples R China.", "countries": "India;China;Singapore", "abstract": "OD bundling is a promising method to identify key origin-destination (OD) patterns, but the bundling can mislead the interpretation of actual trajectories traveled. We present OD Morphing, an interactive OD bundling technique that improves geographical faithfulness to actual trajectories while preserving visual simplicity for OD patterns. OD Morphing iteratively identifies critical waypoints from the actual trajectory network with a min-cut algorithm and transitions OD bundles to pass through the identified waypoints with a smooth morphing method. Furthermore, we extend OD Morphing to support bundling at interaction speeds to enable users to interactively transition between degrees of faithfulness to aid sensemaking. We introduce metrics for faithfulness and simplicity to evaluate their trade-off achieved by OD morphed bundling. We demonstrate OD Morphing on real-world city-scale taxi trajectory and USA domestic planned flight datasets.", "keywords": "OD Visualization,Edge Bundling,Trajectory", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934657", "refList": ["10.1109/tvcg.2008.135", "10.1145/2833165.2833168", "10.1111/cgf.13213", "10.1109/tvcg.2018.2864503", "10.1109/tvcg.2010.44", "10.1145/1653771.1653820", "10.1016/j.comgeo.2015.10.005", "10.1109/tvcg.2014.2337333", "10.1016/j.sbspro.2014.12.218", "10.1109/tvcg.2016.2535234", "10.1109/pacificvis.2011.5742386", "10.1145/321694.321699", "10.1109/tvcg.2013.246", "10.1111/cgf.12881", "10.1111/j.1467-8659.2009.01700.x", "10.1109/tvcg.2015.2467771", "10.1109/tvcg.2011.233", "10.1109/infvis.2003.1249008", "10.1109/itsc.2011.6082850", "10.1111/j.1467-8659.2009.01680.x", "10.1109/tvcg.2014.2346271", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2016.2515611", "10.1016/j.physa.2006.12.003", "10.1147/sj.41.0025", "10.1007/978-3-642-00304-2\\_1", "10.1109/tvcg.2006.147", "10.1109/tvcg.2016.2598958", "10.1109/tpami.2004.60", "10.1109/tvcg.2011.223", "10.1109/tits.2018.2817282", "10.1111/j.1467-8659.2012.03079.x", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2017.2744338", "10.1109/pacificvis.2017.8031594", "10.1179/000870410x12658023467367", "10.1145/2030112.2030126", "10.1109/tvcg.2011.104", "10.1145/2133416.2146416", "10.1109/tvcg.2011.190", "10.1142/s0218195995000064", "10.1002/cnm.1630040603", "10.1109/pacificvis.2011.5742389", "10.1109/pacificvis.2015.7156354", "10.1111/cgf.12778", "10.1109/dsnw.2011.5958797", "10.1109/cvprw.2008.4563095", "10.1109/tvcg.2016.2598885", "10.1109/tvcg.2017.2744322", "10.1111/j.1467-8659.2009.01450.x", "10.1109/tvcg.2016.2598416", "10.1109/cce.2012.6315867"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13728", "year": "2019", "title": "The State of the Art in Visualizing Multivariate Networks", "conferenceName": "EuroVis", "authors": "Carolina Nobre;Miriah D. Meyer;Marc Streit;Alexander Lex", "citationCount": "5", "affiliation": "Nobre, C (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA.\nNobre, C.; Meyer, M.; Lex, A., Univ Utah, Salt Lake City, UT 84112 USA.\nStreit, M., Johannes Kepler Univ Linz, Linz, Austria.", "countries": "USA;Austria", "abstract": "Multivariate networks are made up of nodes and their relationships (links), but also data about those nodes and links as attributes. Most real-world networks are associated with several attributes, and many analysis tasks depend on analyzing both, relationships and attributes. Visualization of multivariate networks, however, is challenging, especially when both the topology of the network and the attributes need to be considered concurrently. In this state-of-the-art report, we analyze current practices and classify techniques along four axes: layouts, view operations, layout operations, and data operations. We also provide an analysis of tasks specific to multivariate networks and give recommendations for which technique to use in which scenario. Finally, we survey application areas and evaluation methodologies.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13728", "refList": ["10.2312/eurovisstar.20151110", "10.1111/cgf.12106", "10.1109/iv.2016.19", "10.1111/j.1467-8659.2011.02087.x", "10.1111/j.1467-8659.2008.01214.x", "10.1109/tvcg.2014.2346893", "10.1145/1124772.1124891", "10.1109/vast.2014.7042484", "10.1109/tvcg.2018.2865149", "10.1080/10447318.2010.516722", "10.1109/icsmc.2011.6084125", "10.1109/mcg.2011.103", "10.1117/12.378894", "10.1145/2207676.2208293", "10.1109/tvcg.2009.122", "10.1111/cgf.12935", "10.1016/s0020-0255(02)00191-3", "10.1038/nmeth.1436", "10.1109/tvcg.2011.187", "10.1109/32.177365", "10.1007/978-3-642-03658-3\\_47", "10.1109/tvcg.2006.147", "10.1109/pacificvis.2011.5742390", "10.1145/2470654.2466444", "10.1109/tvcg.2006.166", "10.1145/2470654.2470724", "10.1109/tvcg.2014.2346441", "10.1109/vizsec.2005.1532070", "10.1101/gr.092759.109", "10.1111/cgf.13184", "10.1109/tvcg.2018.2865940", "10.1016/j.scico.2012.05.002", "10.1109/biovis.2012.6378600", "10.1111/cgf.13213", "10.1109/noms.2006.1687547", "10.1109/tvcg.2008.141", "10.1007/978-3-540-78243-8\\_13", "10.1145/1029208.1029217", "10.1145/345513.345271", "10.1109/tvcg.2015.2467811", "10.1109/visual.1991.175815", "10.1186/1471-2105-10-375", "10.1007/978-3-319-06793-3\\_1", "10.1186/1471-2105-13-275", "10.1109/tvcg.2010.79", "10.1109/tvcg.2011.217", "10.1186/1471-2105-14-s19-s3", "10.3389/fmicb.2017.00010", "10.1109/infvis.2003.1249009", "10.1111/cgf.13187", "10.1111/j.1467-8659.2008.01231.x", "10.1109/tvcg.2011.144", "10.1145/2556288.2557010", "10.1111/j.1467-8659.2009.01710.x", "10.1109/infvis.2004.46", "10.1109/iv.2009.97", "10.1109/tvcg.2008.34", "10.1109/infvis.2003.1249011", "10.1109/iv.2016.41", "10.1109/pacificvis.2010.5429609", "10.1109/tvcg.2006.160", "10.1109/tvcg.2010.205", "10.1109/38.486685", "10.1109/tvcg.2006.106", "10.1057/palgrave.ivs.9500092", "10.1145/568522.568523", "10.1111/j.1467-8659.2008.01221.x", "10.1109/iv.2010.15", "10.1145/22339.22342", "10.1109/csmr.2009.17", "10.1109/tvcg.2017.2744898", "10.1109/tvcg.2008.117", "10.1186/1752-0509-3-82", "10.1109/tvcg.2008.61", "10.1109/pacificvis.2013.6596127", "10.1109/tvcg.2007.70529", "10.1186/1471-2105-15-198", "10.1109/tvcg.2009.128", "10.2312/eurovisshort.20151124", "10.1093/bioinformatics/btq675", "10.1109/tvcg.2009.143", "10.1109/tvcg.2014.2346752", "10.1109/tvcg.2013.124", "10.1007/s00450-007-0036-y", "10.1093/bioinformatics/17.suppl\\_1.s22", "10.2312/eurovisstar.20151109", "10.1007/978-3-319-06793-3\\_5", "10.1109/vissof.2007.4290706", "10.1109/tvcg.2009.145", "10.1109/mcas.2003.1228503", "10.1109/infvis.2005.1532128", "10.1145/989863.989941", "10.1109/infvis.1999.801860", "10.1111/j.1467-8659.2009.01687.x", "10.1109/tvcg.2013.120", "10.1109/tvcg.2007.70582", "10.1109/tvcg.2009.167", "10.1109/pacificvis.2010.5429590", "10.1109/infvis.2000.885091", "10.1109/tvcg.2012.189", "10.1109/tvcg.2009.108", "10.1109/tvcg.2013.154", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2010.159", "10.1016/s0306-4573(98)00024-7", "10.1109/tvcg.2013.223", "10.1002/sam.10071", "10.2312/vissym/eurovis07/083-090", "10.1109/infvis.2002.1173156", "10.1186/1471-2105-7-109", "10.1177/1473871612455983", "10.1073/pnas.95.25.14863", "10.1007/978-3-319-06793-3\\_2", "10.1111/cgf.12883", "10.1007/978-3-540-78243-8\\_9", "10.1057/palgrave.ivs.9500180", "10.1109/pacificvis.2012.6183556", "10.1117/12.872578", "10.1145/1168149.1168169", "10.1109/iv.2013.3", "10.1109/tvcg.2011.186", "10.2307/2685881", "10.1109/pacificvis.2015.7156354", "10.1109/tvcg.2016.2598885", "10.1109/tvcg.2018.2811488", "10.1109/infvis.2005.1532129", "10.1111/j.1467-8659.2009.01450.x", "10.1057/palgrave.ivs.9500162", "10.1109/tvcg.2009.116", "10.1109/2945.468391"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3029413", "title": "A Design Space of Vision Science Methods for Visualization Research", "year": "2020", "conferenceName": "InfoVis", "authors": "Madison A. Elliott;Christine Nothelfer;Cindy Xiong;Danielle Albers Szafir", "citationCount": "0", "affiliation": "Elliott, MA (Corresponding Author), Univ British Columbia, Vancouver, BC, Canada. Elliott, Madison A., Univ British Columbia, Vancouver, BC, Canada. Nothelfer, Christine, Northwestern Univ, Evanston, IL 60208 USA. Xiong, Cindy, Univ Massachusetts, Amherst, MA 01003 USA. Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "Canada;USA", "abstract": "A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.", "keywords": "Perception,human vision,empirical research,evaluation,HCI", "link": "http://dx.doi.org/10.1109/TVCG.2020.3029413", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1177/0886109909354981", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1093/bioinformatics/btq110", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1177/1744987107081254", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1177/174498710501000305", "10.1017/s1049096513001789", "10.1109/tvcg.2012.213", "10.1093/nar/gkz239", "10.1093/sysbio/sys062", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030405", "title": "Insights From Experiments With Rigor in an EvoBio Design Study", "year": "2020", "conferenceName": "InfoVis", "authors": "Jennifer Rogers;Austin H. Patton;Luke Harmon;Alexander Lex;Miriah D. Meyer", "citationCount": "0", "affiliation": "Rogers, J (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Rogers, Jen; Lex, Alexander; Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Patton, Austin H., Washington State Univ, Pullman, WA 99164 USA. Harmon, Luke, Univ Idaho, Moscow, ID 83843 USA.", "countries": "USA", "abstract": "Design study is an established approach of conducting problem-driven visualization research. The academic visualization community has produced a large body of work for reporting on design studies, informed by a handful of theoretical frameworks, and applied to a broad range of application areas. The result is an abundance of reported insights into visualization design, with an emphasis on novel visualization techniques and systems as the primary contribution of these studies. In recent work we proposed a new, interpretivist perspective on design study and six companion criteria for rigor that highlight the opportunities for researchers to contribute knowledge that extends beyond visualization idioms and software. In this work we conducted a year-long collaboration with evolutionary biologists to develop an interactive tool for visual exploration of multivariate datasets and phylogenetic trees. During this design study we experimented with methods to support three of the rigor criteria: ABUNDANT, REFLEXIVE, and TRANSPARENT. As a result we contribute two novel visualization techniques for the analysis of multivariate phylogenetic datasets, three methodological recommendations for conducting design studies drawn from reflections over our process of experimentation, and two writing devices for reporting interpretivist design study. We offer this work as an example for implementing the rigor criteria to produce a diverse range of knowledge contributions.", "keywords": "Methodologies,Application Motivated Visualization,Guidelines,Life Sciences Visualization,Health,Medicine,Biology,Bioinformatics,Genomics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030405", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1080/17493460802276893", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1109/tvcg.2012.213", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13963", "year": "2020", "title": "MotionGlyphs: Visual Abstraction of Spatio-Temporal Networks in Collective Animal Behavior", "conferenceName": "EuroVis", "authors": "Eren Cakmak;Hanna Sch{\\\"{a}}fer;Juri Buchm{\\\"{u}}ller;Johannes Fuchs;Tobias Schreck;A. Jordan;Daniel A. Keim", "citationCount": "0", "affiliation": "Cakmak, E (Corresponding Author), Univ Konstanz, Constance, Germany.\nCakmak, E (Corresponding Author), Ctr Adv Study Collect Behav, Constance, Germany.\nCakmak, E.; Schaefer, H.; Buchmueller, J.; Fuchs, J.; Jordan, A.; Keim, D., Univ Konstanz, Constance, Germany.\nCakmak, E.; Jordan, A.; Keim, D., Ctr Adv Study Collect Behav, Constance, Germany.\nSchreck, T., Graz Univ Technol, Graz, Austria.\nJordan, A., Max Planck Inst Anim Behav, Radolfzell am Bodensee, Germany.", "countries": "Germany;Austria", "abstract": "Domain experts for collective animal behavior analyze relationships between single animal movers and groups of animals over time and space to detect emergent group properties. A common way to interpret this type of data is to visualize it as a spatio-temporal network. Collective behavior data sets are often large, and may hence result in dense and highly connected node-link diagrams, resulting in issues of node-overlap and edge clutter. In this design study, in an iterative design process, we developed glyphs as a design for seamlessly encoding relationships and movement characteristics of a single mover or clusters of movers. Based on these glyph designs, we developed a visual exploration prototype, MotionGlyphs, that supports domain experts in interactively filtering, clustering, and animating spatio-temporal networks for collective animal behavior analysis. By means of an expert evaluation, we show how MotionGlyphs supports important tasks and analysis goals of our domain experts, and we give evidence of the usefulness for analyzing spatio-temporal networks of collective animal behavior.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13963", "refList": ["10.1109/tvcg.2007.70582", "10.1111/cgf.13213", "10.1109/pacificvis.2014.13", "10.1145/2093973.2094038", "10.1111/j.1467-8659.2009.01664.x", "10.1109/tvcg.2010.44", "10.1111/cgf.12106", "10.1111/cgf.12791", "10.1080/15230406.2014.890071", "10.1111/tgis.12100", "10.1111/j.1467-8659.2009.01451.x", "10.1179/000870409x12525737905042", "10.1111/cgf.12923", "10.1006/ijhc.2002.1017", "10.1145/1124772.1124891", "10.1109/tvcg.2011.213", "10.1109/vast.2014.7042484", "10.1007/s12650-016-0375-5", "10.1109/vlhcc.2012.6344514", "10.1073/pnas.1420068112", "10.1006/ijhc.1017", "10.1007/s00371-017-1461-y", "10.1109/infvis.2003.1249008", "10.1111/cgf.13728", "10.2312/conf/eg2013/stars/039-063", "10.1109/tvcg.2014.2346271", "10.1109/hicss.2011.339", "10.1068/p3104", "10.1145/2931002.2931012", "10.1109/asonam.2012.39", "10.1179/000870403235002042", "10.1145/2556288.2557010", "10.1145/2470654.2466443", "10.1109/tvcg.2011.209", "10.1016/j.tree.2013.06.002", "10.1111/1365-2656.12418", "10.1111/cgf.12872", "10.1145/2470654.2466444", "10.1109/tvcg.2014.2322594", "10.1109/tvcg.2008.125", "10.1109/tvcg.2014.2346426", "10.1109/tvcg.2006.166", "10.1111/cgf.12615", "10.1057/palgrave.ivs.9500170", "10.1007/3-540-36151-0", "10.1117/12.872578", "10.1016/j.tics.2008.10.002", "10.1006/jtbi.2002.3065", "10.1109/tvcg.2010.78", "10.1109/iv.2013.3", "10.1073/pnas.1001763107", "10.1109/pacificvis.2015.7156354", "10.1371/journal.pbio.1001805", "10.5220/0005303801230130", "10.1111/j.1467-8659.2009.01687.x", "10.1007/s10844-011-0159-2", "10.1007/s12650-018-00543-4", "10.1145/2669557.2669572", "10.1111/cgf.13184", "10.1186/s40462-015-0032-y", "10.1016/j.ins.2016.06.048", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13987", "year": "2020", "title": "Augmenting Node-Link Diagrams with Topographic Attribute Maps", "conferenceName": "EuroVis", "authors": "Reinhold Preiner;Johanna Schmidt;Katharina Kr{\\\"{o}}sl;Tobias Schreck;Gabriel Mistelbauer", "citationCount": "0", "affiliation": "Preiner, R (Corresponding Author), Graz Univ Technol, Inst Comp Graph \\& Knowledge Visualizat, Graz, Austria.\nPreiner, R.; Schreck, T., Graz Univ Technol, Inst Comp Graph \\& Knowledge Visualizat, Graz, Austria.\nSchmidt, J.; Kroesl, K., Virtual Real \\& Visualisierung Forsch GmbH, VRVis Zentrum, Vienna, Austria.\nKroesl, K., TU Wien, Inst Visual Comp \\& Human Ctr Technol, Vienna, Austria.\nMistelbauer, G., Otto von Guericke Univ, Dept Simulat \\& Graph, Magdeburg, Germany.", "countries": "Germany;Austria", "abstract": "We propose a novel visualization technique for graphs that are attributed with scalar data. In many scenarios, these attributes (e.g., birth date in a family network) provide ambient context information for the graph structure, whose consideration is important for different visual graph analysis tasks. Graph attributes are usually conveyed using different visual representations (e.g., color, size, shape) or by reordering the graph structure according to the attribute domain (e.g., timelines). While visual encodings allow graphs to be arranged in a readable layout, assessing contextual information such as the relative similarities of attributes across the graph is often cumbersome. In contrast, attribute-based graph reordering serves the comparison task of attributes, but typically strongly impairs the readability of the structural information given by the graph's topology. In this work, we augment force-directed node-link diagrams with a continuous ambient representation of the attribute context. This way, we provide a consistent overview of the graph's topological structure as well as its attributes, supporting a wide range of graph-related analysis tasks. We resort to an intuitive height field metaphor, illustrated by a topographic map rendering using contour lines and suitable color maps. Contour lines visually connect nodes of similar attribute values, and depict their relative arrangement within the global context. Moreover, our contextual representation supports visualizing attribute value ranges associated with graph nodes (e.g., lifespans in a family network) as trajectories routed through this height field. We discuss how user interaction with both the structural and the contextual information fosters exploratory graph analysis tasks. The effectiveness and versatility of our technique is confirmed in a user study and case studies from various application domains.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13987", "refList": ["10.1109/tvcg.2013.269", "10.1109/pacificvis.2010.5429590", "10.1073/pnas.0307654100", "10.1145/2505515.2505758", "10.1559/152304082783948286", "10.1109/pacificvis.2014.47", "10.1093/bioinformatics/btp432", "10.1111/j.1467-8659.2011.01898.x", "10.1111/cgf.12931", "10.1111/cgf.12880", "10.1109/tvcg.2014.2346422", "10.1111/j.1467-8659.2009.01706.x", "10.1109/tvcg.2016.2598795", "10.1111/cgf.12800", "10.1109/tvcg.2014.2315995", "10.1111/cgf.12656", "10.1111/cgf.13728", "10.1109/tvcg.2009.122", "10.1111/cgf.13211", "10.1109/tvcg.2007.70596", "10.1109/infvis.2002.1173152", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2003.1196007", "10.1109/infvis.2005.1532150", "10.1145/3243250.3243266", "10.1080/02693799008941549", "10.1371/journal.pone.0058779", "10.1109/infvis.1995.528686", "10.1111/cgf.12872", "10.1002/spe.4380211102", "10.1109/38.974518", "10.1145/3097983.3098130", "10.1002/aris.1440370106", "10.1145/1360612.1360691", "10.1109/mc.2016.145", "10.2307/3006914", "10.1111/j.1467-8659.2009.01683.x", "10.1145/1639714.1639784"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1111/cgf.13712", "year": "2019", "title": "Route-Aware Edge Bundling for Visualizing Origin-Destination Trails in Urban Traffic", "conferenceName": "EuroVis", "authors": "Wei Zeng;Q. Shen;Y. Jiang;A. Telea", "citationCount": "1", "affiliation": "Shen, Q (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.\nZeng, W., Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China.\nShen, Q.; Jiang, Y., Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.\nTelea, A., Univ Groningen, Groningen, Netherlands.", "countries": "China;Netherlands", "abstract": "Origin-destination (OD) trails describe movements across space. Typical visualizations thereof use either straight lines or plot the actual trajectories. To reduce clutter inherent to visualizing large OD datasets, bundling methods can be used. Yet, bundling OD trails in urban traffic data remains challenging. Two specific reasons hereof are the constraints implied by the underlying road network and the difficulty of finding good bundling settings. To cope with these issues, we propose a new approach called Route Aware Edge Bundling (RAEB). To handle road constraints, we first generate a hierarchical model of the road-and-trajectory data. Next, we derive optimal bundling parameters, including kernel size and number of iterations, for a user-selected level of detail of this model, thereby allowing users to explicitly trade off simplification vs accuracy. We demonstrate the added value of RAEB compared to state-of-the-art trail bundling methods on both synthetic and real-world traffic data for tasks that include the preservation of road network topology and the support of multiscale exploration.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13712", "refList": ["10.1109/tvcg.2008.135", "10.1145/2833165.2833168", "10.1111/cgf.13213", "10.1109/mcg.2011.88", "10.1109/tvcg.2013.226", "10.1109/tvcg.2010.44", "10.1145/1653771.1653820", "10.1109/tvcg.2015.2468111", "10.1109/tst.2013.6509098", "10.1111/j.1467-8659.2009.01440.x", "10.1111/cgf.12922", "10.1109/tvcg.2018.2816219", "10.1109/vast.2014.7042486", "10.1198/tas.2009.0033", "10.1111/cgf.12132", "10.1111/j.1467-8659.2009.01700.x", "10.1016/j.trc.2007.05.002", "10.1007/978-3-540-72680-7\\_22", "10.1109/tvcg.2011.233", "10.1109/tvcg.2014.2346271", "10.1109/tvcg.2016.2515611", "10.1109/infvis.2005.1532150", "10.1109/tvcg.2016.2598472", "10.1109/tvcg.2006.147", "10.1109/tvcg.2016.2598958", "10.1109/tits.2017.2683539", "10.1109/tits.2015.2436897", "10.1111/j.1467-8659.2012.03079.x", "10.1109/tvcg.2017.2744338", "10.1109/pacificvis.2017.8031594", "10.1179/000870410x12658023467367", "10.1057/palgrave.ivs.9500182", "10.1109/tvcg.2011.104", "10.1109/tvcg.2011.190", "10.1109/tvcg.2013.114", "10.1111/cgf.12778", "10.1109/tvcg.2015.2467112", "10.1111/cgf.12107", "10.1109/iv.2010.53", "10.1109/42.563664", "10.1109/tvcg.2017.2666146", "10.1145/2530531", "10.1111/j.1467-8659.2009.01450.x", "10.4028/www.scientific.net/kem.342-343.593", "10.1109/tvcg.2011.202", "10.1038/srep00612"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030410", "title": "Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Wei Zeng 0002;Chengqiao Lin;Juncong Lin;Jincheng Jiang;Jiazhi Xia;Cagatay Turkay;Wei Chen", "citationCount": "0", "affiliation": "Lin, JC (Corresponding Author), Xiamen Univ, Xiamen, Peoples R China. Zeng, Wei; Jiang, Jincheng, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Lin, Chengqiao; Lin, Juncong, Xiamen Univ, Xiamen, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Turkay, Cagatay, Univ Warwick, Coventry, W Midlands, England. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China.", "countries": "China;England", "abstract": "Deep learning methods are being increasingly used for urban traffic prediction where spatiotemporal traffic data is aggregated into sequentially organized matrices that are then fed into convolution-based residual neural networks. However, the widely known modifiable areal unit problem within such aggregation processes can lead to perturbations in the network inputs. This issue can significantly destabilize the feature embeddings and the predictions - rendering deep networks much less useful for the experts. This paper approaches this challenge by leveraging unit visualization techniques that enable the investigation of many-to-many relationships between dynamically varied multi-scalar aggregations of urban traffic data and neural network predictions. Through regular exchanges with a domain expert, we design and develop a visual analytics solution that integrates 1) a Bivariate Map equipped with an advanced bivariate colormap to simultaneously depict input traffic and prediction errors across space, 2) a Moran's I Scatterplot that provides local indicators of spatial association analysis, and 3) a Multi-scale Attribution View that arranges non-linear dot plots in a tree layout to promote model analysis and comparison across scales. We evaluate our approach through a series of case studies involving a real-world dataset of Shenzhen taxi trips, and through interviews with domain experts. We observe that geographical scale variations have important impact on prediction performances, and interactive visual exploration of dynamically varying inputs and outputs benefit experts in the development of deep traffic prediction models.", "keywords": "MAUP,traffic prediction,deep learning,model diagnostic,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030410", "refList": ["10.1038/srep26377", "10.1109/mcg.2011.88", "10.1080/13658816.2015.1119279", "10.1109/tvcg.2013.226", "10.1109/pacificvis.2011.5742387", "10.1038/s41467-017-01882-w", "10.1109/tvcg.2019.2934670", "10.1111/j.1467-8659.2009.01440.x", "10.1111/cgf.13712", "10.1016/j.compenvurbsys.2008.09.006", "10.1109/pacificvis.2014.50", "10.1109/tvcg.2018.2816219", "10.1109/tvcg.2016.2535234", "10.1109/tvcg.2014.2346893", "10.3390/ijgi8080344", "10.1109/tvcg.2013.246", "10.1007/s10940-005-9003-6", "10.1016/j.compenvurbsys.2008.05.001", "10.1007/s10661-019-7831-3", "10.1111/j.1538-4632.2007.00699.x", "10.1016/j.aap.2016.08.015", "10.1080/13658816.2018.1541177", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2011.181", "10.1137/090759069", "10.1109/pacificvis.2011.5742390", "10.1214/10-aos799", "10.1109/tits.2017.2683539", "10.1109/tits.2015.2436897", "10.3390/ijerph16071150", "10.1109/tvcg.2009.145", "10.1109/tvcg.2012.265", "10.1080/10106049.2017.1404140", "10.3390/ijgi8020063", "10.3390/info6020134", "10.1080/13658816.2014.955027", "10.1109/tits.2016.2639320", "10.2307/143141", "10.1109/tvcg.2016.2598432"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030469", "title": "Topology Density Map for Urban Data Visualization and Analysis", "year": "2020", "conferenceName": "VAST", "authors": "Zezheng Feng;Haotian Li;Wei Zeng 0004;Shuang-Hua Yang;Huamin Qu", "citationCount": "0", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Feng, Zezheng; Li, Haotian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yang, Shuang-Hua, Southern Univ Sci \\& Technol, Shenzhen, Peoples R China.", "countries": "China", "abstract": "Density map is an effective visualization technique for depicting the scalar field distribution in 2D space. Conventional methods for constructing density maps are mainly based on Euclidean distance, limiting their applicability in urban analysis that shall consider road network and urban traffic. In this work, we propose a new method named Topology Density Map, targeting for accurate and intuitive density maps in the context of urban environment. Based on the various constraints of road connections and traffic conditions, the method first constructs a directed acyclic graph (DAG) that propagates nonlinear scalar fields along 1D road networks. Next, the method extends the scalar fields to a 2D space by identifying key intersecting points in the DAG and calculating the scalar fields for every point, yielding a weighted Voronoi diagram like effect of space division. Two case studies demonstrate that the Topology Density Map supplies accurate information to users and provides an intuitive visualization for decision making. An interview with domain experts demonstrates the feasibility, usability, and effectiveness of our method.", "keywords": "Density map,network topology,urban data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030469", "refList": ["10.1109/vast.2009.5332584", "10.1109/tvcg.2013.193", "10.1080/03081060.2013.844903", "10.1109/tvcg.2018.2864503", "10.1145/2702123.2702419", "10.1109/tvcg.2019.2934670", "10.1109/tits.2015.2496783", "10.1177/1473871615581216", "10.3141/1617-02", "10.1145/2024156.2024169", "10.1111/cgf.13712", "10.1016/j.ejor.2007.02.005", "10.1109/tvcg.2014.2346893", "10.1007/11871842\\_29", "10.1109/vast.2010.5652478", "10.1016/j.visinf.2019.10.002", "10.1109/tvcg.2016.2616404", "10.1109/vl.1996.545307", "10.1145/2629592", "10.1155/2018/2696037", "10.1061/(asce)0733-947x(1998)124:4(368", "10.3141/1899-21", "10.1023/a:1026123329433", "10.1109/mcg.2010.79", "10.1057/palgrave.ivs.9500174", "10.1109/tcyb.2019.2963681", "10.1109/tvcg.2015.2467554", "10.1111/cgf.12114", "10.1145/2814575", "10.1016/j.jcps.2014.08.002", "10.1109/2945.981847", "10.1080/03052150210909", "10.1109/tciaig.2012.2186810", "10.1109/tits.2017.2683539", "10.1109/iv.2004.1320137", "10.1016/0377-2217(80)90126-5", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2015.2467196", "10.1145/3097983.3098056", "10.1007/s11432-018-9801-4", "10.1109/vast.2014.7042490", "10.1061/(asce)0733-947x(2006)132:2(122", "10.1016/j.tra.2008.03.011", "10.1109/tits.2014.2298892", "10.1016/j.trb.2005.12.003", "10.1007/bf01840357", "10.1109/vast.2011.6102454", "10.1109/tvcg.2013.145", "10.1007/bf02289588", "10.1109/pacificvis.2014.56", "10.1109/mcg.2018.053491730", "10.1109/tvcg.2009.111", "10.1057/palgrave.ivs.9500184", "10.1109/tvcg.2013.173", "10.1109/tvcg.2016.2598432", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13963", "year": "2020", "title": "MotionGlyphs: Visual Abstraction of Spatio-Temporal Networks in Collective Animal Behavior", "conferenceName": "EuroVis", "authors": "Eren Cakmak;Hanna Sch{\\\"{a}}fer;Juri Buchm{\\\"{u}}ller;Johannes Fuchs;Tobias Schreck;A. Jordan;Daniel A. Keim", "citationCount": "0", "affiliation": "Cakmak, E (Corresponding Author), Univ Konstanz, Constance, Germany.\nCakmak, E (Corresponding Author), Ctr Adv Study Collect Behav, Constance, Germany.\nCakmak, E.; Schaefer, H.; Buchmueller, J.; Fuchs, J.; Jordan, A.; Keim, D., Univ Konstanz, Constance, Germany.\nCakmak, E.; Jordan, A.; Keim, D., Ctr Adv Study Collect Behav, Constance, Germany.\nSchreck, T., Graz Univ Technol, Graz, Austria.\nJordan, A., Max Planck Inst Anim Behav, Radolfzell am Bodensee, Germany.", "countries": "Germany;Austria", "abstract": "Domain experts for collective animal behavior analyze relationships between single animal movers and groups of animals over time and space to detect emergent group properties. A common way to interpret this type of data is to visualize it as a spatio-temporal network. Collective behavior data sets are often large, and may hence result in dense and highly connected node-link diagrams, resulting in issues of node-overlap and edge clutter. In this design study, in an iterative design process, we developed glyphs as a design for seamlessly encoding relationships and movement characteristics of a single mover or clusters of movers. Based on these glyph designs, we developed a visual exploration prototype, MotionGlyphs, that supports domain experts in interactively filtering, clustering, and animating spatio-temporal networks for collective animal behavior analysis. By means of an expert evaluation, we show how MotionGlyphs supports important tasks and analysis goals of our domain experts, and we give evidence of the usefulness for analyzing spatio-temporal networks of collective animal behavior.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13963", "refList": ["10.1109/tvcg.2007.70582", "10.1111/cgf.13213", "10.1109/pacificvis.2014.13", "10.1145/2093973.2094038", "10.1111/j.1467-8659.2009.01664.x", "10.1109/tvcg.2010.44", "10.1111/cgf.12106", "10.1111/cgf.12791", "10.1080/15230406.2014.890071", "10.1111/tgis.12100", "10.1111/j.1467-8659.2009.01451.x", "10.1179/000870409x12525737905042", "10.1111/cgf.12923", "10.1006/ijhc.2002.1017", "10.1145/1124772.1124891", "10.1109/tvcg.2011.213", "10.1109/vast.2014.7042484", "10.1007/s12650-016-0375-5", "10.1109/vlhcc.2012.6344514", "10.1073/pnas.1420068112", "10.1006/ijhc.1017", "10.1007/s00371-017-1461-y", "10.1109/infvis.2003.1249008", "10.1111/cgf.13728", "10.2312/conf/eg2013/stars/039-063", "10.1109/tvcg.2014.2346271", "10.1109/hicss.2011.339", "10.1068/p3104", "10.1145/2931002.2931012", "10.1109/asonam.2012.39", "10.1179/000870403235002042", "10.1145/2556288.2557010", "10.1145/2470654.2466443", "10.1109/tvcg.2011.209", "10.1016/j.tree.2013.06.002", "10.1111/1365-2656.12418", "10.1111/cgf.12872", "10.1145/2470654.2466444", "10.1109/tvcg.2014.2322594", "10.1109/tvcg.2008.125", "10.1109/tvcg.2014.2346426", "10.1109/tvcg.2006.166", "10.1111/cgf.12615", "10.1057/palgrave.ivs.9500170", "10.1007/3-540-36151-0", "10.1117/12.872578", "10.1016/j.tics.2008.10.002", "10.1006/jtbi.2002.3065", "10.1109/tvcg.2010.78", "10.1109/iv.2013.3", "10.1073/pnas.1001763107", "10.1109/pacificvis.2015.7156354", "10.1371/journal.pbio.1001805", "10.5220/0005303801230130", "10.1111/j.1467-8659.2009.01687.x", "10.1007/s10844-011-0159-2", "10.1007/s12650-018-00543-4", "10.1145/2669557.2669572", "10.1111/cgf.13184", "10.1186/s40462-015-0032-y", "10.1016/j.ins.2016.06.048", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [], "len": 1}], "len": 79}], "len": 291}, "index": 59, "embedding": [-1.9913235902786255, 3.032498836517334, -1.03950035572052, -2.4301393032073975, -0.6195418834686279, 0.16650904715061188, 6.196358680725098, 1.7274249792099, -0.09282401204109192, 6.292224407196045, -1.3509838581085205, 1.919103741645813, 0.9883162975311279, -1.5354515314102173, 7.215931415557861, 2.744638442993164, 1.0050190687179565, 9.175589561462402, -0.5175346732139587, -0.9793055653572083, -0.06630208343267441, 3.1449921131134033, 1.6921380758285522, 3.781575918197632, 0.25567910075187683, 7.435464382171631, 0.6628531813621521, 1.9590117931365967, 2.7893996238708496, 9.918045043945312, -0.9954630732536316, 4.092114448547363], "projection": [-0.12626034021377563, 11.496228218078613], "size": 146, "height": 6, "width": 53}