{"data": {"doi": "10.1111/cgf.12641", "year": "2015", "title": "Visualnostics: Visual Guidance Pictograms for Analyzing Projections of High-dimensional Data", "conferenceName": "EuroVis", "authors": "Dirk J. Lehmann;Fritz Kemmler;Tatsiana Zhyhalava;Marco Kirschke;Holger Theisel", "citationCount": "10", "affiliation": "Lehmann, DJ (Corresponding Author), Univ Magdeburg, Visual Comp Grp, D-39106 Magdeburg, Germany.\nLehmann, Dirk J.; Kemmler, Fritz; Zhyhalava, Tatsiana; Kirschke, Marco; Theisel, Holger, Univ Magdeburg, Visual Comp Grp, D-39106 Magdeburg, Germany.", "countries": "Germany", "abstract": "The visual analysis of multivariate projections is a challenging task, because complex visual structures occur. This causes fatigue or misinterpretations, which distorts the analysis. In fact, the same projection can lead to different analysis results. We provide visual guidance pictograms to improve objectivity of the visual search. A visual guidance pictogram is an iconic visual density map encoding the visual structure of certain data properties. By using them to guide the analysis, structures in the projection can be better understood and mentally linked to properties in the data. We introduce a systematic scheme for designing such pictograms and provide a set of pictograms for standard visual tasks, such as correlation and distribution analysis, for standard projections like scatterplots, RadVis, and Star Coordinates. We conduct a study that compares the visual analysis of real data with and without the support of guidance pictograms. Our tests show that the training effort for a visual search can be decreased and the analysis bias can be reduced by supporting the user's visual search with guidance pictograms.", "keywords": "", "link": "https://doi.org/10.1111/cgf.12641", "refList": ["10.1007/978-1-4419-6515-8\\_19", "10.1109/tvcg.2013.182", "10.1109/tvcg.2010.184", "10.1111/j.1467-8659.2012.03125.x", "10.1109/infvis.2005.1532142", "10.1109/vast.2010.5652433", "10.1111/j.1467-8659.2009.01467.x", "10.1109/vast.2009.5332628", "10.1109/visual.1997.663916", "10.1214/aoms/1177706645", "10.1145/272991.272995", "10.1109/tvcg.2011.200", "10.1109/tvcg.2011.229", "10.1111/bmsp.12006", "10.1109/vast.2011.6102437", "10.1177/1473871612439357", "10.1109/visual.1996.568118", "10.1007/bf01898350"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2016.2598467", "title": "Magnostics: Image-Based Search of Interesting Matrix Views for Guided Network Exploration", "year": "2016", "conferenceName": "VAST", "authors": "Michael Behrisch;Benjamin Bach;Michael Blumenschein;Michael Delz;Laura von R\u00fcden;Jean-Daniel Fekete;Tobias Schreck", "citationCount": "13", "affiliation": "Behrisch, M (Corresponding Author), Univ Konstanz, Constance, Germany. Behrisch, Michael; Hund, Michael; Delz, Michael, Univ Konstanz, Constance, Germany. Bach, Benjamin, Microsoft Res Inria Joint Ctr, Saclay, France. von Rueden, Laura, Capgemini, Paris, France. von Rueden, Laura, Rhein Westfal TH Aachen, Aachen, Germany. Fekete, Jean-Daniel, Inria, Saclay, France. Schreck, Tobias, Graz Univ Technol, Graz, Austria.", "countries": "Germany;France;Austria", "abstract": "In this work we address the problem of retrieving potentially interesting matrix views to support the exploration of networks. We introduce Matrix Diagnostics (or Magnostics), following in spirit related approaches for rating and ranking other visualization techniques, such as Scagnostics for scatter plots. Our approach ranks matrix views according to the appearance of specific visual patterns, such as blocks and lines, indicating the existence of topological motifs in the data, such as clusters, bi-graphs, or central nodes. Magnostics can be used to analyze, query, or search for visually similar matrices in large collections, or to assess the quality of matrix reordering algorithms. While many feature descriptors for image analyzes exist, there is no evidence how they perform for detecting patterns in matrices. In order to make an informed choice of feature descriptors for matrix diagnostics, we evaluate 30 feature descriptors-27 existing ones and three new descriptors that we designed specifically for MAGNOSTICS-with respect to four criteria: pattern response, pattern variability, pattern sensibility, and pattern discrimination. We conclude with an informed set of six descriptors as most appropriate for Magnostics and demonstrate their application in two scenarios; exploring a large collection of matrices and analyzing temporal networks.", "keywords": "Matrix Visualization;Visual Quality Measures;Quality Metrics;Feature Detection/Selection;Relational Data", "link": "http://dx.doi.org/10.1109/TVCG.2016.2598467", "refList": ["10.1109/tvcg.2007.70582", "10.1109/tsmc.1978.4309999", "10.2312/eurova.20141140", "10.1109/tvcg.2011.229", "10.5281/zenodo.31793", "10.1109/vast.2014.7042480", "10.1002/sam.10071", "10.1109/tvcg.2010.184", "10.1006/jvci.1999.0413", "10.1109/tpami.2009.154", "10.1145/2835238.2835242", "10.1109/vast.2012.6400488", "10.1111/cgf.12641", "10.1111/cgf.12935", "10.1109/tvcg.2007.70535", "10.1109/tpami.2008.275", "10.1016/s0019-9958(74)91038-9", "10.1109/tsmc.1973.4309314", "10.1016/s0165-0270(96)00080-5", "10.1109/tip.2002.801585", "10.1109/vast.2010.5652433", "10.1109/vast.2010.5652392", "10.1145/357744.357758", "10.1016/j.cviu.2007.09.014", "10.1109/tvcg.2010.242", "10.1145/1459359.1459577", "10.1109/icip.2001.959135", "10.1145/1998076.1998144", "10.1109/cvpr.1997.609412", "10.1109/inf0vis.2005.14", "10.1109/wiamis.2008.24", "10.1145/1282280.1282340", "10.1109/34.895972", "10.1109/tpami.2006.68"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2017.2743858", "title": "What Would a Graph Look Like in this Layout? A Machine Learning Approach to Large Graph Visualization", "year": "2017", "conferenceName": "InfoVis", "authors": "Oh-Hyun Kwon;Tarik Crnovrsanin;Kwan-Liu Ma", "citationCount": "13", "affiliation": "Kwon, OH (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Kwon, Oh-Hyun; Crnovrsanin, Tarik; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "Using different methods for laying out a graph can lead to very different visual appearances, with which the viewer perceives different information. Selecting a \u201cgood\u201d layout method is thus important for visualizing a graph. The selection can be highly subjective and dependent on the given task. A common approach to selecting a good layout is to use aesthetic criteria and visual inspection. However, fully calculating various layouts and their associated aesthetic metrics is computationally expensive. In this paper, we present a machine learning approach to large graph visualization based on computing the topological similarity of graphs using graph kernels. For a given graph, our approach can show what the graph would look like in different layouts and estimate their corresponding aesthetic metrics. An important contribution of our work is the development of a new framework to design graph kernels. Our experimental study shows that our estimation calculation is considerably faster than computing the actual layouts and their aesthetic metrics. Also, our graph kernels outperform the state-of-the-art ones in both time and accuracy. In addition, we conducted a user study to demonstrate that the topological similarity computed with our graph kernel matches perceptual similarity assessed by human users.", "keywords": "Graph visualization,graph layout,aesthetics,machine learning,graph kernel,graphlet", "link": "http://dx.doi.org/10.1109/TVCG.2017.2743858", "refList": ["10.1080/00018730601170527", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/2783258.2783417", "10.2307/2412323", "10.1109/tvcg.2007.70580", "10.1103/physreve.70.066111", "10.7155/jgaa.00405", "10.1111/j.1467-8659.2011.01898.x", "10.1109/infvis.2002.1173159", "10.1177/1473871612455749", "10.1007/3-540-58950-3", "10.1007/3-540-44541-2\\_17", "10.1109/tvcg.2008.158", "10.1145/2629564", "10.1145/264645.264657", "10.1023/a:1022627411411", "10.1109/tvcg.2015.2467451", "10.1109/2945.841119", "10.1109/72.485670", "10.1007/978-3-540-45167-9\\_11", "10.1016/0020-0190(89)90102-6", "10.1007/978-3-642-36763-2", "10.1145/1014052.1014072", "10.3402/qhw.v6i2.5918", "10.1007/978-3-642-36763-2\\_48", "10.1109/tvcg.2008.155", "10.1016/j.ins.2016.01.074", "10.1007/978-3-662-44043-8\\_3", "10.1093/bioinformatics/bth436", "10.13140/2.1.2393.1847", "10.1016/j.camwa.2004.08.015", "10.1109/tvcg.2010.269", "10.1109/tvcg.2017.2674999", "10.1006/s1045-926x(02)00016-2", "10.1016/0925-7721(94)00014-x", "10.1007/s10115-013-0673-3", "10.1109/tvcg.2007.46", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1016/s0378-8733(96)00299-7", "10.1109/tvcg.2016.2598467", "10.1023/b:stco.0000035301.49549.88", "10.1016/0167-8655(83)90033-8", "10.1093/bioinformatics/btl301", "10.1109/34.868688", "10.1098/rsif.2009.0192", "10.1145/1961189.1961199", "10.7155/jgaa.00051"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865139", "title": "Structure-Based Suggestive Exploration: A New Approach for Effective Exploration of Large Networks", "year": "2018", "conferenceName": "InfoVis", "authors": "Wei Chen;Fangzhou Guo;Dongming Han;Jacheng Pan;Xiaotao Nie;Jiazhi Xia;Xiaolong Zhang", "citationCount": "5", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Labo CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Chen, Wei; Guo, Fangzhou; Han, Dongming; Pan, Jacheng; Nie, Xiaotao, Zhejiang Univ, State Key Labo CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Xia, Jiazhi, Cent S Univ, Changsha, Hunan, Peoples R China. Zhang, Xiaolong, Penn State Univ, University Pk, PA 16802 USA.", "countries": "USA;China", "abstract": "When analyzing a visualized network, users need to explore different sections of the network to gain insight. However, effective exploration of large networks is often a challenge. While various tools are available for users to explore the global and local features of a network, these tools usually require significant interaction activities, such as repetitive navigation actions to follow network nodes and edges. In this paper, we propose a structure-based suggestive exploration approach to support effective exploration of large networks by suggesting appropriate structures upon user request. Encoding nodes with vectorized representations by transforming information of surrounding structures of nodes into a high dimensional space, our approach can identify similar structures within a large network, enable user interaction with multiple similar structures simultaneously, and guide the exploration of unexplored structures. We develop a web-based visual exploration system to incorporate this suggestive exploration approach and compare performances of our approach under different vectorizing methods and networks. We also present the usability and effectiveness of our approach through a controlled user study with two datasets.", "keywords": "Large Network Exploration,Structure-Based Exploration,Suggestive Exploration", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865139", "refList": ["10.1109/tvcg.2007.70582", "10.1109/tvcg.2006.120", "10.1109/mc.2013.242", "10.1109/tvcg.2009.108", "10.1145/2939672.2939754", "10.1016/j.comnet.2011.08.019", "10.1145/2702123.2702476", "10.1109/tvcg.2017.2744938", "10.1007/s00371-013-0892-3", "10.1137/1.9781611974973.67", "10.1109/vast.2009.5333893", "10.1109/tst.2013.6509098", "10.1109/35021bigcomp.2015.7072812", "10.1145/2909132.2909246", "10.1111/j.1467-8659.2011.01957.x", "10.1111/j.1467-8659.2011.01898.x", "10.1177/1473871612455749", "10.1109/tvcg.2013.167", "10.1145/3097983.3098061", "10.1007/978-1-4613-0303-9\\_28", "10.1109/vast.2014.7042485", "10.1109/tmm.2016.2614220", "10.1145/1376616.1376675", "10.1145/2623330.2623732", "10.14778/1920841.1920887", "10.1109/tvcg.2017.2745219", "10.1109/tvcg.2017.2743858", "10.1109/tvcg.2008.151", "10.1145/1150402.1150479", "10.1093/bioinformatics/bth436", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2016.2598958", "10.1111/cgf.12883", "10.1145/1556262.1556300", "10.1109/icdm.2012.159", "10.1145/2470654.2466444", "10.1109/tvcg.2013.109", "10.1109/infvis.2004.1", "10.1109/icdmw.2008.99", "10.1002/aris.1440370106", "10.1007/978-3-319-05813-9\\_11", "10.1371/journal.pone.0098679", "10.1109/tvcg.2006.106", "10.1111/j.1467-8659.2011.01935.x", "10.1111/cgf.12397", "10.1111/cgf.13184", "10.1111/cgf.12642", "10.1109/tvcg.2016.2598831", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934396", "title": "A Deep Generative Model for Graph Layout", "year": "2019", "conferenceName": "InfoVis", "authors": "Oh-Hyun Kwon;Kwan-Liu Ma", "citationCount": "4", "affiliation": "Kwon, OH (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Kwon, Oh-Hyun; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "Different layouts can characterize different aspects of the same graph. Finding a \u201cgood\u201d layout of a graph is thus an important task for graph visualization. In practice, users often visualize a graph in multiple layouts by using different methods and varying parameter settings until they find a layout that best suits the purpose of the visualization. However, this trial-and-error process is often haphazard and time-consuming. To provide users with an intuitive way to navigate the layout design space, we present a technique to systematically visualize a graph in diverse layouts using deep generative models. We design an encoder-decoder architecture to learn a model from a collection of example layouts, where the encoder represents training examples in a latent space and the decoder produces layouts from the latent space. In particular, we train the model to construct a two-dimensional latent space for users to easily explore and generate various layouts. We demonstrate our approach through quantitative and qualitative evaluations of the generated layouts. The results of our evaluations show that our model is capable of learning and generalizing abstract concepts of graph layouts, not just memorizing the training examples. In summary, this paper presents a fundamentally new approach to graph visualization where a machine learning model learns to visualize a graph from examples without manually-defined heuristics.", "keywords": "Graph,network,visualization,layout,machine learning,deep learning,neural network,generative model,autoencoder", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934396", "refList": ["10.1103/physreve.74.036104", "10.1145/234535.234538", "10.2307/2412323", "10.1109/tvcg.2007.70580", "10.7155/jgaa.00405", "10.1177/1473871612455749", "10.1109/tvcg.2011.185", "10.1073/pnas.122653799", "10.1007/3-540-58950-3", "10.1145/2897824.2925974", "10.1007/3-540-44541-2\\_17", "10.1007/bf00410640", "10.1021/acscentsci.7b00572", "10.1007/s10208-011-9093-5", "10.1109/tvcg.2015.2467451", "10.1016/0020-0190(89)90102-6", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13187", "10.1214/aoms/1177729586", "10.1109/tvcg.2014.2346277", "10.1109/tvcg.2017.2743858", "10.1007/978-3-662-44043-8\\_3", "10.1038/30918", "10.1109/mcg.2018.2881501", "10.1016/j.camwa.2004.08.015", "10.1109/tvcg.2010.269", "10.1006/s1045-926x(02)00016-2", "10.1016/0925-7721(94)00014-x", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1103/physrevx.4.011047", "10.1371/journal.pone.0098679", "10.1145/2487788.2488173", "10.1007/978-3-030-01418-6\\_41", "10.1007/978-3-030-04414-5\\_12", "10.1109/tvcg.2018.2865139", "10.1142/s0219525903001067", "10.7155/jgaa.00051"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030367", "title": "Lyra 2: Designing Interactive Visualizations by Demonstration", "year": "2020", "conferenceName": "InfoVis", "authors": "Jonathan Zong;Dhiraj Barnwal;Rupayan Neogy;Arvind Satyanarayan", "citationCount": "0", "affiliation": "Zong, J (Corresponding Author), MIT, Cambridge, MA 02139 USA. Zong, Jonathan; Neogy, Rupayan; Satyanarayan, Arvind, MIT, Cambridge, MA 02139 USA. Barnwal, Dhiraj, Indian Inst Technol Kharagpur, Kharagpur, W Bengal, India.", "countries": "India;USA", "abstract": "Recent graphical interfaces offer direct manipulation mechanisms for authoring visualizations, but are largely restricted to static output. To author interactive visualizations, users must instead turn to textual specification, but such approaches impose a higher technical burden. To bridge this gap, we introduce Lyra 2, a system that extends a prior visualization design environment with novel methods for authoring interaction techniques by demonstration. Users perform an interaction (e.g., button clicks, drags, or key presses) directly on the visualization they are editing. The system interprets this performance using a set of heuristics and enumerates suggestions of possible interaction designs. These heuristics account for the properties of the interaction (e.g., target and event type) as well as the visualization (e.g., mark and scale types, and multiple views). Interaction design suggestions are displayed as thumbnails; users can preview and test these suggestions, iteratively refine them through additional demonstrations, and finally apply and customize them via property inspectors. We evaluate our approach through a gallery of diverse examples, and evaluate its usability through a first-use study and via an analysis of its cognitive dimensions. We find that, in Lyra 2, interaction design by demonstration enables users to rapidly express a wide range of interactive visualizations.", "keywords": "Direct manipulation,interactive visualization,interaction design by demonstration", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030367", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030467", "title": "PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch;Lingyun Yu;Peiran Ren;Thomas Ertl;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Knittel, Johannes; Koch, Steffen; Ertl, Thomas, Univ Stuttgart, VIS VISUS, Stuttgart, Germany. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Dept Comp Sci \\& Software Engn, Suzhou, Peoples R China. Ren, Peiran, Alibaba Grp, Hangzhou, Peoples R China.", "countries": "Germany;China", "abstract": "Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.", "keywords": "Storyline visualization,reinforcement learning,mixed-initiative design", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030467", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030459", "title": "Scalability of Network Visualisation from a Cognitive Load Perspective", "year": "2020", "conferenceName": "InfoVis", "authors": "Vahan Yoghourdjian;Yalong Yang;Tim Dwyer;Lawrence Lee;Michael Wybrow;Kim Marriott", "citationCount": "0", "affiliation": "Yoghourdjian, V (Corresponding Author), Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Yoghourdjian, Vahan; Yang, Yalong; Dwyer, Tim; Wybrow, Michael; Marriott, Kim, Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Lawrence, Lee, Monash Univ, Fac Business \\& Econ, Melbourne, Vic, Australia. Yang, Yalong, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA.", "countries": "USA;Australia", "abstract": "Node-link diagrams are widely used to visualise networks. However, even the best network layout algorithms ultimately result in \u2018hairball\u2019 visualisations when the graph reaches a certain degree of complexity, requiring simplification through aggregation or interaction (such as filtering) to remain usable. Until now, there has been little data to indicate at what level of complexity node-link diagrams become ineffective or how visual complexity affects cognitive load. To this end, we conducted a controlled study to understand workload limits for a task that requires a detailed understanding of the network topology-finding the shortest path between two nodes. We tested performance on graphs with 25 to 175 nodes with varying density. We collected performance measures (accuracy and response time), subjective feedback, and physiological measures (EEG, pupil dilation, and heart rate variability). To the best of our knowledge this is the first network visualisation study to include physiological measures. Our results show that people have significant difficulty finding the shortest path in high density node-link diagrams with more than 50 nodes and even low density graphs with more than 100 nodes. From our collected EEG data we observe functional differences in brain activity between hard and easy tasks. We found that cognitive load increased up to certain level of difficulty after which it decreased, likely because participants had given up. We also explored the effects of global network layout features such as size or number of crossings, and features of the shortest path such as length or straightness on task difficulty. We found that global features generally had a greater impact than those of the shortest path.", "keywords": "Data Visualisation,Network Visualisation,Cognitive Load,EEG", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030459", "refList": ["10.1109/tvcg.2019.2934396", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2016.2570755", "10.1007/s00371-013-0892-3", "10.1007/b98835", "10.1109/isda.2014.7066252", "10.1109/tvcg.2015.2467251", "10.1177/1473871612455749", "10.1109/tvcg.2012.299", "10.1007/3-540-58950-3", "10.1111/cgf.12878", "10.1109/mcse.2007.55", "10.1109/tvcg.2012.238", "10.1145/264645.264657", "10.1109/tvcg.2015.2467451", "10.1109/tvcg.2013.151", "10.1016/0020-0190(89)90102-6", "10.1109/tvcg.2019.2934307", "10.1109/tvcg.2015.2468151", "10.1109/tvcg.2017.2745919", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13440", "10.1109/tvcg.2011.220", "10.1111/cgf.13187", "10.1109/t-c.1969.222678", "10.1109/tvcg.2017.2743858", "10.1126/science.290.5500.2319", "10.1109/cahpc.2018.8645912", "10.1109/tvcg.2015.2465151", "10.1109/tvcg.2016.2598958", "10.1109/tvcg.2017.2751473", "10.1002/spe.4380211102", "10.1109/tpds.2018.2869805", "10.1016/j.jpdc.2019.04.008", "10.1109/pacificvis.2017.8031574", "10.1006/s1045-926x(02)00016-2", "10.1109/tvcg.2017.2674999", "10.1145/2872427.2883041", "10.1145/3292500.3330989", "10.1109/tvcg.2017.2744878", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1109/sbac-pad.2018.00060", "10.1007/978-3-662-45803-7\\_27", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1111/j.1469-1809.1936.tb02137.x", "10.1007/bf02289565", "10.1109/tvcg.2017.2689016", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.4722", "year": "2020", "title": "A Study of Mental Maps in Immersive Network Visualization", "conferenceName": "PacificVis", "authors": "Joseph Kotlarek;Oh{-}Hyun Kwon;Kwan{-}Liu Ma;Peter Eades;Andreas Kerren;Karsten Klein;Falk Schreiber", "citationCount": "0", "affiliation": "Kotlarek, J (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA.\nKotlarek, Joseph; Kwon, Oh-Hyun; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.\nEades, Peter, Univ Sydney, Sydney, NSW, Australia.\nKerren, Andreas, Linnaeus Univ, Vaxjo, Sweden.\nKlein, Karsten; Schreiber, Falk, Univ Konstanz, Constance, Germany.", "countries": "Sweden;Germany;USA;Australia", "abstract": "The visualization of a network influences the quality of the mental map that the viewer develops to understand the network. In this study, we investigate the effects of a 3D immersive visualization environment compared to a traditional 2D desktop environment on the comprehension of a network's structure. We compare the two visualization environments using three tasks-interpreting network structure, memorizing a set of nodes, and identifying the structural changes-commonly used for evaluating the quality of a mental map in network visualization. The results show that participants were able to interpret network structure more accurately when viewing the network in an immersive environment, particularly for larger networks. However, we found that 2D visualizations performed better than immersive visualization for tasks that required spatial memory.", "keywords": "Human-centered computing; Visualization; Visualization techniques; Graph drawings; Human-centered computing; Visualization; Empirical studies in visualization", "link": "https://doi.org/10.1109/PacificVis48177.2020.4722", "refList": ["10.1103/physreve.74.036104", "10.1109/tvcg.2019.2934396", "10.1007/978-3-540-87730-1\\_9", "10.1117/12.2005484", "10.1177/1473871612455749", "10.1109/pacificvis.2017.8031577", "10.1007/978-3-319-73207-7", "10.1109/38.888006", "10.1109/2945.841119", "10.1109/mc.2005.297", "10.1007/s10055-018-0346-3", "10.1109/tvcg.2016.2599107", "10.1109/icsmc.1992.271688", "10.1038/30918", "10.1006/jvlc.1995.1010", "10.1089/109493101300117938", "10.1109/vrais.1998.658488", "10.1109/pacificvis.2015.7156357", "10.1109/tvcg.2010.78", "10.1109/tvcg.2016.2520921", "10.1007/978-3-030-01388-22", "10.1145/229459.229467", "10.1145/1056808.1056875", "10.1109/tvcg.2017.2744079", "10.1109/bdva.2015.7314293", "10.1086/jar.33.4.3629752", "10.1016/j.ijhcs.2013.08.004"], "wos": 1, "children": [], "len": 1}], "len": 11}, {"doi": "10.1109/tvcg.2020.3030440", "title": "Context-aware Sampling of Large Networks via Graph Representation Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Zhiguang Zhou;Chen Shi;Xilong Shen;Lihong Cai;Haoxuan Wang;Yuhua Liu;Ying Zhao;Wei Chen", "citationCount": "0", "affiliation": "Zhao, Y (Corresponding Author), Cent South Univ, Changsha, Peoples R China. Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Zhou, Zhiguang; Shi, Chen; Shen, Xilong; Cai, Lihong; Wang, Haoxuan; Liu, Yuhua, Zhejiang Univ Finance \\& Econ, Sch Informat, Hangzhou, Peoples R China. Zhao, Ying, Cent South Univ, Changsha, Peoples R China. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China.", "countries": "China", "abstract": "Numerous sampling strategies have been proposed to simplify large-scale networks for highly readable visualizations. It is of great challenge to preserve contextual structures formed by nodes and edges with tight relationships in a sampled graph, because they are easily overlooked during the process of sampling due to their irregular distribution and immunity to scale. In this paper, a new graph sampling method is proposed oriented to the preservation of contextual structures. We first utilize a graph representation learning (GRL) model to transform nodes into vectors so that the contextual structures in a network can be effectively extracted and organized. Then, we propose a multi-objective blue noise sampling model to select a subset of nodes in the vectorized space to preserve contextual structures with the retention of relative data and cluster densities in addition to those features of significance, such as bridging nodes and graph connections. We also design a set of visual interfaces enabling users to interactively conduct context-aware sampling, visually compare results with various sampling strategies, and deeply explore large networks. Case studies and quantitative comparisons based on real-world datasets have demonstrated the effectiveness of our method in the abstraction and exploration of large networks.", "keywords": "Graph sampling,Graph representation learning,Blue noise sampling,Graph evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030440", "refList": ["10.1145/2491159.2491168", "10.1016/j.physa.2015.04.035", "10.1145/1830252.1830274", "10.1109/icdmw.2007.91", "10.1002/net.21834", "10.1109/tvcg.2018.2864503", "10.1016/j.cag.2018.01.010", "10.1109/icc.2016.7511156", "10.1111/cgf.13444", "10.1145/956750.956831", "10.1145/364099.364331", "10.1007/s00180-016-0663-5", "10.1109/tvcg.2013.223", "10.1007/s12650-018-0530-2", "10.1103/physreve.73.016102", "10.1109/access.2018.2870684", "10.1007/978-3-319-06793-3\\_1", "10.2312/vissym/eurovis05/239-246", "10.1016/j.ins.2015.02.014", "10.1145/2339530.2339723", "10.1109/icde.2015.7113345", "10.1109/tvcg.2011.233", "10.14778/2809974.2809980", "10.1109/glocom.2015.7417471", "10.1145/2578153.2578175", "10.1214/aoms/1177705148", "10.1109/tvcg.2008.130", "10.14232/actacyb.20.1.2011.6", "10.1504/ijitm.2019.099809", "10.1109/tvcg.2018.2865020", "10.1145/956750", "10.1002/cpe.4330060203", "10.1145/1150402.1150479", "10.1103/physreve.72.036118", "10.1109/tvcg.2017.2744098", "10.1145/2020408.2020512", "10.1142/s0129183114400075", "10.1109/jsac.2011.111005", "10.1016/j.camwa.2011.11.057", "10.1145/2470654.2466444", "10.1109/tvcg.2017.2674999", "10.1214/aos/1013203451", "10.1109/icdcsw.2011.34", "10.1016/j.physa.2013.11.015", "10.1145/1081870.1081893", "10.1109/tnet.2008.2001730", "10.1109/access.2016.2633485", "10.1145/1879141.1879192", "10.1371/journal.pone.0098679", "10.1126/science.220.4598.671", "10.1109/pacificvis.2015.7156355", "10.1088/1475-7516/2011/08/011", "10.1007/978-3-319-27261-0\\_41", "10.1111/cgf.13410", "10.1109/tvcg.2018.2865139", "10.1109/tvcg.2016.2598831", "10.1016/j.physa.2014.06.065"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030447", "title": "DRGraph: An Efficient Graph Layout Algorithm for Large-scale Graphs by Dimensionality Reduction", "year": "2020", "conferenceName": "InfoVis", "authors": "Minfeng Zhu;Wei Chen;Yuanzhe Hu;Yuxuan Hou;Liangjun Liu;Kaiyuan Zhang", "citationCount": "0", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Zhu, Minfeng; Chen, Wei; Hu, Yuanzhe; Hou, Yuxuan; Liu, Liangjun; Zhang, Kaiyuan, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China.", "countries": "China", "abstract": "Efficient layout of large-scale graphs remains a challenging problem: the force-directed and dimensionality reduction-based methods suffer from high overhead for graph distance and gradient computation. In this paper, we present a new graph layout algorithm, called DRGraph, that enhances the nonlinear dimensionality reduction process with three schemes: approximating graph distances by means of a sparse distance matrix, estimating the gradient by using the negative sampling technique, and accelerating the optimization process through a multi-level layout scheme. DRGraph achieves a linear complexity for the computation and memory consumption, and scales up to large-scale graphs with millions of nodes. Experimental results and comparisons with state-of-the-art graph layout methods demonstrate that DRGraph can generate visually comparable layouts with a faster running time and a lower memory requirement.", "keywords": "graph visualization,graph layout,dimensionality reduction,force-directed layout", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030447", "refList": ["10.1109/tvcg.2007.70582", "10.1371/journal.pone.0136497", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/3018661.3018731", "10.1109/34.491619", "10.1145/2939672.2939754", "10.1145/3269206.3271788", "10.1016/j.comnet.2011.08.019", "10.1007/s11263-011-0442-2", "10.1016/s0020-0190(98)00108-2", "10.1109/tvcg.2018.2865151", "10.1007/978-3-319-61188-4\\_2", "10.1177/1473871612455749", "10.1177/1473871616666394", "10.1109/tvcg.2015.2467035", "10.1186/1471-2105-10-375", "10.1145/3097983.3098061", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1109/tvcg.2015.2467451", "10.1109/2945.841119", "10.1016/j.swevo.2015.10.002", "10.1016/0020-0190(89)90102-6", "10.1109/infvis.2003.1249009", "10.1109/tvcg.2017.2745919", "10.1111/cgf.13187", "10.1111/cgf.13440", "10.1109/tvcg.2012.245", "10.1109/tvcg.2017.2743858", "10.1177/1473871618821740", "10.1186/s12859-015-0585-1", "10.1002/nav.3800020109", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1006/s1045-926x(02)00016-2", "10.1109/cvpr.2012.6247667", "10.1023/b:jogo.0000042115.44455.f3", "10.1109/pacificvis.2017.8031607", "10.1002/nav.3800030404", "10.1109/cvpr.2008.4587500", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1090/s0002-9904-1920-03322-7", "10.1109/iv.2013.3", "10.1145/568522.568523", "10.1109/tvcg.2006.156", "10.1109/tvcg.2012.236", "10.1109/tvcg.2018.2865139", "10.1145/3219819.3220025", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030393", "title": "Exemplar-based Layout Fine-tuning for Node-link Diagrams", "year": "2020", "conferenceName": "InfoVis", "authors": "Jiacheng Pan;Wei Chen;Xiaodong Zhao;Shuyue Zhou;Wei Zeng;Minfeng Zhu;Jian Chen;Siwei Fu;Yingcai Wu", "citationCount": "1", "affiliation": "Chen, W; Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Lab, Hangzhou, Peoples R China. Pan, Jiacheng; Chen, Wei; Zhao, Xiaodong; Zhou, Shuyue; Zhu, Minfeng; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Fu, Siwei; Wu, Yingcai, Zhejiang Lab, Hangzhou, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Jian, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA;China", "abstract": "We design and evaluate a novel layout fine-tuning technique for node-link diagrams that facilitates exemplar-based adjustment of a group of substructures in batching mode. The key idea is to transfer user modifications on a local substructure to other substructures in the entire graph that are topologically similar to the exemplar. We first precompute a canonical representation for each substructure with node embedding techniques and then use it for on-the-fly substructure retrieval. We design and develop a light-weight interactive system to enable intuitive adjustment, modification transfer, and visual graph exploration. We also report some results of quantitative comparisons, three case studies, and a within-participant user study.", "keywords": "Node-link diagram,graph layout,graph visualization,user interactions", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030393", "refList": ["10.1109/tvcg.2007.70582", "10.1371/journal.pone.0136497", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/3018661.3018731", "10.1109/34.491619", "10.1145/2939672.2939754", "10.1145/3269206.3271788", "10.1016/j.comnet.2011.08.019", "10.1007/s11263-011-0442-2", "10.1016/s0020-0190(98)00108-2", "10.1109/tvcg.2018.2865151", "10.1177/1473871612455749", "10.1177/1473871616666394", "10.1109/tvcg.2015.2467035", "10.1186/1471-2105-10-375", "10.1145/3097983.3098061", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1109/tvcg.2015.2467451", "10.1016/j.swevo.2015.10.002", "10.1016/0020-0190(89)90102-6", "10.1109/infvis.2003.1249009", "10.1109/tvcg.2017.2745919", "10.1111/cgf.13187", "10.1111/cgf.13440", "10.1109/tvcg.2012.245", "10.1109/tvcg.2017.2743858", "10.1177/1473871618821740", "10.1186/s12859-015-0585-1", "10.1002/nav.3800020109", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1006/s1045-926x(02)00016-2", "10.1109/cvpr.2012.6247667", "10.1023/b:jogo.0000042115.44455.f3", "10.1109/pacificvis.2017.8031607", "10.1002/nav.3800030404", "10.1109/infvis.2004.1", "10.1109/cvpr.2008.4587500", "10.1109/pacificvis.2011.5742389", "10.1140/epjb/e2011-10979-2", "10.1371/journal.pone.0098679", "10.1090/s0002-9904-1920-03322-7", "10.1109/iv.2013.3", "10.1145/568522.568523", "10.1109/tvcg.2006.156", "10.1109/tvcg.2012.236", "10.1109/tvcg.2018.2865139", "10.1145/3219819.3220025", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}], "len": 19}, {"doi": "10.1109/tvcg.2019.2934396", "title": "A Deep Generative Model for Graph Layout", "year": "2019", "conferenceName": "InfoVis", "authors": "Oh-Hyun Kwon;Kwan-Liu Ma", "citationCount": "4", "affiliation": "Kwon, OH (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Kwon, Oh-Hyun; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "Different layouts can characterize different aspects of the same graph. Finding a \u201cgood\u201d layout of a graph is thus an important task for graph visualization. In practice, users often visualize a graph in multiple layouts by using different methods and varying parameter settings until they find a layout that best suits the purpose of the visualization. However, this trial-and-error process is often haphazard and time-consuming. To provide users with an intuitive way to navigate the layout design space, we present a technique to systematically visualize a graph in diverse layouts using deep generative models. We design an encoder-decoder architecture to learn a model from a collection of example layouts, where the encoder represents training examples in a latent space and the decoder produces layouts from the latent space. In particular, we train the model to construct a two-dimensional latent space for users to easily explore and generate various layouts. We demonstrate our approach through quantitative and qualitative evaluations of the generated layouts. The results of our evaluations show that our model is capable of learning and generalizing abstract concepts of graph layouts, not just memorizing the training examples. In summary, this paper presents a fundamentally new approach to graph visualization where a machine learning model learns to visualize a graph from examples without manually-defined heuristics.", "keywords": "Graph,network,visualization,layout,machine learning,deep learning,neural network,generative model,autoencoder", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934396", "refList": ["10.1103/physreve.74.036104", "10.1145/234535.234538", "10.2307/2412323", "10.1109/tvcg.2007.70580", "10.7155/jgaa.00405", "10.1177/1473871612455749", "10.1109/tvcg.2011.185", "10.1073/pnas.122653799", "10.1007/3-540-58950-3", "10.1145/2897824.2925974", "10.1007/3-540-44541-2\\_17", "10.1007/bf00410640", "10.1021/acscentsci.7b00572", "10.1007/s10208-011-9093-5", "10.1109/tvcg.2015.2467451", "10.1016/0020-0190(89)90102-6", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13187", "10.1214/aoms/1177729586", "10.1109/tvcg.2014.2346277", "10.1109/tvcg.2017.2743858", "10.1007/978-3-662-44043-8\\_3", "10.1038/30918", "10.1109/mcg.2018.2881501", "10.1016/j.camwa.2004.08.015", "10.1109/tvcg.2010.269", "10.1006/s1045-926x(02)00016-2", "10.1016/0925-7721(94)00014-x", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1103/physrevx.4.011047", "10.1371/journal.pone.0098679", "10.1145/2487788.2488173", "10.1007/978-3-030-01418-6\\_41", "10.1007/978-3-030-04414-5\\_12", "10.1109/tvcg.2018.2865139", "10.1142/s0219525903001067", "10.7155/jgaa.00051"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030367", "title": "Lyra 2: Designing Interactive Visualizations by Demonstration", "year": "2020", "conferenceName": "InfoVis", "authors": "Jonathan Zong;Dhiraj Barnwal;Rupayan Neogy;Arvind Satyanarayan", "citationCount": "0", "affiliation": "Zong, J (Corresponding Author), MIT, Cambridge, MA 02139 USA. Zong, Jonathan; Neogy, Rupayan; Satyanarayan, Arvind, MIT, Cambridge, MA 02139 USA. Barnwal, Dhiraj, Indian Inst Technol Kharagpur, Kharagpur, W Bengal, India.", "countries": "India;USA", "abstract": "Recent graphical interfaces offer direct manipulation mechanisms for authoring visualizations, but are largely restricted to static output. To author interactive visualizations, users must instead turn to textual specification, but such approaches impose a higher technical burden. To bridge this gap, we introduce Lyra 2, a system that extends a prior visualization design environment with novel methods for authoring interaction techniques by demonstration. Users perform an interaction (e.g., button clicks, drags, or key presses) directly on the visualization they are editing. The system interprets this performance using a set of heuristics and enumerates suggestions of possible interaction designs. These heuristics account for the properties of the interaction (e.g., target and event type) as well as the visualization (e.g., mark and scale types, and multiple views). Interaction design suggestions are displayed as thumbnails; users can preview and test these suggestions, iteratively refine them through additional demonstrations, and finally apply and customize them via property inspectors. We evaluate our approach through a gallery of diverse examples, and evaluate its usability through a first-use study and via an analysis of its cognitive dimensions. We find that, in Lyra 2, interaction design by demonstration enables users to rapidly express a wide range of interactive visualizations.", "keywords": "Direct manipulation,interactive visualization,interaction design by demonstration", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030367", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030467", "title": "PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch;Lingyun Yu;Peiran Ren;Thomas Ertl;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Knittel, Johannes; Koch, Steffen; Ertl, Thomas, Univ Stuttgart, VIS VISUS, Stuttgart, Germany. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Dept Comp Sci \\& Software Engn, Suzhou, Peoples R China. Ren, Peiran, Alibaba Grp, Hangzhou, Peoples R China.", "countries": "Germany;China", "abstract": "Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.", "keywords": "Storyline visualization,reinforcement learning,mixed-initiative design", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030467", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030459", "title": "Scalability of Network Visualisation from a Cognitive Load Perspective", "year": "2020", "conferenceName": "InfoVis", "authors": "Vahan Yoghourdjian;Yalong Yang;Tim Dwyer;Lawrence Lee;Michael Wybrow;Kim Marriott", "citationCount": "0", "affiliation": "Yoghourdjian, V (Corresponding Author), Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Yoghourdjian, Vahan; Yang, Yalong; Dwyer, Tim; Wybrow, Michael; Marriott, Kim, Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Lawrence, Lee, Monash Univ, Fac Business \\& Econ, Melbourne, Vic, Australia. Yang, Yalong, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA.", "countries": "USA;Australia", "abstract": "Node-link diagrams are widely used to visualise networks. However, even the best network layout algorithms ultimately result in \u2018hairball\u2019 visualisations when the graph reaches a certain degree of complexity, requiring simplification through aggregation or interaction (such as filtering) to remain usable. Until now, there has been little data to indicate at what level of complexity node-link diagrams become ineffective or how visual complexity affects cognitive load. To this end, we conducted a controlled study to understand workload limits for a task that requires a detailed understanding of the network topology-finding the shortest path between two nodes. We tested performance on graphs with 25 to 175 nodes with varying density. We collected performance measures (accuracy and response time), subjective feedback, and physiological measures (EEG, pupil dilation, and heart rate variability). To the best of our knowledge this is the first network visualisation study to include physiological measures. Our results show that people have significant difficulty finding the shortest path in high density node-link diagrams with more than 50 nodes and even low density graphs with more than 100 nodes. From our collected EEG data we observe functional differences in brain activity between hard and easy tasks. We found that cognitive load increased up to certain level of difficulty after which it decreased, likely because participants had given up. We also explored the effects of global network layout features such as size or number of crossings, and features of the shortest path such as length or straightness on task difficulty. We found that global features generally had a greater impact than those of the shortest path.", "keywords": "Data Visualisation,Network Visualisation,Cognitive Load,EEG", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030459", "refList": ["10.1109/tvcg.2019.2934396", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2016.2570755", "10.1007/s00371-013-0892-3", "10.1007/b98835", "10.1109/isda.2014.7066252", "10.1109/tvcg.2015.2467251", "10.1177/1473871612455749", "10.1109/tvcg.2012.299", "10.1007/3-540-58950-3", "10.1111/cgf.12878", "10.1109/mcse.2007.55", "10.1109/tvcg.2012.238", "10.1145/264645.264657", "10.1109/tvcg.2015.2467451", "10.1109/tvcg.2013.151", "10.1016/0020-0190(89)90102-6", "10.1109/tvcg.2019.2934307", "10.1109/tvcg.2015.2468151", "10.1109/tvcg.2017.2745919", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13440", "10.1109/tvcg.2011.220", "10.1111/cgf.13187", "10.1109/t-c.1969.222678", "10.1109/tvcg.2017.2743858", "10.1126/science.290.5500.2319", "10.1109/cahpc.2018.8645912", "10.1109/tvcg.2015.2465151", "10.1109/tvcg.2016.2598958", "10.1109/tvcg.2017.2751473", "10.1002/spe.4380211102", "10.1109/tpds.2018.2869805", "10.1016/j.jpdc.2019.04.008", "10.1109/pacificvis.2017.8031574", "10.1006/s1045-926x(02)00016-2", "10.1109/tvcg.2017.2674999", "10.1145/2872427.2883041", "10.1145/3292500.3330989", "10.1109/tvcg.2017.2744878", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1109/sbac-pad.2018.00060", "10.1007/978-3-662-45803-7\\_27", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1111/j.1469-1809.1936.tb02137.x", "10.1007/bf02289565", "10.1109/tvcg.2017.2689016", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.4722", "year": "2020", "title": "A Study of Mental Maps in Immersive Network Visualization", "conferenceName": "PacificVis", "authors": "Joseph Kotlarek;Oh{-}Hyun Kwon;Kwan{-}Liu Ma;Peter Eades;Andreas Kerren;Karsten Klein;Falk Schreiber", "citationCount": "0", "affiliation": "Kotlarek, J (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA.\nKotlarek, Joseph; Kwon, Oh-Hyun; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.\nEades, Peter, Univ Sydney, Sydney, NSW, Australia.\nKerren, Andreas, Linnaeus Univ, Vaxjo, Sweden.\nKlein, Karsten; Schreiber, Falk, Univ Konstanz, Constance, Germany.", "countries": "Sweden;Germany;USA;Australia", "abstract": "The visualization of a network influences the quality of the mental map that the viewer develops to understand the network. In this study, we investigate the effects of a 3D immersive visualization environment compared to a traditional 2D desktop environment on the comprehension of a network's structure. We compare the two visualization environments using three tasks-interpreting network structure, memorizing a set of nodes, and identifying the structural changes-commonly used for evaluating the quality of a mental map in network visualization. The results show that participants were able to interpret network structure more accurately when viewing the network in an immersive environment, particularly for larger networks. However, we found that 2D visualizations performed better than immersive visualization for tasks that required spatial memory.", "keywords": "Human-centered computing; Visualization; Visualization techniques; Graph drawings; Human-centered computing; Visualization; Empirical studies in visualization", "link": "https://doi.org/10.1109/PacificVis48177.2020.4722", "refList": ["10.1103/physreve.74.036104", "10.1109/tvcg.2019.2934396", "10.1007/978-3-540-87730-1\\_9", "10.1117/12.2005484", "10.1177/1473871612455749", "10.1109/pacificvis.2017.8031577", "10.1007/978-3-319-73207-7", "10.1109/38.888006", "10.1109/2945.841119", "10.1109/mc.2005.297", "10.1007/s10055-018-0346-3", "10.1109/tvcg.2016.2599107", "10.1109/icsmc.1992.271688", "10.1038/30918", "10.1006/jvlc.1995.1010", "10.1089/109493101300117938", "10.1109/vrais.1998.658488", "10.1109/pacificvis.2015.7156357", "10.1109/tvcg.2010.78", "10.1109/tvcg.2016.2520921", "10.1007/978-3-030-01388-22", "10.1145/229459.229467", "10.1145/1056808.1056875", "10.1109/tvcg.2017.2744079", "10.1109/bdva.2015.7314293", "10.1086/jar.33.4.3629752", "10.1016/j.ijhcs.2013.08.004"], "wos": 1, "children": [], "len": 1}], "len": 11}, {"doi": "10.1109/tvcg.2019.2934798", "title": "DeepDrawing: A Deep Learning Approach to Graph Drawing", "year": "2019", "conferenceName": "InfoVis", "authors": "Yong Wang;Zhihua Jin;Qianwen Wang;Weiwei Cui;Tengfei Ma;Huamin Qu", "citationCount": "4", "affiliation": "Wang, Y (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Yong; Jin, Zhihua; Wang, Qianwen; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Jin, Zhihua, Zhejiang Univ, Hangzhou, Peoples R China. Cui, Weiwei, Microsoft Res Asia, Beijing, Peoples R China. Ma, Tengfei, IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.", "countries": "USA;China", "abstract": "Node-link diagrams are widely used to facilitate network explorations. However, when using a graph drawing technique to visualize networks, users often need to tune different algorithm-specific parameters iteratively by comparing the corresponding drawing results in order to achieve a desired visual effect. This trial and error process is often tedious and time-consuming, especially for non-expert users. Inspired by the powerful data modelling and prediction capabilities of deep learning techniques, we explore the possibility of applying deep learning techniques to graph drawing. Specifically, we propose using a graph-LSTM-based approach to directly map network structures to graph drawings. Given a set of layout examples as the training dataset, we train the proposed graph-LSTM-based model to capture their layout characteristics. Then, the trained model is used to generate graph drawings in a similar style for new networks. We evaluated the proposed approach on two special types of layouts (i.e., grid layouts and star layouts) and two general types of layouts (i.e., ForceAtlas2 and PivotMDS) in both qualitative and quantitative ways. The results provide support for the effectiveness of our approach. We also conducted a time cost assessment on the drawings of small graphs with 20 to 50 nodes. We further report the lessons we learned and discuss the limitations and future work.", "keywords": "Graph Drawing,Deep Learning,LSTM,Procrustes Analysis", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934798", "refList": ["10.1057/ivs.2009.10", "10.1145/2939672.2939754", "10.1016/s0020-0190(98)00108-2", "10.5555/3157382.3157527", "10.1145/2939672.2939753", "10.1111/j.1467-8659.2011.01898.x", "10.1016/j.visinf.2018.12.006", "10.1177/1473871612455749", "10.1111/j.2517-6161.1991.tb01825.x", "10.1109/cvpr.2017.576", "10.1631/fitee.1700808", "10.1109/2945.841119", "10.1109/72.485670", "10.1007/3-540-58950-3\\_393", "10.1007/978-3-642-36763-2", "10.1162/089976600300015015", "10.1016/0020-0190(89)90102-6", "10.1145/2623330.2623732", "10.3402/qhw.v6i2.5918", "10.1007/978-3-642-36763-2\\_48", "10.1111/cgf.13187", "10.1038/nature14539", "10.1016/s0020-0255(02)00191-3", "10.1109/tvcg.2015.2467691", "10.1109/72.279181", "10.1109/tvcg.2017.2743858", "10.1007/978-3-662-44043-8\\_3", "10.1002/spe.4380211102", "10.1007/3-540-44541-2\\_18", "10.1162/neco.1997.9.8.1735", "10.1103/physreve.78.046110", "10.1006/s1045-926x(02)00016-2", "10.1109/pacificvis.2012.6183571", "10.1016/0925-7721(94)00014-x", "10.1109/infvis.2004.1", "10.1145/1830483.1830503", "10.1112/plms/s3-13.1.743", "10.1371/journal.pone.0098679", "10.1007/978-3-030-04414-5\\_12", "10.1147/jrd.2015.2411412"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030367", "title": "Lyra 2: Designing Interactive Visualizations by Demonstration", "year": "2020", "conferenceName": "InfoVis", "authors": "Jonathan Zong;Dhiraj Barnwal;Rupayan Neogy;Arvind Satyanarayan", "citationCount": "0", "affiliation": "Zong, J (Corresponding Author), MIT, Cambridge, MA 02139 USA. Zong, Jonathan; Neogy, Rupayan; Satyanarayan, Arvind, MIT, Cambridge, MA 02139 USA. Barnwal, Dhiraj, Indian Inst Technol Kharagpur, Kharagpur, W Bengal, India.", "countries": "India;USA", "abstract": "Recent graphical interfaces offer direct manipulation mechanisms for authoring visualizations, but are largely restricted to static output. To author interactive visualizations, users must instead turn to textual specification, but such approaches impose a higher technical burden. To bridge this gap, we introduce Lyra 2, a system that extends a prior visualization design environment with novel methods for authoring interaction techniques by demonstration. Users perform an interaction (e.g., button clicks, drags, or key presses) directly on the visualization they are editing. The system interprets this performance using a set of heuristics and enumerates suggestions of possible interaction designs. These heuristics account for the properties of the interaction (e.g., target and event type) as well as the visualization (e.g., mark and scale types, and multiple views). Interaction design suggestions are displayed as thumbnails; users can preview and test these suggestions, iteratively refine them through additional demonstrations, and finally apply and customize them via property inspectors. We evaluate our approach through a gallery of diverse examples, and evaluate its usability through a first-use study and via an analysis of its cognitive dimensions. We find that, in Lyra 2, interaction design by demonstration enables users to rapidly express a wide range of interactive visualizations.", "keywords": "Direct manipulation,interactive visualization,interaction design by demonstration", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030367", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030467", "title": "PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch;Lingyun Yu;Peiran Ren;Thomas Ertl;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Knittel, Johannes; Koch, Steffen; Ertl, Thomas, Univ Stuttgart, VIS VISUS, Stuttgart, Germany. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Dept Comp Sci \\& Software Engn, Suzhou, Peoples R China. Ren, Peiran, Alibaba Grp, Hangzhou, Peoples R China.", "countries": "Germany;China", "abstract": "Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.", "keywords": "Storyline visualization,reinforcement learning,mixed-initiative design", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030467", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2020.3030447", "title": "DRGraph: An Efficient Graph Layout Algorithm for Large-scale Graphs by Dimensionality Reduction", "year": "2020", "conferenceName": "InfoVis", "authors": "Minfeng Zhu;Wei Chen;Yuanzhe Hu;Yuxuan Hou;Liangjun Liu;Kaiyuan Zhang", "citationCount": "0", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Zhu, Minfeng; Chen, Wei; Hu, Yuanzhe; Hou, Yuxuan; Liu, Liangjun; Zhang, Kaiyuan, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China.", "countries": "China", "abstract": "Efficient layout of large-scale graphs remains a challenging problem: the force-directed and dimensionality reduction-based methods suffer from high overhead for graph distance and gradient computation. In this paper, we present a new graph layout algorithm, called DRGraph, that enhances the nonlinear dimensionality reduction process with three schemes: approximating graph distances by means of a sparse distance matrix, estimating the gradient by using the negative sampling technique, and accelerating the optimization process through a multi-level layout scheme. DRGraph achieves a linear complexity for the computation and memory consumption, and scales up to large-scale graphs with millions of nodes. Experimental results and comparisons with state-of-the-art graph layout methods demonstrate that DRGraph can generate visually comparable layouts with a faster running time and a lower memory requirement.", "keywords": "graph visualization,graph layout,dimensionality reduction,force-directed layout", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030447", "refList": ["10.1109/tvcg.2007.70582", "10.1371/journal.pone.0136497", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/3018661.3018731", "10.1109/34.491619", "10.1145/2939672.2939754", "10.1145/3269206.3271788", "10.1016/j.comnet.2011.08.019", "10.1007/s11263-011-0442-2", "10.1016/s0020-0190(98)00108-2", "10.1109/tvcg.2018.2865151", "10.1007/978-3-319-61188-4\\_2", "10.1177/1473871612455749", "10.1177/1473871616666394", "10.1109/tvcg.2015.2467035", "10.1186/1471-2105-10-375", "10.1145/3097983.3098061", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1109/tvcg.2015.2467451", "10.1109/2945.841119", "10.1016/j.swevo.2015.10.002", "10.1016/0020-0190(89)90102-6", "10.1109/infvis.2003.1249009", "10.1109/tvcg.2017.2745919", "10.1111/cgf.13187", "10.1111/cgf.13440", "10.1109/tvcg.2012.245", "10.1109/tvcg.2017.2743858", "10.1177/1473871618821740", "10.1186/s12859-015-0585-1", "10.1002/nav.3800020109", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1006/s1045-926x(02)00016-2", "10.1109/cvpr.2012.6247667", "10.1023/b:jogo.0000042115.44455.f3", "10.1109/pacificvis.2017.8031607", "10.1002/nav.3800030404", "10.1109/cvpr.2008.4587500", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1090/s0002-9904-1920-03322-7", "10.1109/iv.2013.3", "10.1145/568522.568523", "10.1109/tvcg.2006.156", "10.1109/tvcg.2012.236", "10.1109/tvcg.2018.2865139", "10.1145/3219819.3220025", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030393", "title": "Exemplar-based Layout Fine-tuning for Node-link Diagrams", "year": "2020", "conferenceName": "InfoVis", "authors": "Jiacheng Pan;Wei Chen;Xiaodong Zhao;Shuyue Zhou;Wei Zeng;Minfeng Zhu;Jian Chen;Siwei Fu;Yingcai Wu", "citationCount": "1", "affiliation": "Chen, W; Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Lab, Hangzhou, Peoples R China. Pan, Jiacheng; Chen, Wei; Zhao, Xiaodong; Zhou, Shuyue; Zhu, Minfeng; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Fu, Siwei; Wu, Yingcai, Zhejiang Lab, Hangzhou, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Jian, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA;China", "abstract": "We design and evaluate a novel layout fine-tuning technique for node-link diagrams that facilitates exemplar-based adjustment of a group of substructures in batching mode. The key idea is to transfer user modifications on a local substructure to other substructures in the entire graph that are topologically similar to the exemplar. We first precompute a canonical representation for each substructure with node embedding techniques and then use it for on-the-fly substructure retrieval. We design and develop a light-weight interactive system to enable intuitive adjustment, modification transfer, and visual graph exploration. We also report some results of quantitative comparisons, three case studies, and a within-participant user study.", "keywords": "Node-link diagram,graph layout,graph visualization,user interactions", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030393", "refList": ["10.1109/tvcg.2007.70582", "10.1371/journal.pone.0136497", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/3018661.3018731", "10.1109/34.491619", "10.1145/2939672.2939754", "10.1145/3269206.3271788", "10.1016/j.comnet.2011.08.019", "10.1007/s11263-011-0442-2", "10.1016/s0020-0190(98)00108-2", "10.1109/tvcg.2018.2865151", "10.1177/1473871612455749", "10.1177/1473871616666394", "10.1109/tvcg.2015.2467035", "10.1186/1471-2105-10-375", "10.1145/3097983.3098061", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1109/tvcg.2015.2467451", "10.1016/j.swevo.2015.10.002", "10.1016/0020-0190(89)90102-6", "10.1109/infvis.2003.1249009", "10.1109/tvcg.2017.2745919", "10.1111/cgf.13187", "10.1111/cgf.13440", "10.1109/tvcg.2012.245", "10.1109/tvcg.2017.2743858", "10.1177/1473871618821740", "10.1186/s12859-015-0585-1", "10.1002/nav.3800020109", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1006/s1045-926x(02)00016-2", "10.1109/cvpr.2012.6247667", "10.1023/b:jogo.0000042115.44455.f3", "10.1109/pacificvis.2017.8031607", "10.1002/nav.3800030404", "10.1109/infvis.2004.1", "10.1109/cvpr.2008.4587500", "10.1109/pacificvis.2011.5742389", "10.1140/epjb/e2011-10979-2", "10.1371/journal.pone.0098679", "10.1090/s0002-9904-1920-03322-7", "10.1109/iv.2013.3", "10.1145/568522.568523", "10.1109/tvcg.2006.156", "10.1109/tvcg.2012.236", "10.1109/tvcg.2018.2865139", "10.1145/3219819.3220025", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030459", "title": "Scalability of Network Visualisation from a Cognitive Load Perspective", "year": "2020", "conferenceName": "InfoVis", "authors": "Vahan Yoghourdjian;Yalong Yang;Tim Dwyer;Lawrence Lee;Michael Wybrow;Kim Marriott", "citationCount": "0", "affiliation": "Yoghourdjian, V (Corresponding Author), Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Yoghourdjian, Vahan; Yang, Yalong; Dwyer, Tim; Wybrow, Michael; Marriott, Kim, Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Lawrence, Lee, Monash Univ, Fac Business \\& Econ, Melbourne, Vic, Australia. Yang, Yalong, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA.", "countries": "USA;Australia", "abstract": "Node-link diagrams are widely used to visualise networks. However, even the best network layout algorithms ultimately result in \u2018hairball\u2019 visualisations when the graph reaches a certain degree of complexity, requiring simplification through aggregation or interaction (such as filtering) to remain usable. Until now, there has been little data to indicate at what level of complexity node-link diagrams become ineffective or how visual complexity affects cognitive load. To this end, we conducted a controlled study to understand workload limits for a task that requires a detailed understanding of the network topology-finding the shortest path between two nodes. We tested performance on graphs with 25 to 175 nodes with varying density. We collected performance measures (accuracy and response time), subjective feedback, and physiological measures (EEG, pupil dilation, and heart rate variability). To the best of our knowledge this is the first network visualisation study to include physiological measures. Our results show that people have significant difficulty finding the shortest path in high density node-link diagrams with more than 50 nodes and even low density graphs with more than 100 nodes. From our collected EEG data we observe functional differences in brain activity between hard and easy tasks. We found that cognitive load increased up to certain level of difficulty after which it decreased, likely because participants had given up. We also explored the effects of global network layout features such as size or number of crossings, and features of the shortest path such as length or straightness on task difficulty. We found that global features generally had a greater impact than those of the shortest path.", "keywords": "Data Visualisation,Network Visualisation,Cognitive Load,EEG", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030459", "refList": ["10.1109/tvcg.2019.2934396", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2016.2570755", "10.1007/s00371-013-0892-3", "10.1007/b98835", "10.1109/isda.2014.7066252", "10.1109/tvcg.2015.2467251", "10.1177/1473871612455749", "10.1109/tvcg.2012.299", "10.1007/3-540-58950-3", "10.1111/cgf.12878", "10.1109/mcse.2007.55", "10.1109/tvcg.2012.238", "10.1145/264645.264657", "10.1109/tvcg.2015.2467451", "10.1109/tvcg.2013.151", "10.1016/0020-0190(89)90102-6", "10.1109/tvcg.2019.2934307", "10.1109/tvcg.2015.2468151", "10.1109/tvcg.2017.2745919", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13440", "10.1109/tvcg.2011.220", "10.1111/cgf.13187", "10.1109/t-c.1969.222678", "10.1109/tvcg.2017.2743858", "10.1126/science.290.5500.2319", "10.1109/cahpc.2018.8645912", "10.1109/tvcg.2015.2465151", "10.1109/tvcg.2016.2598958", "10.1109/tvcg.2017.2751473", "10.1002/spe.4380211102", "10.1109/tpds.2018.2869805", "10.1016/j.jpdc.2019.04.008", "10.1109/pacificvis.2017.8031574", "10.1006/s1045-926x(02)00016-2", "10.1109/tvcg.2017.2674999", "10.1145/2872427.2883041", "10.1145/3292500.3330989", "10.1109/tvcg.2017.2744878", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1109/sbac-pad.2018.00060", "10.1007/978-3-662-45803-7\\_27", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1111/j.1469-1809.1936.tb02137.x", "10.1007/bf02289565", "10.1109/tvcg.2017.2689016", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.3756", "year": "2020", "title": "BatchLayout: A Batch-Parallel Force-Directed Graph Layout Algorithm in Shared Memory", "conferenceName": "PacificVis", "authors": "Md. Khaledur Rahman;Majedul Haque Sujon;Ariful Azad", "citationCount": "0", "affiliation": "Azad, A (Corresponding Author), Indiana Univ, Dept Intelligent Syst Engn, Bloomington, IN 47405 USA.\nRahman, Md Khaledur, Indiana Univ, Dept Comp Sci, Bloomington, IN 47405 USA.\nSujon, Majedul Haque; Azad, Ariful, Indiana Univ, Dept Intelligent Syst Engn, Bloomington, IN 47405 USA.", "countries": "USA", "abstract": "Force-directed algorithms are widely used to generate aesthetically-pleasing layouts of graphs or networks arisen in many scientific disciplines. To visualize large-scale graphs, several parallel algorithms have been discussed in the literature. However, existing parallel algorithms do not utilize memory hierarchy efficiently and often offer limited parallelism. This paper addresses these limitations with BatchLayout, an algorithm that groups vertices into minibatches and processes them in parallel. BatchLayout also employs cache blocking techniques to utilize memory hierarchy efficiently. More parallelism and improved memory accesses coupled with force approximating techniques, better initialization, and optimized learning rate make BatchLayout significantly faster than other state-of-the-art algorithms such as ForceAtlas2 and OpenOrd. The visualization quality of layouts from BatchLayout is comparable or better than similar visualization tools.", "keywords": "Human-centered computing; Visualization; Visualization techniques; Graph drawings; Human-centered computing; Visualization; Visualization systems and tools; Visualization toolkits", "link": "https://doi.org/10.1109/PacificVis48177.2020.3756", "refList": ["10.1101/gr.1239303", "10.1109/tvcg.2012.299", "10.1109/tpds.2014.2331243", "10.1145/169627.169640", "10.1038/324446a0", "10.1016/j.ins.2016.11.012", "10.1016/0020-0190(89)90102-6", "10.1109/tvcg.2018.2859997", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13187", "10.1109/icpp.2017.47", "10.1109/tvcg.2017.2743858", "10.1103/physreve.79.026102", "10.1002/spe.4380211102", "10.1109/tpds.2018.2869805", "10.1103/physreve.78.046110", "10.1006/s1045-926x(02)00016-2", "10.1117/12.871402", "10.1137/s154034590241370x", "10.1371/journal.pone.0098679", "10.1007/978-3-642-00219-9\\_21"], "wos": 1, "children": [], "len": 1}], "len": 51}, {"doi": "10.1109/tvcg.2017.2745978", "title": "HiPiler: Visual Exploration of Large Genome Interaction Matrices with Interactive Small Multiples", "year": "2017", "conferenceName": "InfoVis", "authors": "Fritz Lekschas;Benjamin Bach;Peter Kerpedjiev;Nils Gehlenborg;Hanspeter Pfister", "citationCount": "4", "affiliation": "Lekschas, F (Corresponding Author), Harvard Univ, Cambridge, MA 02138 USA. Lekschas, Fritz; Bach, Benjamin; Pfister, Hanspeter, Harvard Univ, Cambridge, MA 02138 USA. Kerpedjiev, Peter; Gehlenborg, Nils, Harvard Med Sch, Boston, MA USA.", "countries": "USA", "abstract": "This paper presents an interactive visualization interface-HiPiler-for the exploration and visualization of regions-of-interest in large genome interaction matrices. Genome interaction matrices approximate the physical distance of pairs of regions on the genome to each other and can contain up to 3 million rows and columns with many sparse regions. Regions of interest (ROIs) can be defined, e.g., by sets of adjacent rows and columns, or by specific visual patterns in the matrix. However, traditional matrix aggregation or pan-and-zoom interfaces fail in supporting search, inspection, and comparison of ROIs in such large matrices. In HiPiler, ROIs are first-class objects, represented as thumbnail-like \u201csnippets\u201d. Snippets can be interactively explored and grouped or laid out automatically in scatterplots, or through dimension reduction methods. Snippets are linked to the entire navigable genome interaction matrix through brushing and linking. The design of HiPiler is based on a series of semi-structured interviews with 10 domain experts involved in the analysis and interpretation of genome interaction matrices. We describe six exploration tasks that are crucial for analysis of interaction matrices and demonstrate how HiPiler supports these tasks. We report on a user study with a series of data exploration sessions with domain experts to assess the usability of HiPiler as well as to demonstrate respective findings in the data.", "keywords": "Interactive Small Multiples,Matrix Comparison,Biomedical Visualization,Genomics", "link": "http://dx.doi.org/10.1109/TVCG.2017.2745978", "refList": ["10.1109/tvcg.2007.70582", "10.1016/j.cels.2015.07.012", "10.1101/121889", "10.1109/vast.2009.5333893", "10.1128/mmbr.00006-15", "10.1016/j.cell.2015.04.004", "10.1145/345513.345304", "10.1186/1756-8935-7-25", "10.1111/j.1467-8659.2011.01898.x", "10.1126/science.1181369", "10.1073/pnas.1320308111", "10.1109/mcse.2007.55", "10.1186/s13059-017-1161-y", "10.1126/science.1067799", "10.1111/cgf.12925", "10.1093/bioinformatics/bti556", "10.1111/cgf.12935", "10.1145/2556288.2557010", "10.1016/j.cell.2014.11.021", "10.1007/s11665-016-2173-6", "10.1126/science.298.5594.824", "10.1145/2470654.2466444", "10.1016/s0925-2312(98)00030-7", "10.1126/science.aad9024", "10.1111/cgf.12615", "10.1016/j.cell.2013.04.010", "10.1038/nature11243", "10.1109/tvcg.2016.2598467", "10.1038/nrg3454", "10.1038/nature05916", "10.1093/bioinformatics/bts378", "10.1109/tvcg.2015.2467851", "10.1038/nrg.2016.4", "10.1109/tvcg.2012.208"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934300", "title": "GUIRO: User-Guided Matrix Reordering", "year": "2019", "conferenceName": "VAST", "authors": "Michael Behrisch;Tobias Schreck;Hanspeter Pfister", "citationCount": "1", "affiliation": "Behrisch, M (Corresponding Author), Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Behrisch, Michael; Pfister, Hanspeter, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Schreck, Tobias, Graz Univ Technol, Graz, Austria.", "countries": "USA;Austria", "abstract": "Matrix representations are one of the main established and empirically proven to be effective visualization techniques for relational (or network) data. However, matrices\u2014similar to node-link diagrams\u2014are most effective if their layout reveals the underlying data topology. Given the many developed algorithms, a practical problem arises: \u201cWhich matrix reordering algorithm should I choose for my dataset at hand?\u201d To make matters worse, different reordering algorithms applied to the same dataset may let significantly different visual matrix patterns emerge. This leads to the question of trustworthiness and explainability of these fully automated, often heuristic, black-box processes. We present GUIRO, a Visual Analytics system that helps novices, network analysts, and algorithm designers to open the black-box. Users can investigate the usefulness and expressiveness of 70 accessible matrix reordering algorithms. For network analysts, we introduce a novel model space representation and two interaction techniques for a user-guided reordering of rows or columns, and especially groups thereof (submatrix reordering). These novel techniques contribute to the understanding of the global and local dataset topology. We support algorithm designers by giving them access to 16 reordering quality metrics and visual exploration means for comparing reordering implementations on a row/column permutation level. We evaluated GUIRO in a guided explorative user study with 12 subjects, a case study demonstrating its usefulness in a real-world scenario, and through an expert study gathering feedback on our design decisions. We found that our proposed methods help even inexperienced users to understand matrix patterns and allow a user-guided steering of reordering algorithms. GUIRO helps to increase the transparency of matrix reordering algorithms, thus helping a broad range of users to get a better insight into the complex reordering process, in turn supporting data and reordering algorithm insights.", "keywords": "Visual Analytics,matrix,black-box algorithms,seriation,ordering,sorting,steerable algorithm,interaction,2D projection", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934300", "refList": ["10.1109/tvcg.2007.70582", "10.2307/276978", "10.1109/tvcg.2008.61", "10.3390/su10040973", "10.1101/121889", "10.1145/1345448.1345453", "10.1186/s12879-014-0695-9", "10.1186/1471-2105-9-155", "10.1145/1124772.1124891", "10.1109/tvcg.2010.159", "10.1111/j.2044-8317.1974.tb00534.x", "10.1109/tpami.2015.2470671", "10.1198/000313005x22770", "10.1109/tvcg.2012.219", "10.1002/sam.10071", "10.1016/j.ejor.2016.08.066", "10.1007/s00265-003-0651-y", "10.1109/biovis.2013.6664342", "10.3115/v1/p14-1062", "10.3389/fpsyg.2017.01349", "10.1109/tpami.2004.1265866", "10.1057/palgrave.ivs.9500086", "10.1007/s004260000031", "10.1111/cgf.12935", "10.1145/800195.805928", "10.1109/tvcg.2014.2346279", "10.2312/eurovisstar.20141174", "10.1109/tvcg.2012.256", "10.1093/bioinformatics/17.suppl\\_1.s22", "10.1109/infvis.2004.46", "10.1007/978-3-319-06793-3\\_5", "10.1109/tvcg.2006.147", "10.1109/tvcg.2015.2468078", "10.1093/bioinformatics/bti141", "10.1145/1168149.1168168", "10.1109/tvcg.2017.2745978", "10.1177/1473871613513228", "10.1109/mcg.2014.62", "10.1109/tvcg.2006.166", "10.1145/2470654.2470724", "10.1177/154193120605000909", "10.1109/sibgrapi.2007.21", "10.1145/3065386", "10.1109/tvcg.2006.160", "10.1287/opre.20.5.993", "10.1111/cgf.13446", "10.1057/palgrave.ivs.9500092", "10.1145/568522.568523", "10.1109/mcg.2013.66", "10.1109/tvcg.2018.2865940", "10.1109/tvcg.2012.208"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934555", "title": "Pattern-Driven Navigation in 2D Multiscale Visualizations with Scalable Insets", "year": "2019", "conferenceName": "InfoVis", "authors": "Fritz Lekschas;Michael Behrisch;Benjamin Bach;Peter Kerpedjiev;Nils Gehlenborg;Hanspeter Pfister", "citationCount": "2", "affiliation": "Lekschas, F (Corresponding Author), Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Lekschas, Fritz; Behrisch, Michael; Pfister, Hanspeter, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Bach, Benjamin, Univ Edinburgh, Edinburgh, Midlothian, Scotland. Kerpedjiev, Peter; Gehlenborg, Nils, Harvard Med Sch, Boston, MA 02115 USA.", "countries": "Scotland;USA", "abstract": "We present Scalable Insets, a technique for interactively exploring and navigating large numbers of annotated patterns in multiscale visualizations such as gigapixel images, matrices, or maps. Exploration of many but sparsely-distributed patterns in multiscale visualizations is challenging as visual representations change across zoom levels, context and navigational cues get lost upon zooming, and navigation is time consuming. Our technique visualizes annotated patterns too small to be identifiable at certain zoom levels using insets, i.e., magnified thumbnail views of the annotated patterns. Insets support users in searching, comparing, and contextualizing patterns while reducing the amount of navigation needed. They are dynamically placed either within the viewport or along the boundary of the viewport to offer a compromise between locality and context preservation. Annotated patterns are interactively clustered by location and type. They are visually represented as an aggregated inset to provide scalable exploration within a single viewport. In a controlled user study with 18 participants, we found that Scalable Insets can speed up visual search and improve the accuracy of pattern comparison at the cost of slower frequency estimation compared to a baseline technique. A second study with 6 experts in the field of genomics showed that Scalable Insets is easy to learn and provides first insights into how Scalable Insets can be applied in an open-ended data exploration scenario.", "keywords": "Guided Navigation,Pattern Exploration,Multiscale Visualizations,Gigapixel Images,Geospatial Maps,Genomics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934555", "refList": ["10.1109/tvcg.2011.231", "10.1111/cgf.12333", "10.1109/vast.2009.5333443", "10.1109/tvcg.2013.154", "10.1109/tvcg.2011.185", "10.1016/j.comgeo.2009.03.006", "10.1109/tvcg.2013.213", "10.1111/cgf.12871", "10.1063/1.1699114", "10.1145/22339.22342", "10.1145/2984511.2984578", "10.1016/j.comgeo.2006.05.003", "10.1145/223355.223749", "10.1016/j.cell.2014.11.021", "10.1145/302979.303148", "10.1111/j.1467-8659.2009.01450.x", "10.1126/science.298.5594.824", "10.1145/2470654.2466444", "10.1109/tvcg.2017.2745978", "10.1109/tvcg.2007.70589", "10.1145/212332.212334", "10.1007/s00453-015-0028-4", "10.1109/tvcg.2014.2346441", "10.1111/cgf.12615", "10.1109/tvcg.2006.136", "10.1007/978-3-319-27261-0\\_42", "10.1559/15230406384373", "10.1145/1618452.1618513", "10.1109/tvcg.2014.2346352", "10.1117/12.2231191", "10.1007/978-3-642-12670-3.3", "10.1038/nature23884", "10.1111/j.1467-8659.2011.01935.x", "10.3138/u3n2-6363-130n-h870", "10.1186/s13059-018-1486-1", "10.1111/cgf.12936", "10.1109/tvcg.2009.65", "10.1111/cgf.13207"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030378", "title": "NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries", "year": "2020", "conferenceName": "InfoVis", "authors": "Arpit Narechania;Arjun Srinivasan;John T. Stasko", "citationCount": "1", "affiliation": "Narechania, A (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA. Narechania, Arpit; Srinivasan, Arjun; Stasko, John, Georgia Inst Technol, Atlanta, GA 30332 USA.", "countries": "USA", "abstract": "Natural language interfaces (NLls) have shown great promise for visual data analysis, allowing people to flexibly specify and interact with visualizations. However, developing visualization NLIs remains a challenging task, requiring low-level implementation of natural language processing (NLP) techniques as well as knowledge of visual analytic tasks and visualization design. We present NL4DV, a toolkit for natural language-driven data visualization. NL4DV is a Python package that takes as input a tabular dataset and a natural language query about that dataset. In response, the toolkit returns an analytic specification modeled as a JSON object containing data attributes, analytic tasks, and a list of Vega-Lite specifications relevant to the input query. In doing so, NL4DV aids visualization developers who may not have a background in NLP, enabling them to create new visualization NLIs or incorporate natural language input within their existing systems. We demonstrate NL4DV's usage and capabilities through four examples: 1) rendering visualizations using natural language in a Jupyter notebook, 2) developing a NLI to specify and edit Vega-Lite charts, 3) recreating data ambiguity widgets from the DataTone system, and 4) incorporating speech input to create a multimodal visualization system.", "keywords": "Natural Language Interfaces,Visualization Toolkits", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030378", "refList": ["10.1109/tvcg.2014.2346249", "10.1145/3230623", "10.1111/cgf.12106", "10.1109/visual.2005.1532788", "10.1016/j.infsof.2018.01.005", "10.1109/tvcg.2012.237", "10.1109/tvcg.2011.185", "10.1109/vlhcc.2004.13", "10.1109/oecc.2012.6276803", "10.1111/j.1467-8659.2012.03093.x", "10.1029/2018jd029522", "10.1109/tvcg.2007.70535", "10.1080/14626260701532033", "10.1038/s41592-018-0175-z", "10.1016/j.cell.2014.11.021", "10.1080/07370024.2013.812411", "10.1109/tvcg.2017.2745978", "10.1007/s11263-009-0275-4", "10.1109/tvcg.2015.2467851", "10.1109/tvcg.2008.125", "10.1111/cgf.12615", "10.1109/tvcg.2015.2502587", "10.1109/2945.942695", "10.1109/cvprw.2009.5206848", "10.1145/376929.376932", "10.1109/tvcg.2019.2934555", "10.1057/palgrave.ivs.9500025", "10.1145/357423.357430"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2020.3030378", "title": "NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries", "year": "2020", "conferenceName": "InfoVis", "authors": "Arpit Narechania;Arjun Srinivasan;John T. Stasko", "citationCount": "1", "affiliation": "Narechania, A (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA. Narechania, Arpit; Srinivasan, Arjun; Stasko, John, Georgia Inst Technol, Atlanta, GA 30332 USA.", "countries": "USA", "abstract": "Natural language interfaces (NLls) have shown great promise for visual data analysis, allowing people to flexibly specify and interact with visualizations. However, developing visualization NLIs remains a challenging task, requiring low-level implementation of natural language processing (NLP) techniques as well as knowledge of visual analytic tasks and visualization design. We present NL4DV, a toolkit for natural language-driven data visualization. NL4DV is a Python package that takes as input a tabular dataset and a natural language query about that dataset. In response, the toolkit returns an analytic specification modeled as a JSON object containing data attributes, analytic tasks, and a list of Vega-Lite specifications relevant to the input query. In doing so, NL4DV aids visualization developers who may not have a background in NLP, enabling them to create new visualization NLIs or incorporate natural language input within their existing systems. We demonstrate NL4DV's usage and capabilities through four examples: 1) rendering visualizations using natural language in a Jupyter notebook, 2) developing a NLI to specify and edit Vega-Lite charts, 3) recreating data ambiguity widgets from the DataTone system, and 4) incorporating speech input to create a multimodal visualization system.", "keywords": "Natural Language Interfaces,Visualization Toolkits", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030378", "refList": ["10.1109/tvcg.2014.2346249", "10.1145/3230623", "10.1111/cgf.12106", "10.1109/visual.2005.1532788", "10.1016/j.infsof.2018.01.005", "10.1109/tvcg.2012.237", "10.1109/tvcg.2011.185", "10.1109/vlhcc.2004.13", "10.1109/oecc.2012.6276803", "10.1111/j.1467-8659.2012.03093.x", "10.1029/2018jd029522", "10.1109/tvcg.2007.70535", "10.1080/14626260701532033", "10.1038/s41592-018-0175-z", "10.1016/j.cell.2014.11.021", "10.1080/07370024.2013.812411", "10.1109/tvcg.2017.2745978", "10.1007/s11263-009-0275-4", "10.1109/tvcg.2015.2467851", "10.1109/tvcg.2008.125", "10.1111/cgf.12615", "10.1109/tvcg.2015.2502587", "10.1109/2945.942695", "10.1109/cvprw.2009.5206848", "10.1145/376929.376932", "10.1109/tvcg.2019.2934555", "10.1057/palgrave.ivs.9500025", "10.1145/357423.357430"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13446", "year": "2018", "title": "Quality Metrics for Information Visualization", "conferenceName": "EuroVis", "authors": "Michael Behrisch;Michael Blumenschein;Nam Wook Kim;Lin Shao;Mennatallah El{-}Assady;Johannes Fuchs;Daniel Seebacher;Alexandra Diehl;Ulrik Brandes;Hanspeter Pfister;Tobias Schreck;Daniel Weiskopf;Daniel A. Keim", "citationCount": "25", "affiliation": "Behrisch, M (Corresponding Author), Harvard Univ, Cambridge, MA 02138 USA.\nBehrisch, M.; Kim, N. W.; Pfister, H., Harvard Univ, Cambridge, MA 02138 USA.\nBlumenschein, M.; El-Assady, M.; Fuchs, J.; Seebacher, D.; Diehl, A.; Keim, D. A., Univ Konstanz, Constance, Germany.\nShao, L.; Schreck, T., Graz Univ Technol, Graz, Austria.\nBrandes, U., Swiss Fed Inst Technol, Zurich, Switzerland.\nWeiskopf, D., Univ Stuttgart, Stuttgart, Germany.", "countries": "Switzerland;Germany;USA;Austria", "abstract": "The visualization community has developed to date many intuitions and understandings of how to judge the quality of views in visualizing data. The computation of a visualization's quality and usefulness ranges from measuring clutter and overlap, up to the existence and perception of specific (visual) patterns. This survey attempts to report, categorize and unify the diverse understandings and aims to establish a common vocabulary that will enable a wide audience to understand their differences and subtleties. For this purpose, we present a commonly applicable quality metric formalization that should detail and relate all constituting parts of a quality metric. We organize our corpus of reviewed research papers along the data types established in the information visualization community: multi- and high-dimensional, relational, sequential, geospatial and text data. For each data type, we select the visualization subdomains in which quality metrics are an active research field and report their findings, reason on the underlying concepts, describe goals and outline the constraints and requirements. One central goal of this survey is to provide guidance on future research opportunities for the field and outline how different visualization communities could benefit from each other by applying or transferring knowledge to their respective subdomain. Additionally, we aim to motivate the visualization community to compare computed measures to the perception of humans.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13446", "refList": ["10.2307/276978", "10.1057/ivs.2009.10", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2014.2346420", "10.1016/j.cag.2018.01.010", "10.1109/tvcg.2017.2743959", "10.1109/tvcg.2011.127", "10.1109/tvcg.2015.2467759", "10.1109/tvcg.2013.187", "10.1109/tvcg.2007.70594", "10.1186/1471-2105-9-155", "10.1145/2702123.2702545", "10.1145/1645953.1646023", "10.1093/bioinformatics/btm312", "10.1109/pacificvis.2016.7465245", "10.1145/3038462.3038463", "10.1007/978-3-540-70956-5", "10.1109/tvcg.2017.2743939", "10.1111/cgf.12932", "10.1111/j.1467-8659.2012.03106.x", "10.1145/1133265.1133318", "10.1109/tvcg.2010.186", "10.1109/iv.2013.101", "10.1109/tvcg.2016.2598590", "10.2312/conf/eg2013/stars/039-063", "10.3758/bf03201236", "10.1111/cgf.12935", "10.1145/1056808.1056914", "10.1007/bf01898350", "10.2307/1390686", "10.1109/2945.981848", "10.1109/hicss.2008.422", "10.1109/tvcg.2011.201", "10.1109/pacificvis.2011.5742390", "10.2307/2288400", "10.1111/cgf.12872", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2006.163", "10.1109/tvcg.2013.150", "10.1109/tvcg.2009.171", "10.1109/tvcg.2017.2723397", "10.1117/12.2079841", "10.1109/pacificvis.2009.4906838", "10.1109/infvis.2004.1", "10.1109/tvcg.2008.166", "10.1287/opre.20.5.993", "10.7155/jgaa.00370", "10.1111/j.1467-8659.2008.01240.x", "10.1109/inf0vis.2005.14", "10.1109/tvcg.2009.23", "10.1109/infvis.1998.729559", "10.1177/0165551506078083", "10.1109/t-c.1974.224051", "10.1109/tvcg.2017.2744339", "10.1109/tvcg.2014.2346433", "10.1145/989863.989940", "10.1109/tvcg.2015.2509990", "10.2307/2288843", "10.1016/j.neucom.2017.01.105", "10.1073/pnas.43.10.923", "10.1111/cgf.12647", "10.1016/j.rse.2017.06.031", "10.1109/infvis.2005.1532145", "10.1109/2945.841121", "10.1109/tvcg.2011.229", "10.2312/eurova.20141140", "10.1111/j.2044-8317.1974.tb00534.x", "10.1109/pacificvis.2015.7156366", "10.1109/vast.2009.5332628", "10.1109/vast.2014.7042480", "10.1145/2993901.2993907", "10.1177/0951692899011001004", "10.1007/978-3-642-03655-2\\_43", "10.1198/106186008x320465", "10.1109/visual.1997.663916", "10.1111/cgf.12125", "10.1109/infvis.2003.1249009", "10.1109/icdm.2003.1250978", "10.1109/tvcg.2017.2745919", "10.1109/tvcg.2012.128", "10.1109/vast.2010.5652450", "10.1109/tvcg.2006.161", "10.1145/571647.571649", "10.1109/tvcg.2011.193", "10.1111/cgf.12633", "10.1109/tvcg.2017.2674978", "10.1109/vast.2010.5652433", "10.1109/tvcg.2017.2745978", "10.1515/itit-2014-1070", "10.1145/1374489.1374501", "10.1109/tvcg.2009.153", "10.2312/eurovisshort.20171128", "10.2312/eurovisshort.20151130", "10.1016/0925-7721(94)00014-x", "10.1057/palgrave.ivs.95000/3", "10.1145/302979.303030", "10.1002/widm.1071", "10.1109/pacificvis.2014.42", "10.1109/tvcg.2016.2598467", "10.1111/j.1467-8659.2009.01667.x", "10.2307/2287708", "10.1177/1473871613477091", "10.1145/967900.968153", "10.1109/tvcg.2010.162", "10.1145/568522.568523", "10.1016/j.cag.2004.03.022", "10.1109/tvcg.2015.2466992", "10.1145/2993901.2993903", "10.1147/jrd.2015.2411412", "10.1177/154193120504900508", "10.1109/tvcg.2006.138", "10.1111/cgf.13168", "10.1111/cgf.12380", "10.2312/conf/eg2013/stars/095-116", "10.1109/tvcg.2017.2745859", "10.1109/tvcg.2017.2701829", "10.1109/tvcg.2007.70529", "10.1007/bf01199431", "10.1117/12.697548", "10.1109/tvcg.2017.2653106", "10.1207/s15327906mbr2701\\_4", "10.1109/vl.1996.545307", "10.1109/tvcg.2017.2745140", "10.1111/j.1467-8659.2012.03069.x", "10.1111/j.1467-8659.2011.01961.x", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.124", "10.1057/ivs.2008.13", "10.2312/vissym/eurovis06/195-202", "10.2312/vissym/eurovis07/163-170", "10.1109/tvcg.2015.2467324", "10.1109/tvcg.2011.167", "10.1109/pacificvis.2014.40", "10.1109/pacificvis.2012.6183570", "10.1111/j.1467-8659.2012.03125.x", "10.1075/idj.20.1.02bei", "10.1111/1467-8306.00061", "10.1145/2858036.2858155", "10.1109/tvcg.2012.108", "10.2312/vmv.20171261", "10.1109/tvcg.2015.2467531", "10.1145/1379092.1379130", "10.1109/tpami.1979.4766909", "10.1080/13875860903039172", "10.1109/mcg.2006.70", "10.1109/tvcg.2010.242", "10.1109/vast.2007.4389004", "10.1109/tvcg.2015.2467191", "10.2312/cgvc.20171276", "10.1016/0169-7439(87)80084-9", "10.2312/pe/eurovisshort/eurovisshort2012/097-101", "10.1016/j.neucom.2014.07.073", "10.2307/2685263", "10.1145/996546.996554", "10.1111/j.1467-8659.2009.01666.x", "10.1117/12.2083444", "10.1109/tvcg.2014.2346572", "10.1007/978-0-387-39940-9\\_262", "10.1007/bf00988593", "10.1145/1054972.1055078", "10.1111/j.1467-8659.2009.01467.x", "10.1111/cgf.13181", "10.1145/502512.502530", "10.1109/pacificvis.2010.5429600", "10.1111/j.1467-8659.2011.01923.x", "10.1109/mcg.2004.41", "10.1109/tvcg.2015.2467971", "10.1111/j.1467-8659.2011.01919.x", "10.1002/sam.10071", "10.1109/tvcg.2010.184", "10.1007/978-3-642-27848-8\\_648-1", "10.1109/tvcg.2010.132", "10.1145/22949.22950", "10.1145/1842993.1843002", "10.1109/infvis.2000.885096", "10.1016/j.ins.2015.04.017", "10.1109/iv.2009.43", "10.1016/j.visinf.2017.11.001", "10.1109/tvcg.2011.239", "10.1109/infovis.2005.40", "10.1111/cgf.12641", "10.1109/pacificvis.2016.7465262", "10.1145/2470654.2466443", "10.1007/s10844-011-0157-4", "10.1111/cgf.12919", "10.1016/j.jvlc.2016.07.003", "10.1109/tvcg.2016.2549018", "10.1057/palgrave.ivs.9500166", "10.3406/colan.1981.1409.1", "10.1093/bioinformatics/bti141", "10.1111/j.1467-8306.2004.09401004.x", "10.1145/1168149.1168168", "10.1109/tvcg.2017.2744184", "10.1117/12.2079420", "10.1109/iv.2005.62", "10.1145/102377.115768", "10.1109/tvcg.2014.2346677", "10.1109/tvcg.2014.2346426", "10.1111/j.1467-8659.2012.03107.x", "10.5220/0006097400400051", "10.2991/978-94-6239-186-4", "10.2307/2284077", "10.1109/iv.2008.89", "10.1016/j.jvlc.2015.12.001", "10.1109/pacificvis.2013.6596147", "10.1145/1242572.1242826", "10.1016/j.cag.2007.01.030", "10.1111/cgf.12632", "10.2312/eurovisstar.20151113", "10.1559/152304009788188808"], "wos": 1, "children": [{"doi": "10.1109/vast.2018.8802486", "title": "SMARTexplore: Simplifying High-Dimensional Data Analysis through a Table-Based Visual Analytics Approach", "year": "2018", "conferenceName": "VAST", "authors": "Michael Blumenschein;Michael Behrisch;Stefanie Schmid;Simon Butscher;Deborah Wahl;Karoline Villinger;Britta Renner;Harald Reiterer;Daniel A. Keim", "citationCount": "0", "affiliation": "Blumenschein, M (Corresponding Author), Univ Konstanz, Constance, Germany. Blumenschein, Michael; Schmid, Stefanie; Butscher, Simon; Wahl, Deborah R.; Villinger, Karoline; Renner, Britta; Reiterer, Harald; Keim, Daniel A., Univ Konstanz, Constance, Germany. Behrisch, Michael, Harvard Univ, Cambridge, MA 02138 USA.", "countries": "Germany;USA", "abstract": "We present SMARTEXPLORE, a novel visual analytics technique that simplifies the identification and understanding of clusters, correlations, and complex patterns in high-dimensional data. The analysis is integrated into an interactive table-based visualization that maintains a consistent and familiar representation throughout the analysis. The visualization is tightly coupled with pattern matching, subspace analysis, reordering, and layout algorithms. To increase the analyst's trust in the revealed patterns, SMARTEXPLORE automatically selects and computes statistical measures based on dimension and data properties. While existing approaches to analyzing high-dimensional data (e.g., planar projections and Parallel coordinates) have proven effective, they typically have steep learning curves for non-visualization experts. Our evaluation, based on three expert case studies, confirms that non-visualization experts successfully reveal patterns in high-dimensional data when using SMARTEXPLORE.", "keywords": "High-dimensional data,visual exploration,pattern-driven analysis,tabular visualization,subspace,aggregation", "link": "http://dx.doi.org/10.1109/VAST.2018.8802486", "refList": ["10.1177/1473871612460526", "10.1109/tvcg.2015.2489649", "10.1111/j.1467-8659.2008.01241.x", "10.1007/978-3-319-25087-8\\_29", "10.1007/b98835", "10.13140/rg.2.2.16570.90567", "10.1109/tvcg.2014.2346260", "10.1109/vast.2009.5332628", "10.2307/2528823", "10.1109/tvcg.2010.184", "10.1145/1133265.1133318", "10.1057/palgrave.ivs.9500072", "10.1111/j.1467-8659.2012.03110.x", "10.1145/1007730.1007731", "10.1057/palgrave.ivs.9500086", "10.1111/cgf.12935", "10.1109/tvcg.2014.2346279", "10.1007/bf01898350", "10.1109/tvcg.2013.173", "10.1109/tvcg.2017.2672987", "10.1109/tvcg.2014.2346248", "10.1109/infvis.2004.46", "10.1109/vast.2010.5652433", "10.1109/vast.2009.5332611", "10.1109/tvcg.2015.2467553", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2013.150", "10.1109/tvcg.2016.2640960", "10.1111/cgf.12630", "10.1007/978-1-4757-1904-8", "10.1109/tvcg.2011.188", "10.1109/tvcg.2010.138", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2017.2743978", "10.1111/cgf.12879", "10.1109/iv.2008.33", "10.1111/cgf.13446", "10.1007/s00371-018-1483-0", "10.1109/infvis.1998.729559", "10.1145/2669557.2669572", "10.1111/j.1467-8659.2008.01239.x"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934541", "title": "A Recursive Subdivision Technique for Sampling Multi-class Scatterplots", "year": "2019", "conferenceName": "InfoVis", "authors": "Xin Chen;Tong Ge;Jian Zhang 0070;Baoquan Chen;Chi-Wing Fu;Oliver Deussen;Yunhai Wang", "citationCount": "5", "affiliation": "Chen, X (Corresponding Author), Shandong Univ, Jinan, Shandong, Peoples R China. Chen, Xin; Ge, Tong; Wang, Yunhai, Shandong Univ, Jinan, Shandong, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China. Fu, Chi-Wing, SIAT, Guangdong Prov Key Lab CV \\& VR Tech, Shenzhen, Guangdong, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Deussen, Oliver, Konstanz Univ, Constance, Germany. Deussen, Oliver, SIAT, Shenzhen VIsuCA Key Lab, Shenzhen, Guangdong, Peoples R China.", "countries": "Germany;China", "abstract": "We present a non-uniform recursive sampling technique for multi-class scatterplots, with the specific goal of faithfully presenting relative data and class densities, while preserving major outliers in the plots. Our technique is based on a customized binary kd-tree, in which leaf nodes are created by recursively subdividing the underlying multi-class density map. By backtracking, we merge leaf nodes until they encompass points of all classes for our subsequently applied outlier-aware multi-class sampling strategy. A quantitative evaluation shows that our approach can better preserve outliers and at the same time relative densities in multi-class scatterplots compared to the previous approaches, several case studies demonstrate the effectiveness of our approach in exploring complex and real world data.", "keywords": "Scatterplot,multi-class sampling,kd-tree,outlier,relative density", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934541", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2006.170", "10.1109/tvcg.2008.119", "10.1109/tvcg.2007.70580", "10.2307/2289444", "10.1145/1964921.1964943", "10.1109/tvcg.2013.65", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/itoec.2018.8740621", "10.1145/1778765.1778816", "10.1109/tvcg.2010.197", "10.1201/b17511", "10.1109/tvcg.2018.2864912", "10.1007/0-387-28695-0", "10.1109/5.726791", "10.1109/visual.1998.745301", "10.1016/j.physa.2011.12.004", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2004.1272729", "10.1145/335191.335388", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1109/iv.2002.1028760", "10.1145/1842993.1842999", "10.1109/tvcg.2018.2869149", "10.1038/nmeth.2490", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2010.176", "10.1145/2702123.2702585", "10.1057/ivs.2009.34"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14001", "year": "2020", "title": "Sunspot Plots: Model-based Structure Enhancement for Dense Scatter Plots", "conferenceName": "EuroVis", "authors": "Thomas Trautner;Fabian Bolte;Sergej Stoppel;Stefan Bruckner", "citationCount": "0", "affiliation": "Trautner, T (Corresponding Author), Univ Bergen, Bergen, Norway.\nTrautner, T.; Bolte, F.; Stoppel, S.; Bruckner, S., Univ Bergen, Bergen, Norway.", "countries": "Norway", "abstract": "Scatter plots are a powerful and well-established technique for visualizing the relationships between two variables as a collection of discrete points. However, especially when dealing with large and dense data, scatter plots often exhibit problems such as overplotting, making the data interpretation arduous. Density plots are able to overcome these limitations in highly populated regions, but fail to provide accurate information of individual data points. This is particularly problematic in sparse regions where the density estimate may not provide a good representation of the underlying data. In this paper, we present sunspot plots, a visualization technique that communicates dense data as a continuous data distribution, while preserving the discrete nature of data samples in sparsely populated areas. We furthermore demonstrate the advantages of our approach on typical failure cases of scatter plots within synthetic and real-world data sets and validate its effectiveness in a user study.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14001", "refList": ["10.1057/palgrave.ivs.9500122", "10.2312/eggh/hpg12/097-103", "10.1109/tvcg.2008.119", "10.1109/pacificvis.2011.5742387", "10.1038/331163a0", "10.1109/visual.2019.8933620", "10.2307/2683294", "10.1201/9781351072304", "10.2307/2289444", "10.1109/tvcg.2019.2934541", "10.1080/00949657508810123", "10.1109/tvcg.2013.65", "10.1109/infvis.1997.636789", "10.1109/2945.841121", "10.1109/mcse.2007.55", "10.1109/tvcg.2010.197", "10.1111/cgf.12871", "10.1109/infvis.2003.1249018", "10.1145/3173574.3173991", "10.1109/tvcg.2012.238", "10.1007/0-387-28695-0", "10.1109/pacificvis.2010.5429604", "10.18637/jss.v008.i03", "10.1109/tvcg.2007.70596", "10.1109/visual.1998.745301", "10.1007/0-387-37977-0\\_3", "10.1145/1556262.1556289", "10.2312/wiced.20161094", "10.1109/tvcg.2007.70535", "10.1145/1056808.1056914", "10.1109/tvcg.2003.1196007", "10.1109/iv.2004.1320190", "10.1111/cgf.12877", "10.1162/leon.2007.40.2.202a", "10.1109/tvcg.2017.2674978", "10.1057/ivs.2010.4", "10.1016/j.cag.2018.02.008", "10.1201/9781315140919", "10.1109/visual.2000.885677", "10.1002/jhbs.20078", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1109/visual.2001.964495", "10.1109/iv.2002.1028760", "10.1111/cgf.13684", "10.1109/hicss.2013.197", "10.1109/tvcg.2019.2903956", "10.1093/mnras/stt961", "10.1109/tvcg.2017.2668409", "10.1111/j.1467-8659.2009.01478.x", "10.1145/2702123.2702585", "10.2307/1418003", "10.2307/1390742", "10.1145/360825.360839", "10.1109/pacificvis.2009.4906843", "10.1057/ivs.2009.34", "10.2307/2288711"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934284", "title": "Color Crafting: Automating the Construction of Designer Quality Color Ramps", "year": "2019", "conferenceName": "InfoVis", "authors": "Stephen Smart;Keke Wu;Danielle Albers Szafir", "citationCount": "3", "affiliation": "Smart, S (Corresponding Author), Univ Colorado, Boulder, CO 80309 USA. Smart, Stephen; Wu, Keke; Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Visualizations often encode numeric data using sequential and diverging color ramps. Effective ramps use colors that are sufficiently discriminable, align well with the data, and are aesthetically pleasing. Designers rely on years of experience to create high-quality color ramps. However, it is challenging for novice visualization developers that lack this experience to craft effective ramps as most guidelines for constructing ramps are loosely defined qualitative heuristics that are often difficult to apply. Our goal is to enable visualization developers to readily create effective color encodings using a single seed color. We do this using an algorithmic approach that models designer practices by analyzing patterns in the structure of designer-crafted color ramps. We construct these models from a corpus of 222 expert-designed color ramps, and use the results to automatically generate ramps that mimic designer practices. We evaluate our approach through an empirical study comparing the outputs of our approach with designer-crafted color ramps. Our models produce ramps that support accurate and aesthetically pleasing visualizations at least as well as designer ramps and that outperform conventional mathematical approaches.", "keywords": "Visualization,Aesthetics in Visualization,Color Perception,Visual Design,Design Mining", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934284", "refList": ["10.1145/3009924", "10.1109/tvcg.2015.2489649", "10.1109/tvcg.2017.2744359", "10.1002/(sici)1098-1098(199622)7:2", "10.1016/s0734-189x(83)80046-2", "10.1109/tvcg.2016.2599106", "10.1364/josaa.29.000313", "10.1109/tvcg.2016.2598918", "10.2307/2683294", "10.1109/tvcg.2017.2653106", "10.1016/j.ijhcs.2010.05.006", "10.1016/0146-664x(81)90006-x", "10.1007/978-3-642-10520-3\\_9", "10.1109/38.135886", "10.1146/annurev-psych-120710-100504", "10.1016/j.csda.2008.11.033", "10.1145/2461912.2461988", "10.1016/j.cub.2007.06.022", "10.1016/s0146-664x(79)80040-4", "10.1109/mcg.2004.1297012", "10.1109/iv.2009.94", "10.1109/tpami.2010.184", "10.3758/s13414-010-0027-0", "10.1145/22949.22950", "10.1109/visual.1995.480803", "10.1109/tvcg.2014.2346277", "10.2307/2684111", "10.1109/tvcg.2018.2865240", "10.1111/cgf.12633", "10.1109/38.7760", "10.1016/j.jspi.2015.04.007", "10.1109/iv.2008.24", "10.1145/2939502.2939506", "10.1111/cgf.12127", "10.1016/j.cag.2010.11.015", "10.1145/3025453.3026041", "10.1109/tvcg.2012.315", "10.2307/2288400", "10.1511/2005.5.436", "10.1016/s0097-8493(96)00072-6", "10.1109/mcg.2018.011461525", "10.1145/2470654.2466420", "10.1109/tvcg.2012.279", "10.1109/tvcg.2014.2346978", "10.1109/tvcg.2017.2743978", "10.1145/3406601.3406602", "10.1117/12.2084548", "10.1109/tvcg.2018.2865147", "10.1109/tvcg.2015.2467191", "10.1111/cgf.13446", "10.1109/tvcg.2017.2744320", "10.1111/j.1467-8659.2008.01203.x", "10.1145/2702613.2702975", "10.1007/978-3-319-26633-6\\_13", "10.1145/2207676.2208547", "10.1109/tvcg.2008.174", "10.1179/000870403235002042"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028891", "title": "A Structured Review of Data Management Technology for Interactive Visualization and Analysis", "year": "2020", "conferenceName": "InfoVis", "authors": "Leilani Battle;Carlos Scheidegger", "citationCount": "0", "affiliation": "Battle, L (Corresponding Author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA. Battle, Leilani, Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA. Scheidegger, Carlos, Univ Arizona, Dept Comp Sci, HDC Lab, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "In the last two decades, interactive visualization and analysis have become a central tool in data-driven decision making. Concurrently to the contributions in data visualization, research in data management has produced technology that directly benefits interactive analysis. Here, we contribute a systematic review of 30 years of work in this adjacent field, and highlight techniques and principles we believe to be underappreciated in visualization work. We structure our review along two axes. First, we use task taxonomies from the visualization literature to structure the space of interactions in usual systems. Second, we created a categorization of data management work that strikes a balance between specificity and generality. Concretely, we contribute a characterization of 131 research papers along these two axes. We find that five notions in data management venues fit interactive visualization systems well: materialized views, approximate query processing, user modeling and query prediction, muiti-query optimization, lineage techniques, and indexing techniques. In addition, we find a preponderance of work in materialized views and approximate query processing, most targeting a limited subset of the interaction tasks in the taxonomy we used. This suggests natural avenues of future research both in visualization and data management. Our categorization both changes how we visualization researchers design and build our systems, and highlights where future work is necessary.", "keywords": "", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028891", "refList": ["10.1109/tvcg.2012.233", "10.1016/s0022-5371(74)80015-0", "10.1109/tvcg.2017.2744359", "10.1037/0096-3445.136.4.623", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2012.196", "10.1037/a0029856", "10.1109/tvcg.2014.2346979", "10.1037/h0030300", "10.1109/tvcg.2016.2598918", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2018.2864909", "10.1111/cgf.13079", "10.3389/fpsyg.2012.00355", "10.1145/2858036.2858465", "10.1080/01621459.1989.10478821", "10.1037/0278-7393.24.3.732", "10.1109/tvcg.2011.127", "10.1145/2858036.2858063", "10.4249/scholarpedia.3325", "10.4324/9781410611949", "10.1111/cgf.13444", "10.1145/2993901.2993909", "10.1037/0033-295x.96.2.267", "10.1006/ijhc.1017", "10.1086/405615", "10.1109/tvcg.2019.2934801", "10.1038/17953", "10.1037/xhp0000314", "10.1109/tvcg.2019.2934400", "10.1145/2470654.2470723", "10.1037/0096-1523.16.2.332", "10.1167/16.5.11", "10.3758/s13423-016-1174-7", "10.3758/bf03207704", "10.1146/annurev.psych.55.090902.141415", "10.2307/2288400", "10.3758/bf03204258", "10.1109/tvcg.2011.279", "10.1109/vissoft.2014.36", "10.3758/s13423-011-0055-3", "10.1145/3025453.3025922", "10.1109/tvcg.2019.2934284", "10.3758/bf03210498", "10.3758/bf03200774", "10.2307/1419876", "10.1038/s41562-017-0058", "10.1109/tvcg.2010.237", "10.1109/pacificvis.2012.6183556", "10.1109/infvis.1997.636792", "10.1093/acprof:oso/9780198523192.003.0005", "10.1073/pnas.1117465109", "10.1109/tvcg.2013.234", "10.1038/nn.3655", "10.1111/cgf.12379", "10.1146/annurev-psych-010416-044232", "10.1111/cgf.13695", "10.1037/0033-295x.107.3.500", "10.1109/tvcg.2013.183", "10.1146/annurev.psych.53.100901.135125", "10.1037//0022-3514.79.6.995", "10.1559/152304003100010929", "10.1109/tvcg.2018.2865147", "10.1037/0096-1523.18.3.849", "10.1111/j.1467-8659.2009.01694.x", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934799", "title": "Data Sampling in Multi-view and Multi-class Scatterplots via Set Cover Optimization", "year": "2019", "conferenceName": "InfoVis", "authors": "Ruizhen Hu;Tingkai Sha;Oliver van Kaick;Oliver Deussen;Hui Huang 0004", "citationCount": "4", "affiliation": "Hu, RZ (Corresponding Author), Shenzhen Univ, Visual Comp Res Ctr, Shenzhen, Guangdong, Peoples R China. Hu, Ruizhen; Sha, Tingkai; Huang, Hui, Shenzhen Univ, Visual Comp Res Ctr, Shenzhen, Guangdong, Peoples R China. van Kaick, Oliver, Carleton Univ, Sch Comp Sci, Ottawa, ON, Canada. Deussen, Oliver, Konstanz Univ, Constance, Germany. Deussen, Oliver, SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Guangdong, Peoples R China.", "countries": "Canada;Germany;China", "abstract": "We present a method for data sampling in scatterplots by jointly optimizing point selection for different views or classes. Our method uses space-filling curves (Z-order curves) that partition a point set into subsets that, when covered each by one sample, provide a sampling or coreset with good approximation guarantees in relation to the original point set. For scatterplot matrices with multiple views, different views provide different space-filling curves, leading to different partitions of the given point set. For multi-class scatterplots, the focus on either per-class distribution or global distribution provides two different partitions of the given point set that need to be considered in the selection of the coreset. For both cases, we convert the coreset selection problem into an Exact Cover Problem (ECP), and demonstrate with quantitative and qualitative evaluations that an approximate solution that solves the ECP efficiently is able to provide high-quality samplings.", "keywords": "Sampling,Scatterplot,SPLOM,Exact Cover Problem", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934799", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2006.170", "10.1109/tvcg.2008.119", "10.1111/j.1467-8659.2009.01467.x", "10.2307/2289444", "10.1109/tvcg.2013.65", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/itoec.2018.8740621", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2864912", "10.1007/0-387-28695-0", "10.1109/visual.1998.745301", "10.1287/moor.4.3.233", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.2307/2284239", "10.1109/tvcg.2017.2674978", "10.1016/j.dss.2009.05.016", "10.1109/tvcg.2014.2346594", "10.1109/vds.2017.8573446", "10.1007/978-1-4684-2001-2\\_9", "10.1109/iv.2002.1028760", "10.1145/1842993.1842999", "10.1038/nmeth.2490", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2010.176", "10.1145/2702123.2702585", "10.1057/ivs.2009.34", "10.1179/000870403235002042"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934432", "title": "Discriminability Tests for Visualization Effectiveness and Scalability", "year": "2019", "conferenceName": "InfoVis", "authors": "Rafael Veras;Christopher Collins", "citationCount": "1", "affiliation": "Veras, R (Corresponding Author), Ontario Tech Univ, Oshawa, ON, Canada. Veras, Rafael; Collins, Christopher, Ontario Tech Univ, Oshawa, ON, Canada.", "countries": "Canada", "abstract": "The scalability of a particular visualization approach is limited by the ability for people to discern differences between plots made with different datasets. Ideally, when the data changes, the visualization changes in perceptible ways. This relation breaks down when there is a mismatch between the encoding and the character of the dataset being viewed. Unfortunately, visualizations are often designed and evaluated without fully exploring how they will respond to a wide variety of datasets. We explore the use of an image similarity measure, the Multi-Scale Structural Similarity Index (MS-SSIM), for testing the discriminability of a data visualization across a variety of datasets. MS-SSIM is able to capture the similarity of two visualizations across multiple scales, including low level granular changes and high level patterns. Significant data changes that are not captured by the MS-SSIM indicate visualizations of low discriminability and effectiveness. The measure's utility is demonstrated with two empirical studies. In the first, we compare human similarity judgments and MS-SSIM scores for a collection of scatterplots. In the second, we compute the discriminability values for a set of basic visualizations and compare them with empirical measurements of effectiveness. In both cases, the analyses show that the computational measure is able to approximate empirical results. Our approach can be used to rank competing encodings on their discriminability and to aid in selecting visualizations for a particular type of data distribution.", "keywords": "Scalability,Discriminability,Simulation,Perception", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934432", "refList": ["10.1109/tvcg.2012.233", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2012.230", "10.1111/cgf.12647", "10.1109/tvcg.2016.2598918", "10.1109/tvcg.2007.70529", "10.1145/2858036.2858435", "10.1109/vast.2009.5332628", "10.1145/3025453.3025912", "10.1145/1133265.1133318", "10.1109/tip.2010.2092435", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2865264", "10.1175/jtech-d-11-00103.1", "10.1145/1190036.1190039", "10.1111/cgf.12127", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1109/mcg.2014.18", "10.1145/3173574.3174172", "10.1109/tvcg.2009.153", "10.1057/palgrave.ivs.9500070", "10.1109/tvcg.2018.2790961", "10.1109/tvcg.2014.2346978", "10.1109/tvcg.2018.2810918", "10.1111/cgf.13409", "10.1109/tip.2003.819861", "10.1145/2642918.2647411", "10.1080/15230406.2016.1140074", "10.1109/jstsp.2009.2015374", "10.1109/tvcg.2010.237", "10.1109/mcg.2009.6", "10.1111/j.1467-8659.2009.01667.x", "10.1109/tvcg.2010.161", "10.1111/cgf.13446", "10.1109/tvcg.2014.2346325", "10.1109/tvcg.2009.111", "10.1147/jrd.2015.2411412"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030394", "title": "Direct Volume Rendering with Nonparametric Models of Uncertainty", "year": "2020", "conferenceName": "SciVis", "authors": "Tushar M. Athawale;Bo Ma 0002;Elham Sakhaee;Christopher R. Johnson;Alireza Entezari", "citationCount": "0", "affiliation": "Athawale, TM (Corresponding Author), Univ Utah, Sci Comp \\& Imaging SCI Inst, Salt Lake City, UT 84112 USA. Athawale, Tushar M.; Johnson, Chris R., Univ Utah, Sci Comp \\& Imaging SCI Inst, Salt Lake City, UT 84112 USA. Ma, Bo; Sakhaee, Elham; Entezari, Alireza, Univ Florida, Dept CISE, Gainesville, FL 32611 USA.", "countries": "USA", "abstract": "We present a nonparametric statistical framework for the quantification, analysis, and propagation of data uncertainty in direct volume rendering (DVR). The state-of-the-art statistical DVR framework allows for preserving the transfer function (TF) of the ground truth function when visualizing uncertain data; however, the existing framework is restricted to parametric models of uncertainty. In this paper, we address the limitations of the existing DVR framework by extending the DVR framework for nonparametric distributions. We exploit the quantile interpolation technique to derive probability distributions representing uncertainty in viewing-ray sample intensities in closed form, which allows for accurate and efficient computation. We evaluate our proposed nonparametric statistical models through qualitative and quantitative comparisons with the mean-field and parametric statistical models, such as uniform and Gaussian, as well as Gaussian mixtures. In addition, we present an extension of the state-of-the-art rendering parametric framework to 2D TFs for improved DVR classifications. We show the applicability of our uncertainty quantification framework to ensemble, downsampled, and bivariate versions of scalar field datasets.", "keywords": "Volumes,uncertainty,nonparametric,2D transfer function", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030394", "refList": ["10.1109/icdm.2012.80", "10.1145/1401890.1401904", "10.1109/tvcg.2017.2745139", "10.18128/d030.v6.0", "10.1111/cgf.12142", "10.1109/tvcg.2009.114", "10.1111/j.1467-8659.2009.01677.x", "10.1109/mcse.2007.55", "10.1109/icde.2012.16", "10.1198/106186008x320465", "10.1145/1835804.1835868", "10.1559/1523040054738936", "10.1109/tvcg.2019.2934432", "10.1109/34.1000236", "10.1109/infvis.2005.1532136", "10.1109/tvcg.2012.128", "10.1109/pacificvis.2017.8031573", "10.1109/tvcg.2016.2598920", "10.1109/tvcg.2018.2865021", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1109/sp.2009.22", "10.1109/tvcg.2017.2744184", "10.1145/773153.773173", "10.1145/2660267.2660348", "10.1109/icdmw.2009.93", "10.1016/j.jtrangeo.2015.09.001", "10.1109/icde.2010.5447831", "10.1007/11681878\\_14", "10.1111/cgf.13409", "10.1007/s13278-014-0205-5", "10.1109/tvcg.2011.163", "10.1145/3035918.3035940", "10.1109/sp.2008.33", "10.1145/2882903.2882931"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934208", "title": "Evaluating Perceptual Bias During Geometric Scaling of Scatterplots", "year": "2019", "conferenceName": "VAST", "authors": "Yating Wei;Honghui Mei;Ying Zhao;Shuyue Zhou;Bingru Lin;Haojing Jiang;Wei Chen", "citationCount": "5", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou 310058, Zhejiang, Peoples R China. Zhao, Y (Corresponding Author), Cent South Univ, Sch Comp Sci \\& Engn, Changsha 410083, Hunan, Peoples R China. Wei, Yating; Mei, Honghui; Zhou, Shuyue; Lin, Bingru; Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou 310058, Zhejiang, Peoples R China. Zhao, Ying; Jiang, Haojing, Cent South Univ, Sch Comp Sci \\& Engn, Changsha 410083, Hunan, Peoples R China.", "countries": "China", "abstract": "Scatterplots are frequently scaled to fit display areas in multi-view and multi-device data analysis environments. A common method used for scaling is to enlarge or shrink the entire scatterplot together with the inside points synchronously and proportionally. This process is called geometric scaling. However, geometric scaling of scatterplots may cause a perceptual bias, that is, the perceived and physical values of visual features may be dissociated with respect to geometric scaling. For example, if a scatterplot is projected from a laptop to a large projector screen, then observers may feel that the scatterplot shown on the projector has fewer points than that viewed on the laptop. This paper presents an evaluation study on the perceptual bias of visual features in scatterplots caused by geometric scaling. The study focuses on three fundamental visual features (i.e., numerosity, correlation, and cluster separation) and three hypotheses that are formulated on the basis of our experience. We carefully design three controlled experiments by using well-prepared synthetic data and recruit participants to complete the experiments on the basis of their subjective experience. With a detailed analysis of the experimental results, we obtain a set of instructive findings. First, geometric scaling causes a bias that has a linear relationship with the scale ratio. Second, no significant difference exists between the biases measured from normally and uniformly distributed scatterplots. Third, changing the point radius can correct the bias to a certain extent. These findings can be used to inspire the design decisions of scatterplots in various scenarios.", "keywords": "Evaluation,scatterplot,geometric scaling,bias,perceptual consistency", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934208", "refList": ["10.2307/2288843", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2014.2346979", "10.1126/science.216.4550.1138", "10.1109/tvcg.2017.2744138", "10.1111/j.1467-8659.2009.01467.x", "10.1145/2491568.2491577", "10.1109/mwc.2018.1700325", "10.1145/2449396.2449439", "10.1109/tvcg.2011.127", "10.1109/tvcg.2011.229", "10.1167/15.5.4", "10.1073/pnas.1113195108", "10.1145/2702123.2702545", "10.1109/vast.2009.5332628", "10.1109/tvcg.2018.2800013", "10.1007/s12650-018-0530-2", "10.1016/s0042-6989(97)00340-4", "10.1109/vast.2010.5652460", "10.1109/mcg.2018.2879067", "10.1109/tvcg.2018.2864912", "10.1109/pacificvis.2010.5429604", "10.1109/tcst.2018.2819965", "10.1167/10.2.10", "10.1145/2470654.2481318", "10.1145/1842993.1843002", "10.1167/12.6.8", "10.1109/tvcg.2013.124", "10.1057/ivs.2008.13", "10.1109/tvcg.2007.70596", "10.1109/tvcg.2018.2865266", "10.1145/3173574.3173664", "10.1109/tvcg.2015.2467671", "10.1016/j.cag.2017.07.004", "10.1177/0956797613501520", "10.1109/tvcg.2018.2865020", "10.1111/j.1467-8659.2012.03125.x", "10.3758/bf03205986", "10.3758/s13423-016-1174-7", "10.1109/tvcg.2017.2680452", "10.1109/mc.2006.109", "10.1109/tvcg.2017.2744098", "10.1038/srep32810", "10.1016/j.visres.2013.06.006", "10.1002/jhbs.20078", "10.1109/tvcg.2006.163", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1016/j.cognition.2007.10.009", "10.1109/tvcg.2018.2865142", "10.3758/app.72.7.1839", "10.1109/tvcg.2018.2810918", "10.1016/j.jvlc.2017.10.001", "10.1145/2682623", "10.1109/tvcg.2018.2864884", "10.1145/3025453.3025984", "10.1109/tvcg.2006.184", "10.1109/tvcg.2013.153", "10.1016/j.jvlc.2018.08.003", "10.1109/tvcg.2016.2520921", "10.1111/cgf.13446", "10.1017/s0022381612000187", "10.1145/2702123.2702406", "10.1109/vast.2012.6400487", "10.1109/tvcg.2013.183", "10.1177/1473871611415997", "10.1145/2702123.2702585", "10.1145/2993901.2993903", "10.1109/tvcg.2013.120", "10.1111/cgf.12632", "10.1145/1385569.1385602", "10.1109/tvcg.2017.2754480", "10.1111/j.1467-8659.2009.01694.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934300", "title": "GUIRO: User-Guided Matrix Reordering", "year": "2019", "conferenceName": "VAST", "authors": "Michael Behrisch;Tobias Schreck;Hanspeter Pfister", "citationCount": "1", "affiliation": "Behrisch, M (Corresponding Author), Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Behrisch, Michael; Pfister, Hanspeter, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Schreck, Tobias, Graz Univ Technol, Graz, Austria.", "countries": "USA;Austria", "abstract": "Matrix representations are one of the main established and empirically proven to be effective visualization techniques for relational (or network) data. However, matrices\u2014similar to node-link diagrams\u2014are most effective if their layout reveals the underlying data topology. Given the many developed algorithms, a practical problem arises: \u201cWhich matrix reordering algorithm should I choose for my dataset at hand?\u201d To make matters worse, different reordering algorithms applied to the same dataset may let significantly different visual matrix patterns emerge. This leads to the question of trustworthiness and explainability of these fully automated, often heuristic, black-box processes. We present GUIRO, a Visual Analytics system that helps novices, network analysts, and algorithm designers to open the black-box. Users can investigate the usefulness and expressiveness of 70 accessible matrix reordering algorithms. For network analysts, we introduce a novel model space representation and two interaction techniques for a user-guided reordering of rows or columns, and especially groups thereof (submatrix reordering). These novel techniques contribute to the understanding of the global and local dataset topology. We support algorithm designers by giving them access to 16 reordering quality metrics and visual exploration means for comparing reordering implementations on a row/column permutation level. We evaluated GUIRO in a guided explorative user study with 12 subjects, a case study demonstrating its usefulness in a real-world scenario, and through an expert study gathering feedback on our design decisions. We found that our proposed methods help even inexperienced users to understand matrix patterns and allow a user-guided steering of reordering algorithms. GUIRO helps to increase the transparency of matrix reordering algorithms, thus helping a broad range of users to get a better insight into the complex reordering process, in turn supporting data and reordering algorithm insights.", "keywords": "Visual Analytics,matrix,black-box algorithms,seriation,ordering,sorting,steerable algorithm,interaction,2D projection", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934300", "refList": ["10.1109/tvcg.2007.70582", "10.2307/276978", "10.1109/tvcg.2008.61", "10.3390/su10040973", "10.1101/121889", "10.1145/1345448.1345453", "10.1186/s12879-014-0695-9", "10.1186/1471-2105-9-155", "10.1145/1124772.1124891", "10.1109/tvcg.2010.159", "10.1111/j.2044-8317.1974.tb00534.x", "10.1109/tpami.2015.2470671", "10.1198/000313005x22770", "10.1109/tvcg.2012.219", "10.1002/sam.10071", "10.1016/j.ejor.2016.08.066", "10.1007/s00265-003-0651-y", "10.1109/biovis.2013.6664342", "10.3115/v1/p14-1062", "10.3389/fpsyg.2017.01349", "10.1109/tpami.2004.1265866", "10.1057/palgrave.ivs.9500086", "10.1007/s004260000031", "10.1111/cgf.12935", "10.1145/800195.805928", "10.1109/tvcg.2014.2346279", "10.2312/eurovisstar.20141174", "10.1109/tvcg.2012.256", "10.1093/bioinformatics/17.suppl\\_1.s22", "10.1109/infvis.2004.46", "10.1007/978-3-319-06793-3\\_5", "10.1109/tvcg.2006.147", "10.1109/tvcg.2015.2468078", "10.1093/bioinformatics/bti141", "10.1145/1168149.1168168", "10.1109/tvcg.2017.2745978", "10.1177/1473871613513228", "10.1109/mcg.2014.62", "10.1109/tvcg.2006.166", "10.1145/2470654.2470724", "10.1177/154193120605000909", "10.1109/sibgrapi.2007.21", "10.1145/3065386", "10.1109/tvcg.2006.160", "10.1287/opre.20.5.993", "10.1111/cgf.13446", "10.1057/palgrave.ivs.9500092", "10.1145/568522.568523", "10.1109/mcg.2013.66", "10.1109/tvcg.2018.2865940", "10.1109/tvcg.2012.208"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934796", "title": "Improving the Robustness of Scagnostics", "year": "2019", "conferenceName": "InfoVis", "authors": "Yunhai Wang;Zeyu Wang 0005;Tingting Liu;Michael Correll;Zhanglin Cheng;Oliver Deussen;Michael Sedlmair", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Jinan, Peoples R China. Wang, Yunhai; Wang, Zeyu; Liu, Tingting, Shandong Univ, Jinan, Peoples R China. Wang, Zeyu; Cheng, Zhanglin; Deussen, Oliver, SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Peoples R China. Correll, Michael, Tableau Res, Seattle, WA USA. Deussen, Oliver, Konstanz Univ, Constance, Germany. Sedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "In this paper, we examine the robustness of scagnostics through a series of theoretical and empirical studies. First, we investigate the sensitivity of scagnostics by employing perturbing operations on more than 60M synthetic and real-world scatterplots. We found that two scagnostic measures, Outlying and Clumpy, are overly sensitive to data binning. To understand how these measures align with human judgments of visual features, we conducted a study with 24 participants, which reveals that i) humans are not sensitive to small perturbations of the data that cause large changes in both measures, and ii) the perception of clumpiness heavily depends on per-cluster topologies and structures. Motivated by these results, we propose Robust Scagnostics (RScag) by combining adaptive binning with a hierarchy-based form of scagnostics. An analysis shows that RScag improves on the robustness of original scagnostics, aligns better with human judgments, and is equally fast as the traditional scagnostic measures.", "keywords": "Scagnostics,scatterplots,sensitivity analysis,Robust Scagnostics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934796", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2346979", "10.1111/j.1467-8659.2009.01467.x", "10.1109/vast.2008.4677368", "10.2307/2289444", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/vast.2009.5332628", "10.1111/insr.12095\\_11", "10.1109/tvcg.2010.184", "10.1109/tvcg.2015.2467323", "10.1109/vast.2010.5652460", "10.1111/j.1467-8659.2012.03069.x", "10.1145/1842993.1843002", "10.1198/106186008x320465", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.20", "10.1111/cgf.12641", "10.1109/tvcg.2012.128", "10.1109/tvcg.2015.2467671", "10.1057/palgrave.ivs.9500091", "10.1007/978-1-4612-4400-4", "10.1111/cgf.13176", "10.1002/0470870958", "10.1109/tvcg.2018.2864907", "10.1111/j.1467-8659.2012.03125.x", "10.1109/vast.2012.6400490", "10.1109/infvis.2005.1532142", "10.1109/vast.2010.5652433", "10.1109/vast.2009.5332611", "10.1201/9781315140919", "10.1145/2858036.2858155", "10.1109/ldav.2013.6675164", "10.1515/itit-2014-1070", "10.1111/cgf.13684", "10.1109/tpami.1979.4766909", "10.2307/2289936", "10.1109/pacificvis.2014.42", "10.1109/tvcg.2016.2598467", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2006.94", "10.1109/tvcg.2014.2346572", "10.1111/cgf.12632", "10.1109/tvcg.2017.2744339"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/vast47406.2019.8986917", "title": "VIANA: Visual Interactive Annotation of Argumentation", "year": "2019", "conferenceName": "VAST", "authors": "Fabian Sperrle;Rita Sevastjanova;Rebecca Kehlbeck;Mennatallah El-Assady", "citationCount": "0", "affiliation": "Sperrle, F (Corresponding Author), Univ Konstanz, Constance, Germany. Sperrle, Fabian; Sevastjanova, Rita; Kehlbeck, Rebecca; El-Assady, Mennatallah, Univ Konstanz, Constance, Germany.", "countries": "Germany", "abstract": "Argumentation Mining addresses the challenging tasks of identifying boundaries of argumentative text fragments and extracting their relationships. Fully automated solutions do not reach satisfactory accuracy due to their insufficient incorporation of semantics and domain knowledge. Therefore, experts currently rely on time-consuming manual annotations. In this paper, we present a visual analytics system that augments the manual annotation process by automatically suggesting which text fragments to annotate next. The accuracy of those suggestions is improved over time by incorporating linguistic knowledge and language modeling to learn a measure of argument similarity from user interactions. Based on a long-term collaboration with domain experts, we identify and model five high-level analysis tasks. We enable close reading and note-taking, annotation of arguments, argument reconstruction, extraction of argument relations, and exploration of argument graphs. To avoid context switches, we transition between all views through seamless morphing, visually anchoring all text- and graph-based layers. We evaluate our system with a two-stage expert user study based on a corpus of presidential debates. The results show that experts prefer our system over existing solutions due to the speedup provided by the automatic suggestions and the tight integration between text and graph views.", "keywords": "Argumentation annotation,machine learning,user interaction,layered interfaces,semantic transitions", "link": "http://dx.doi.org/10.1109/VAST47406.2019.8986917", "refList": ["10.1007/978-3-642-40624-9\\_1", "10.3233/978-1-61499-436-7-185", "10.1145/371920.372071", "10.3233/978-1-61499-906-5-4", "10.1109/tvcg.2015.2467759", "10.18653/v1/d15-1050", "10.2307/2529310", "10.1109/mic.2003.1167344", "10.1145/312624.312682", "10.1145/2850417", "10.18653/v1/p17-2039", "10.1016/j.eswa.2016.02.013", "10.1007/978-0-387-85820-3\\_3", "10.1007/s11412-009-9080-x", "10.1109/tvcg.2008.127", "10.1145/2207676.2207741", "10.1109/cit.2012.217", "10.3233/aac-170022", "10.1109/tvcg.2017.2745080", "10.1007/978-3-319-44039-2\\_6", "10.1145/3290605.3300233", "10.1007/978-94-017-0431-1", "10.3233/978-1-61499-906-5-313", "10.1142/s0218213004001922", "10.1109/tvcg.2012.262", "10.1177/001316446002000104", "10.1109/tvcg.2018.2834341", "10.3233/978-1-61499-436-7-463", "10.1145/1772690.1772773", "10.1109/tvcg.2014.2346677", "10.1145/2523813", "10.1162/153244303322533223", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2015.2467531", "10.1109/tvcg.2018.2864769", "10.1007/978-3-319-90092-6\\_14", "10.1137/1.9781611972801.19", "10.1109/vast.2012.6400485", "10.1007/s10579-013-9215-6", "10.3115/v1/d14-1162", "10.1111/cgf.13446", "10.1109/tvcg.2006.156", "10.1111/cgf.13092", "10.1145/2645710.2645759", "10.1109/bigdata.2017.8258140", "10.1145/2669557.2669572", "10.2312/eurovisstar.20151113"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030376", "title": "Insight Beyond Numbers: The Impact of Qualitative Factors on Visual Data Analysis", "year": "2020", "conferenceName": "VAST", "authors": "Benjamin Karer;Hans Hagen;Dirk J. Lehmann", "citationCount": "0", "affiliation": "Karer, B (Corresponding Author), Fed Criminal Police Off Germany, Wiesbaden, Germany. Karer, Benjamin, Fed Criminal Police Off Germany, Wiesbaden, Germany. Hagen, Hans, TU Kaiserslautern, Kaiserslautern, Germany. Lehmann, Dirk J., Ostfalia Univ Appl Sci, Wolfenbuttel, Germany. Lehmann, Dirk J., IAV GmbH, Berlin, Germany.", "countries": "Germany", "abstract": "As of today, data analysis focuses primarily on the findings to be made inside the data and concentrates less on how those findings relate to the domain of investigation. Contemporary visualization as a field of research shows a strong tendency to adopt this data-centrism. Despite their decisive influence on the analysis result, qualitative aspects of the analysis process such as the structure, soundness, and complexity of the applied reasoning strategy are rarely discussed explicitly. We argue that if the purpose of visualization is the provision of domain insight rather than the depiction of data analysis results, a holistic perspective requires a qualitative component to to be added to the discussion of quantitative and human factors. To support this point, we demonstrate how considerations of qualitative factors in visual analysis can be applied to obtain explanations and possible solutions for a number of practical limitations inherent to the data-centric perspective on analysis. Based on this discussion of what we call qualitative visual analysis, we develop an inside-outside principle of nested levels of context that can serve as a conceptual basis for the development of visualization systems that optimally support the emergence of insight during analysis.", "keywords": "Visualization,Reasoning,Qualitative Aspects", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030376", "refList": ["10.1057/ivs.2009.22", "10.1109/tvcg.2015.2467732", "10.1109/infvis.2000.885092", "10.1109/vast.2014.7042482", "10.1145/1498700.1498704", "10.1109/tvcg.2009.108", "10.1109/tvcg.2018.2829750", "10.1037/0033-295x.111.4.1036", "10.1109/tvcg.2012.199", "10.1109/38.267476", "10.1007/978-3-540-70956-5\\_2", "10.1109/tvcg.2015.2467613", "10.1109/tvcg.2015.2467195", "10.1109/tvcg.2014.2346419", "10.1109/vast.2017.8585669", "10.1109/tvcg.2010.132", "10.1145/22949.22950", "10.1109/tvcg.2014.2346984", "10.1017/cbo9780511816772", "10.1109/mcg.2003.1231171", "10.1111/coin.12227", "10.1109/tvcg.2018.2865138", "10.1109/38.788803", "10.1109/tvcg.2018.2864849", "10.1109/mcg.2012.120", "10.1109/tvcg.2007.70535", "10.1162/neco.2008.12-06-420", "10.1109/tvcg.2018.2865240", "10.1089/tmj.2010.0114", "10.1109/beliv.2018.8634267", "10.1109/tvcg.2014.2346481", "10.1207/s15327809jls0402\\_2", "10.1109/tvcg.2006.80", "10.1145/2858036.2858280", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2011.52", "10.1016/j.cag.2014.03.002", "10.1109/tvcg.2015.2513410", "10.1111/j.1756-8765.2011.01150.x", "10.1109/infvis.2005.1532142", "10.1002/spe.4380211102", "10.1186/s41235-018-0120-9", "10.1073/pnas.1807180116", "10.1109/tvcg.2012.133", "10.1109/tvcg.2012.273", "10.1111/j.1467-8659.2011.01928.x", "10.1145/353485.353486", "10.1109/tvcg.2015.2462356", "10.1109/mcg.2019.2923483", "10.1111/cgf.13264", "10.1057/palgrave.ivs.9500070", "10.1145/1168149.1168158", "10.1109/mcg.2019.2961716", "10.1016/s0167-739x(96)00029-5", "10.1111/j.1467-8659.2009.01667.x", "10.1177/1473871611415989", "10.1109/tvcg.2013.234", "10.1109/iv.2012.33", "10.1111/cgf.13446", "10.1057/ivs.2008.28", "10.1111/cgf.12379", "10.1037/0033-295x.112.1.159", "10.1109/tvcg.2010.161", "10.1109/tvcg.2017.2744319", "10.1145/989863.989880", "10.1007/s11390-016-1663-1", "10.1177/1473871615609787", "10.1016/j.cola.2019.100911"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030395", "title": "Towards Modeling Visualization Processes as Dynamic Bayesian Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Christian Heine 0002", "citationCount": "0", "affiliation": "Heine, C (Corresponding Author), Univ Leipzig, Leipzig, Germany. Heine, Christian, Univ Leipzig, Leipzig, Germany.", "countries": "Germany", "abstract": "Visualization designs typically need to be evaluated with user studies, because their suitability for a particular task is hard to predict. What the field of visualization is currently lacking are theories and models that can be used to explain why certain designs work and others do not. This paper outlines a general framework for modeling visualization processes that can serve as the first step towards such a theory. It surveys related research in mathematical and computational psychology and argues for the use of dynamic Bayesian networks to describe these time-dependent, probabilistic processes. It is discussed how these models could be used to aid in design evaluation. The development of concrete models will be a long process. Thus, the paper outlines a research program sketching how to develop prototypes and their extensions from existing models, controlled experiments, and observational studies.", "keywords": "Visualization,model building,perception,cognition,dynamic Bayesian networks", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030395", "refList": ["10.1057/ivs.2009.22", "10.1109/tvcg.2015.2467732", "10.1109/infvis.2000.885092", "10.1109/vast.2014.7042482", "10.1145/1498700.1498704", "10.1109/tvcg.2009.108", "10.1109/tvcg.2018.2829750", "10.1037/0033-295x.111.4.1036", "10.1109/tvcg.2012.199", "10.1111/cgf.13899", "10.1109/38.267476", "10.1007/978-3-540-70956-5\\_2", "10.1109/tvcg.2015.2467613", "10.1109/tvcg.2015.2467195", "10.1109/tvcg.2014.2346419", "10.1145/3290605.3300562", "10.1109/vl.1996.545307", "10.1109/vast.2017.8585669", "10.1109/infvis.2003.1249004", "10.1109/tvcg.2010.132", "10.1145/22949.22950", "10.1109/tvcg.2014.2346984", "10.1145/3290605.3300418", "10.1017/cbo9780511816772", "10.1109/mcg.2003.1231171", "10.1111/coin.12227", "10.1109/tvcg.2018.2865138", "10.1109/38.788803", "10.1109/tvcg.2018.2864849", "10.1109/mcg.2012.120", "10.1109/tvcg.2007.70535", "10.1162/neco.2008.12-06-420", "10.1109/tvcg.2018.2865240", "10.1089/tmj.2010.0114", "10.1109/beliv.2018.8634267", "10.1109/tvcg.2014.2346481", "10.1207/s15327809jls0402\\_2", "10.1109/tvcg.2006.80", "10.1145/2858036.2858280", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2011.52", "10.1016/j.cag.2014.03.002", "10.1109/tvcg.2015.2513410", "10.1111/j.1756-8765.2011.01150.x", "10.1109/infvis.2005.1532142", "10.1002/spe.4380211102", "10.1186/s41235-018-0120-9", "10.1073/pnas.1807180116", "10.1109/tvcg.2012.133", "10.1109/tvcg.2012.273", "10.1111/j.1467-8659.2011.01928.x", "10.1145/353485.353486", "10.1109/tvcg.2015.2462356", "10.1109/mcg.2019.2923483", "10.1111/cgf.13264", "10.1057/palgrave.ivs.9500070", "10.1145/1168149.1168158", "10.1109/mcg.2019.2961716", "10.1016/s0167-739x(96)00029-5", "10.1111/j.1467-8659.2009.01667.x", "10.1177/1473871611415989", "10.1109/tvcg.2013.234", "10.1109/iv.2012.33", "10.1111/cgf.13446", "10.1057/ivs.2008.28", "10.1111/cgf.12379", "10.1037/0033-295x.112.1.159", "10.1017/cb09781139033916.005", "10.1109/tvcg.2010.161", "10.1109/tvcg.2017.2744319", "10.1145/989863.989880", "10.1007/s11390-016-1663-1", "10.1177/1473871615609787", "10.1016/j.cola.2019.100911", "10.1057/palgrave.ivs.9500025"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.8199", "year": "2020", "title": "Efficient Morphing of Shape-preserving Star Coordinates", "conferenceName": "PacificVis", "authors": "Vladimir Molchanov;Sagad Hamid;Lars Linsen", "citationCount": "0", "affiliation": "Molchanov, V (Corresponding Author), Westfalische Wilhelms Univ Munster, Munster, Germany.\nMolchanov, Vladimir; Hamid, Sagad; Linsen, Lars, Westfalische Wilhelms Univ Munster, Munster, Germany.", "countries": "Germany", "abstract": "Data tours follow an exploratory multi-dimensional data visualization concept that provides animations of projections of the multidimensional data to a 2D visual space. To create an animation, a sequence of key projections is provided and morphings between each pair of consecutive key projections are computed, which then can be stitched together to form the data tour. The morphings should be smooth so that a user can easily follow the transformations, and their computations shall be fast to allow for their integration into an interactive visual exploration process. Moreover, if the key projections are chosen to satisfy additional conditions, it is desirable that these conditions are maintained during morphing. Shape preservation is such a desirable condition, as it avoids shape distortions that may otherwise be caused by a projection. We develop a novel efficient morphing algorithms for computing shape-preserving data tours, i.e., data tours constructed for a sequence of shape-preserving linear projections. We propose a stepping strategy for the morphing to avoid discontinuities in the evolution of the projections, where we represent the linear projections using a star-coordinates system. Our algorithms are less computationally involved, produce smoother morphings, and require less user-defined parameter settings than existing state-of-the-art approaches.", "keywords": "Human-centered computing; Visualization; Visualization techniques", "link": "https://doi.org/10.1109/PacificVis48177.2020.8199", "refList": ["10.2312/eurovisshort.20141152", "10.1109/tvcg.2013.182", "10.1109/tvcg.2008.153", "10.1109/tvcg.2015.2467591", "10.4135/9781412985130", "10.1111/cgf.12878", "10.1016/j.cag.2016.08.007", "10.1080/10618600.1995.10474674", "10.1109/infvis.2003.1249004", "10.1002/9781118445112.stat06472", "10.1109/visual.1997.663916", "10.1111/cgf.12117", "10.1137/0906011", "10.1109/tvcg.2018.2865118", "10.1126/science.290.5500.2319", "10.1111/cgf.12845", "10.1111/j.1467-8659.2012.03125.x", "10.1109/tvcg.2017.2705189", "10.1002/0471725293", "10.1111/cgf.12876", "10.1111/cgf.13404", "10.2307/2289161", "10.1111/cgf.13446", "10.1198/106186008x318440", "10.2307/1390747", "10.1109/tvcg.2012.35", "10.1109/tvcg.2015.2467132", "10.1109/tvcg.2006.94", "10.1109/t-c.1974.224051", "10.1109/pacificvis.2019.00018", "10.1109/tvcg.2017.2744339", "10.1109/tsmcb.2005.850151"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2019.00018", "year": "2019", "title": "Scatterplot Summarization by Constructing Fast and Robust Principal Graphs from Skeletons", "conferenceName": "PacificVis", "authors": "Jos{\\'{e}} Matute;Marcel Fischer;Alexandru C. Telea;Lars Linsen", "citationCount": "1", "affiliation": "Matute, J (Corresponding Author), Univ Munster, Munster, Germany.\nMatute, Jose; Fischer, Marcel; Linsen, Lars, Univ Munster, Munster, Germany.\nTelea, Alexandru C., Univ Groningen, Groningen, Netherlands.", "countries": "Germany;Netherlands", "abstract": "Principal curves are a long-standing and well-known method for summarizing large scatterplots. They are defined as self-consistent curves (or curve sets in the more general case) that locally pass through the middle of the scatterplot data. However, computing principal curves that capture well complex scatterplot topologies and are robust to noise is hard and/or slow for large scatterplots. We present a fast and robust approach for computing principal graphs (a generalization of principal curves for more complex topologies) inspired by the similarity to medial descriptors (curves locally centered in a shape). Compared to state-of-the-art methods for computing principal graphs, we outperform these in terms of computational scalability and robustness to noise and resolution. We also demonstrate the advantages of our method over other scatterplot summarization approaches.", "keywords": "", "link": "https://doi.org/10.1109/PacificVis.2019.00018", "refList": ["10.1016/j.cag.2014.01.006", "10.1016/s0167-8655(02)00032-6", "10.1006/jmva.2000.1917", "10.1111/cgf.12386", "10.1109/vast.2008.4677367", "10.1109/tpami.2004.1261076", "10.1111/cgf.12865", "10.1109/tvcg.2010.213", "10.1109/tvcg.2014.2346455", "10.1109/tvcg.2011.233", "10.1111/j.1467-8659.2009.01680.x", "10.1109/tvcg.2016.2515611", "10.1109/tit.1982.1056489", "10.1109/34.899944", "10.1007/bf01889678", "10.1137/s0036144599352836", "10.1145/2858036.2858155", "10.1002/jhbs.20078", "10.1198/jcgs.2011.09224", "10.1109/tvcg.2017.2744184", "10.1057/ivs.2010.2", "10.1111/j.1467-8659.2012.03107.x", "10.2307/2289936", "10.1109/pacificvis.2014.42", "10.1109/tpami.2008.21", "10.1111/cgf.13446", "10.2307/2290446", "10.1109/34.982884", "10.1109/tvcg.2006.94", "10.2307/2288711", "10.1117/12.304651", "10.5201/ipol.2013.87", "10.1109/mlsp.2008.4685520"], "wos": 1, "children": [{"doi": "10.1109/pacificvis48177.2020.8199", "year": "2020", "title": "Efficient Morphing of Shape-preserving Star Coordinates", "conferenceName": "PacificVis", "authors": "Vladimir Molchanov;Sagad Hamid;Lars Linsen", "citationCount": "0", "affiliation": "Molchanov, V (Corresponding Author), Westfalische Wilhelms Univ Munster, Munster, Germany.\nMolchanov, Vladimir; Hamid, Sagad; Linsen, Lars, Westfalische Wilhelms Univ Munster, Munster, Germany.", "countries": "Germany", "abstract": "Data tours follow an exploratory multi-dimensional data visualization concept that provides animations of projections of the multidimensional data to a 2D visual space. To create an animation, a sequence of key projections is provided and morphings between each pair of consecutive key projections are computed, which then can be stitched together to form the data tour. The morphings should be smooth so that a user can easily follow the transformations, and their computations shall be fast to allow for their integration into an interactive visual exploration process. Moreover, if the key projections are chosen to satisfy additional conditions, it is desirable that these conditions are maintained during morphing. Shape preservation is such a desirable condition, as it avoids shape distortions that may otherwise be caused by a projection. We develop a novel efficient morphing algorithms for computing shape-preserving data tours, i.e., data tours constructed for a sequence of shape-preserving linear projections. We propose a stepping strategy for the morphing to avoid discontinuities in the evolution of the projections, where we represent the linear projections using a star-coordinates system. Our algorithms are less computationally involved, produce smoother morphings, and require less user-defined parameter settings than existing state-of-the-art approaches.", "keywords": "Human-centered computing; Visualization; Visualization techniques", "link": "https://doi.org/10.1109/PacificVis48177.2020.8199", "refList": ["10.2312/eurovisshort.20141152", "10.1109/tvcg.2013.182", "10.1109/tvcg.2008.153", "10.1109/tvcg.2015.2467591", "10.4135/9781412985130", "10.1111/cgf.12878", "10.1016/j.cag.2016.08.007", "10.1080/10618600.1995.10474674", "10.1109/infvis.2003.1249004", "10.1002/9781118445112.stat06472", "10.1109/visual.1997.663916", "10.1111/cgf.12117", "10.1137/0906011", "10.1109/tvcg.2018.2865118", "10.1126/science.290.5500.2319", "10.1111/cgf.12845", "10.1111/j.1467-8659.2012.03125.x", "10.1109/tvcg.2017.2705189", "10.1002/0471725293", "10.1111/cgf.12876", "10.1111/cgf.13404", "10.2307/2289161", "10.1111/cgf.13446", "10.1198/106186008x318440", "10.2307/1390747", "10.1109/tvcg.2012.35", "10.1109/tvcg.2015.2467132", "10.1109/tvcg.2006.94", "10.1109/t-c.1974.224051", "10.1109/pacificvis.2019.00018", "10.1109/tvcg.2017.2744339", "10.1109/tsmcb.2005.850151"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.13684", "year": "2019", "title": "ClustMe: A Visual Quality Measure for Ranking Monochrome Scatterplots based on Cluster Patterns", "conferenceName": "EuroVis", "authors": "Mostafa M. Abbas;Micha{\\\"{e}}l Aupetit;Michael Sedlmair;Halima Bensmail", "citationCount": "4", "affiliation": "Abbas, MM (Corresponding Author), HBKU, QCRI, Doha, Qatar.\nAbbas, Mostafa M.; Aupetit, Michael; Bensmail, Halima, HBKU, QCRI, Doha, Qatar.\nSedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany.", "countries": "Qatar;Germany", "abstract": "We propose ClustMe, a new visual quality measure to rank monochrome scatterplots based on cluster patterns. ClustMe is based on data collected from a human-subjects study, in which 34 participants judged synthetically generated cluster patterns in 1000 scatterplots. We generated these patterns by carefully varying the free parameters of a simple Gaussian Mixture Model with two components, and asked the participants to count the number of clusters they could see (1 or more than 1). Based on the results, we form ClustMe by selecting the model that best predicts these human judgments among 7 different state-of-the-art merging techniques (Demp). To quantitatively evaluate ClustMe, we conducted a second study, in which 31 human subjects ranked 435 pairs of scatterplots of real and synthetic data in terms of cluster patterns complexity. We use this data to compare ClustMe's performance to 4 other state-of-the-art clustering measures, including the well-known Clumpiness scagnostics. We found that of all measures, ClustMe is in strongest agreement with the human rankings.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13684", "refList": ["10.1109/tvcg.2014.2330617", "10.1109/tvcg.2014.2346979", "10.1109/tvcg.2017.2744138", "10.1109/tvcg.2017.2701829", "10.2307/2529310", "10.1109/tvcg.2011.229", "10.1109/vast.2011.6102437", "10.1068/p030033", "10.1016/j.jvcir.2011.01.005", "10.1109/tnn.2005.845141", "10.1145/2669557.2669559", "10.1167/8.7.6", "10.1145/2993901.2993907", "10.1145/2803140.2803143", "10.1145/1842993.1843002", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.124", "10.1057/ivs.2008.13", "10.1109/vast.2012.6400488", "10.1111/j.1467-9574.2008.00412.x", "10.1214/aos/1176344136", "10.1109/tvcg.2015.2467671", "10.3138/y308-2422-8615-1233", "10.1111/j.1467-8659.2012.03125.x", "10.1177/001316446002000104", "10.1145/2858036.2858155", "10.1007/s11634-010-0058-3", "10.1214/009053605000000417", "10.1109/tvcg.2009.153", "10.1023/a:1018510926151", "10.1109/tvcg.2014.2346978", "10.1007/s10816-016-9307-x", "10.1177/0301006615602599", "10.1109/pacificvis.2014.42", "10.1162/neco.1991.3.2.246", "10.1007/3-540-44491-2\\_3", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2018.2846735", "10.2307/2291604", "10.1109/inf0vis.2005.14", "10.1198/016214502760047131", "10.1007/s00180-008-0119-7", "10.1109/t-c.1974.224051", "10.1111/cgf.12632", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934796", "title": "Improving the Robustness of Scagnostics", "year": "2019", "conferenceName": "InfoVis", "authors": "Yunhai Wang;Zeyu Wang 0005;Tingting Liu;Michael Correll;Zhanglin Cheng;Oliver Deussen;Michael Sedlmair", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Jinan, Peoples R China. Wang, Yunhai; Wang, Zeyu; Liu, Tingting, Shandong Univ, Jinan, Peoples R China. Wang, Zeyu; Cheng, Zhanglin; Deussen, Oliver, SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Peoples R China. Correll, Michael, Tableau Res, Seattle, WA USA. Deussen, Oliver, Konstanz Univ, Constance, Germany. Sedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "In this paper, we examine the robustness of scagnostics through a series of theoretical and empirical studies. First, we investigate the sensitivity of scagnostics by employing perturbing operations on more than 60M synthetic and real-world scatterplots. We found that two scagnostic measures, Outlying and Clumpy, are overly sensitive to data binning. To understand how these measures align with human judgments of visual features, we conducted a study with 24 participants, which reveals that i) humans are not sensitive to small perturbations of the data that cause large changes in both measures, and ii) the perception of clumpiness heavily depends on per-cluster topologies and structures. Motivated by these results, we propose Robust Scagnostics (RScag) by combining adaptive binning with a hierarchy-based form of scagnostics. An analysis shows that RScag improves on the robustness of original scagnostics, aligns better with human judgments, and is equally fast as the traditional scagnostic measures.", "keywords": "Scagnostics,scatterplots,sensitivity analysis,Robust Scagnostics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934796", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2346979", "10.1111/j.1467-8659.2009.01467.x", "10.1109/vast.2008.4677368", "10.2307/2289444", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/vast.2009.5332628", "10.1111/insr.12095\\_11", "10.1109/tvcg.2010.184", "10.1109/tvcg.2015.2467323", "10.1109/vast.2010.5652460", "10.1111/j.1467-8659.2012.03069.x", "10.1145/1842993.1843002", "10.1198/106186008x320465", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.20", "10.1111/cgf.12641", "10.1109/tvcg.2012.128", "10.1109/tvcg.2015.2467671", "10.1057/palgrave.ivs.9500091", "10.1007/978-1-4612-4400-4", "10.1111/cgf.13176", "10.1002/0470870958", "10.1109/tvcg.2018.2864907", "10.1111/j.1467-8659.2012.03125.x", "10.1109/vast.2012.6400490", "10.1109/infvis.2005.1532142", "10.1109/vast.2010.5652433", "10.1109/vast.2009.5332611", "10.1201/9781315140919", "10.1145/2858036.2858155", "10.1109/ldav.2013.6675164", "10.1515/itit-2014-1070", "10.1111/cgf.13684", "10.1109/tpami.1979.4766909", "10.2307/2289936", "10.1109/pacificvis.2014.42", "10.1109/tvcg.2016.2598467", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2006.94", "10.1109/tvcg.2014.2346572", "10.1111/cgf.12632", "10.1109/tvcg.2017.2744339"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.14001", "year": "2020", "title": "Sunspot Plots: Model-based Structure Enhancement for Dense Scatter Plots", "conferenceName": "EuroVis", "authors": "Thomas Trautner;Fabian Bolte;Sergej Stoppel;Stefan Bruckner", "citationCount": "0", "affiliation": "Trautner, T (Corresponding Author), Univ Bergen, Bergen, Norway.\nTrautner, T.; Bolte, F.; Stoppel, S.; Bruckner, S., Univ Bergen, Bergen, Norway.", "countries": "Norway", "abstract": "Scatter plots are a powerful and well-established technique for visualizing the relationships between two variables as a collection of discrete points. However, especially when dealing with large and dense data, scatter plots often exhibit problems such as overplotting, making the data interpretation arduous. Density plots are able to overcome these limitations in highly populated regions, but fail to provide accurate information of individual data points. This is particularly problematic in sparse regions where the density estimate may not provide a good representation of the underlying data. In this paper, we present sunspot plots, a visualization technique that communicates dense data as a continuous data distribution, while preserving the discrete nature of data samples in sparsely populated areas. We furthermore demonstrate the advantages of our approach on typical failure cases of scatter plots within synthetic and real-world data sets and validate its effectiveness in a user study.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14001", "refList": ["10.1057/palgrave.ivs.9500122", "10.2312/eggh/hpg12/097-103", "10.1109/tvcg.2008.119", "10.1109/pacificvis.2011.5742387", "10.1038/331163a0", "10.1109/visual.2019.8933620", "10.2307/2683294", "10.1201/9781351072304", "10.2307/2289444", "10.1109/tvcg.2019.2934541", "10.1080/00949657508810123", "10.1109/tvcg.2013.65", "10.1109/infvis.1997.636789", "10.1109/2945.841121", "10.1109/mcse.2007.55", "10.1109/tvcg.2010.197", "10.1111/cgf.12871", "10.1109/infvis.2003.1249018", "10.1145/3173574.3173991", "10.1109/tvcg.2012.238", "10.1007/0-387-28695-0", "10.1109/pacificvis.2010.5429604", "10.18637/jss.v008.i03", "10.1109/tvcg.2007.70596", "10.1109/visual.1998.745301", "10.1007/0-387-37977-0\\_3", "10.1145/1556262.1556289", "10.2312/wiced.20161094", "10.1109/tvcg.2007.70535", "10.1145/1056808.1056914", "10.1109/tvcg.2003.1196007", "10.1109/iv.2004.1320190", "10.1111/cgf.12877", "10.1162/leon.2007.40.2.202a", "10.1109/tvcg.2017.2674978", "10.1057/ivs.2010.4", "10.1016/j.cag.2018.02.008", "10.1201/9781315140919", "10.1109/visual.2000.885677", "10.1002/jhbs.20078", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1109/visual.2001.964495", "10.1109/iv.2002.1028760", "10.1111/cgf.13684", "10.1109/hicss.2013.197", "10.1109/tvcg.2019.2903956", "10.1093/mnras/stt961", "10.1109/tvcg.2017.2668409", "10.1111/j.1467-8659.2009.01478.x", "10.1145/2702123.2702585", "10.2307/1418003", "10.2307/1390742", "10.1145/360825.360839", "10.1109/pacificvis.2009.4906843", "10.1057/ivs.2009.34", "10.2307/2288711"], "wos": 1, "children": [], "len": 1}], "len": 13}, {"doi": "10.1111/cgf.14000", "year": "2020", "title": "Evaluating Reordering Strategies for Cluster Identification in Parallel Coordinates", "conferenceName": "EuroVis", "authors": "Michael Blumenschein;Xuan Zhang;David Pomerenke;Daniel A. Keim;Johannes Fuchs", "citationCount": "0", "affiliation": "Blumenschein, M (Corresponding Author), Univ Konstanz, Constance, Germany.\nBlumenschein, Michael; Pomerenke, David; Keim, Daniel A.; Fuchs, Johannes, Univ Konstanz, Constance, Germany.\nZhang, Xuan, Rhein Westfal TH Aachen, Aachen, Germany.", "countries": "Germany", "abstract": "The ability to perceive patterns in parallel coordinates plots (PCPs) is heavily influenced by the ordering of the dimensions. While the community has proposed over 30 automatic ordering strategies, we still lack empirical guidance for choosing an appropriate strategy for a given task. In this paper, we first propose a classification of tasks and patterns and analyze which PCP reordering strategies help in detecting them. Based on our classification, we then conduct an empirical user study with 31 participants to evaluate reordering strategies for cluster identification tasks. We particularly measure time, identification quality, and the users' confidence for two different strategies using both synthetic and real-world datasets. Our results show that, somewhat unexpectedly, participants tend to focus on dissimilar rather than similar dimension pairs when detecting clusters, and are more confident in their answers. This is especially true when increasing the amount of clutter in the data. As a result of these findings, we propose a new reordering strategy based on the dissimilarity of neighboring dimension pairs.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14000", "refList": ["10.1111/j.1467-8659.2008.01241.x", "10.2312/conf/eg2013/stars/095-116", "10.1109/tvcg.2014.2346979", "10.1117/12.838819", "10.2307/2290001", "10.1111/cgf.12638", "10.1007/978-3-642-30217-6\\_42", "10.1109/sc.2005.47", "10.1002/wics.145", "10.1109/tvcg.2011.229", "10.1109/vast.2009.5332628", "10.1109/vast.2017.8585613", "10.1145/2993901.2993907", "10.1007/bf00410640", "10.1109/tvcg.2010.184", "10.11591/ijece.v5i6", "10.1111/j.1467-8659.2011.01961.x", "10.1109/visual.1997.663916", "10.1111/j.1467-8659.2012.03129.x", "10.1109/visual.1999.809866", "10.1057/ivs.2008.13", "10.1016/j.visinf.2017.11.001", "10.2307/2282967", "10.1007/978-0-387-68628-8\\_10", "10.5888/pcd9.120082", "10.1109/tvcg.2007.70535", "10.1109/vast.2010.5652450", "10.1007/bf01898350", "10.1016/0010-0285(91)90009-d", "10.1007/978-3-642-24958-7\\_12", "10.1109/infvis.2005.1532142", "10.1057/palgrave.ivs.9500166", "10.1111/j.1467-8659.2008.01239.x", "10.1109/mcse.2015.55", "10.2312/pe/eurovast/eurova12/007-011.7", "10.1109/tvcg.2009.153", "10.1198/jcgs.2010.09136", "10.1145/2463676.2463696", "10.1109/infvis.2005.1532138", "10.1109/tvcg.2010.242", "10.5220/0006097400400051", "10.1109/visual.2019.8933706", "10.1109/atc.2015.7388338", "10.2312/pe/eurovast/eurova12/007-011", "10.5120/5044-7370", "10.1111/cgf.13446", "10.1109/icmla.2012.148", "10.1016/j.jvlc.2015.12.001", "10.1109/tvcg.2015.2466992", "10.1111/j.1467-8659.2009.01666.x", "10.5169/seals-266450", "10.1109/infvis.1998.729559", "10.1016/j.jvlc.2017.10.003", "10.1109/visual.1995.485139", "10.1007/978-0-387-39940-9\\_262", "10.1109/tvcg.2006.138"], "wos": 1, "children": [], "len": 1}], "len": 71}, {"doi": "10.1111/cgf.13727", "year": "2019", "title": "Tasks, Techniques, and Tools for Genomic Data Visualization", "conferenceName": "EuroVis", "authors": "Sabrina Nusrat;Theresa Harbig;Nils Gehlenborg", "citationCount": "2", "affiliation": "Nusrat, S (Corresponding Author), Harvard Med Sch, Dept Biomed Informat, Boston, MA 02115 USA.\nNusrat, S.; Harbig, T.; Gehlenborg, N., Harvard Med Sch, Dept Biomed Informat, Boston, MA 02115 USA.", "countries": "USA", "abstract": "Genomic data visualization is essential for interpretation and hypothesis generation as well as a valuable aid in communicating discoveries. Visual tools bridge the gap between algorithmic approaches and the cognitive skills of investigators. Addressing this need has become crucial in genomics, as biomedical research is increasingly data-driven and many studies lack well-defined hypotheses. A key challenge in data-driven research is to discover unexpected patterns and to formulate hypotheses in an unbiased manner in vast amounts of genomic and other associated data. Over the past two decades, this has driven the development of numerous data visualization techniques and tools for visualizing genomic data. Based on a comprehensive literature survey, we propose taxonomies for data, visualization, and tasks involved in genomic data visualization. Furthermore, we provide a comprehensive review of published genomic visualization tools in the context of the proposed taxonomies.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13727", "refList": ["10.1093/bioinformatics/btu205", "10.1093/bioinformatics/btv433", "10.1016/j.cels.2015.07.012", "10.1186/1471-2164-14-209", "10.1093/bioinformatics/btm039", "10.1093/bioinformatics/btl151", "10.1101/gr.849004", "10.1186/s13059-018-1519-9", "10.1093/nar/gkv1290", "10.1093/bioinformatics/btr309", "10.1186/1471-2105-8-82", "10.1038/nature08987", "10.1146/annurev-biodatasci-080917-013424", "10.1101/gr.229102", "10.1073/pnas.0500398102", "10.1093/bioinformatics/bth159", "10.1093/nar/gks1189", "10.1177/1176935119835546", "10.1093/bioinformatics/btr020", "10.1093/nar/gkl198", "10.1093/bioinformatics/btx359", "10.1093/bioinformatics/btt733", "10.1186/1741-7007-8-49", "10.1109/tvcg.2010.163", "10.1002/1873-3468.12778", "10.1101/gr.1304504", "10.1186/s13059-018-1465-6", "10.1093/nar/gkw255", "10.1038/nsmb.3180", "10.1002/0471250953.bi0909s28", "10.1101/gr.092759.109", "10.1186/s13059-016-0924-1", "10.1038/nature23884", "10.1093/bioinformatics/btw474", "10.1186/gb-2012-13-1-r4", "10.1093/bioinformatics/btu842", "10.1093/nar/gkr407", "10.1109/tvcg.2013.214", "10.1038/464679a", "10.1158/0008-5472.can-17-0802", "10.1038/464678a", "10.1093/nar/gkx1095", "10.1093/nar/30.1.38", "10.1038/nmeth.3038", "10.1038/nmeth.1772", "10.1093/bioinformatics/btt114", "10.1016/j.cels.2018.01.001", "10.1186/1471-2105-13-2", "10.1002/ajmg.c.30206", "10.1002/0471250953.bi0912s31", "10.1093/bib/bbs017", "10.1093/bioinformatics/btx144", "10.1109/tvcg.2015.2467911", "10.1109/tvcg.2017.2745978", "10.1186/1471-2105-16-s11-s6", "10.1093/bioinformatics/btq588", "10.1109/tvcg.2016.2598789", "10.1093/bioinformatics/btw161", "10.1109/tvcg.2011.232", "10.1093/nar/18.20.6097", "10.1093/bioinformatics/bts217", "10.1186/gb-2012-13-8-r77", "10.1093/bioinformatics/btp033", "10.1038/nmeth1009-690", "10.1093/nar/gkh458", "10.1093/bioinformatics/bti054", "10.1109/vl.1996.545307", "10.1186/s13059-017-1161-y", "10.1093/nar/gkv401", "10.1038/ng.2764", "10.1109/tvcg.2013.124", "10.1158/2159-8290.cd-12-0095", "10.1186/1471-2105-9-536", "10.1186/1471-2105-12-494", "10.1093/bioinformatics/btl193", "10.1126/science.270.5235.397", "10.1093/bioinformatics/16.11.1046", "10.1093/nar/gkn179", "10.1093/bioinformatics/btu531", "10.1038/nature11247", "10.1093/nar/gkw257", "10.1093/bioinformatics/bts452", "10.1186/s13059-018-1486-1", "10.1038/nmeth.2646", "10.1038/nmeth.3412", "10.1109/tvcg.2009.167", "10.1186/s13742-015-0077-2", "10.1038/nmeth.1422", "10.7717/peerj.4218", "10.1038/nature09725", "10.1093/nar/gky504", "10.1093/bioinformatics/btw069", "10.1142/9789813207813\\_0025", "10.1093/bioinformatics/btv034", "10.1186/gm413", "10.1093/bioinformatics/btn578", "10.1093/nar/gks427", "10.1093/bioinformatics/btp152", "10.1186/s12864-018-4625-x", "10.1093/nar/gkw022", "10.1093/nar/gkv476", "10.1186/s12887-018-1200-1", "10.1093/bioinformatics/btv249", "10.1093/nar/gkr995"], "wos": 1, "children": [], "len": 1}], "len": 85}, {"doi": "10.1109/vast47406.2019.8986940", "title": "FDive: Learning Relevance Models Using Pattern-based Similarity Measures", "year": "2019", "conferenceName": "VAST", "authors": "Frederik L. Dennig;Tom Polk;Zudi Lin;Tobias Schreck;Hanspeter Pfister;Michael Behrisch", "citationCount": "0", "affiliation": "Dennig, FL (Corresponding Author), Univ Konstanz, Constance, Germany. Dennig, Frederik L.; Polk, Tom, Univ Konstanz, Constance, Germany. Lin, Zudi; Pfister, Hanspeter; Behrisch, Michael, Harvard Univ, Cambridge, MA 02138 USA. Schreck, Tobias, Graz Univ Technol, Graz, Austria.", "countries": "Germany;USA;Austria", "abstract": "The detection of interesting patterns in large high-dimensional datasets is difficult because of their dimensionality and pattern complexity. Therefore, analysts require automated support for the extraction of relevant patterns. In this paper, we present FDive, a visual active learning system that helps to create visually explorable relevance models, assisted by learning a pattern-based similarity. We use a small set of user-provided labels to rank similarity measures, consisting of feature descriptor and distance function combinations, by their ability to distinguish relevant from irrelevant data. Based on the best-ranked similarity measure, the system calculates an interactive Self-Organizing Map-based relevance model, which classifies data according to the cluster affiliation. It also automatically prompts further relevance feedback to improve its accuracy. Uncertain areas, especially near the decision boundaries, are highlighted and can be refined by the user. We evaluate our approach by comparison to state-of-the-art feature selection techniques and demonstrate the usefulness of our approach by a case study classifying electron microscopy images of brain cells. The results show that FDive enhances both the quality and understanding of relevance models and can thus lead to new insights for brain research.", "keywords": "Visual analytics,similarity measure selection,relevance feedback,active learning,self-organizing maps", "link": "http://dx.doi.org/10.1109/VAST47406.2019.8986940", "refList": ["10.1016/j.ijhcs.2006.07.005", "10.1109/tsmc.1978.4309999", "10.1109/igarss.2016.7729190", "10.1145/3301275.3302280", "10.1007/bf00337288", "10.1162/153244303322753616", "10.1186/1471-2105-13-s8-s4", "10.1109/vast.2014.7042480", "10.1006/jvci.1999.0413", "10.1109/tpami.2009.154", "10.1109/wiamis.2008.24", "10.1109/tvcg.2017.2744805", "10.1111/j.1467-8659.2011.01938.x", "10.1109/tvcg.2013.157", "10.1080/01969727308546046", "10.1109/tsmc.1973.4309314", "10.1016/j.jchromb.2012.05.020", "10.1109/tvcg.2014.2346481", "10.1007/978-3-319-14364-4\\_70", "10.2307/2287820", "10.1109/infvis.2005.1532142", "10.1109/tip.2002.801585", "10.1007/978-1-4614-7138-7\\_1", "10.1145/1390156.1390302", "10.1109/tvcg.2012.277", "10.3390/jimaging4080097", "10.1109/cbmi.2015.7153629", "10.1145/2836034.2836035", "10.1145/2362456.2362485", "10.1016/j.visinf.2019.03.002", "10.1109/tpami.1979.4766909", "10.1145/1459359.1459577", "10.1109/tkde.2009.191", "10.1109/icip.2001.959135", "10.1109/tvcg.2016.2598467", "10.1007/978-3-540-87481-2\\_21", "10.1109/cvpr.1997.609412", "10.2307/2685881", "10.1109/cvpr.2016.90", "10.1145/347090.347124", "10.1016/b978-044450270-4/50003-6", "10.1109/tvcg.2017.2744818", "10.1145/989863.989880", "10.1145/3185517", "10.1145/1282280.1282340", "10.1109/vast.2012.6400486", "10.1109/tpami.2006.68", "10.1109/vast.2011.6102453"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934796", "title": "Improving the Robustness of Scagnostics", "year": "2019", "conferenceName": "InfoVis", "authors": "Yunhai Wang;Zeyu Wang 0005;Tingting Liu;Michael Correll;Zhanglin Cheng;Oliver Deussen;Michael Sedlmair", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Jinan, Peoples R China. Wang, Yunhai; Wang, Zeyu; Liu, Tingting, Shandong Univ, Jinan, Peoples R China. Wang, Zeyu; Cheng, Zhanglin; Deussen, Oliver, SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Peoples R China. Correll, Michael, Tableau Res, Seattle, WA USA. Deussen, Oliver, Konstanz Univ, Constance, Germany. Sedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "In this paper, we examine the robustness of scagnostics through a series of theoretical and empirical studies. First, we investigate the sensitivity of scagnostics by employing perturbing operations on more than 60M synthetic and real-world scatterplots. We found that two scagnostic measures, Outlying and Clumpy, are overly sensitive to data binning. To understand how these measures align with human judgments of visual features, we conducted a study with 24 participants, which reveals that i) humans are not sensitive to small perturbations of the data that cause large changes in both measures, and ii) the perception of clumpiness heavily depends on per-cluster topologies and structures. Motivated by these results, we propose Robust Scagnostics (RScag) by combining adaptive binning with a hierarchy-based form of scagnostics. An analysis shows that RScag improves on the robustness of original scagnostics, aligns better with human judgments, and is equally fast as the traditional scagnostic measures.", "keywords": "Scagnostics,scatterplots,sensitivity analysis,Robust Scagnostics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934796", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2346979", "10.1111/j.1467-8659.2009.01467.x", "10.1109/vast.2008.4677368", "10.2307/2289444", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/vast.2009.5332628", "10.1111/insr.12095\\_11", "10.1109/tvcg.2010.184", "10.1109/tvcg.2015.2467323", "10.1109/vast.2010.5652460", "10.1111/j.1467-8659.2012.03069.x", "10.1145/1842993.1843002", "10.1198/106186008x320465", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.20", "10.1111/cgf.12641", "10.1109/tvcg.2012.128", "10.1109/tvcg.2015.2467671", "10.1057/palgrave.ivs.9500091", "10.1007/978-1-4612-4400-4", "10.1111/cgf.13176", "10.1002/0470870958", "10.1109/tvcg.2018.2864907", "10.1111/j.1467-8659.2012.03125.x", "10.1109/vast.2012.6400490", "10.1109/infvis.2005.1532142", "10.1109/vast.2010.5652433", "10.1109/vast.2009.5332611", "10.1201/9781315140919", "10.1145/2858036.2858155", "10.1109/ldav.2013.6675164", "10.1515/itit-2014-1070", "10.1111/cgf.13684", "10.1109/tpami.1979.4766909", "10.2307/2289936", "10.1109/pacificvis.2014.42", "10.1109/tvcg.2016.2598467", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2006.94", "10.1109/tvcg.2014.2346572", "10.1111/cgf.12632", "10.1109/tvcg.2017.2744339"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13446", "year": "2018", "title": "Quality Metrics for Information Visualization", "conferenceName": "EuroVis", "authors": "Michael Behrisch;Michael Blumenschein;Nam Wook Kim;Lin Shao;Mennatallah El{-}Assady;Johannes Fuchs;Daniel Seebacher;Alexandra Diehl;Ulrik Brandes;Hanspeter Pfister;Tobias Schreck;Daniel Weiskopf;Daniel A. Keim", "citationCount": "25", "affiliation": "Behrisch, M (Corresponding Author), Harvard Univ, Cambridge, MA 02138 USA.\nBehrisch, M.; Kim, N. W.; Pfister, H., Harvard Univ, Cambridge, MA 02138 USA.\nBlumenschein, M.; El-Assady, M.; Fuchs, J.; Seebacher, D.; Diehl, A.; Keim, D. A., Univ Konstanz, Constance, Germany.\nShao, L.; Schreck, T., Graz Univ Technol, Graz, Austria.\nBrandes, U., Swiss Fed Inst Technol, Zurich, Switzerland.\nWeiskopf, D., Univ Stuttgart, Stuttgart, Germany.", "countries": "Switzerland;Germany;USA;Austria", "abstract": "The visualization community has developed to date many intuitions and understandings of how to judge the quality of views in visualizing data. The computation of a visualization's quality and usefulness ranges from measuring clutter and overlap, up to the existence and perception of specific (visual) patterns. This survey attempts to report, categorize and unify the diverse understandings and aims to establish a common vocabulary that will enable a wide audience to understand their differences and subtleties. For this purpose, we present a commonly applicable quality metric formalization that should detail and relate all constituting parts of a quality metric. We organize our corpus of reviewed research papers along the data types established in the information visualization community: multi- and high-dimensional, relational, sequential, geospatial and text data. For each data type, we select the visualization subdomains in which quality metrics are an active research field and report their findings, reason on the underlying concepts, describe goals and outline the constraints and requirements. One central goal of this survey is to provide guidance on future research opportunities for the field and outline how different visualization communities could benefit from each other by applying or transferring knowledge to their respective subdomain. Additionally, we aim to motivate the visualization community to compare computed measures to the perception of humans.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13446", "refList": ["10.2307/276978", "10.1057/ivs.2009.10", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2014.2346420", "10.1016/j.cag.2018.01.010", "10.1109/tvcg.2017.2743959", "10.1109/tvcg.2011.127", "10.1109/tvcg.2015.2467759", "10.1109/tvcg.2013.187", "10.1109/tvcg.2007.70594", "10.1186/1471-2105-9-155", "10.1145/2702123.2702545", "10.1145/1645953.1646023", "10.1093/bioinformatics/btm312", "10.1109/pacificvis.2016.7465245", "10.1145/3038462.3038463", "10.1007/978-3-540-70956-5", "10.1109/tvcg.2017.2743939", "10.1111/cgf.12932", "10.1111/j.1467-8659.2012.03106.x", "10.1145/1133265.1133318", "10.1109/tvcg.2010.186", "10.1109/iv.2013.101", "10.1109/tvcg.2016.2598590", "10.2312/conf/eg2013/stars/039-063", "10.3758/bf03201236", "10.1111/cgf.12935", "10.1145/1056808.1056914", "10.1007/bf01898350", "10.2307/1390686", "10.1109/2945.981848", "10.1109/hicss.2008.422", "10.1109/tvcg.2011.201", "10.1109/pacificvis.2011.5742390", "10.2307/2288400", "10.1111/cgf.12872", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2006.163", "10.1109/tvcg.2013.150", "10.1109/tvcg.2009.171", "10.1109/tvcg.2017.2723397", "10.1117/12.2079841", "10.1109/pacificvis.2009.4906838", "10.1109/infvis.2004.1", "10.1109/tvcg.2008.166", "10.1287/opre.20.5.993", "10.7155/jgaa.00370", "10.1111/j.1467-8659.2008.01240.x", "10.1109/inf0vis.2005.14", "10.1109/tvcg.2009.23", "10.1109/infvis.1998.729559", "10.1177/0165551506078083", "10.1109/t-c.1974.224051", "10.1109/tvcg.2017.2744339", "10.1109/tvcg.2014.2346433", "10.1145/989863.989940", "10.1109/tvcg.2015.2509990", "10.2307/2288843", "10.1016/j.neucom.2017.01.105", "10.1073/pnas.43.10.923", "10.1111/cgf.12647", "10.1016/j.rse.2017.06.031", "10.1109/infvis.2005.1532145", "10.1109/2945.841121", "10.1109/tvcg.2011.229", "10.2312/eurova.20141140", "10.1111/j.2044-8317.1974.tb00534.x", "10.1109/pacificvis.2015.7156366", "10.1109/vast.2009.5332628", "10.1109/vast.2014.7042480", "10.1145/2993901.2993907", "10.1177/0951692899011001004", "10.1007/978-3-642-03655-2\\_43", "10.1198/106186008x320465", "10.1109/visual.1997.663916", "10.1111/cgf.12125", "10.1109/infvis.2003.1249009", "10.1109/icdm.2003.1250978", "10.1109/tvcg.2017.2745919", "10.1109/tvcg.2012.128", "10.1109/vast.2010.5652450", "10.1109/tvcg.2006.161", "10.1145/571647.571649", "10.1109/tvcg.2011.193", "10.1111/cgf.12633", "10.1109/tvcg.2017.2674978", "10.1109/vast.2010.5652433", "10.1109/tvcg.2017.2745978", "10.1515/itit-2014-1070", "10.1145/1374489.1374501", "10.1109/tvcg.2009.153", "10.2312/eurovisshort.20171128", "10.2312/eurovisshort.20151130", "10.1016/0925-7721(94)00014-x", "10.1057/palgrave.ivs.95000/3", "10.1145/302979.303030", "10.1002/widm.1071", "10.1109/pacificvis.2014.42", "10.1109/tvcg.2016.2598467", "10.1111/j.1467-8659.2009.01667.x", "10.2307/2287708", "10.1177/1473871613477091", "10.1145/967900.968153", "10.1109/tvcg.2010.162", "10.1145/568522.568523", "10.1016/j.cag.2004.03.022", "10.1109/tvcg.2015.2466992", "10.1145/2993901.2993903", "10.1147/jrd.2015.2411412", "10.1177/154193120504900508", "10.1109/tvcg.2006.138", "10.1111/cgf.13168", "10.1111/cgf.12380", "10.2312/conf/eg2013/stars/095-116", "10.1109/tvcg.2017.2745859", "10.1109/tvcg.2017.2701829", "10.1109/tvcg.2007.70529", "10.1007/bf01199431", "10.1117/12.697548", "10.1109/tvcg.2017.2653106", "10.1207/s15327906mbr2701\\_4", "10.1109/vl.1996.545307", "10.1109/tvcg.2017.2745140", "10.1111/j.1467-8659.2012.03069.x", "10.1111/j.1467-8659.2011.01961.x", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.124", "10.1057/ivs.2008.13", "10.2312/vissym/eurovis06/195-202", "10.2312/vissym/eurovis07/163-170", "10.1109/tvcg.2015.2467324", "10.1109/tvcg.2011.167", "10.1109/pacificvis.2014.40", "10.1109/pacificvis.2012.6183570", "10.1111/j.1467-8659.2012.03125.x", "10.1075/idj.20.1.02bei", "10.1111/1467-8306.00061", "10.1145/2858036.2858155", "10.1109/tvcg.2012.108", "10.2312/vmv.20171261", "10.1109/tvcg.2015.2467531", "10.1145/1379092.1379130", "10.1109/tpami.1979.4766909", "10.1080/13875860903039172", "10.1109/mcg.2006.70", "10.1109/tvcg.2010.242", "10.1109/vast.2007.4389004", "10.1109/tvcg.2015.2467191", "10.2312/cgvc.20171276", "10.1016/0169-7439(87)80084-9", "10.2312/pe/eurovisshort/eurovisshort2012/097-101", "10.1016/j.neucom.2014.07.073", "10.2307/2685263", "10.1145/996546.996554", "10.1111/j.1467-8659.2009.01666.x", "10.1117/12.2083444", "10.1109/tvcg.2014.2346572", "10.1007/978-0-387-39940-9\\_262", "10.1007/bf00988593", "10.1145/1054972.1055078", "10.1111/j.1467-8659.2009.01467.x", "10.1111/cgf.13181", "10.1145/502512.502530", "10.1109/pacificvis.2010.5429600", "10.1111/j.1467-8659.2011.01923.x", "10.1109/mcg.2004.41", "10.1109/tvcg.2015.2467971", "10.1111/j.1467-8659.2011.01919.x", "10.1002/sam.10071", "10.1109/tvcg.2010.184", "10.1007/978-3-642-27848-8\\_648-1", "10.1109/tvcg.2010.132", "10.1145/22949.22950", "10.1145/1842993.1843002", "10.1109/infvis.2000.885096", "10.1016/j.ins.2015.04.017", "10.1109/iv.2009.43", "10.1016/j.visinf.2017.11.001", "10.1109/tvcg.2011.239", "10.1109/infovis.2005.40", "10.1111/cgf.12641", "10.1109/pacificvis.2016.7465262", "10.1145/2470654.2466443", "10.1007/s10844-011-0157-4", "10.1111/cgf.12919", "10.1016/j.jvlc.2016.07.003", "10.1109/tvcg.2016.2549018", "10.1057/palgrave.ivs.9500166", "10.3406/colan.1981.1409.1", "10.1093/bioinformatics/bti141", "10.1111/j.1467-8306.2004.09401004.x", "10.1145/1168149.1168168", "10.1109/tvcg.2017.2744184", "10.1117/12.2079420", "10.1109/iv.2005.62", "10.1145/102377.115768", "10.1109/tvcg.2014.2346677", "10.1109/tvcg.2014.2346426", "10.1111/j.1467-8659.2012.03107.x", "10.5220/0006097400400051", "10.2991/978-94-6239-186-4", "10.2307/2284077", "10.1109/iv.2008.89", "10.1016/j.jvlc.2015.12.001", "10.1109/pacificvis.2013.6596147", "10.1145/1242572.1242826", "10.1016/j.cag.2007.01.030", "10.1111/cgf.12632", "10.2312/eurovisstar.20151113", "10.1559/152304009788188808"], "wos": 1, "children": [{"doi": "10.1109/vast.2018.8802486", "title": "SMARTexplore: Simplifying High-Dimensional Data Analysis through a Table-Based Visual Analytics Approach", "year": "2018", "conferenceName": "VAST", "authors": "Michael Blumenschein;Michael Behrisch;Stefanie Schmid;Simon Butscher;Deborah Wahl;Karoline Villinger;Britta Renner;Harald Reiterer;Daniel A. Keim", "citationCount": "0", "affiliation": "Blumenschein, M (Corresponding Author), Univ Konstanz, Constance, Germany. Blumenschein, Michael; Schmid, Stefanie; Butscher, Simon; Wahl, Deborah R.; Villinger, Karoline; Renner, Britta; Reiterer, Harald; Keim, Daniel A., Univ Konstanz, Constance, Germany. Behrisch, Michael, Harvard Univ, Cambridge, MA 02138 USA.", "countries": "Germany;USA", "abstract": "We present SMARTEXPLORE, a novel visual analytics technique that simplifies the identification and understanding of clusters, correlations, and complex patterns in high-dimensional data. The analysis is integrated into an interactive table-based visualization that maintains a consistent and familiar representation throughout the analysis. The visualization is tightly coupled with pattern matching, subspace analysis, reordering, and layout algorithms. To increase the analyst's trust in the revealed patterns, SMARTEXPLORE automatically selects and computes statistical measures based on dimension and data properties. While existing approaches to analyzing high-dimensional data (e.g., planar projections and Parallel coordinates) have proven effective, they typically have steep learning curves for non-visualization experts. Our evaluation, based on three expert case studies, confirms that non-visualization experts successfully reveal patterns in high-dimensional data when using SMARTEXPLORE.", "keywords": "High-dimensional data,visual exploration,pattern-driven analysis,tabular visualization,subspace,aggregation", "link": "http://dx.doi.org/10.1109/VAST.2018.8802486", "refList": ["10.1177/1473871612460526", "10.1109/tvcg.2015.2489649", "10.1111/j.1467-8659.2008.01241.x", "10.1007/978-3-319-25087-8\\_29", "10.1007/b98835", "10.13140/rg.2.2.16570.90567", "10.1109/tvcg.2014.2346260", "10.1109/vast.2009.5332628", "10.2307/2528823", "10.1109/tvcg.2010.184", "10.1145/1133265.1133318", "10.1057/palgrave.ivs.9500072", "10.1111/j.1467-8659.2012.03110.x", "10.1145/1007730.1007731", "10.1057/palgrave.ivs.9500086", "10.1111/cgf.12935", "10.1109/tvcg.2014.2346279", "10.1007/bf01898350", "10.1109/tvcg.2013.173", "10.1109/tvcg.2017.2672987", "10.1109/tvcg.2014.2346248", "10.1109/infvis.2004.46", "10.1109/vast.2010.5652433", "10.1109/vast.2009.5332611", "10.1109/tvcg.2015.2467553", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2013.150", "10.1109/tvcg.2016.2640960", "10.1111/cgf.12630", "10.1007/978-1-4757-1904-8", "10.1109/tvcg.2011.188", "10.1109/tvcg.2010.138", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2017.2743978", "10.1111/cgf.12879", "10.1109/iv.2008.33", "10.1111/cgf.13446", "10.1007/s00371-018-1483-0", "10.1109/infvis.1998.729559", "10.1145/2669557.2669572", "10.1111/j.1467-8659.2008.01239.x"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934541", "title": "A Recursive Subdivision Technique for Sampling Multi-class Scatterplots", "year": "2019", "conferenceName": "InfoVis", "authors": "Xin Chen;Tong Ge;Jian Zhang 0070;Baoquan Chen;Chi-Wing Fu;Oliver Deussen;Yunhai Wang", "citationCount": "5", "affiliation": "Chen, X (Corresponding Author), Shandong Univ, Jinan, Shandong, Peoples R China. Chen, Xin; Ge, Tong; Wang, Yunhai, Shandong Univ, Jinan, Shandong, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China. Fu, Chi-Wing, SIAT, Guangdong Prov Key Lab CV \\& VR Tech, Shenzhen, Guangdong, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Deussen, Oliver, Konstanz Univ, Constance, Germany. Deussen, Oliver, SIAT, Shenzhen VIsuCA Key Lab, Shenzhen, Guangdong, Peoples R China.", "countries": "Germany;China", "abstract": "We present a non-uniform recursive sampling technique for multi-class scatterplots, with the specific goal of faithfully presenting relative data and class densities, while preserving major outliers in the plots. Our technique is based on a customized binary kd-tree, in which leaf nodes are created by recursively subdividing the underlying multi-class density map. By backtracking, we merge leaf nodes until they encompass points of all classes for our subsequently applied outlier-aware multi-class sampling strategy. A quantitative evaluation shows that our approach can better preserve outliers and at the same time relative densities in multi-class scatterplots compared to the previous approaches, several case studies demonstrate the effectiveness of our approach in exploring complex and real world data.", "keywords": "Scatterplot,multi-class sampling,kd-tree,outlier,relative density", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934541", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2006.170", "10.1109/tvcg.2008.119", "10.1109/tvcg.2007.70580", "10.2307/2289444", "10.1145/1964921.1964943", "10.1109/tvcg.2013.65", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/itoec.2018.8740621", "10.1145/1778765.1778816", "10.1109/tvcg.2010.197", "10.1201/b17511", "10.1109/tvcg.2018.2864912", "10.1007/0-387-28695-0", "10.1109/5.726791", "10.1109/visual.1998.745301", "10.1016/j.physa.2011.12.004", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2004.1272729", "10.1145/335191.335388", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1109/iv.2002.1028760", "10.1145/1842993.1842999", "10.1109/tvcg.2018.2869149", "10.1038/nmeth.2490", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2010.176", "10.1145/2702123.2702585", "10.1057/ivs.2009.34"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14001", "year": "2020", "title": "Sunspot Plots: Model-based Structure Enhancement for Dense Scatter Plots", "conferenceName": "EuroVis", "authors": "Thomas Trautner;Fabian Bolte;Sergej Stoppel;Stefan Bruckner", "citationCount": "0", "affiliation": "Trautner, T (Corresponding Author), Univ Bergen, Bergen, Norway.\nTrautner, T.; Bolte, F.; Stoppel, S.; Bruckner, S., Univ Bergen, Bergen, Norway.", "countries": "Norway", "abstract": "Scatter plots are a powerful and well-established technique for visualizing the relationships between two variables as a collection of discrete points. However, especially when dealing with large and dense data, scatter plots often exhibit problems such as overplotting, making the data interpretation arduous. Density plots are able to overcome these limitations in highly populated regions, but fail to provide accurate information of individual data points. This is particularly problematic in sparse regions where the density estimate may not provide a good representation of the underlying data. In this paper, we present sunspot plots, a visualization technique that communicates dense data as a continuous data distribution, while preserving the discrete nature of data samples in sparsely populated areas. We furthermore demonstrate the advantages of our approach on typical failure cases of scatter plots within synthetic and real-world data sets and validate its effectiveness in a user study.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14001", "refList": ["10.1057/palgrave.ivs.9500122", "10.2312/eggh/hpg12/097-103", "10.1109/tvcg.2008.119", "10.1109/pacificvis.2011.5742387", "10.1038/331163a0", "10.1109/visual.2019.8933620", "10.2307/2683294", "10.1201/9781351072304", "10.2307/2289444", "10.1109/tvcg.2019.2934541", "10.1080/00949657508810123", "10.1109/tvcg.2013.65", "10.1109/infvis.1997.636789", "10.1109/2945.841121", "10.1109/mcse.2007.55", "10.1109/tvcg.2010.197", "10.1111/cgf.12871", "10.1109/infvis.2003.1249018", "10.1145/3173574.3173991", "10.1109/tvcg.2012.238", "10.1007/0-387-28695-0", "10.1109/pacificvis.2010.5429604", "10.18637/jss.v008.i03", "10.1109/tvcg.2007.70596", "10.1109/visual.1998.745301", "10.1007/0-387-37977-0\\_3", "10.1145/1556262.1556289", "10.2312/wiced.20161094", "10.1109/tvcg.2007.70535", "10.1145/1056808.1056914", "10.1109/tvcg.2003.1196007", "10.1109/iv.2004.1320190", "10.1111/cgf.12877", "10.1162/leon.2007.40.2.202a", "10.1109/tvcg.2017.2674978", "10.1057/ivs.2010.4", "10.1016/j.cag.2018.02.008", "10.1201/9781315140919", "10.1109/visual.2000.885677", "10.1002/jhbs.20078", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1109/visual.2001.964495", "10.1109/iv.2002.1028760", "10.1111/cgf.13684", "10.1109/hicss.2013.197", "10.1109/tvcg.2019.2903956", "10.1093/mnras/stt961", "10.1109/tvcg.2017.2668409", "10.1111/j.1467-8659.2009.01478.x", "10.1145/2702123.2702585", "10.2307/1418003", "10.2307/1390742", "10.1145/360825.360839", "10.1109/pacificvis.2009.4906843", "10.1057/ivs.2009.34", "10.2307/2288711"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934284", "title": "Color Crafting: Automating the Construction of Designer Quality Color Ramps", "year": "2019", "conferenceName": "InfoVis", "authors": "Stephen Smart;Keke Wu;Danielle Albers Szafir", "citationCount": "3", "affiliation": "Smart, S (Corresponding Author), Univ Colorado, Boulder, CO 80309 USA. Smart, Stephen; Wu, Keke; Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Visualizations often encode numeric data using sequential and diverging color ramps. Effective ramps use colors that are sufficiently discriminable, align well with the data, and are aesthetically pleasing. Designers rely on years of experience to create high-quality color ramps. However, it is challenging for novice visualization developers that lack this experience to craft effective ramps as most guidelines for constructing ramps are loosely defined qualitative heuristics that are often difficult to apply. Our goal is to enable visualization developers to readily create effective color encodings using a single seed color. We do this using an algorithmic approach that models designer practices by analyzing patterns in the structure of designer-crafted color ramps. We construct these models from a corpus of 222 expert-designed color ramps, and use the results to automatically generate ramps that mimic designer practices. We evaluate our approach through an empirical study comparing the outputs of our approach with designer-crafted color ramps. Our models produce ramps that support accurate and aesthetically pleasing visualizations at least as well as designer ramps and that outperform conventional mathematical approaches.", "keywords": "Visualization,Aesthetics in Visualization,Color Perception,Visual Design,Design Mining", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934284", "refList": ["10.1145/3009924", "10.1109/tvcg.2015.2489649", "10.1109/tvcg.2017.2744359", "10.1002/(sici)1098-1098(199622)7:2", "10.1016/s0734-189x(83)80046-2", "10.1109/tvcg.2016.2599106", "10.1364/josaa.29.000313", "10.1109/tvcg.2016.2598918", "10.2307/2683294", "10.1109/tvcg.2017.2653106", "10.1016/j.ijhcs.2010.05.006", "10.1016/0146-664x(81)90006-x", "10.1007/978-3-642-10520-3\\_9", "10.1109/38.135886", "10.1146/annurev-psych-120710-100504", "10.1016/j.csda.2008.11.033", "10.1145/2461912.2461988", "10.1016/j.cub.2007.06.022", "10.1016/s0146-664x(79)80040-4", "10.1109/mcg.2004.1297012", "10.1109/iv.2009.94", "10.1109/tpami.2010.184", "10.3758/s13414-010-0027-0", "10.1145/22949.22950", "10.1109/visual.1995.480803", "10.1109/tvcg.2014.2346277", "10.2307/2684111", "10.1109/tvcg.2018.2865240", "10.1111/cgf.12633", "10.1109/38.7760", "10.1016/j.jspi.2015.04.007", "10.1109/iv.2008.24", "10.1145/2939502.2939506", "10.1111/cgf.12127", "10.1016/j.cag.2010.11.015", "10.1145/3025453.3026041", "10.1109/tvcg.2012.315", "10.2307/2288400", "10.1511/2005.5.436", "10.1016/s0097-8493(96)00072-6", "10.1109/mcg.2018.011461525", "10.1145/2470654.2466420", "10.1109/tvcg.2012.279", "10.1109/tvcg.2014.2346978", "10.1109/tvcg.2017.2743978", "10.1145/3406601.3406602", "10.1117/12.2084548", "10.1109/tvcg.2018.2865147", "10.1109/tvcg.2015.2467191", "10.1111/cgf.13446", "10.1109/tvcg.2017.2744320", "10.1111/j.1467-8659.2008.01203.x", "10.1145/2702613.2702975", "10.1007/978-3-319-26633-6\\_13", "10.1145/2207676.2208547", "10.1109/tvcg.2008.174", "10.1179/000870403235002042"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028891", "title": "A Structured Review of Data Management Technology for Interactive Visualization and Analysis", "year": "2020", "conferenceName": "InfoVis", "authors": "Leilani Battle;Carlos Scheidegger", "citationCount": "0", "affiliation": "Battle, L (Corresponding Author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA. Battle, Leilani, Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA. Scheidegger, Carlos, Univ Arizona, Dept Comp Sci, HDC Lab, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "In the last two decades, interactive visualization and analysis have become a central tool in data-driven decision making. Concurrently to the contributions in data visualization, research in data management has produced technology that directly benefits interactive analysis. Here, we contribute a systematic review of 30 years of work in this adjacent field, and highlight techniques and principles we believe to be underappreciated in visualization work. We structure our review along two axes. First, we use task taxonomies from the visualization literature to structure the space of interactions in usual systems. Second, we created a categorization of data management work that strikes a balance between specificity and generality. Concretely, we contribute a characterization of 131 research papers along these two axes. We find that five notions in data management venues fit interactive visualization systems well: materialized views, approximate query processing, user modeling and query prediction, muiti-query optimization, lineage techniques, and indexing techniques. In addition, we find a preponderance of work in materialized views and approximate query processing, most targeting a limited subset of the interaction tasks in the taxonomy we used. This suggests natural avenues of future research both in visualization and data management. Our categorization both changes how we visualization researchers design and build our systems, and highlights where future work is necessary.", "keywords": "", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028891", "refList": ["10.1109/tvcg.2012.233", "10.1016/s0022-5371(74)80015-0", "10.1109/tvcg.2017.2744359", "10.1037/0096-3445.136.4.623", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2012.196", "10.1037/a0029856", "10.1109/tvcg.2014.2346979", "10.1037/h0030300", "10.1109/tvcg.2016.2598918", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2018.2864909", "10.1111/cgf.13079", "10.3389/fpsyg.2012.00355", "10.1145/2858036.2858465", "10.1080/01621459.1989.10478821", "10.1037/0278-7393.24.3.732", "10.1109/tvcg.2011.127", "10.1145/2858036.2858063", "10.4249/scholarpedia.3325", "10.4324/9781410611949", "10.1111/cgf.13444", "10.1145/2993901.2993909", "10.1037/0033-295x.96.2.267", "10.1006/ijhc.1017", "10.1086/405615", "10.1109/tvcg.2019.2934801", "10.1038/17953", "10.1037/xhp0000314", "10.1109/tvcg.2019.2934400", "10.1145/2470654.2470723", "10.1037/0096-1523.16.2.332", "10.1167/16.5.11", "10.3758/s13423-016-1174-7", "10.3758/bf03207704", "10.1146/annurev.psych.55.090902.141415", "10.2307/2288400", "10.3758/bf03204258", "10.1109/tvcg.2011.279", "10.1109/vissoft.2014.36", "10.3758/s13423-011-0055-3", "10.1145/3025453.3025922", "10.1109/tvcg.2019.2934284", "10.3758/bf03210498", "10.3758/bf03200774", "10.2307/1419876", "10.1038/s41562-017-0058", "10.1109/tvcg.2010.237", "10.1109/pacificvis.2012.6183556", "10.1109/infvis.1997.636792", "10.1093/acprof:oso/9780198523192.003.0005", "10.1073/pnas.1117465109", "10.1109/tvcg.2013.234", "10.1038/nn.3655", "10.1111/cgf.12379", "10.1146/annurev-psych-010416-044232", "10.1111/cgf.13695", "10.1037/0033-295x.107.3.500", "10.1109/tvcg.2013.183", "10.1146/annurev.psych.53.100901.135125", "10.1037//0022-3514.79.6.995", "10.1559/152304003100010929", "10.1109/tvcg.2018.2865147", "10.1037/0096-1523.18.3.849", "10.1111/j.1467-8659.2009.01694.x", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934799", "title": "Data Sampling in Multi-view and Multi-class Scatterplots via Set Cover Optimization", "year": "2019", "conferenceName": "InfoVis", "authors": "Ruizhen Hu;Tingkai Sha;Oliver van Kaick;Oliver Deussen;Hui Huang 0004", "citationCount": "4", "affiliation": "Hu, RZ (Corresponding Author), Shenzhen Univ, Visual Comp Res Ctr, Shenzhen, Guangdong, Peoples R China. Hu, Ruizhen; Sha, Tingkai; Huang, Hui, Shenzhen Univ, Visual Comp Res Ctr, Shenzhen, Guangdong, Peoples R China. van Kaick, Oliver, Carleton Univ, Sch Comp Sci, Ottawa, ON, Canada. Deussen, Oliver, Konstanz Univ, Constance, Germany. Deussen, Oliver, SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Guangdong, Peoples R China.", "countries": "Canada;Germany;China", "abstract": "We present a method for data sampling in scatterplots by jointly optimizing point selection for different views or classes. Our method uses space-filling curves (Z-order curves) that partition a point set into subsets that, when covered each by one sample, provide a sampling or coreset with good approximation guarantees in relation to the original point set. For scatterplot matrices with multiple views, different views provide different space-filling curves, leading to different partitions of the given point set. For multi-class scatterplots, the focus on either per-class distribution or global distribution provides two different partitions of the given point set that need to be considered in the selection of the coreset. For both cases, we convert the coreset selection problem into an Exact Cover Problem (ECP), and demonstrate with quantitative and qualitative evaluations that an approximate solution that solves the ECP efficiently is able to provide high-quality samplings.", "keywords": "Sampling,Scatterplot,SPLOM,Exact Cover Problem", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934799", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2006.170", "10.1109/tvcg.2008.119", "10.1111/j.1467-8659.2009.01467.x", "10.2307/2289444", "10.1109/tvcg.2013.65", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/itoec.2018.8740621", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2864912", "10.1007/0-387-28695-0", "10.1109/visual.1998.745301", "10.1287/moor.4.3.233", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.2307/2284239", "10.1109/tvcg.2017.2674978", "10.1016/j.dss.2009.05.016", "10.1109/tvcg.2014.2346594", "10.1109/vds.2017.8573446", "10.1007/978-1-4684-2001-2\\_9", "10.1109/iv.2002.1028760", "10.1145/1842993.1842999", "10.1038/nmeth.2490", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2010.176", "10.1145/2702123.2702585", "10.1057/ivs.2009.34", "10.1179/000870403235002042"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934432", "title": "Discriminability Tests for Visualization Effectiveness and Scalability", "year": "2019", "conferenceName": "InfoVis", "authors": "Rafael Veras;Christopher Collins", "citationCount": "1", "affiliation": "Veras, R (Corresponding Author), Ontario Tech Univ, Oshawa, ON, Canada. Veras, Rafael; Collins, Christopher, Ontario Tech Univ, Oshawa, ON, Canada.", "countries": "Canada", "abstract": "The scalability of a particular visualization approach is limited by the ability for people to discern differences between plots made with different datasets. Ideally, when the data changes, the visualization changes in perceptible ways. This relation breaks down when there is a mismatch between the encoding and the character of the dataset being viewed. Unfortunately, visualizations are often designed and evaluated without fully exploring how they will respond to a wide variety of datasets. We explore the use of an image similarity measure, the Multi-Scale Structural Similarity Index (MS-SSIM), for testing the discriminability of a data visualization across a variety of datasets. MS-SSIM is able to capture the similarity of two visualizations across multiple scales, including low level granular changes and high level patterns. Significant data changes that are not captured by the MS-SSIM indicate visualizations of low discriminability and effectiveness. The measure's utility is demonstrated with two empirical studies. In the first, we compare human similarity judgments and MS-SSIM scores for a collection of scatterplots. In the second, we compute the discriminability values for a set of basic visualizations and compare them with empirical measurements of effectiveness. In both cases, the analyses show that the computational measure is able to approximate empirical results. Our approach can be used to rank competing encodings on their discriminability and to aid in selecting visualizations for a particular type of data distribution.", "keywords": "Scalability,Discriminability,Simulation,Perception", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934432", "refList": ["10.1109/tvcg.2012.233", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2012.230", "10.1111/cgf.12647", "10.1109/tvcg.2016.2598918", "10.1109/tvcg.2007.70529", "10.1145/2858036.2858435", "10.1109/vast.2009.5332628", "10.1145/3025453.3025912", "10.1145/1133265.1133318", "10.1109/tip.2010.2092435", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2865264", "10.1175/jtech-d-11-00103.1", "10.1145/1190036.1190039", "10.1111/cgf.12127", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1109/mcg.2014.18", "10.1145/3173574.3174172", "10.1109/tvcg.2009.153", "10.1057/palgrave.ivs.9500070", "10.1109/tvcg.2018.2790961", "10.1109/tvcg.2014.2346978", "10.1109/tvcg.2018.2810918", "10.1111/cgf.13409", "10.1109/tip.2003.819861", "10.1145/2642918.2647411", "10.1080/15230406.2016.1140074", "10.1109/jstsp.2009.2015374", "10.1109/tvcg.2010.237", "10.1109/mcg.2009.6", "10.1111/j.1467-8659.2009.01667.x", "10.1109/tvcg.2010.161", "10.1111/cgf.13446", "10.1109/tvcg.2014.2346325", "10.1109/tvcg.2009.111", "10.1147/jrd.2015.2411412"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030394", "title": "Direct Volume Rendering with Nonparametric Models of Uncertainty", "year": "2020", "conferenceName": "SciVis", "authors": "Tushar M. Athawale;Bo Ma 0002;Elham Sakhaee;Christopher R. Johnson;Alireza Entezari", "citationCount": "0", "affiliation": "Athawale, TM (Corresponding Author), Univ Utah, Sci Comp \\& Imaging SCI Inst, Salt Lake City, UT 84112 USA. Athawale, Tushar M.; Johnson, Chris R., Univ Utah, Sci Comp \\& Imaging SCI Inst, Salt Lake City, UT 84112 USA. Ma, Bo; Sakhaee, Elham; Entezari, Alireza, Univ Florida, Dept CISE, Gainesville, FL 32611 USA.", "countries": "USA", "abstract": "We present a nonparametric statistical framework for the quantification, analysis, and propagation of data uncertainty in direct volume rendering (DVR). The state-of-the-art statistical DVR framework allows for preserving the transfer function (TF) of the ground truth function when visualizing uncertain data; however, the existing framework is restricted to parametric models of uncertainty. In this paper, we address the limitations of the existing DVR framework by extending the DVR framework for nonparametric distributions. We exploit the quantile interpolation technique to derive probability distributions representing uncertainty in viewing-ray sample intensities in closed form, which allows for accurate and efficient computation. We evaluate our proposed nonparametric statistical models through qualitative and quantitative comparisons with the mean-field and parametric statistical models, such as uniform and Gaussian, as well as Gaussian mixtures. In addition, we present an extension of the state-of-the-art rendering parametric framework to 2D TFs for improved DVR classifications. We show the applicability of our uncertainty quantification framework to ensemble, downsampled, and bivariate versions of scalar field datasets.", "keywords": "Volumes,uncertainty,nonparametric,2D transfer function", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030394", "refList": ["10.1109/icdm.2012.80", "10.1145/1401890.1401904", "10.1109/tvcg.2017.2745139", "10.18128/d030.v6.0", "10.1111/cgf.12142", "10.1109/tvcg.2009.114", "10.1111/j.1467-8659.2009.01677.x", "10.1109/mcse.2007.55", "10.1109/icde.2012.16", "10.1198/106186008x320465", "10.1145/1835804.1835868", "10.1559/1523040054738936", "10.1109/tvcg.2019.2934432", "10.1109/34.1000236", "10.1109/infvis.2005.1532136", "10.1109/tvcg.2012.128", "10.1109/pacificvis.2017.8031573", "10.1109/tvcg.2016.2598920", "10.1109/tvcg.2018.2865021", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1109/sp.2009.22", "10.1109/tvcg.2017.2744184", "10.1145/773153.773173", "10.1145/2660267.2660348", "10.1109/icdmw.2009.93", "10.1016/j.jtrangeo.2015.09.001", "10.1109/icde.2010.5447831", "10.1007/11681878\\_14", "10.1111/cgf.13409", "10.1007/s13278-014-0205-5", "10.1109/tvcg.2011.163", "10.1145/3035918.3035940", "10.1109/sp.2008.33", "10.1145/2882903.2882931"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934208", "title": "Evaluating Perceptual Bias During Geometric Scaling of Scatterplots", "year": "2019", "conferenceName": "VAST", "authors": "Yating Wei;Honghui Mei;Ying Zhao;Shuyue Zhou;Bingru Lin;Haojing Jiang;Wei Chen", "citationCount": "5", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou 310058, Zhejiang, Peoples R China. Zhao, Y (Corresponding Author), Cent South Univ, Sch Comp Sci \\& Engn, Changsha 410083, Hunan, Peoples R China. Wei, Yating; Mei, Honghui; Zhou, Shuyue; Lin, Bingru; Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou 310058, Zhejiang, Peoples R China. Zhao, Ying; Jiang, Haojing, Cent South Univ, Sch Comp Sci \\& Engn, Changsha 410083, Hunan, Peoples R China.", "countries": "China", "abstract": "Scatterplots are frequently scaled to fit display areas in multi-view and multi-device data analysis environments. A common method used for scaling is to enlarge or shrink the entire scatterplot together with the inside points synchronously and proportionally. This process is called geometric scaling. However, geometric scaling of scatterplots may cause a perceptual bias, that is, the perceived and physical values of visual features may be dissociated with respect to geometric scaling. For example, if a scatterplot is projected from a laptop to a large projector screen, then observers may feel that the scatterplot shown on the projector has fewer points than that viewed on the laptop. This paper presents an evaluation study on the perceptual bias of visual features in scatterplots caused by geometric scaling. The study focuses on three fundamental visual features (i.e., numerosity, correlation, and cluster separation) and three hypotheses that are formulated on the basis of our experience. We carefully design three controlled experiments by using well-prepared synthetic data and recruit participants to complete the experiments on the basis of their subjective experience. With a detailed analysis of the experimental results, we obtain a set of instructive findings. First, geometric scaling causes a bias that has a linear relationship with the scale ratio. Second, no significant difference exists between the biases measured from normally and uniformly distributed scatterplots. Third, changing the point radius can correct the bias to a certain extent. These findings can be used to inspire the design decisions of scatterplots in various scenarios.", "keywords": "Evaluation,scatterplot,geometric scaling,bias,perceptual consistency", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934208", "refList": ["10.2307/2288843", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2014.2346979", "10.1126/science.216.4550.1138", "10.1109/tvcg.2017.2744138", "10.1111/j.1467-8659.2009.01467.x", "10.1145/2491568.2491577", "10.1109/mwc.2018.1700325", "10.1145/2449396.2449439", "10.1109/tvcg.2011.127", "10.1109/tvcg.2011.229", "10.1167/15.5.4", "10.1073/pnas.1113195108", "10.1145/2702123.2702545", "10.1109/vast.2009.5332628", "10.1109/tvcg.2018.2800013", "10.1007/s12650-018-0530-2", "10.1016/s0042-6989(97)00340-4", "10.1109/vast.2010.5652460", "10.1109/mcg.2018.2879067", "10.1109/tvcg.2018.2864912", "10.1109/pacificvis.2010.5429604", "10.1109/tcst.2018.2819965", "10.1167/10.2.10", "10.1145/2470654.2481318", "10.1145/1842993.1843002", "10.1167/12.6.8", "10.1109/tvcg.2013.124", "10.1057/ivs.2008.13", "10.1109/tvcg.2007.70596", "10.1109/tvcg.2018.2865266", "10.1145/3173574.3173664", "10.1109/tvcg.2015.2467671", "10.1016/j.cag.2017.07.004", "10.1177/0956797613501520", "10.1109/tvcg.2018.2865020", "10.1111/j.1467-8659.2012.03125.x", "10.3758/bf03205986", "10.3758/s13423-016-1174-7", "10.1109/tvcg.2017.2680452", "10.1109/mc.2006.109", "10.1109/tvcg.2017.2744098", "10.1038/srep32810", "10.1016/j.visres.2013.06.006", "10.1002/jhbs.20078", "10.1109/tvcg.2006.163", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1016/j.cognition.2007.10.009", "10.1109/tvcg.2018.2865142", "10.3758/app.72.7.1839", "10.1109/tvcg.2018.2810918", "10.1016/j.jvlc.2017.10.001", "10.1145/2682623", "10.1109/tvcg.2018.2864884", "10.1145/3025453.3025984", "10.1109/tvcg.2006.184", "10.1109/tvcg.2013.153", "10.1016/j.jvlc.2018.08.003", "10.1109/tvcg.2016.2520921", "10.1111/cgf.13446", "10.1017/s0022381612000187", "10.1145/2702123.2702406", "10.1109/vast.2012.6400487", "10.1109/tvcg.2013.183", "10.1177/1473871611415997", "10.1145/2702123.2702585", "10.1145/2993901.2993903", "10.1109/tvcg.2013.120", "10.1111/cgf.12632", "10.1145/1385569.1385602", "10.1109/tvcg.2017.2754480", "10.1111/j.1467-8659.2009.01694.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934300", "title": "GUIRO: User-Guided Matrix Reordering", "year": "2019", "conferenceName": "VAST", "authors": "Michael Behrisch;Tobias Schreck;Hanspeter Pfister", "citationCount": "1", "affiliation": "Behrisch, M (Corresponding Author), Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Behrisch, Michael; Pfister, Hanspeter, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Schreck, Tobias, Graz Univ Technol, Graz, Austria.", "countries": "USA;Austria", "abstract": "Matrix representations are one of the main established and empirically proven to be effective visualization techniques for relational (or network) data. However, matrices\u2014similar to node-link diagrams\u2014are most effective if their layout reveals the underlying data topology. Given the many developed algorithms, a practical problem arises: \u201cWhich matrix reordering algorithm should I choose for my dataset at hand?\u201d To make matters worse, different reordering algorithms applied to the same dataset may let significantly different visual matrix patterns emerge. This leads to the question of trustworthiness and explainability of these fully automated, often heuristic, black-box processes. We present GUIRO, a Visual Analytics system that helps novices, network analysts, and algorithm designers to open the black-box. Users can investigate the usefulness and expressiveness of 70 accessible matrix reordering algorithms. For network analysts, we introduce a novel model space representation and two interaction techniques for a user-guided reordering of rows or columns, and especially groups thereof (submatrix reordering). These novel techniques contribute to the understanding of the global and local dataset topology. We support algorithm designers by giving them access to 16 reordering quality metrics and visual exploration means for comparing reordering implementations on a row/column permutation level. We evaluated GUIRO in a guided explorative user study with 12 subjects, a case study demonstrating its usefulness in a real-world scenario, and through an expert study gathering feedback on our design decisions. We found that our proposed methods help even inexperienced users to understand matrix patterns and allow a user-guided steering of reordering algorithms. GUIRO helps to increase the transparency of matrix reordering algorithms, thus helping a broad range of users to get a better insight into the complex reordering process, in turn supporting data and reordering algorithm insights.", "keywords": "Visual Analytics,matrix,black-box algorithms,seriation,ordering,sorting,steerable algorithm,interaction,2D projection", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934300", "refList": ["10.1109/tvcg.2007.70582", "10.2307/276978", "10.1109/tvcg.2008.61", "10.3390/su10040973", "10.1101/121889", "10.1145/1345448.1345453", "10.1186/s12879-014-0695-9", "10.1186/1471-2105-9-155", "10.1145/1124772.1124891", "10.1109/tvcg.2010.159", "10.1111/j.2044-8317.1974.tb00534.x", "10.1109/tpami.2015.2470671", "10.1198/000313005x22770", "10.1109/tvcg.2012.219", "10.1002/sam.10071", "10.1016/j.ejor.2016.08.066", "10.1007/s00265-003-0651-y", "10.1109/biovis.2013.6664342", "10.3115/v1/p14-1062", "10.3389/fpsyg.2017.01349", "10.1109/tpami.2004.1265866", "10.1057/palgrave.ivs.9500086", "10.1007/s004260000031", "10.1111/cgf.12935", "10.1145/800195.805928", "10.1109/tvcg.2014.2346279", "10.2312/eurovisstar.20141174", "10.1109/tvcg.2012.256", "10.1093/bioinformatics/17.suppl\\_1.s22", "10.1109/infvis.2004.46", "10.1007/978-3-319-06793-3\\_5", "10.1109/tvcg.2006.147", "10.1109/tvcg.2015.2468078", "10.1093/bioinformatics/bti141", "10.1145/1168149.1168168", "10.1109/tvcg.2017.2745978", "10.1177/1473871613513228", "10.1109/mcg.2014.62", "10.1109/tvcg.2006.166", "10.1145/2470654.2470724", "10.1177/154193120605000909", "10.1109/sibgrapi.2007.21", "10.1145/3065386", "10.1109/tvcg.2006.160", "10.1287/opre.20.5.993", "10.1111/cgf.13446", "10.1057/palgrave.ivs.9500092", "10.1145/568522.568523", "10.1109/mcg.2013.66", "10.1109/tvcg.2018.2865940", "10.1109/tvcg.2012.208"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934796", "title": "Improving the Robustness of Scagnostics", "year": "2019", "conferenceName": "InfoVis", "authors": "Yunhai Wang;Zeyu Wang 0005;Tingting Liu;Michael Correll;Zhanglin Cheng;Oliver Deussen;Michael Sedlmair", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Jinan, Peoples R China. Wang, Yunhai; Wang, Zeyu; Liu, Tingting, Shandong Univ, Jinan, Peoples R China. Wang, Zeyu; Cheng, Zhanglin; Deussen, Oliver, SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Peoples R China. Correll, Michael, Tableau Res, Seattle, WA USA. Deussen, Oliver, Konstanz Univ, Constance, Germany. Sedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "In this paper, we examine the robustness of scagnostics through a series of theoretical and empirical studies. First, we investigate the sensitivity of scagnostics by employing perturbing operations on more than 60M synthetic and real-world scatterplots. We found that two scagnostic measures, Outlying and Clumpy, are overly sensitive to data binning. To understand how these measures align with human judgments of visual features, we conducted a study with 24 participants, which reveals that i) humans are not sensitive to small perturbations of the data that cause large changes in both measures, and ii) the perception of clumpiness heavily depends on per-cluster topologies and structures. Motivated by these results, we propose Robust Scagnostics (RScag) by combining adaptive binning with a hierarchy-based form of scagnostics. An analysis shows that RScag improves on the robustness of original scagnostics, aligns better with human judgments, and is equally fast as the traditional scagnostic measures.", "keywords": "Scagnostics,scatterplots,sensitivity analysis,Robust Scagnostics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934796", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2346979", "10.1111/j.1467-8659.2009.01467.x", "10.1109/vast.2008.4677368", "10.2307/2289444", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/vast.2009.5332628", "10.1111/insr.12095\\_11", "10.1109/tvcg.2010.184", "10.1109/tvcg.2015.2467323", "10.1109/vast.2010.5652460", "10.1111/j.1467-8659.2012.03069.x", "10.1145/1842993.1843002", "10.1198/106186008x320465", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.20", "10.1111/cgf.12641", "10.1109/tvcg.2012.128", "10.1109/tvcg.2015.2467671", "10.1057/palgrave.ivs.9500091", "10.1007/978-1-4612-4400-4", "10.1111/cgf.13176", "10.1002/0470870958", "10.1109/tvcg.2018.2864907", "10.1111/j.1467-8659.2012.03125.x", "10.1109/vast.2012.6400490", "10.1109/infvis.2005.1532142", "10.1109/vast.2010.5652433", "10.1109/vast.2009.5332611", "10.1201/9781315140919", "10.1145/2858036.2858155", "10.1109/ldav.2013.6675164", "10.1515/itit-2014-1070", "10.1111/cgf.13684", "10.1109/tpami.1979.4766909", "10.2307/2289936", "10.1109/pacificvis.2014.42", "10.1109/tvcg.2016.2598467", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2006.94", "10.1109/tvcg.2014.2346572", "10.1111/cgf.12632", "10.1109/tvcg.2017.2744339"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/vast47406.2019.8986917", "title": "VIANA: Visual Interactive Annotation of Argumentation", "year": "2019", "conferenceName": "VAST", "authors": "Fabian Sperrle;Rita Sevastjanova;Rebecca Kehlbeck;Mennatallah El-Assady", "citationCount": "0", "affiliation": "Sperrle, F (Corresponding Author), Univ Konstanz, Constance, Germany. Sperrle, Fabian; Sevastjanova, Rita; Kehlbeck, Rebecca; El-Assady, Mennatallah, Univ Konstanz, Constance, Germany.", "countries": "Germany", "abstract": "Argumentation Mining addresses the challenging tasks of identifying boundaries of argumentative text fragments and extracting their relationships. Fully automated solutions do not reach satisfactory accuracy due to their insufficient incorporation of semantics and domain knowledge. Therefore, experts currently rely on time-consuming manual annotations. In this paper, we present a visual analytics system that augments the manual annotation process by automatically suggesting which text fragments to annotate next. The accuracy of those suggestions is improved over time by incorporating linguistic knowledge and language modeling to learn a measure of argument similarity from user interactions. Based on a long-term collaboration with domain experts, we identify and model five high-level analysis tasks. We enable close reading and note-taking, annotation of arguments, argument reconstruction, extraction of argument relations, and exploration of argument graphs. To avoid context switches, we transition between all views through seamless morphing, visually anchoring all text- and graph-based layers. We evaluate our system with a two-stage expert user study based on a corpus of presidential debates. The results show that experts prefer our system over existing solutions due to the speedup provided by the automatic suggestions and the tight integration between text and graph views.", "keywords": "Argumentation annotation,machine learning,user interaction,layered interfaces,semantic transitions", "link": "http://dx.doi.org/10.1109/VAST47406.2019.8986917", "refList": ["10.1007/978-3-642-40624-9\\_1", "10.3233/978-1-61499-436-7-185", "10.1145/371920.372071", "10.3233/978-1-61499-906-5-4", "10.1109/tvcg.2015.2467759", "10.18653/v1/d15-1050", "10.2307/2529310", "10.1109/mic.2003.1167344", "10.1145/312624.312682", "10.1145/2850417", "10.18653/v1/p17-2039", "10.1016/j.eswa.2016.02.013", "10.1007/978-0-387-85820-3\\_3", "10.1007/s11412-009-9080-x", "10.1109/tvcg.2008.127", "10.1145/2207676.2207741", "10.1109/cit.2012.217", "10.3233/aac-170022", "10.1109/tvcg.2017.2745080", "10.1007/978-3-319-44039-2\\_6", "10.1145/3290605.3300233", "10.1007/978-94-017-0431-1", "10.3233/978-1-61499-906-5-313", "10.1142/s0218213004001922", "10.1109/tvcg.2012.262", "10.1177/001316446002000104", "10.1109/tvcg.2018.2834341", "10.3233/978-1-61499-436-7-463", "10.1145/1772690.1772773", "10.1109/tvcg.2014.2346677", "10.1145/2523813", "10.1162/153244303322533223", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2015.2467531", "10.1109/tvcg.2018.2864769", "10.1007/978-3-319-90092-6\\_14", "10.1137/1.9781611972801.19", "10.1109/vast.2012.6400485", "10.1007/s10579-013-9215-6", "10.3115/v1/d14-1162", "10.1111/cgf.13446", "10.1109/tvcg.2006.156", "10.1111/cgf.13092", "10.1145/2645710.2645759", "10.1109/bigdata.2017.8258140", "10.1145/2669557.2669572", "10.2312/eurovisstar.20151113"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030376", "title": "Insight Beyond Numbers: The Impact of Qualitative Factors on Visual Data Analysis", "year": "2020", "conferenceName": "VAST", "authors": "Benjamin Karer;Hans Hagen;Dirk J. Lehmann", "citationCount": "0", "affiliation": "Karer, B (Corresponding Author), Fed Criminal Police Off Germany, Wiesbaden, Germany. Karer, Benjamin, Fed Criminal Police Off Germany, Wiesbaden, Germany. Hagen, Hans, TU Kaiserslautern, Kaiserslautern, Germany. Lehmann, Dirk J., Ostfalia Univ Appl Sci, Wolfenbuttel, Germany. Lehmann, Dirk J., IAV GmbH, Berlin, Germany.", "countries": "Germany", "abstract": "As of today, data analysis focuses primarily on the findings to be made inside the data and concentrates less on how those findings relate to the domain of investigation. Contemporary visualization as a field of research shows a strong tendency to adopt this data-centrism. Despite their decisive influence on the analysis result, qualitative aspects of the analysis process such as the structure, soundness, and complexity of the applied reasoning strategy are rarely discussed explicitly. We argue that if the purpose of visualization is the provision of domain insight rather than the depiction of data analysis results, a holistic perspective requires a qualitative component to to be added to the discussion of quantitative and human factors. To support this point, we demonstrate how considerations of qualitative factors in visual analysis can be applied to obtain explanations and possible solutions for a number of practical limitations inherent to the data-centric perspective on analysis. Based on this discussion of what we call qualitative visual analysis, we develop an inside-outside principle of nested levels of context that can serve as a conceptual basis for the development of visualization systems that optimally support the emergence of insight during analysis.", "keywords": "Visualization,Reasoning,Qualitative Aspects", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030376", "refList": ["10.1057/ivs.2009.22", "10.1109/tvcg.2015.2467732", "10.1109/infvis.2000.885092", "10.1109/vast.2014.7042482", "10.1145/1498700.1498704", "10.1109/tvcg.2009.108", "10.1109/tvcg.2018.2829750", "10.1037/0033-295x.111.4.1036", "10.1109/tvcg.2012.199", "10.1109/38.267476", "10.1007/978-3-540-70956-5\\_2", "10.1109/tvcg.2015.2467613", "10.1109/tvcg.2015.2467195", "10.1109/tvcg.2014.2346419", "10.1109/vast.2017.8585669", "10.1109/tvcg.2010.132", "10.1145/22949.22950", "10.1109/tvcg.2014.2346984", "10.1017/cbo9780511816772", "10.1109/mcg.2003.1231171", "10.1111/coin.12227", "10.1109/tvcg.2018.2865138", "10.1109/38.788803", "10.1109/tvcg.2018.2864849", "10.1109/mcg.2012.120", "10.1109/tvcg.2007.70535", "10.1162/neco.2008.12-06-420", "10.1109/tvcg.2018.2865240", "10.1089/tmj.2010.0114", "10.1109/beliv.2018.8634267", "10.1109/tvcg.2014.2346481", "10.1207/s15327809jls0402\\_2", "10.1109/tvcg.2006.80", "10.1145/2858036.2858280", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2011.52", "10.1016/j.cag.2014.03.002", "10.1109/tvcg.2015.2513410", "10.1111/j.1756-8765.2011.01150.x", "10.1109/infvis.2005.1532142", "10.1002/spe.4380211102", "10.1186/s41235-018-0120-9", "10.1073/pnas.1807180116", "10.1109/tvcg.2012.133", "10.1109/tvcg.2012.273", "10.1111/j.1467-8659.2011.01928.x", "10.1145/353485.353486", "10.1109/tvcg.2015.2462356", "10.1109/mcg.2019.2923483", "10.1111/cgf.13264", "10.1057/palgrave.ivs.9500070", "10.1145/1168149.1168158", "10.1109/mcg.2019.2961716", "10.1016/s0167-739x(96)00029-5", "10.1111/j.1467-8659.2009.01667.x", "10.1177/1473871611415989", "10.1109/tvcg.2013.234", "10.1109/iv.2012.33", "10.1111/cgf.13446", "10.1057/ivs.2008.28", "10.1111/cgf.12379", "10.1037/0033-295x.112.1.159", "10.1109/tvcg.2010.161", "10.1109/tvcg.2017.2744319", "10.1145/989863.989880", "10.1007/s11390-016-1663-1", "10.1177/1473871615609787", "10.1016/j.cola.2019.100911"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030395", "title": "Towards Modeling Visualization Processes as Dynamic Bayesian Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Christian Heine 0002", "citationCount": "0", "affiliation": "Heine, C (Corresponding Author), Univ Leipzig, Leipzig, Germany. Heine, Christian, Univ Leipzig, Leipzig, Germany.", "countries": "Germany", "abstract": "Visualization designs typically need to be evaluated with user studies, because their suitability for a particular task is hard to predict. What the field of visualization is currently lacking are theories and models that can be used to explain why certain designs work and others do not. This paper outlines a general framework for modeling visualization processes that can serve as the first step towards such a theory. It surveys related research in mathematical and computational psychology and argues for the use of dynamic Bayesian networks to describe these time-dependent, probabilistic processes. It is discussed how these models could be used to aid in design evaluation. The development of concrete models will be a long process. Thus, the paper outlines a research program sketching how to develop prototypes and their extensions from existing models, controlled experiments, and observational studies.", "keywords": "Visualization,model building,perception,cognition,dynamic Bayesian networks", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030395", "refList": ["10.1057/ivs.2009.22", "10.1109/tvcg.2015.2467732", "10.1109/infvis.2000.885092", "10.1109/vast.2014.7042482", "10.1145/1498700.1498704", "10.1109/tvcg.2009.108", "10.1109/tvcg.2018.2829750", "10.1037/0033-295x.111.4.1036", "10.1109/tvcg.2012.199", "10.1111/cgf.13899", "10.1109/38.267476", "10.1007/978-3-540-70956-5\\_2", "10.1109/tvcg.2015.2467613", "10.1109/tvcg.2015.2467195", "10.1109/tvcg.2014.2346419", "10.1145/3290605.3300562", "10.1109/vl.1996.545307", "10.1109/vast.2017.8585669", "10.1109/infvis.2003.1249004", "10.1109/tvcg.2010.132", "10.1145/22949.22950", "10.1109/tvcg.2014.2346984", "10.1145/3290605.3300418", "10.1017/cbo9780511816772", "10.1109/mcg.2003.1231171", "10.1111/coin.12227", "10.1109/tvcg.2018.2865138", "10.1109/38.788803", "10.1109/tvcg.2018.2864849", "10.1109/mcg.2012.120", "10.1109/tvcg.2007.70535", "10.1162/neco.2008.12-06-420", "10.1109/tvcg.2018.2865240", "10.1089/tmj.2010.0114", "10.1109/beliv.2018.8634267", "10.1109/tvcg.2014.2346481", "10.1207/s15327809jls0402\\_2", "10.1109/tvcg.2006.80", "10.1145/2858036.2858280", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2011.52", "10.1016/j.cag.2014.03.002", "10.1109/tvcg.2015.2513410", "10.1111/j.1756-8765.2011.01150.x", "10.1109/infvis.2005.1532142", "10.1002/spe.4380211102", "10.1186/s41235-018-0120-9", "10.1073/pnas.1807180116", "10.1109/tvcg.2012.133", "10.1109/tvcg.2012.273", "10.1111/j.1467-8659.2011.01928.x", "10.1145/353485.353486", "10.1109/tvcg.2015.2462356", "10.1109/mcg.2019.2923483", "10.1111/cgf.13264", "10.1057/palgrave.ivs.9500070", "10.1145/1168149.1168158", "10.1109/mcg.2019.2961716", "10.1016/s0167-739x(96)00029-5", "10.1111/j.1467-8659.2009.01667.x", "10.1177/1473871611415989", "10.1109/tvcg.2013.234", "10.1109/iv.2012.33", "10.1111/cgf.13446", "10.1057/ivs.2008.28", "10.1111/cgf.12379", "10.1037/0033-295x.112.1.159", "10.1017/cb09781139033916.005", "10.1109/tvcg.2010.161", "10.1109/tvcg.2017.2744319", "10.1145/989863.989880", "10.1007/s11390-016-1663-1", "10.1177/1473871615609787", "10.1016/j.cola.2019.100911", "10.1057/palgrave.ivs.9500025"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.8199", "year": "2020", "title": "Efficient Morphing of Shape-preserving Star Coordinates", "conferenceName": "PacificVis", "authors": "Vladimir Molchanov;Sagad Hamid;Lars Linsen", "citationCount": "0", "affiliation": "Molchanov, V (Corresponding Author), Westfalische Wilhelms Univ Munster, Munster, Germany.\nMolchanov, Vladimir; Hamid, Sagad; Linsen, Lars, Westfalische Wilhelms Univ Munster, Munster, Germany.", "countries": "Germany", "abstract": "Data tours follow an exploratory multi-dimensional data visualization concept that provides animations of projections of the multidimensional data to a 2D visual space. To create an animation, a sequence of key projections is provided and morphings between each pair of consecutive key projections are computed, which then can be stitched together to form the data tour. The morphings should be smooth so that a user can easily follow the transformations, and their computations shall be fast to allow for their integration into an interactive visual exploration process. Moreover, if the key projections are chosen to satisfy additional conditions, it is desirable that these conditions are maintained during morphing. Shape preservation is such a desirable condition, as it avoids shape distortions that may otherwise be caused by a projection. We develop a novel efficient morphing algorithms for computing shape-preserving data tours, i.e., data tours constructed for a sequence of shape-preserving linear projections. We propose a stepping strategy for the morphing to avoid discontinuities in the evolution of the projections, where we represent the linear projections using a star-coordinates system. Our algorithms are less computationally involved, produce smoother morphings, and require less user-defined parameter settings than existing state-of-the-art approaches.", "keywords": "Human-centered computing; Visualization; Visualization techniques", "link": "https://doi.org/10.1109/PacificVis48177.2020.8199", "refList": ["10.2312/eurovisshort.20141152", "10.1109/tvcg.2013.182", "10.1109/tvcg.2008.153", "10.1109/tvcg.2015.2467591", "10.4135/9781412985130", "10.1111/cgf.12878", "10.1016/j.cag.2016.08.007", "10.1080/10618600.1995.10474674", "10.1109/infvis.2003.1249004", "10.1002/9781118445112.stat06472", "10.1109/visual.1997.663916", "10.1111/cgf.12117", "10.1137/0906011", "10.1109/tvcg.2018.2865118", "10.1126/science.290.5500.2319", "10.1111/cgf.12845", "10.1111/j.1467-8659.2012.03125.x", "10.1109/tvcg.2017.2705189", "10.1002/0471725293", "10.1111/cgf.12876", "10.1111/cgf.13404", "10.2307/2289161", "10.1111/cgf.13446", "10.1198/106186008x318440", "10.2307/1390747", "10.1109/tvcg.2012.35", "10.1109/tvcg.2015.2467132", "10.1109/tvcg.2006.94", "10.1109/t-c.1974.224051", "10.1109/pacificvis.2019.00018", "10.1109/tvcg.2017.2744339", "10.1109/tsmcb.2005.850151"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2019.00018", "year": "2019", "title": "Scatterplot Summarization by Constructing Fast and Robust Principal Graphs from Skeletons", "conferenceName": "PacificVis", "authors": "Jos{\\'{e}} Matute;Marcel Fischer;Alexandru C. Telea;Lars Linsen", "citationCount": "1", "affiliation": "Matute, J (Corresponding Author), Univ Munster, Munster, Germany.\nMatute, Jose; Fischer, Marcel; Linsen, Lars, Univ Munster, Munster, Germany.\nTelea, Alexandru C., Univ Groningen, Groningen, Netherlands.", "countries": "Germany;Netherlands", "abstract": "Principal curves are a long-standing and well-known method for summarizing large scatterplots. They are defined as self-consistent curves (or curve sets in the more general case) that locally pass through the middle of the scatterplot data. However, computing principal curves that capture well complex scatterplot topologies and are robust to noise is hard and/or slow for large scatterplots. We present a fast and robust approach for computing principal graphs (a generalization of principal curves for more complex topologies) inspired by the similarity to medial descriptors (curves locally centered in a shape). Compared to state-of-the-art methods for computing principal graphs, we outperform these in terms of computational scalability and robustness to noise and resolution. We also demonstrate the advantages of our method over other scatterplot summarization approaches.", "keywords": "", "link": "https://doi.org/10.1109/PacificVis.2019.00018", "refList": ["10.1016/j.cag.2014.01.006", "10.1016/s0167-8655(02)00032-6", "10.1006/jmva.2000.1917", "10.1111/cgf.12386", "10.1109/vast.2008.4677367", "10.1109/tpami.2004.1261076", "10.1111/cgf.12865", "10.1109/tvcg.2010.213", "10.1109/tvcg.2014.2346455", "10.1109/tvcg.2011.233", "10.1111/j.1467-8659.2009.01680.x", "10.1109/tvcg.2016.2515611", "10.1109/tit.1982.1056489", "10.1109/34.899944", "10.1007/bf01889678", "10.1137/s0036144599352836", "10.1145/2858036.2858155", "10.1002/jhbs.20078", "10.1198/jcgs.2011.09224", "10.1109/tvcg.2017.2744184", "10.1057/ivs.2010.2", "10.1111/j.1467-8659.2012.03107.x", "10.2307/2289936", "10.1109/pacificvis.2014.42", "10.1109/tpami.2008.21", "10.1111/cgf.13446", "10.2307/2290446", "10.1109/34.982884", "10.1109/tvcg.2006.94", "10.2307/2288711", "10.1117/12.304651", "10.5201/ipol.2013.87", "10.1109/mlsp.2008.4685520"], "wos": 1, "children": [{"doi": "10.1109/pacificvis48177.2020.8199", "year": "2020", "title": "Efficient Morphing of Shape-preserving Star Coordinates", "conferenceName": "PacificVis", "authors": "Vladimir Molchanov;Sagad Hamid;Lars Linsen", "citationCount": "0", "affiliation": "Molchanov, V (Corresponding Author), Westfalische Wilhelms Univ Munster, Munster, Germany.\nMolchanov, Vladimir; Hamid, Sagad; Linsen, Lars, Westfalische Wilhelms Univ Munster, Munster, Germany.", "countries": "Germany", "abstract": "Data tours follow an exploratory multi-dimensional data visualization concept that provides animations of projections of the multidimensional data to a 2D visual space. To create an animation, a sequence of key projections is provided and morphings between each pair of consecutive key projections are computed, which then can be stitched together to form the data tour. The morphings should be smooth so that a user can easily follow the transformations, and their computations shall be fast to allow for their integration into an interactive visual exploration process. Moreover, if the key projections are chosen to satisfy additional conditions, it is desirable that these conditions are maintained during morphing. Shape preservation is such a desirable condition, as it avoids shape distortions that may otherwise be caused by a projection. We develop a novel efficient morphing algorithms for computing shape-preserving data tours, i.e., data tours constructed for a sequence of shape-preserving linear projections. We propose a stepping strategy for the morphing to avoid discontinuities in the evolution of the projections, where we represent the linear projections using a star-coordinates system. Our algorithms are less computationally involved, produce smoother morphings, and require less user-defined parameter settings than existing state-of-the-art approaches.", "keywords": "Human-centered computing; Visualization; Visualization techniques", "link": "https://doi.org/10.1109/PacificVis48177.2020.8199", "refList": ["10.2312/eurovisshort.20141152", "10.1109/tvcg.2013.182", "10.1109/tvcg.2008.153", "10.1109/tvcg.2015.2467591", "10.4135/9781412985130", "10.1111/cgf.12878", "10.1016/j.cag.2016.08.007", "10.1080/10618600.1995.10474674", "10.1109/infvis.2003.1249004", "10.1002/9781118445112.stat06472", "10.1109/visual.1997.663916", "10.1111/cgf.12117", "10.1137/0906011", "10.1109/tvcg.2018.2865118", "10.1126/science.290.5500.2319", "10.1111/cgf.12845", "10.1111/j.1467-8659.2012.03125.x", "10.1109/tvcg.2017.2705189", "10.1002/0471725293", "10.1111/cgf.12876", "10.1111/cgf.13404", "10.2307/2289161", "10.1111/cgf.13446", "10.1198/106186008x318440", "10.2307/1390747", "10.1109/tvcg.2012.35", "10.1109/tvcg.2015.2467132", "10.1109/tvcg.2006.94", "10.1109/t-c.1974.224051", "10.1109/pacificvis.2019.00018", "10.1109/tvcg.2017.2744339", "10.1109/tsmcb.2005.850151"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.13684", "year": "2019", "title": "ClustMe: A Visual Quality Measure for Ranking Monochrome Scatterplots based on Cluster Patterns", "conferenceName": "EuroVis", "authors": "Mostafa M. Abbas;Micha{\\\"{e}}l Aupetit;Michael Sedlmair;Halima Bensmail", "citationCount": "4", "affiliation": "Abbas, MM (Corresponding Author), HBKU, QCRI, Doha, Qatar.\nAbbas, Mostafa M.; Aupetit, Michael; Bensmail, Halima, HBKU, QCRI, Doha, Qatar.\nSedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany.", "countries": "Qatar;Germany", "abstract": "We propose ClustMe, a new visual quality measure to rank monochrome scatterplots based on cluster patterns. ClustMe is based on data collected from a human-subjects study, in which 34 participants judged synthetically generated cluster patterns in 1000 scatterplots. We generated these patterns by carefully varying the free parameters of a simple Gaussian Mixture Model with two components, and asked the participants to count the number of clusters they could see (1 or more than 1). Based on the results, we form ClustMe by selecting the model that best predicts these human judgments among 7 different state-of-the-art merging techniques (Demp). To quantitatively evaluate ClustMe, we conducted a second study, in which 31 human subjects ranked 435 pairs of scatterplots of real and synthetic data in terms of cluster patterns complexity. We use this data to compare ClustMe's performance to 4 other state-of-the-art clustering measures, including the well-known Clumpiness scagnostics. We found that of all measures, ClustMe is in strongest agreement with the human rankings.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13684", "refList": ["10.1109/tvcg.2014.2330617", "10.1109/tvcg.2014.2346979", "10.1109/tvcg.2017.2744138", "10.1109/tvcg.2017.2701829", "10.2307/2529310", "10.1109/tvcg.2011.229", "10.1109/vast.2011.6102437", "10.1068/p030033", "10.1016/j.jvcir.2011.01.005", "10.1109/tnn.2005.845141", "10.1145/2669557.2669559", "10.1167/8.7.6", "10.1145/2993901.2993907", "10.1145/2803140.2803143", "10.1145/1842993.1843002", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.124", "10.1057/ivs.2008.13", "10.1109/vast.2012.6400488", "10.1111/j.1467-9574.2008.00412.x", "10.1214/aos/1176344136", "10.1109/tvcg.2015.2467671", "10.3138/y308-2422-8615-1233", "10.1111/j.1467-8659.2012.03125.x", "10.1177/001316446002000104", "10.1145/2858036.2858155", "10.1007/s11634-010-0058-3", "10.1214/009053605000000417", "10.1109/tvcg.2009.153", "10.1023/a:1018510926151", "10.1109/tvcg.2014.2346978", "10.1007/s10816-016-9307-x", "10.1177/0301006615602599", "10.1109/pacificvis.2014.42", "10.1162/neco.1991.3.2.246", "10.1007/3-540-44491-2\\_3", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2018.2846735", "10.2307/2291604", "10.1109/inf0vis.2005.14", "10.1198/016214502760047131", "10.1007/s00180-008-0119-7", "10.1109/t-c.1974.224051", "10.1111/cgf.12632", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934796", "title": "Improving the Robustness of Scagnostics", "year": "2019", "conferenceName": "InfoVis", "authors": "Yunhai Wang;Zeyu Wang 0005;Tingting Liu;Michael Correll;Zhanglin Cheng;Oliver Deussen;Michael Sedlmair", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Jinan, Peoples R China. Wang, Yunhai; Wang, Zeyu; Liu, Tingting, Shandong Univ, Jinan, Peoples R China. Wang, Zeyu; Cheng, Zhanglin; Deussen, Oliver, SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Peoples R China. Correll, Michael, Tableau Res, Seattle, WA USA. Deussen, Oliver, Konstanz Univ, Constance, Germany. Sedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "In this paper, we examine the robustness of scagnostics through a series of theoretical and empirical studies. First, we investigate the sensitivity of scagnostics by employing perturbing operations on more than 60M synthetic and real-world scatterplots. We found that two scagnostic measures, Outlying and Clumpy, are overly sensitive to data binning. To understand how these measures align with human judgments of visual features, we conducted a study with 24 participants, which reveals that i) humans are not sensitive to small perturbations of the data that cause large changes in both measures, and ii) the perception of clumpiness heavily depends on per-cluster topologies and structures. Motivated by these results, we propose Robust Scagnostics (RScag) by combining adaptive binning with a hierarchy-based form of scagnostics. An analysis shows that RScag improves on the robustness of original scagnostics, aligns better with human judgments, and is equally fast as the traditional scagnostic measures.", "keywords": "Scagnostics,scatterplots,sensitivity analysis,Robust Scagnostics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934796", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2346979", "10.1111/j.1467-8659.2009.01467.x", "10.1109/vast.2008.4677368", "10.2307/2289444", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/vast.2009.5332628", "10.1111/insr.12095\\_11", "10.1109/tvcg.2010.184", "10.1109/tvcg.2015.2467323", "10.1109/vast.2010.5652460", "10.1111/j.1467-8659.2012.03069.x", "10.1145/1842993.1843002", "10.1198/106186008x320465", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.20", "10.1111/cgf.12641", "10.1109/tvcg.2012.128", "10.1109/tvcg.2015.2467671", "10.1057/palgrave.ivs.9500091", "10.1007/978-1-4612-4400-4", "10.1111/cgf.13176", "10.1002/0470870958", "10.1109/tvcg.2018.2864907", "10.1111/j.1467-8659.2012.03125.x", "10.1109/vast.2012.6400490", "10.1109/infvis.2005.1532142", "10.1109/vast.2010.5652433", "10.1109/vast.2009.5332611", "10.1201/9781315140919", "10.1145/2858036.2858155", "10.1109/ldav.2013.6675164", "10.1515/itit-2014-1070", "10.1111/cgf.13684", "10.1109/tpami.1979.4766909", "10.2307/2289936", "10.1109/pacificvis.2014.42", "10.1109/tvcg.2016.2598467", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2006.94", "10.1109/tvcg.2014.2346572", "10.1111/cgf.12632", "10.1109/tvcg.2017.2744339"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.14001", "year": "2020", "title": "Sunspot Plots: Model-based Structure Enhancement for Dense Scatter Plots", "conferenceName": "EuroVis", "authors": "Thomas Trautner;Fabian Bolte;Sergej Stoppel;Stefan Bruckner", "citationCount": "0", "affiliation": "Trautner, T (Corresponding Author), Univ Bergen, Bergen, Norway.\nTrautner, T.; Bolte, F.; Stoppel, S.; Bruckner, S., Univ Bergen, Bergen, Norway.", "countries": "Norway", "abstract": "Scatter plots are a powerful and well-established technique for visualizing the relationships between two variables as a collection of discrete points. However, especially when dealing with large and dense data, scatter plots often exhibit problems such as overplotting, making the data interpretation arduous. Density plots are able to overcome these limitations in highly populated regions, but fail to provide accurate information of individual data points. This is particularly problematic in sparse regions where the density estimate may not provide a good representation of the underlying data. In this paper, we present sunspot plots, a visualization technique that communicates dense data as a continuous data distribution, while preserving the discrete nature of data samples in sparsely populated areas. We furthermore demonstrate the advantages of our approach on typical failure cases of scatter plots within synthetic and real-world data sets and validate its effectiveness in a user study.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14001", "refList": ["10.1057/palgrave.ivs.9500122", "10.2312/eggh/hpg12/097-103", "10.1109/tvcg.2008.119", "10.1109/pacificvis.2011.5742387", "10.1038/331163a0", "10.1109/visual.2019.8933620", "10.2307/2683294", "10.1201/9781351072304", "10.2307/2289444", "10.1109/tvcg.2019.2934541", "10.1080/00949657508810123", "10.1109/tvcg.2013.65", "10.1109/infvis.1997.636789", "10.1109/2945.841121", "10.1109/mcse.2007.55", "10.1109/tvcg.2010.197", "10.1111/cgf.12871", "10.1109/infvis.2003.1249018", "10.1145/3173574.3173991", "10.1109/tvcg.2012.238", "10.1007/0-387-28695-0", "10.1109/pacificvis.2010.5429604", "10.18637/jss.v008.i03", "10.1109/tvcg.2007.70596", "10.1109/visual.1998.745301", "10.1007/0-387-37977-0\\_3", "10.1145/1556262.1556289", "10.2312/wiced.20161094", "10.1109/tvcg.2007.70535", "10.1145/1056808.1056914", "10.1109/tvcg.2003.1196007", "10.1109/iv.2004.1320190", "10.1111/cgf.12877", "10.1162/leon.2007.40.2.202a", "10.1109/tvcg.2017.2674978", "10.1057/ivs.2010.4", "10.1016/j.cag.2018.02.008", "10.1201/9781315140919", "10.1109/visual.2000.885677", "10.1002/jhbs.20078", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1109/visual.2001.964495", "10.1109/iv.2002.1028760", "10.1111/cgf.13684", "10.1109/hicss.2013.197", "10.1109/tvcg.2019.2903956", "10.1093/mnras/stt961", "10.1109/tvcg.2017.2668409", "10.1111/j.1467-8659.2009.01478.x", "10.1145/2702123.2702585", "10.2307/1418003", "10.2307/1390742", "10.1145/360825.360839", "10.1109/pacificvis.2009.4906843", "10.1057/ivs.2009.34", "10.2307/2288711"], "wos": 1, "children": [], "len": 1}], "len": 13}, {"doi": "10.1111/cgf.14000", "year": "2020", "title": "Evaluating Reordering Strategies for Cluster Identification in Parallel Coordinates", "conferenceName": "EuroVis", "authors": "Michael Blumenschein;Xuan Zhang;David Pomerenke;Daniel A. Keim;Johannes Fuchs", "citationCount": "0", "affiliation": "Blumenschein, M (Corresponding Author), Univ Konstanz, Constance, Germany.\nBlumenschein, Michael; Pomerenke, David; Keim, Daniel A.; Fuchs, Johannes, Univ Konstanz, Constance, Germany.\nZhang, Xuan, Rhein Westfal TH Aachen, Aachen, Germany.", "countries": "Germany", "abstract": "The ability to perceive patterns in parallel coordinates plots (PCPs) is heavily influenced by the ordering of the dimensions. While the community has proposed over 30 automatic ordering strategies, we still lack empirical guidance for choosing an appropriate strategy for a given task. In this paper, we first propose a classification of tasks and patterns and analyze which PCP reordering strategies help in detecting them. Based on our classification, we then conduct an empirical user study with 31 participants to evaluate reordering strategies for cluster identification tasks. We particularly measure time, identification quality, and the users' confidence for two different strategies using both synthetic and real-world datasets. Our results show that, somewhat unexpectedly, participants tend to focus on dissimilar rather than similar dimension pairs when detecting clusters, and are more confident in their answers. This is especially true when increasing the amount of clutter in the data. As a result of these findings, we propose a new reordering strategy based on the dissimilarity of neighboring dimension pairs.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14000", "refList": ["10.1111/j.1467-8659.2008.01241.x", "10.2312/conf/eg2013/stars/095-116", "10.1109/tvcg.2014.2346979", "10.1117/12.838819", "10.2307/2290001", "10.1111/cgf.12638", "10.1007/978-3-642-30217-6\\_42", "10.1109/sc.2005.47", "10.1002/wics.145", "10.1109/tvcg.2011.229", "10.1109/vast.2009.5332628", "10.1109/vast.2017.8585613", "10.1145/2993901.2993907", "10.1007/bf00410640", "10.1109/tvcg.2010.184", "10.11591/ijece.v5i6", "10.1111/j.1467-8659.2011.01961.x", "10.1109/visual.1997.663916", "10.1111/j.1467-8659.2012.03129.x", "10.1109/visual.1999.809866", "10.1057/ivs.2008.13", "10.1016/j.visinf.2017.11.001", "10.2307/2282967", "10.1007/978-0-387-68628-8\\_10", "10.5888/pcd9.120082", "10.1109/tvcg.2007.70535", "10.1109/vast.2010.5652450", "10.1007/bf01898350", "10.1016/0010-0285(91)90009-d", "10.1007/978-3-642-24958-7\\_12", "10.1109/infvis.2005.1532142", "10.1057/palgrave.ivs.9500166", "10.1111/j.1467-8659.2008.01239.x", "10.1109/mcse.2015.55", "10.2312/pe/eurovast/eurova12/007-011.7", "10.1109/tvcg.2009.153", "10.1198/jcgs.2010.09136", "10.1145/2463676.2463696", "10.1109/infvis.2005.1532138", "10.1109/tvcg.2010.242", "10.5220/0006097400400051", "10.1109/visual.2019.8933706", "10.1109/atc.2015.7388338", "10.2312/pe/eurovast/eurova12/007-011", "10.5120/5044-7370", "10.1111/cgf.13446", "10.1109/icmla.2012.148", "10.1016/j.jvlc.2015.12.001", "10.1109/tvcg.2015.2466992", "10.1111/j.1467-8659.2009.01666.x", "10.5169/seals-266450", "10.1109/infvis.1998.729559", "10.1016/j.jvlc.2017.10.003", "10.1109/visual.1995.485139", "10.1007/978-0-387-39940-9\\_262", "10.1109/tvcg.2006.138"], "wos": 1, "children": [], "len": 1}], "len": 71}], "len": 215}, {"doi": "10.1109/tvcg.2016.2598468", "title": "Characterizing Guidance in Visual Analytics", "year": "2016", "conferenceName": "VAST", "authors": "Davide Ceneda;Theresia Gschwandtner;Thorsten May;Silvia Miksch;Hans-J\u00f6rg Schulz;Marc Streit;Christian Tominski", "citationCount": "55", "affiliation": "Ceneda, D (Corresponding Author), Theresia Gschwandtner, Vienna, Austria. Ceneda, Davide, Theresia Gschwandtner, Vienna, Austria. Miksch, Silvia, Vienna Univ Technol, Vienna, Austria. May, Thorsten, Fraunhofer IGD, Darmstadt, Germany. Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria. Schulz, Hans-Jorg; Tominski, Christian, Univ Rostock, Rostock, Germany.", "countries": "Germany;Austria", "abstract": "Visual analytics (VA) is typically applied in scenarios where complex data has to be analyzed. Unfortunately, there is a natural correlation between the complexity of the data and the complexity of the tools to study them. An adverse effect of complicated tools is that analytical goals are more difficult to reach. Therefore, it makes sense to consider methods that guide or assist users in the visual analysis process. Several such methods already exist in the literature, yet we are lacking a general model that facilitates in-depth reasoning about guidance. We establish such a model by extending van Wijk's model of visualization with the fundamental components of guidance. Guidance is defined as a process that gradually narrows the gap that hinders effective continuation of the data analysis. We describe diverse inputs based on which guidance can be generated and discuss different degrees of guidance and means to incorporate guidance into VA tools. We use existing guidance approaches from the literature to illustrate the various aspects of our model. As a conclusion, we identify research challenges and suggest directions for future studies. With our work we take a necessary step to pave the way to a systematic development of guidance techniques that effectively support users in the context of VA.", "keywords": "Visual analytics;guidance model;assistance;user support", "link": "http://dx.doi.org/10.1109/TVCG.2016.2598468", "refList": ["10.1145/1345448.1345453", "10.1145/501317.501343", "10.1109/tvcg.2014.2346260", "10.1109/visual.1990.146375", "10.1145/1502650.1502695", "10.1145/108360.108361", "10.1057/palgrave.ivs.9500167", "10.1111/j.1467-8659.2012.03091.x", "10.1109/tvcg.2011.108", "10.1109/visual.2002.1183803", "10.1111/j.1467-8659.2012.03110.x", "10.1145/1378773.1378788", "10.1137/0906011", "10.1145/964442.964480", "10.1111/cgf.12641", "10.2312/pe/eurovast/eurova12/001-005", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2006.80", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2007.70589", "10.1111/j.1467-8659.2010.01816.x", "10.1109/tvcg.2012.23", "10.1145/302979.303030", "10.1007/978-3-642-36763-2\\_42", "10.1109/visual.2000.885678", "10.1109/biovis.2012.6378590", "10.1007/978-3-642-41939-3\\_4", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2005.63", "10.1109/visual.1997.663889", "10.1109/tkde.2005.67", "10.1145/253769.253801", "10.1109/infvis.2004.2", "10.1109/tvcg.2013.120", "10.1109/tvcg.2008.174", "10.2312/vissym/eurovis07/091-098.3,8"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2017.2744805", "title": "SOMFlow: Guided Exploratory Cluster Analysis with Self-Organizing Maps and Analytic Provenance", "year": "2017", "conferenceName": "VAST", "authors": "Dominik Sacha;Matthias Kraus;J\u00fcrgen Bernard;Michael Behrisch;Tobias Schreck;Yuki Asano;Daniel A. Keim", "citationCount": "10", "affiliation": "Sacha, D (Corresponding Author), Univ Konstanz, Constance, Germany. Sacha, Dominik; Kraus, Matthias; Behrisch, Michael; Keim, Daniel A., Univ Konstanz, Constance, Germany. Bernard, Juergen, Tech Univ Darmstadt, Darmstadt, Germany. Schreck, Tobias, Graz Univ Technol, Graz, Austria. Asano, Yuki, Univ Tubingen, Tubingen, Germany.", "countries": "Germany;Austria", "abstract": "Clustering is a core building block for data analysis, aiming to extract otherwise hidden structures and relations from raw datasets, such as particular groups that can be effectively related, compared, and interpreted. A plethora of visual-interactive cluster analysis techniques has been proposed to date, however, arriving at useful clusterings often requires several rounds of user interactions to fine-tune the data preprocessing and algorithms. We present a multi-stage Visual Analytics (VA) approach for iterative cluster refinement together with an implementation (SOMFlow) that uses Self-Organizing Maps (SOM) to analyze time series data. It supports exploration by offering the analyst a visual platform to analyze intermediate results, adapt the underlying computations, iteratively partition the data, and to reflect previous analytical activities. The history of previous decisions is explicitly visualized within a flow graph, allowing to compare earlier cluster refinements and to explore relations. We further leverage quality and interestingness measures to guide the analyst in the discovery of useful patterns, relations, and data partitions. We conducted two pair analytics experiments together with a subject matter expert in speech intonation research to demonstrate that the approach is effective for interactive data analysis, supporting enhanced understanding of clustering results as well as the interactive process itself.", "keywords": "Visual Analytics,Interaction,Visual Cluster Analysis,Quality Metrics,Guidance,Self-Organizing Maps,Time Series", "link": "http://dx.doi.org/10.1109/TVCG.2017.2744805", "refList": ["10.1109/tvcg.2007.70582", "10.1109/tvcg.2015.2467591", "10.1109/tnn.2002.804221", "10.1111/j.1467-8659.2009.01467.x", "10.1111/cgf.12106", "10.1109/tvcg.2014.2346260", "10.1109/tvcg.2011.229", "10.1109/vast.2010.5653042", "10.1111/cgf.12924", "10.1145/2232817.2232844", "10.1109/icdew.2006.75", "10.1109/vast.2014.7042480", "10.1109/tvcg.2013.178", "10.1007/978-3-642-21602-2\\_67", "10.1109/pacificvis.2016.7465244", "10.1145/2207676.2208293", "10.1117/12.872545", "10.1109/hicss.2011.339", "10.1109/vast.2015.7347625", "10.1007/s00799-014-0134-y", "10.1109/tvcg.2014.2346481", "10.1145/1963192.1963283", "10.1007/978-3-642-00304-2\\_1", "10.1016/0377-0427(87)90125-7", "10.1145/2362456.2362485", "10.1016/j.neunet.2012.09.018", "10.1109/tvcg.2011.188", "10.1117/12.2079841", "10.1057/palgrave.ivs.9500170", "10.1057/ivs.2008.29", "10.1109/tvcg.2010.242", "10.1002/(sici)1097-4571(199307)44:6", "10.1007/978-0-387-09823-4\\_56", "10.2200/s00730ed1v01y201608vis007", "10.1016/s1088-467x(99)00013-x", "10.1109/vast.2010.5652443", "10.1109/mcg.2015.50", "10.1016/b978-044450270-4/50003-6", "10.1016/j.neucom.2017.01.105", "10.1109/tvcg.2016.2598468", "10.1007/978-0-85729-079-3", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2864477", "title": "Clustrophile 2: Guided Visual Clustering Analysis", "year": "2018", "conferenceName": "VAST", "authors": "Marco Cavallo;\u00c7agatay Demiralp", "citationCount": "8", "affiliation": "Cavallo, M (Corresponding Author), IBM Res, Yorktown Hts, NY 10598 USA. Cavallo, Marco, IBM Res, Yorktown Hts, NY 10598 USA. Demiralp, Cagatay, MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA. Demiralp, Cagatay, Fitnescity Labs, Cambridge, MA USA.", "countries": "USA", "abstract": "Data clustering is a common unsupervised learning method frequently used in exploratory data analysis. However, identifying relevant structures in unlabeled, high-dimensional data is nontrivial, requiring iterative experimentation with clustering parameters as well as data features and instances. The number of possible clusterings for a typical dataset is vast, and navigating in this vast space is also challenging. The absence of ground-truth labels makes it impossible to define an optimal solution, thus requiring user judgment to establish what can be considered a satisfiable clustering result. Data scientists need adequate interactive tools to effectively explore and navigate the large clustering space so as to improve the effectiveness of exploratory clustering analysis. We introduce Clustrophile 2, a new interactive tool for guided clustering analysis. Clustrophile 2 guides users in clustering-based exploratory analysis, adapts user feedback to improve user guidance, facilitates the interpretation of clusters, and helps quickly reason about differences between clusterings. To this end, Clustrophile 2 contributes a novel feature, the Clustering Tour, to help users choose clustering parameters and assess the quality of different clustering results in relation to current analysis goals and user expectations. We evaluate Clustrophile 2 through a user study with 12 data scientists, who used our tool to explore and interpret sub-cohorts in a dataset of Parkinson's disease patients. Results suggest that Clustrophile 2 improves the speed and effectiveness of exploratory clustering analysis for both experts and non-experts.", "keywords": "Clustering tour,Guided data analysis,Unsupervised learning,Exploratory data analysis,Interactive clustering analysis,Interpretability,Explainability,Visual data exploration recommendation,Dimensionality reduction,What-if analysis,Clustrophile", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864477", "refList": ["10.1002/mds.22340", "10.1186/1471-2105-15-s6-s4", "10.1101/cshperspect.a009282", "10.14778/2831360.2831371", "10.1186/1471-2105-16-s11-s5", "10.1109/tvcg.2012.219", "10.1007/978-0-387-84858-7", "10.1109/icdm.2010.35", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2012.03110.x", "10.1111/1467-9868.00196", "10.1137/0906011", "10.1109/tvcg.2017.2744805", "10.1016/s0306-4379(01)00008-4", "10.1016/j.neucom.2014.09.062", "10.1023/a:1012487302797", "10.1007/bf02985802", "10.1109/tvcg.2013.119", "10.1093/nar/gkv468", "10.1126/science.290.5500.2319", "10.1016/0377-0427(87)90125-7", "10.1109/tvcg.2010.138", "10.1109/tvcg.2011.188", "10.1057/ivs.2008.29", "10.1126/science.290.5500.2323", "10.1109/tvcg.2012.207", "10.1109/34.868688", "10.1007/bf02289565", "10.1109/t-c.1974.224051"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934261", "title": "Ablate, Variate, and Contemplate: Visual Analytics for Discovering Neural Architectures", "year": "2019", "conferenceName": "VAST", "authors": "Dylan Cashman;Adam Perer;Remco Chang;Hendrik Strobelt", "citationCount": "1", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Perer, Adam, Carnegie Mellon Univ, Pittsburgh, PA 15213 USA. Strobelt, Hendrik, MIT IBM Watson AI Lab, Cambridge, MA USA.", "countries": "USA", "abstract": "The performance of deep learning models is dependent on the precise configuration of many layers and parameters. However, there are currently few systematic guidelines for how to configure a successful model. This means model builders often have to experiment with different configurations by manually programming different architectures (which is tedious and time consuming) or rely on purely automated approaches to generate and train the architectures (which is expensive). In this paper, we present Rapid Exploration of Model Architectures and Parameters, or REMAP, a visual analytics tool that allows a model builder to discover a deep learning model quickly via exploration and rapid experimentation of neural network architectures. In REMAP, the user explores the large and complex parameter space for neural network architectures using a combination of global inspection and local experimentation. Through a visual overview of a set of models, the user identifies interesting clusters of architectures. Based on their findings, the user can run ablation and variation experiments to identify the effects of adding, removing, or replacing layers in a given architecture and generate new models accordingly. They can also handcraft new models using a simple graphical interface. As a result, a model builder can build deep learning models quickly, efficiently, and without manual programming. We inform the design of REMAP through a design study with four deep learning model builders. Through a use case, we demonstrate that REMAP allows users to discover performant neural network architectures efficiently using visual exploration and user-defined semi-automated searches through the model space.", "keywords": "visual analytics,neural networks,parameter space exploration", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934261", "refList": ["10.1109/mcg.2018.2878902", "10.1111/cgf.12639", "10.1109/tvcg.2017.2744938", "10.1117/12.2007316", "10.1109/cvpr.2014.81", "10.1016/j.csda.2008.02.031", "10.1080/00994480.2000.10748487", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2013.125", "10.1109/cvpr.2015.7298594", "10.1111/j.1467-8659.2009.01475.x", "10.1109/tvcg.2012.65", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2017.2744805", "10.1145/2487575.2487629", "10.1109/tvcg.2018.2865044", "10.23915/distill.00010", "10.1109/72.279181", "10.1109/tvcg.2017.2744199", "10.1007/s13398-014-0173-7.2", "10.1109/tvcg.2018.2864504", "10.1109/vast.2012.6400490", "10.1007/978-3-319-10590-1\\_53", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2014.2346321", "10.1094/pdis-11-11-0999-pdn", "10.1109/ijcnn.2015.7280767", "10.1109/tvcg.2017.2744878", "10.5555/3326943.3327130", "10.1109/tvcg.2017.2744718", "10.1109/iccv.2015.169", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2014.223", "10.1109/cvpr.2016.90", "10.1109/vast.2010.5652443", "10.1111/cgf.13681", "10.1109/tvcg.2016.2598831", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028888", "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes", "year": "2020", "conferenceName": "VAST", "authors": "Yuxin Ma;Arlen Fan;Jingrui He;Arun Reddy Nelakurthi;Ross Maciejewski", "citationCount": "0", "affiliation": "Ma, YX (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Ma, Yuxin; Fan, Arlen; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. He, Jingrui, Univ Illinois, Champaign, IL USA. Nelakurthi, Arun Reddy, Samsung Res Amer, Mountain View, CA USA.", "countries": "USA", "abstract": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.", "keywords": "Transfer learning,deep learning,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028888", "refList": ["10.1109/tvcg.2014.2346578", "10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1109/tpami.2018.2868685", "10.1145/2702123.2702509", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2016.2598828", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2013.65", "10.1145/2976749.2978318", "10.1007/978-3-030-01424-7\\_27", "10.1109/tvcg.2019.2934261", "10.1007/s11704-016-6028-y", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/iccv.2015.279", "10.1109/mci.2018.2840738", "10.1109/tvcg.2019.2892483", "10.1109/vast.2018.8802509", "10.1109/tvcg.2013.124", "10.1186/s40537-016-0043-6", "10.1109/tvcg.2018.2864475", "10.1145/3200489", "10.1109/tvcg.2018.2865044", "10.1111/cgf.13210", "10.1109/tvcg.2018.2816223", "10.23915/distill.00007", "10.1109/tvcg.2017.2744199", "10.1109/tkde.2018.2876857", "10.1109/tvcg.2019.2934631", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2011.188", "10.1007/978-3-642-15561-1\\_16", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1109/tvcg.2017.2744718", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2016.2598541", "10.1109/tkde.2009.191", "10.1145/3065386", "10.1016/j.ins.2016.03.021", "10.1109/tvcg.2019.2903943", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/iccv.2017.74", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2012.207", "10.1111/cgf.13092", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2017.2754480", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934395", "title": "The Impact of Immersion on Cluster Identification Tasks", "year": "2019", "conferenceName": "InfoVis", "authors": "Matthias Kraus;Niklas Weiler;Daniela Oelke;Johannes Kehrer;Daniel A. Keim;Johannes Fuchs", "citationCount": "4", "affiliation": "Kraus, M (Corresponding Author), Univ Konstanz, Constance, Germany. Kraus, M.; Weiler, N.; Keim, D. A.; Fuchs, J., Univ Konstanz, Constance, Germany. Oelke, D.; Kehrer, J., Siemens Corp Technol, Munich, Germany.", "countries": "Germany", "abstract": "Recent developments in technology encourage the use of head-mounted displays (HMDs) as a medium to explore visualizations in virtual realities (VRs). VR environments (VREs) enable new, more immersive visualization design spaces compared to traditional computer screens. Previous studies in different domains, such as medicine, psychology, and geology, report a positive effect of immersion, e.g., on learning performance or phobia treatment effectiveness. Our work presented in this paper assesses the applicability of those findings to a common task from the information visualization (InfoVis) domain. We conducted a quantitative user study to investigate the impact of immersion on cluster identification tasks in scatterplot visualizations. The main experiment was carried out with 18 participants in a within-subjects setting using four different visualizations, (1) a 2D scatterplot matrix on a screen, (2) a 3D scatterplot on a screen, (3) a 3D scatterplot miniature in a VRE and (4) a fully immersive 3D scatterplot in a VRE. The four visualization design spaces vary in their level of immersion, as shown in a supplementary study. The results of our main study indicate that task performance differs between the investigated visualization design spaces in terms of accuracy, efficiency, memorability, sense of orientation, and user preference. In particular, the 2D visualization on the screen performed worse compared to the 3D visualizations with regard to the measured variables. The study shows that an increased level of immersion can be a substantial benefit in the context of 3D data and cluster detection.", "keywords": "Virtual reality,evaluation,visual analytics,clustering", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934395", "refList": ["10.1111/j.1467-9671.2010.01194.x", "10.1177/1473871614556393", "10.1109/tvcg.2008.153", "10.1111/cgf.13430", "10.1109/tvcg.2004.17", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2007.70433", "10.1089/109493103322011641", "10.2307/2290001", "10.2307/2289444", "10.1016/j.ijhcs.2008.04.004", "10.1109/tvcg.2012.42", "10.1109/vr.2018.8447558", "10.1162/105474602760204318", "10.2307/2986199", "10.3758/bf03200735", "10.1097/opx.0b013e31825da430", "10.1109/infvis.1999.801851", "10.1109/vast.2008.4677350", "10.1162/105474698565686", "10.1109/iv.2013.51", "10.1109/tvcg.2017.2745941", "10.1109/infvis.1998.729555", "10.1111/j.1467-8659.2012.03125.x", "10.1109/2945.506223", "10.1016/s1045-926x(03)00046-6", "10.1097/00042871-200701010-00099", "10.1109/iv.2004.1320137", "10.1109/visual.2002.1183816", "10.1109/tvcg.2018.2864477", "10.1109/vast.2007.4389000", "10.1016/j.ijms.2006.06.015", "10.1007/pl00022704", "10.1111/cgf.13072", "10.1145/3290605.3300555", "10.1109/bdva.2016.7787042", "10.1109/hicss.2013.197", "10.2312/vissym/vissym04/255-260", "10.1115/imece2007-43781", "10.1007/978-4-431-68057-43", "10.1109/tvcg.2016.2520921", "10.1109/mcg.2004.1255801", "10.1109/tvcg.2013.153", "10.1162/pres.1996.5.3.274", "10.1198/106186004x12425", "10.1162/105474601300343612", "10.1162/pres.1997.6.6.603", "10.1111/cgf.12804", "10.1111/j.1467-8659.2009.01666.x", "10.1109/vr.2004.1310069", "10.1109/vr.1999.756938", "10.1007/978-3-658-02897-8\\_16", "10.1162/pres\\_a\\_00016", "10.2307/2288711"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030450", "title": "Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment", "year": "2020", "conferenceName": "InfoVis", "authors": "Benjamin Lee;Xiaoyun Hu;Maxime Cordeil;Arnaud Prouzeau;Bernhard Jenny;Tim Dwyer", "citationCount": "0", "affiliation": "Lee, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Lee, Benjamin; Hu, Xiaoyun; Cordeil, Maxime; Prouzeau, Arnaud; Jenny, Bernhard; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia.", "countries": "Australia", "abstract": "Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA: the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner's personal workspace.", "keywords": "Immersive analytics,collaboration,virtual reality,qualitative study,multivariate data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030450", "refList": ["10.1109/tvcg.2008.153", "10.1007/s10606-004-5062-8", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2019.2914677", "10.1016/0020-7373(91)90039-a", "10.1109/tvcg.2011.287", "10.1117/12.2005484", "10.1109/mmul.2009.35", "10.1109/tvcg.2019.2934803", "10.1145/3359996.3364242", "10.1109/immersive.2016.7932384", "10.1016/j.future.2008.07.015", "10.1057/palgrave.ivs.9500167", "10.1109/mcg.2019.2898941", "10.1145/3343055.3360746", "10.1007/978-3-319-45853-3\\_8", "10.1145/2576099", "10.1145/2858036.2858039", "10.1109/tvcg.2016.2599107", "10.1145/3173574.3173664", "10.1007/978-3-030-01388-2\\_2", "10.1109/vr.2019.8797978", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1007/978-3-030-01388-2\\_8", "10.1023/a:1021271517844", "10.1109/bigdata.2014.7004282", "10.1109/tvcg.2019.2934395", "10.1109/ismar.2010.5643530", "10.1007/978-3-030-01388-22", "10.1145/2133806.2133821", "10.1109/3dvis.2014.7160093", "10.1145/2556288.2557058", "10.1109/vr.2019.8797845"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/tvcg.2020.3030347", "title": "Integrating Prior Knowledge in Mixed-Initiative Social Network Clustering", "year": "2020", "conferenceName": "VAST", "authors": "Alexis Pister;Paolo Buono;Jean-Daniel Fekete;Catherine Plaisant;Paola Valdivia", "citationCount": "0", "affiliation": "Pister, A (Corresponding Author), Univ Paris Saclay, CNRS, Inria, LRI, Gif Sur Yvette, France. Pister, Alexis; Fekete, Jean-Daniel; Plaisant, Catherine; Valdivia, Paola, Univ Paris Saclay, CNRS, Inria, LRI, Gif Sur Yvette, France. Buono, Paolo, Univ Bari, Bari, Italy. Plaisant, Catherine, Univ Maryland, College Pk, MD USA.", "countries": "Italy;USA;France", "abstract": "We propose a new approach-called PK-clustering-to help social scientists create meaningful clusters in social networks. Many clustering algorithms exist but most social scientists find them difficult to understand, and tools do not provide any guidance to choose algorithms, or to evaluate results taking into account the prior knowledge of the scientists. Our work introduces a new clustering approach and a visual analytics user interface that address this issue. It is based on a process that 1) captures the prior knowledge of the scientists as a set of incomplete clusters, 2) runs multiple clustering algorithms (similarly to clustering ensemble methods), 3) visualizes the results of all the algorithms ranked and summarized by how well each algorithm matches the prior knowledge, 4) evaluates the consensus between user-selected algorithms and 5) allows users to review details and iteratively update the acquired knowledge. We describe our approach using an initial functional prototype, then provide two examples of use and early feedback from social scientists. We believe our clustering approach offers a novel constructive method to iteratively build knowledge while avoiding being overly influenced by the results of often randomly selected black-box clustering algorithms.", "keywords": "Social network analysis,network visualization,clustering,mixed-initiative,prior knowledge,user interface", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030347", "refList": ["10.1109/tvcg.2016.2615308", "10.2312/eurovisstar.20151110", "10.1145/1353343.1353398", "10.1109/tvcg.2017.2745178", "10.1162/153244303321897735", "10.4230/dagrep.8.10.1", "10.1007/978-3-662-47824-0\\_2", "10.1145/1124772.1124830", "10.1109/tvcg.2014.2346260", "10.1109/2945.841121", "10.1109/hicss.2016.181", "10.1073/pnas.122653799", "10.1103/physreve.76.036102", "10.2312/eur0vissh0rt.20141162", "10.1186/1471-2105-16-s11-s5", "10.1109/tvcg.2014.2315995", "10.1007/s41109-019-0165-9", "10.1109/icdm.2013.167", "10.1559/1523040054738936", "10.1109/tpami.2002.1017616", "10.1007/978-1-4471-6497-5\\_1", "10.1057/palgrave/ivs/9500006", "10.1007/s13042-015-0338-5", "10.1145/3172867", "10.1109/vast.2015.7347625", "10.1103/physreve.76.046115", "10.1109/ijcnn.2008.4634108", "10.1109/tvcg.2014.2346248", "10.1145/3340960", "10.1109/icdm.2009.143", "10.1109/tvcg.2006.147", "10.2312/eurovisshort.20141162", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2014.2346321", "10.1145/2493102.2499786", "10.1002/wics.1270", "10.1016/j.physrep.2009.11.002", "10.1145/302979.303030", "10.1145/1124772.1124890", "10.2312/eur0visstar.20151110", "10.1145/3110025.3110030", "10.1016/0031-3203(95)00120-4", "10.1093/bioinformatics/bti373", "10.1109/tkde.2007.190689", "10.1016/j.ejrs.2018.11.001", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2019.2933196", "10.1109/tvcg.2006.76"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030369", "title": "Investigating Visual Analysis of Differentially Private Data", "year": "2020", "conferenceName": "InfoVis", "authors": "Dan Zhang;Ali Sarvghad;Gerome Miklau", "citationCount": "0", "affiliation": "Zhang, D (Corresponding Author), Univ Massachusetts, Amherst, MA 01003 USA. Zhang, Dan; Sarvghad, Ali; Miklau, Gerome, Univ Massachusetts, Amherst, MA 01003 USA.", "countries": "USA", "abstract": "Differential Privacy is an emerging privacy model with increasing popularity in many domains. It functions by adding carefully calibrated noise to data that blurs information about individuals while preserving overall statistics about the population. Theoretically, it is possible to produce robust privacy-preserving visualizations by plotting differentially private data. However, noise-induced data perturbations can alter visual patterns and impact the utility of a private visualization. We still know little about the challenges and opportunities for visual data exploration and analysis using private visualizations. As a first step towards filling this gap, we conducted a crowdsourced experiment, measuring participants' performance under three levels of privacy (high, low, non-private) for combinations of eight analysis tasks and four visualization types (bar chart, pie chart, line chart, scatter plot). Our findings show that for participants' accuracy for summary tasks (e.g., find clusters in data) was higher that value tasks (e.g., retrieve a certain value). We also found that under DP, pie chart and line chart offer similar or better accuracy than bar chart. In this work, we contribute the results of our empirical study, investigating the task-based effectiveness of basic private visualizations, a dichotomous model for defining and measuring user success in performing visual analysis tasks under DP, and a set of distribution metrics for tuning the injection to improve the utility of private visualizations.", "keywords": "Differential privacy,information visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030369", "refList": ["10.1109/tvcg.2016.2615308", "10.2312/eurovisstar.20151110", "10.1145/1353343.1353398", "10.1109/tvcg.2017.2745178", "10.1162/153244303321897735", "10.4230/dagrep.8.10.1", "10.1145/1124772.1124830", "10.1007/978-3-662-47824-0\\_2", "10.1109/2945.841121", "10.1109/hicss.2016.181", "10.1073/pnas.122653799", "10.1103/physreve.76.036102", "10.1186/1471-2105-16-s11-s5", "10.1109/tvcg.2014.2315995", "10.1007/s41109-019-0165-9", "10.1109/icdm.2013.167", "10.1559/1523040054738936", "10.1109/tpami.2002.1017616", "10.1007/978-1-4471-6497-5\\_1", "10.1057/palgrave/ivs/9500006", "10.1007/s13042-015-0338-5", "10.1145/3172867", "10.1109/vast.2015.7347625", "10.1103/physreve.76.046115", "10.1109/ijcnn.2008.4634108", "10.1109/tvcg.2014.2346248", "10.1145/3340960", "10.1109/tvcg.2006.147", "10.2312/eurovisshort.20141162", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2014.2346321", "10.1145/2493102.2499786", "10.1002/wics.1270", "10.1016/j.physrep.2009.11.002", "10.1145/1124772.1124890", "10.1145/3110025.3110030", "10.1016/0031-3203(95)00120-4", "10.1093/bioinformatics/bti373", "10.1109/tkde.2007.190689", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2019.2933196"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13681", "year": "2019", "title": "A User-based Visual Analytics Workflow for Exploratory Model Analysis", "conferenceName": "EuroVis", "authors": "Dylan Cashman;Shah Rukh Humayoun;Florian Heimerl;Kendall Park;Subhajit Das;John Thompson;Bahador Saket;Abigail Mosca;John T. Stasko;Alex Endert;Michael Gleicher;Remco Chang", "citationCount": "6", "affiliation": "Cashman, D; Humayoun, SR (Corresponding Author), Tufts Univ, Medford, MA 02155 USA.\nCashman, Dylan; Humayoun, Shah Rukh; Mosca, Abigail; Chang, Remco, Tufts Univ, Medford, MA 02155 USA.\nHeimerl, Florian; Park, Kendall; Gleicher, Michael, Georgia Tech, Atlanta, GA USA.\nDas, Subhajit; Thompson, John; Saket, Bahador; Stasko, John; Endert, Alex, Univ Wisconsin, Madison, WI USA.", "countries": "USA", "abstract": "Many visual analytics systems allow users to interact with machine learning models towards the goals of data exploration and insight generation on a given dataset. However, in some situations, insights may be less important than the production of an accurate predictive model for future use. In that case, users are more interested in generating of diverse and robust predictive models, verifying their performance on holdout data, and selecting the most suitable model for their usage scenario. In this paper, we consider the concept of Exploratory Model Analysis (EMA), which is defined as the process of discovering and selecting relevant models that can be used to make predictions on a data source. We delineate the differences between EMA and the well-known term exploratory data analysis in terms of the desired outcome of the analytic process: insights into the data or a set of deployable models. The contributions of this work are a visual analytics system workflow for EMA, a user study, and two use cases validating the effectiveness of the workflow. We found that our system workflow enabled users to generate complex models, to assess them for various qualities, and to select the most relevant model for their task.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13681", "refList": ["10.1109/tvcg.2017.2744683", "10.1111/cgf.12639", "10.1007/s11390-016-1663-1", "10.1109/tvcg.2017.2744938", "10.1117/12.2007316", "10.1109/tvcg.2017.2745178", "10.1145/2702123.2702509", "10.1080/02701367.1992.10608764", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2011.185", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2013.125", "10.1109/mcse.2007.55", "10.1007/978-3-540-70956-5", "10.1109/vl.1996.545307", "10.1901/jeab.1979.31-433", "10.1111/j.1467-8659.2009.01475.x", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2012.65", "10.1145/1835804.1835827", "10.1109/tvcg.2017.2745085", "10.1145/2939672.2939778", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2017.2744805", "10.1145/2487575.2487629", "10.1109/tvcg.2013.157", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2011.209", "10.1109/vast.2012.6400490", "10.1109/tvcg.2015.2513410", "10.1007/978-3-540-79347-2\\_3", "10.1109/tvcg.2014.2346431", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2012.277", "10.1109/infvis.1998.729560", "10.1109/infvis.2004.64", "10.1109/mcg.2006.70", "10.1109/tvcg.2012.260", "10.1109/tvcg.2014.2346660", "10.1145/1743546.1743567", "10.1111/cgf.13417", "10.1109/tvcg.2014.2346325", "10.1109/tvcg.2018.2864838", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2003.1207445", "10.1109/vast.2010.5652443", "10.1145/2641190.2641198", "10.1109/tvcg.2016.2598830", "10.1109/tvcg.2017.2744378", "10.1111/cgf.13324", "10.1109/mcg.2009.22", "10.1109/tvcg.2016.2599030", "10.1109/vast.2012.6400486", "10.1109/tvcg.2016.2598831", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934261", "title": "Ablate, Variate, and Contemplate: Visual Analytics for Discovering Neural Architectures", "year": "2019", "conferenceName": "VAST", "authors": "Dylan Cashman;Adam Perer;Remco Chang;Hendrik Strobelt", "citationCount": "1", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Perer, Adam, Carnegie Mellon Univ, Pittsburgh, PA 15213 USA. Strobelt, Hendrik, MIT IBM Watson AI Lab, Cambridge, MA USA.", "countries": "USA", "abstract": "The performance of deep learning models is dependent on the precise configuration of many layers and parameters. However, there are currently few systematic guidelines for how to configure a successful model. This means model builders often have to experiment with different configurations by manually programming different architectures (which is tedious and time consuming) or rely on purely automated approaches to generate and train the architectures (which is expensive). In this paper, we present Rapid Exploration of Model Architectures and Parameters, or REMAP, a visual analytics tool that allows a model builder to discover a deep learning model quickly via exploration and rapid experimentation of neural network architectures. In REMAP, the user explores the large and complex parameter space for neural network architectures using a combination of global inspection and local experimentation. Through a visual overview of a set of models, the user identifies interesting clusters of architectures. Based on their findings, the user can run ablation and variation experiments to identify the effects of adding, removing, or replacing layers in a given architecture and generate new models accordingly. They can also handcraft new models using a simple graphical interface. As a result, a model builder can build deep learning models quickly, efficiently, and without manual programming. We inform the design of REMAP through a design study with four deep learning model builders. Through a use case, we demonstrate that REMAP allows users to discover performant neural network architectures efficiently using visual exploration and user-defined semi-automated searches through the model space.", "keywords": "visual analytics,neural networks,parameter space exploration", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934261", "refList": ["10.1109/mcg.2018.2878902", "10.1111/cgf.12639", "10.1109/tvcg.2017.2744938", "10.1117/12.2007316", "10.1109/cvpr.2014.81", "10.1016/j.csda.2008.02.031", "10.1080/00994480.2000.10748487", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2013.125", "10.1109/cvpr.2015.7298594", "10.1111/j.1467-8659.2009.01475.x", "10.1109/tvcg.2012.65", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2017.2744805", "10.1145/2487575.2487629", "10.1109/tvcg.2018.2865044", "10.23915/distill.00010", "10.1109/72.279181", "10.1109/tvcg.2017.2744199", "10.1007/s13398-014-0173-7.2", "10.1109/tvcg.2018.2864504", "10.1109/vast.2012.6400490", "10.1007/978-3-319-10590-1\\_53", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2014.2346321", "10.1094/pdis-11-11-0999-pdn", "10.1109/ijcnn.2015.7280767", "10.1109/tvcg.2017.2744878", "10.5555/3326943.3327130", "10.1109/tvcg.2017.2744718", "10.1109/iccv.2015.169", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2014.223", "10.1109/cvpr.2016.90", "10.1109/vast.2010.5652443", "10.1111/cgf.13681", "10.1109/tvcg.2016.2598831", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028888", "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes", "year": "2020", "conferenceName": "VAST", "authors": "Yuxin Ma;Arlen Fan;Jingrui He;Arun Reddy Nelakurthi;Ross Maciejewski", "citationCount": "0", "affiliation": "Ma, YX (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Ma, Yuxin; Fan, Arlen; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. He, Jingrui, Univ Illinois, Champaign, IL USA. Nelakurthi, Arun Reddy, Samsung Res Amer, Mountain View, CA USA.", "countries": "USA", "abstract": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.", "keywords": "Transfer learning,deep learning,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028888", "refList": ["10.1109/tvcg.2014.2346578", "10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1109/tpami.2018.2868685", "10.1145/2702123.2702509", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2016.2598828", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2013.65", "10.1145/2976749.2978318", "10.1007/978-3-030-01424-7\\_27", "10.1109/tvcg.2019.2934261", "10.1007/s11704-016-6028-y", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/iccv.2015.279", "10.1109/mci.2018.2840738", "10.1109/tvcg.2019.2892483", "10.1109/vast.2018.8802509", "10.1109/tvcg.2013.124", "10.1186/s40537-016-0043-6", "10.1109/tvcg.2018.2864475", "10.1145/3200489", "10.1109/tvcg.2018.2865044", "10.1111/cgf.13210", "10.1109/tvcg.2018.2816223", "10.23915/distill.00007", "10.1109/tvcg.2017.2744199", "10.1109/tkde.2018.2876857", "10.1109/tvcg.2019.2934631", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2011.188", "10.1007/978-3-642-15561-1\\_16", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1109/tvcg.2017.2744718", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2016.2598541", "10.1109/tkde.2009.191", "10.1145/3065386", "10.1016/j.ins.2016.03.021", "10.1109/tvcg.2019.2903943", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/iccv.2017.74", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2012.207", "10.1111/cgf.13092", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2017.2754480", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 9}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13970", "year": "2020", "title": "QUESTO: Interactive Construction of Objective Functions for Classification Tasks", "conferenceName": "EuroVis", "authors": "Subhajit Das;Shenyu Xu;Michael Gleicher;Remco Chang;Alex Endert", "citationCount": "0", "affiliation": "Das, S (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA.\nDas, Subhajit; Xu, Shenyu; Endert, Alex, Georgia Inst Technol, Atlanta, GA 30332 USA.\nGleicher, Michael, Univ Wisconsin, Madison, WI USA.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.", "countries": "USA", "abstract": "Building effective classifiers requires providing the modeling algorithms with information about the training data and modeling goals in order to create a model that makes proper tradeoffs. Machine learning algorithms allow for flexible specification of such meta-information through the design of the objective functions that they solve. However, such objective functions are hard for users to specify as they are a specific mathematical formulation of their intents. In this paper, we present an approach that allows users to generate objective functions for classification problems through an interactive visual interface. Our approach adopts a semantic interaction design in that user interactions over data elements in the visualization are translated into objective function terms. The generated objective functions are solved by a machine learning solver that provides candidate models, which can be inspected by the user, and used to suggest refinements to the specifications. We demonstrate a visual analytics system QUESTO for users to manipulate objective functions to define domain-specific constraints. Through a user study we show that QUESTO helps users create various objective functions that satisfy their goals.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13970", "refList": ["10.1016/j.neucom.2017.01.105", "10.1371/journal.pone.0050474", "10.1109/tvcg.2016.2598839", "10.1007/978-3-642-21530-8\\_14", "10.1109/tvcg.2016.2598828", "10.5555/2969442.2969547", "10.1007/s00371-015-1132-9", "10.1145/2851581.2856492", "10.1126/scirobotics.aao6760", "10.1109/vast.2011.6102453", "10.1109/vast.2011.6102449", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2016.2598446", "10.1145/359784.360332", "10.1111/j.1467-8659.2009.01475.x", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2016.2598460", "10.1109/tevc.2015.2472283", "10.1145/2207676.2207741", "10.1109/tvcg.2017.2745085", "10.1145/2939672.2939778", "10.1145/1866029.1866038", "10.1145/2487575.2487629", "10.1145/3077257.3077259", "10.2312/eurova.20171123", "10.1145/2983924", "10.1109/mcg.2013.53", "10.1109/tvcg.2015.2467615", "10.1109/vast.2014.7042492", "10.1145/2675133.2675214", "10.1109/tvcg.2014.2346482", "10.1145/3180308.3180362", "10.1109/tvcg.2014.2346321", "10.24963/ijcai.2017/202", "10.1109/tvcg.2014.2346291", "10.1109/tbme.2012.2212278", "10.1007/s40708-016-0042-6", "10.1609/aimag.v35i4.2513", "10.1016/j.ijhcs.2009.03.004", "10.1109/tevc.2012.2225064", "10.1016/s0890-6955(02)00074-3", "10.1109/icmlde.2018.00014", "10.1111/cgf.13681", "10.1109/tvcg.2013.173", "10.1145/3025171.3025208", "10.1145/3025453.3026044", "10.1109/cec.2017.7969334", "10.1109/vast.2012.6400486", "10.1109/tvcg.2016.2598831", "10.1109/tcyb.2014.2310651"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13972", "year": "2020", "title": "Boxer: Interactive Comparison of Classifier Results", "conferenceName": "EuroVis", "authors": "Michael Gleicher;Aditya Barve;Xinyi Yu;Florian Heimerl", "citationCount": "0", "affiliation": "Gleicher, M (Corresponding Author), Univ Wisconsin, Madison, WI 53706 USA.\nGleicher, Michael; Barve, Aditya; Yu, Xinyi; Heimerl, Florian, Univ Wisconsin, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Machine learning practitioners often compare the results of different classifiers to help select, diagnose and tune models. We present Boxer, a system to enable such comparison. Our system facilitates interactive exploration of the experimental results obtained by applying multiple classifiers to a common set of model inputs. The approach focuses on allowing the user to identify interesting subsets of training and testing instances and comparing performance of the classifiers on these subsets. The system couples standard visual designs with set algebra interactions and comparative elements. This allows the user to compose and coordinate views to specify subsets and assess classifier performance on them. The flexibility of these compositions allow the user to address a wide range of scenarios in developing and assessing classifiers. We demonstrate Boxer in use cases including model selection, tuning, fairness assessment, and data quality diagnosis.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13972", "refList": ["10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2016.2598838", "10.1007/s10618-014-0368-8", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1145/3287560.3287589", "10.1109/vast.2017.8585721", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2009.128", "10.1109/tvcg.2017.2744018", "10.1080/00994480.2000.10748487", "10.5555/3305890.3306024", "10.1109/iccv.2015.329", "10.1109/tvcg.2013.125", "10.1089/big.2016.0007", "10.1109/memsys.2019.8870817", "10.1145/2939672.2939778", "10.1007/s11104-019-04156-0", "10.1371/journal.pone.0181142", "10.1145/3301275.3302324", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2018.2865044", "10.1023/a:1010933404324", "10.1145/2487575.2487579", "10.1109/tvcg.2013.157", "10.1145/2783258.2788613", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934631", "10.1016/s0304-3800(02)00064-9", "10.1007/s10115-013-0679-x", "10.1109/tvcg.2019.2934267", "10.1007/978-3-319-10590-1\\_53", "10.1109/vast.2017.8585720", "10.1016/0004-3702(80)90021-1", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2009.84", "10.1007/s11263-016-0911-8", "10.1111/cgf.12918", "10.1111/cgf.12373", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1080/00207176808905715", "10.1002/er.3827", "10.1109/tvcg.2014.2346660", "10.1111/cgf.13417", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2017.2744158", "10.1016/b978-0-12-815849-4.00004-9", "10.1097/ede.0b013e3181c30fb2", "10.1111/cgf.13681", "10.1016/j.ejor.2006.04.051", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2016.2598468", "10.9735/2229-3981", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 21}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13972", "year": "2020", "title": "Boxer: Interactive Comparison of Classifier Results", "conferenceName": "EuroVis", "authors": "Michael Gleicher;Aditya Barve;Xinyi Yu;Florian Heimerl", "citationCount": "0", "affiliation": "Gleicher, M (Corresponding Author), Univ Wisconsin, Madison, WI 53706 USA.\nGleicher, Michael; Barve, Aditya; Yu, Xinyi; Heimerl, Florian, Univ Wisconsin, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Machine learning practitioners often compare the results of different classifiers to help select, diagnose and tune models. We present Boxer, a system to enable such comparison. Our system facilitates interactive exploration of the experimental results obtained by applying multiple classifiers to a common set of model inputs. The approach focuses on allowing the user to identify interesting subsets of training and testing instances and comparing performance of the classifiers on these subsets. The system couples standard visual designs with set algebra interactions and comparative elements. This allows the user to compose and coordinate views to specify subsets and assess classifier performance on them. The flexibility of these compositions allow the user to address a wide range of scenarios in developing and assessing classifiers. We demonstrate Boxer in use cases including model selection, tuning, fairness assessment, and data quality diagnosis.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13972", "refList": ["10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2016.2598838", "10.1007/s10618-014-0368-8", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1145/3287560.3287589", "10.1109/vast.2017.8585721", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2009.128", "10.1109/tvcg.2017.2744018", "10.1080/00994480.2000.10748487", "10.5555/3305890.3306024", "10.1109/iccv.2015.329", "10.1109/tvcg.2013.125", "10.1089/big.2016.0007", "10.1109/memsys.2019.8870817", "10.1145/2939672.2939778", "10.1007/s11104-019-04156-0", "10.1371/journal.pone.0181142", "10.1145/3301275.3302324", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2018.2865044", "10.1023/a:1010933404324", "10.1145/2487575.2487579", "10.1109/tvcg.2013.157", "10.1145/2783258.2788613", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934631", "10.1016/s0304-3800(02)00064-9", "10.1007/s10115-013-0679-x", "10.1109/tvcg.2019.2934267", "10.1007/978-3-319-10590-1\\_53", "10.1109/vast.2017.8585720", "10.1016/0004-3702(80)90021-1", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2009.84", "10.1007/s11263-016-0911-8", "10.1111/cgf.12918", "10.1111/cgf.12373", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1080/00207176808905715", "10.1002/er.3827", "10.1109/tvcg.2014.2346660", "10.1111/cgf.13417", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2017.2744158", "10.1016/b978-0-12-815849-4.00004-9", "10.1097/ede.0b013e3181c30fb2", "10.1111/cgf.13681", "10.1016/j.ejor.2006.04.051", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2016.2598468", "10.9735/2229-3981", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 53}, {"doi": "10.1109/tvcg.2018.2864838", "title": "VIS4ML: An Ontology for Visual Analytics Assisted Machine Learning", "year": "2018", "conferenceName": "VAST", "authors": "Dominik Sacha;Matthias Kraus;Daniel A. Keim;Min Chen", "citationCount": "11", "affiliation": "Sacha, D (Corresponding Author), Univ Konstanz, Constance, Germany. Sacha, Dominik; Kraus, Matthias; Keim, Daniel A., Univ Konstanz, Constance, Germany. Chen, Min, Univ Oxford, Oxford, England.", "countries": "Germany;England", "abstract": "While many VA workflows make use of machine-learned models to support analytical tasks, VA workflows have become increasingly important in understanding and improving Machine Learning (ML) processes. In this paper, we propose an ontology (VIS4ML) for a subarea of VA, namely \u201cVA-assisted ML\u201d. The purpose of VIS4ML is to describe and understand existing VA workflows used in ML as well as to detect gaps in ML processes and the potential of introducing advanced VA techniques to such processes. Ontologies have been widely used to map out the scope of a topic in biology, medicine, and many other disciplines. We adopt the scholarly methodologies for constructing VIS4ML, including the specification, conceptualization, formalization, implementation, and validation of ontologies. In particular, we reinterpret the traditional VA pipeline to encompass model-development workflows. We introduce necessary definitions, rules, syntaxes, and visual notations for formulating VIS4ML and make use of semantic web technologies for implementing it in the Web Ontology Language (OWL). VIS4ML captures the high-level knowledge about previous workflows where VA is used to assist in ML. It is consistent with the established VA concepts and will continue to evolve along with the future developments in VA and ML. While this ontology is an effort for building the theoretical foundation of VA, it can be used by practitioners in real-world applications to optimize model-development workflows by systematically examining the potential benefits that can be brought about by either machine or human capabilities. Meanwhile, VIS4ML is intended to be extensible and will continue to be updated to reflect future advancements in using VA for building high-quality data-analytical models or for building such models rapidly.", "keywords": "Visual Analytics,Visualization,Machine Learning,Human-Computer Interaction,Ontology,VIS4ML", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864838", "refList": ["10.1109/tvcg.2015.2467591", "10.1109/38.31462", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2017.2745178", "10.1145/1287620.1287621", "10.1109/mcg.2017.3271463", "10.1109/tvcg.2016.2598828", "10.1109/icdmw.2008.62", "10.1145/3011141.3011207", "10.1007/978-3-540-70956-5", "10.1145/2512208", "10.1016/j.aei.2016.04.003", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2017.2745158", "10.1007/978-3-642-40897-7\\_9", "10.1109/tvcg.2017.2744805", "10.23915/distill.00010", "10.1111/j.1467-8659.2008.01230.x", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2016.2598829", "10.1109/tvcg.2017.2744718", "10.1057/ivs.2008.29", "10.1109/mcg.2005.55", "10.1109/mcg.2018.042731661", "10.2200/s00429ed1v01y201207aim018", "10.1145/2468356.2468677", "10.1609/aimag.v35i4.2513", "10.1057/ivs.2008.28", "10.1201/9781315139470", "10.1109/tvcg.2017.2744158", "10.1109/vast.2008.4677361", "10.1007/s10844-014-0304-9", "10.1109/mcg.2014.33", "10.1016/j.ineucom.2017.01.105", "10.1111/cgf.13092", "10.1109/tvcg.2016.2598830", "10.1109/visual.2004.10", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1111/cgf.13324", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934614", "title": "Interactive Learning for Identifying Relevant Tweets to Support Real-time Situational Awareness", "year": "2019", "conferenceName": "VAST", "authors": "Luke S. Snyder;Yi-Shan Lin;Morteza Karimzadeh;Dan Goldwasser;David S. Ebert", "citationCount": "1", "affiliation": "Snyder, LS (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Snyder, Luke S.; Lin, Yi-Shan; Karimzadeh, Morteza; Goldwasser, Dan; Ebert, David S., Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Various domain users are increasingly leveraging real-time social media data to gain rapid situational awareness. However, due to the high noise in the deluge of data, effectively determining semantically relevant information can be difficult, further complicated by the changing definition of relevancy by each end user for different events. The majority of existing methods for short text relevance classification fail to incorporate users' knowledge into the classification process. Existing methods that incorporate interactive user feedback focus on historical datasets. Therefore, classifiers cannot be interactively retrained for specific events or user-dependent needs in real-time. This limits real-time situational awareness, as streaming data that is incorrectly classified cannot be corrected immediately, permitting the possibility for important incoming data to be incorrectly classified as well. We present a novel interactive learning framework to improve the classification process in which the user iteratively corrects the relevancy of tweets in real-time to train the classification model on-the-fly for immediate predictive improvements. We computationally evaluate our classification model adapted to learn at interactive rates. Our results show that our approach outperforms state-of-the-art machine learning models. In addition, we integrate our framework with the extended Social Media Analytics and Reporting Toolkit (SMART) 2.0 system, allowing the use of our interactive learning framework within a visual analytics system tailored for real-time situational awareness. To demonstrate our framework's effectiveness, we provide domain expert feedback from first responders who used the extended SMART 2.0 system.", "keywords": "Interactive machine learning,human-computer interaction,social media analytics,emergency/disaster management,situational awareness", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934614", "refList": ["10.1145/1835449.1835643", "10.1109/icip.2014.7025078", "10.1162/coli.08-012-r1-06-90", "10.1016/j.ijinfomgt.2018.07.008", "10.1207/s15516709cog1402\\_1", "10.1145/2806416.2806485", "10.1109/vast.2010.5652922", "10.1145/1645953.1646071", "10.1145/3155133.3155206", "10.3115/v1/n15-1142", "10.1109/tvcg.2013.186", "10.1145/1367497.1367510", "10.18653/v1/d15-1167", "10.1109/5.726791", "10.1145/2207676.2207741", "10.1109/icci-cc.2015.7259377", "10.1109/tcbb.2017.2701379", "10.1080/15230406.2017.1370391", "10.1109/infvis.2004.37", "10.1162/jmlr.2003.3.4-5.951", "10.18653/v1/s16-1024", "10.1109/tvcg.2012.277", "10.1162/neco.1997.9.8.1735", "10.1145/347090.347168", "10.1109/tvcg.2017.2744718", "10.1145/2675133.2675242", "10.1002/smr.1874", "10.1109/icassp.2018.8461907", "10.1109/bigmm.2017.82", "10.1109/tvcg.2018.2864838", "10.1109/ssci.2015.33", "10.1109/tvcg.2017.2744818", "10.1145/2872518.2889365", "10.1109/asonam.2016.7752432", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934267", "title": "ProtoSteer: Steering Deep Sequence Model with Prototypes", "year": "2019", "conferenceName": "VAST", "authors": "Yao Ming;Panpan Xu;Furui Cheng;Huamin Qu;Ren Liu", "citationCount": "2", "affiliation": "Ming, Y (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Ming, Yao; Cheng, Furui; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xu, Panpan; Ren, Liu, Bosch Res North Amer, Palo Alto, CA USA.", "countries": "USA;China", "abstract": "Recently we have witnessed growing adoption of deep sequence models (e.g. LSTMs) in many application domains, including predictive health care, natural language processing, and log analysis. However, the intricate working mechanism of these models confines their accessibility to the domain experts. Their black-box nature also makes it a challenging task to incorporate domain-specific knowledge of the experts into the model. In ProtoSteer (Prototype Steering), we tackle the challenge of directly involving the domain experts to steer a deep sequence model without relying on model developers as intermediaries. Our approach originates in case-based reasoning, which imitates the common human problem-solving process of consulting past experiences to solve new problems. We utilize ProSeNet (Prototype Sequence Network), which learns a small set of exemplar cases (i.e., prototypes) from historical data. In ProtoSteer they serve both as an efficient visual summary of the original data and explanations of model decisions. With ProtoSteer the domain experts can inspect, critique, and revise the prototypes interactively. The system then incorporates user-specified prototypes and incrementally updates the model. We conduct extensive case studies and expert interviews in application domains including sentiment analysis on texts and predictive diagnostics based on vehicle fault logs. The results demonstrate that involvements of domain users can help obtain more interpretable models with concise prototypes while retaining similar accuracy.", "keywords": "Sequence Data,Explainable Artificial Intelligence (XAI),Recurrent Neural Networks (RNNs),Prototype Learning", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934267", "refList": ["10.1109/infvis.2000.885091", "10.1145/2468356.2468434", "10.1109/mcg.2018.2878902", "10.1145/2858036.2858107", "10.1145/2702123.2702419", "10.1109/vast.2015.7347682", "10.1109/tvcg.2016.2598797", "10.1145/2939672.2939778", "10.1109/tvcg.2018.2864885", "10.1109/tvcg.2018.2865044", "10.1073/pnas.95.25.14863", "10.1109/tvcg.2016.2539960", "10.1145/3025453.3025456", "10.1145/2557500.2557508", "10.1109/tvcg.2012.225", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2018.2865230", "10.1109/tvcg.2017.2745320", "10.1109/tvcg.2017.2744718", "10.18653/v1/n16-1082", "10.1016/j.neucom.2013.11.045", "10.1109/tvcg.2017.2745083", "10.1609/aimag.v35i4.2513", "10.1007/bf00155578", "10.1007/978-3-319-90403-0\\_17", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2018.2864838", "10.1109/tvcg.2018.2865027", "10.1145/312129.312298", "10.1145/3185517", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13972", "year": "2020", "title": "Boxer: Interactive Comparison of Classifier Results", "conferenceName": "EuroVis", "authors": "Michael Gleicher;Aditya Barve;Xinyi Yu;Florian Heimerl", "citationCount": "0", "affiliation": "Gleicher, M (Corresponding Author), Univ Wisconsin, Madison, WI 53706 USA.\nGleicher, Michael; Barve, Aditya; Yu, Xinyi; Heimerl, Florian, Univ Wisconsin, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Machine learning practitioners often compare the results of different classifiers to help select, diagnose and tune models. We present Boxer, a system to enable such comparison. Our system facilitates interactive exploration of the experimental results obtained by applying multiple classifiers to a common set of model inputs. The approach focuses on allowing the user to identify interesting subsets of training and testing instances and comparing performance of the classifiers on these subsets. The system couples standard visual designs with set algebra interactions and comparative elements. This allows the user to compose and coordinate views to specify subsets and assess classifier performance on them. The flexibility of these compositions allow the user to address a wide range of scenarios in developing and assessing classifiers. We demonstrate Boxer in use cases including model selection, tuning, fairness assessment, and data quality diagnosis.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13972", "refList": ["10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2016.2598838", "10.1007/s10618-014-0368-8", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1145/3287560.3287589", "10.1109/vast.2017.8585721", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2009.128", "10.1109/tvcg.2017.2744018", "10.1080/00994480.2000.10748487", "10.5555/3305890.3306024", "10.1109/iccv.2015.329", "10.1109/tvcg.2013.125", "10.1089/big.2016.0007", "10.1109/memsys.2019.8870817", "10.1145/2939672.2939778", "10.1007/s11104-019-04156-0", "10.1371/journal.pone.0181142", "10.1145/3301275.3302324", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2018.2865044", "10.1023/a:1010933404324", "10.1145/2487575.2487579", "10.1109/tvcg.2013.157", "10.1145/2783258.2788613", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934631", "10.1016/s0304-3800(02)00064-9", "10.1007/s10115-013-0679-x", "10.1109/tvcg.2019.2934267", "10.1007/978-3-319-10590-1\\_53", "10.1109/vast.2017.8585720", "10.1016/0004-3702(80)90021-1", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2009.84", "10.1007/s11263-016-0911-8", "10.1111/cgf.12918", "10.1111/cgf.12373", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1080/00207176808905715", "10.1002/er.3827", "10.1109/tvcg.2014.2346660", "10.1111/cgf.13417", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2017.2744158", "10.1016/b978-0-12-815849-4.00004-9", "10.1097/ede.0b013e3181c30fb2", "10.1111/cgf.13681", "10.1016/j.ejor.2006.04.051", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2016.2598468", "10.9735/2229-3981", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 11}, {"doi": "10.1109/tvcg.2019.2934595", "title": "Visual Interaction with Deep Learning Models through Collaborative Semantic Inference", "year": "2019", "conferenceName": "VAST", "authors": "Sebastian Gehrmann;Hendrik Strobelt;Robert Kr\u00fcger;Hanspeter Pfister;Alexander M. Rush", "citationCount": "3", "affiliation": "Gehrmann, S (Corresponding Author), Harvard NLP Grp, Cambridge, MA 02138 USA. Gehrmann, Sebastian; Rush, Alexander M., Harvard NLP Grp, Cambridge, MA 02138 USA. Strobelt, Hendrik, IBM Res Cambridge, Cambridge, MA USA. Kruger, Robert, MIT IBM Watson AI Lab, Cambridge, MA USA. Pfister, Hanspeter, Harvard Visual Comp Grp, Cambridge, MA USA.", "countries": "USA", "abstract": "Automation of tasks can have critical consequences when humans lose agency over decision processes. Deep learning models are particularly susceptible since current black-box approaches lack explainable reasoning. We argue that both the visual interface and model structure of deep learning systems need to take into account interaction design. We propose a framework of collaborative semantic inference (CSI) for the co-design of interactions and models to enable visual collaboration between humans and algorithms. The approach exposes the intermediate reasoning process of models which allows semantic interactions with the visual metaphors of a problem, which means that a user can both understand and control parts of the model reasoning process. We demonstrate the feasibility of CSI with a co-designed case study of a document summarization system.", "keywords": "Human-Computer Collaboration,Deep Learning,Neural Networks,Interaction Design,Human-Centered Design", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934595", "refList": ["10.1109/vast.2017.8585721", "10.1109/tvcg.2012.195", "10.1613/jair.295", "10.1162/tacl\\textbackslash{}a\\textbackslash{}00254", "10.1007/978-3-540-70956-5", "10.1145/2678025.2701399", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.5555/645526.657137", "10.1146/annurev.neur0.26.041002.131047.issn", "10.1145/2207676.2207741", "10.1145/2939672.2939778", "10.1109/vlhcc.2010.15", "10.3115/v1/d14-1130", "10.18653/v1/p17-1099", "10.1109/tvcg.2018.2865044", "10.18653/v1/p17-1080", "10.1111/cgf.13210", "10.1162/tacl\\_a\\_00254", "10.1109/tvcg.2018.2816223", "10.1109/72.279181", "10.1145/2783258.2788613", "10.2112/si85-057.1", "10.1145/1866029.1866078", "10.1007/978-3-319-10590-1\\_53", "10.1109/tvcg.2018.2865230", "10.1007/s12650-018-0531-1", "10.1145/2365952.2365964", "10.1017/s026988890200019x", "10.1109/iccv.2015.337", "10.1109/tvcg.2017.2744878", "10.1145/3027063.3053103", "10.1109/tvcg.2017.2744718", "10.1145/302979.303030", "10.1109/cvpr.2018.00917", "10.1007/s40708-016-0042-6", "10.1073/pnas.1807184115", "10.1609/aimag.v35i4.2513", "10.1016/j.ijhcs.2009.03.004", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2018.2864838", "10.1111/cgf.13092", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2014.2346574", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1016/s0167-739x(97)00022-8", "10.1093/bi0inf0rmatics/bth267"], "wos": 1, "children": [{"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 7}, {"doi": "10.1109/tvcg.2020.3030380", "title": "HyperTendril: Visual Analytics for User-Driven Hyperparameter Optimization of Deep Neural Networks", "year": "2020", "conferenceName": "VAST", "authors": "Heungseok Park;Yoonsoo Nam;Jihoon Kim;Jaegul Choo", "citationCount": "0", "affiliation": "Kim, JH (Corresponding Author), NAVER Corp, Clova AI Res, Seongnam Si, South Korea. Choo, J (Corresponding Author), Korea Adv Inst Sci \\& Technol, Daejeon, South Korea. Park, Heungseok; Nam, Yoonsoo; Kim, Ji-Hoon, NAVER Corp, Clova AI Res, Seongnam Si, South Korea. Choo, Jaegul, Korea Adv Inst Sci \\& Technol, Daejeon, South Korea.", "countries": "Korea", "abstract": "To mitigate the pain of manually tuning hyperparameters of deep neural networks, automated machine learning (AutoML) methods have been developed to search for an optimal set of hyperparameters in large combinatorial search spaces. However, the search results of AutoML methods significantly depend on initial configurations, making it a non-trivial task to find a proper configuration. Therefore, human intervention via a visual analytic approach bears huge potential in this task. In response, we propose HyperTendril, a web-based visual analytics system that supports user-driven hyperparameter tuning processes in a model-agnostic environment. HyperTendril takes a novel approach to effectively steering hyperparameter optimization through an iterative, interactive tuning procedure that allows users to refine the search spaces and the configuration of the AutoML method based on their own insights from given results. Using HyperTendril, users can obtain insights into the complex behaviors of various hyperparameter search algorithms and diagnose their configurations. In addition, HyperTendril supports variable importance analysis to help the users refine their search spaces based on the analysis of relative importance of different hyperparameters and their interaction effects. We present the evaluation demonstrating how HyperTendril helps users steer their tuning processes via a longitudinal user study based on the analysis of interaction logs and in-depth interviews while we deploy our system in a professional industrial environment.", "keywords": "Visual analytics,deep learning,machine learning,automated machine learning,human-centered computing", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030380", "refList": ["10.1109/tvcg.2019.2934629", "10.1145/2834892.2834896", "10.1007/s10732-014-9275-9", "10.1109/icdis.2019.00018", "10.1145/3147.3165", "10.1109/bigdata.2017.8257923", "10.1145/3379336.3381474", "10.1007/978-3-319-47099-3\\_15", "10.1109/tvcg.2014.2346248", "10.1109/jproc.2015.2494218", "10.1145/3292500.3330701", "10.1162/neco.1997.9.8.1735", "10.1109/tvcg.2016.2598829", "10.1145/3097983.3098043", "10.1109/cvpr.2016.90", "10.1109/tvcg.2018.2864838", "10.1007/978-4-431-68057-4\\_3", "10.1111/cgf.13092", "10.1145/3219819.3220058", "10.1109/tvcg.2017.2744358"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030449", "title": "HypoML: Visual Analysis for Hypothesis-based Evaluation of Machine Learning Models", "year": "2020", "conferenceName": "VAST", "authors": "Qianwen Wang;William Alexander;Jack Pegg;Huamin Qu;Min Chen", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Alexander, William; Pegg, Jack; Chen, Min, Univ Oxford, Oxford, England.", "countries": "China;England", "abstract": "In this paper, we present a visual analytics tool for enabling hypothesis-based evaluation of machine learning (ML) models. We describe a novel ML-testing framework that combines the traditional statistical hypothesis testing (commonly used in empirical research) with logical reasoning about the conclusions of multiple hypotheses. The framework defines a controlled configuration for testing a number of hypotheses as to whether and how some extra information about a \u201cconcept\u201d or \u201cfeature\u201d may benefit or hinder an ML model. Because reasoning multiple hypotheses is not always straightforward, we provide HypoML as a visual analysis tool, with which, the multi-thread testing results are first transformed to analytical results using statistical and logical inferences, and then to a visual representation for rapid observation of the conclusions and the logical flow between the testing results and hypotheses. We have applied HypoML to a number of hypothesized concepts, demonstrating the intuitive and explainable nature of the visual analysis.", "keywords": "Visual analytics,model-developmental visualization,machine learning,neural network,hypothesis test,HypoML", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030449", "refList": ["10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1145/2702123.2702509", "10.1109/tvcg.2016.2598828", "10.1016/j.inffus.2018.07.007", "10.1109/tvcg.2012.197", "10.1631/fitee.1700808", "10.1145/2858036.2858529", "10.1038/nature14539", "10.1109/tvcg.2018.2816223", "10.2307/2288400", "10.3390/s19092212", "10.1109/tvcg.2016.2598829", "10.1109/tvcg.2017.2744718", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/iccv.2017.74", "10.1109/tvcg.2018.2864838", "10.1111/cgf.13092", "10.1016/j.inffus.2018.09.014", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2019.2934619", "10.24963/ijcai.2018/430", "10.1109/tvcg.2018.2864499", "10.1109/cvpr.2016.319", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13681", "year": "2019", "title": "A User-based Visual Analytics Workflow for Exploratory Model Analysis", "conferenceName": "EuroVis", "authors": "Dylan Cashman;Shah Rukh Humayoun;Florian Heimerl;Kendall Park;Subhajit Das;John Thompson;Bahador Saket;Abigail Mosca;John T. Stasko;Alex Endert;Michael Gleicher;Remco Chang", "citationCount": "6", "affiliation": "Cashman, D; Humayoun, SR (Corresponding Author), Tufts Univ, Medford, MA 02155 USA.\nCashman, Dylan; Humayoun, Shah Rukh; Mosca, Abigail; Chang, Remco, Tufts Univ, Medford, MA 02155 USA.\nHeimerl, Florian; Park, Kendall; Gleicher, Michael, Georgia Tech, Atlanta, GA USA.\nDas, Subhajit; Thompson, John; Saket, Bahador; Stasko, John; Endert, Alex, Univ Wisconsin, Madison, WI USA.", "countries": "USA", "abstract": "Many visual analytics systems allow users to interact with machine learning models towards the goals of data exploration and insight generation on a given dataset. However, in some situations, insights may be less important than the production of an accurate predictive model for future use. In that case, users are more interested in generating of diverse and robust predictive models, verifying their performance on holdout data, and selecting the most suitable model for their usage scenario. In this paper, we consider the concept of Exploratory Model Analysis (EMA), which is defined as the process of discovering and selecting relevant models that can be used to make predictions on a data source. We delineate the differences between EMA and the well-known term exploratory data analysis in terms of the desired outcome of the analytic process: insights into the data or a set of deployable models. The contributions of this work are a visual analytics system workflow for EMA, a user study, and two use cases validating the effectiveness of the workflow. We found that our system workflow enabled users to generate complex models, to assess them for various qualities, and to select the most relevant model for their task.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13681", "refList": ["10.1109/tvcg.2017.2744683", "10.1111/cgf.12639", "10.1007/s11390-016-1663-1", "10.1109/tvcg.2017.2744938", "10.1117/12.2007316", "10.1109/tvcg.2017.2745178", "10.1145/2702123.2702509", "10.1080/02701367.1992.10608764", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2011.185", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2013.125", "10.1109/mcse.2007.55", "10.1007/978-3-540-70956-5", "10.1109/vl.1996.545307", "10.1901/jeab.1979.31-433", "10.1111/j.1467-8659.2009.01475.x", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2012.65", "10.1145/1835804.1835827", "10.1109/tvcg.2017.2745085", "10.1145/2939672.2939778", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2017.2744805", "10.1145/2487575.2487629", "10.1109/tvcg.2013.157", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2011.209", "10.1109/vast.2012.6400490", "10.1109/tvcg.2015.2513410", "10.1007/978-3-540-79347-2\\_3", "10.1109/tvcg.2014.2346431", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2012.277", "10.1109/infvis.1998.729560", "10.1109/infvis.2004.64", "10.1109/mcg.2006.70", "10.1109/tvcg.2012.260", "10.1109/tvcg.2014.2346660", "10.1145/1743546.1743567", "10.1111/cgf.13417", "10.1109/tvcg.2014.2346325", "10.1109/tvcg.2018.2864838", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2003.1207445", "10.1109/vast.2010.5652443", "10.1145/2641190.2641198", "10.1109/tvcg.2016.2598830", "10.1109/tvcg.2017.2744378", "10.1111/cgf.13324", "10.1109/mcg.2009.22", "10.1109/tvcg.2016.2599030", "10.1109/vast.2012.6400486", "10.1109/tvcg.2016.2598831", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934261", "title": "Ablate, Variate, and Contemplate: Visual Analytics for Discovering Neural Architectures", "year": "2019", "conferenceName": "VAST", "authors": "Dylan Cashman;Adam Perer;Remco Chang;Hendrik Strobelt", "citationCount": "1", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Perer, Adam, Carnegie Mellon Univ, Pittsburgh, PA 15213 USA. Strobelt, Hendrik, MIT IBM Watson AI Lab, Cambridge, MA USA.", "countries": "USA", "abstract": "The performance of deep learning models is dependent on the precise configuration of many layers and parameters. However, there are currently few systematic guidelines for how to configure a successful model. This means model builders often have to experiment with different configurations by manually programming different architectures (which is tedious and time consuming) or rely on purely automated approaches to generate and train the architectures (which is expensive). In this paper, we present Rapid Exploration of Model Architectures and Parameters, or REMAP, a visual analytics tool that allows a model builder to discover a deep learning model quickly via exploration and rapid experimentation of neural network architectures. In REMAP, the user explores the large and complex parameter space for neural network architectures using a combination of global inspection and local experimentation. Through a visual overview of a set of models, the user identifies interesting clusters of architectures. Based on their findings, the user can run ablation and variation experiments to identify the effects of adding, removing, or replacing layers in a given architecture and generate new models accordingly. They can also handcraft new models using a simple graphical interface. As a result, a model builder can build deep learning models quickly, efficiently, and without manual programming. We inform the design of REMAP through a design study with four deep learning model builders. Through a use case, we demonstrate that REMAP allows users to discover performant neural network architectures efficiently using visual exploration and user-defined semi-automated searches through the model space.", "keywords": "visual analytics,neural networks,parameter space exploration", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934261", "refList": ["10.1109/mcg.2018.2878902", "10.1111/cgf.12639", "10.1109/tvcg.2017.2744938", "10.1117/12.2007316", "10.1109/cvpr.2014.81", "10.1016/j.csda.2008.02.031", "10.1080/00994480.2000.10748487", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2013.125", "10.1109/cvpr.2015.7298594", "10.1111/j.1467-8659.2009.01475.x", "10.1109/tvcg.2012.65", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2017.2744805", "10.1145/2487575.2487629", "10.1109/tvcg.2018.2865044", "10.23915/distill.00010", "10.1109/72.279181", "10.1109/tvcg.2017.2744199", "10.1007/s13398-014-0173-7.2", "10.1109/tvcg.2018.2864504", "10.1109/vast.2012.6400490", "10.1007/978-3-319-10590-1\\_53", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2014.2346321", "10.1094/pdis-11-11-0999-pdn", "10.1109/ijcnn.2015.7280767", "10.1109/tvcg.2017.2744878", "10.5555/3326943.3327130", "10.1109/tvcg.2017.2744718", "10.1109/iccv.2015.169", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2014.223", "10.1109/cvpr.2016.90", "10.1109/vast.2010.5652443", "10.1111/cgf.13681", "10.1109/tvcg.2016.2598831", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028888", "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes", "year": "2020", "conferenceName": "VAST", "authors": "Yuxin Ma;Arlen Fan;Jingrui He;Arun Reddy Nelakurthi;Ross Maciejewski", "citationCount": "0", "affiliation": "Ma, YX (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Ma, Yuxin; Fan, Arlen; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. He, Jingrui, Univ Illinois, Champaign, IL USA. Nelakurthi, Arun Reddy, Samsung Res Amer, Mountain View, CA USA.", "countries": "USA", "abstract": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.", "keywords": "Transfer learning,deep learning,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028888", "refList": ["10.1109/tvcg.2014.2346578", "10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1109/tpami.2018.2868685", "10.1145/2702123.2702509", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2016.2598828", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2013.65", "10.1145/2976749.2978318", "10.1007/978-3-030-01424-7\\_27", "10.1109/tvcg.2019.2934261", "10.1007/s11704-016-6028-y", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/iccv.2015.279", "10.1109/mci.2018.2840738", "10.1109/tvcg.2019.2892483", "10.1109/vast.2018.8802509", "10.1109/tvcg.2013.124", "10.1186/s40537-016-0043-6", "10.1109/tvcg.2018.2864475", "10.1145/3200489", "10.1109/tvcg.2018.2865044", "10.1111/cgf.13210", "10.1109/tvcg.2018.2816223", "10.23915/distill.00007", "10.1109/tvcg.2017.2744199", "10.1109/tkde.2018.2876857", "10.1109/tvcg.2019.2934631", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2011.188", "10.1007/978-3-642-15561-1\\_16", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1109/tvcg.2017.2744718", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2016.2598541", "10.1109/tkde.2009.191", "10.1145/3065386", "10.1016/j.ins.2016.03.021", "10.1109/tvcg.2019.2903943", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/iccv.2017.74", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2012.207", "10.1111/cgf.13092", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2017.2754480", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 9}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13970", "year": "2020", "title": "QUESTO: Interactive Construction of Objective Functions for Classification Tasks", "conferenceName": "EuroVis", "authors": "Subhajit Das;Shenyu Xu;Michael Gleicher;Remco Chang;Alex Endert", "citationCount": "0", "affiliation": "Das, S (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA.\nDas, Subhajit; Xu, Shenyu; Endert, Alex, Georgia Inst Technol, Atlanta, GA 30332 USA.\nGleicher, Michael, Univ Wisconsin, Madison, WI USA.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.", "countries": "USA", "abstract": "Building effective classifiers requires providing the modeling algorithms with information about the training data and modeling goals in order to create a model that makes proper tradeoffs. Machine learning algorithms allow for flexible specification of such meta-information through the design of the objective functions that they solve. However, such objective functions are hard for users to specify as they are a specific mathematical formulation of their intents. In this paper, we present an approach that allows users to generate objective functions for classification problems through an interactive visual interface. Our approach adopts a semantic interaction design in that user interactions over data elements in the visualization are translated into objective function terms. The generated objective functions are solved by a machine learning solver that provides candidate models, which can be inspected by the user, and used to suggest refinements to the specifications. We demonstrate a visual analytics system QUESTO for users to manipulate objective functions to define domain-specific constraints. Through a user study we show that QUESTO helps users create various objective functions that satisfy their goals.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13970", "refList": ["10.1016/j.neucom.2017.01.105", "10.1371/journal.pone.0050474", "10.1109/tvcg.2016.2598839", "10.1007/978-3-642-21530-8\\_14", "10.1109/tvcg.2016.2598828", "10.5555/2969442.2969547", "10.1007/s00371-015-1132-9", "10.1145/2851581.2856492", "10.1126/scirobotics.aao6760", "10.1109/vast.2011.6102453", "10.1109/vast.2011.6102449", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2016.2598446", "10.1145/359784.360332", "10.1111/j.1467-8659.2009.01475.x", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2016.2598460", "10.1109/tevc.2015.2472283", "10.1145/2207676.2207741", "10.1109/tvcg.2017.2745085", "10.1145/2939672.2939778", "10.1145/1866029.1866038", "10.1145/2487575.2487629", "10.1145/3077257.3077259", "10.2312/eurova.20171123", "10.1145/2983924", "10.1109/mcg.2013.53", "10.1109/tvcg.2015.2467615", "10.1109/vast.2014.7042492", "10.1145/2675133.2675214", "10.1109/tvcg.2014.2346482", "10.1145/3180308.3180362", "10.1109/tvcg.2014.2346321", "10.24963/ijcai.2017/202", "10.1109/tvcg.2014.2346291", "10.1109/tbme.2012.2212278", "10.1007/s40708-016-0042-6", "10.1609/aimag.v35i4.2513", "10.1016/j.ijhcs.2009.03.004", "10.1109/tevc.2012.2225064", "10.1016/s0890-6955(02)00074-3", "10.1109/icmlde.2018.00014", "10.1111/cgf.13681", "10.1109/tvcg.2013.173", "10.1145/3025171.3025208", "10.1145/3025453.3026044", "10.1109/cec.2017.7969334", "10.1109/vast.2012.6400486", "10.1109/tvcg.2016.2598831", "10.1109/tcyb.2014.2310651"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13972", "year": "2020", "title": "Boxer: Interactive Comparison of Classifier Results", "conferenceName": "EuroVis", "authors": "Michael Gleicher;Aditya Barve;Xinyi Yu;Florian Heimerl", "citationCount": "0", "affiliation": "Gleicher, M (Corresponding Author), Univ Wisconsin, Madison, WI 53706 USA.\nGleicher, Michael; Barve, Aditya; Yu, Xinyi; Heimerl, Florian, Univ Wisconsin, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Machine learning practitioners often compare the results of different classifiers to help select, diagnose and tune models. We present Boxer, a system to enable such comparison. Our system facilitates interactive exploration of the experimental results obtained by applying multiple classifiers to a common set of model inputs. The approach focuses on allowing the user to identify interesting subsets of training and testing instances and comparing performance of the classifiers on these subsets. The system couples standard visual designs with set algebra interactions and comparative elements. This allows the user to compose and coordinate views to specify subsets and assess classifier performance on them. The flexibility of these compositions allow the user to address a wide range of scenarios in developing and assessing classifiers. We demonstrate Boxer in use cases including model selection, tuning, fairness assessment, and data quality diagnosis.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13972", "refList": ["10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2016.2598838", "10.1007/s10618-014-0368-8", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1145/3287560.3287589", "10.1109/vast.2017.8585721", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2009.128", "10.1109/tvcg.2017.2744018", "10.1080/00994480.2000.10748487", "10.5555/3305890.3306024", "10.1109/iccv.2015.329", "10.1109/tvcg.2013.125", "10.1089/big.2016.0007", "10.1109/memsys.2019.8870817", "10.1145/2939672.2939778", "10.1007/s11104-019-04156-0", "10.1371/journal.pone.0181142", "10.1145/3301275.3302324", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2018.2865044", "10.1023/a:1010933404324", "10.1145/2487575.2487579", "10.1109/tvcg.2013.157", "10.1145/2783258.2788613", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934631", "10.1016/s0304-3800(02)00064-9", "10.1007/s10115-013-0679-x", "10.1109/tvcg.2019.2934267", "10.1007/978-3-319-10590-1\\_53", "10.1109/vast.2017.8585720", "10.1016/0004-3702(80)90021-1", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2009.84", "10.1007/s11263-016-0911-8", "10.1111/cgf.12918", "10.1111/cgf.12373", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1080/00207176808905715", "10.1002/er.3827", "10.1109/tvcg.2014.2346660", "10.1111/cgf.13417", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2017.2744158", "10.1016/b978-0-12-815849-4.00004-9", "10.1097/ede.0b013e3181c30fb2", "10.1111/cgf.13681", "10.1016/j.ejor.2006.04.051", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2016.2598468", "10.9735/2229-3981", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 21}, {"doi": "10.1111/cgf.13677", "year": "2019", "title": "An Ontological Framework for Supporting the Design and Evaluation of Visual Analytics Systems", "conferenceName": "EuroVis", "authors": "Min Chen;David S. Ebert", "citationCount": "5", "affiliation": "Chen, M (Corresponding Author), Univ Oxford, Oxford, England.\nChen, Min, Univ Oxford, Oxford, England.\nEbert, David S., Purdue Univ, W Lafayette, IN 47907 USA.", "countries": "USA;England", "abstract": "Designing, evaluating, and improving visual analytics (VA) systems is a primary area of activities in our discipline. In this paper, we present an ontological framework for recording and categorizing technical shortcomings to be addressed in a VA workflow, reasoning about the causes of such problems, identifying technical solutions, and anticipating secondary effects of the solutions. The methodology is built on the theoretical premise that designing a VA workflow is an optimization of the cost-benefit ratio of the processes in the workflow. It makes uses three fundamental measures to group and connect symptoms, causes, remedies, and side-effects, and guide the search for potential solutions to the problems. In terms of requirement analysis and system design, the proposed methodology can enable system designers to explore the decision space in a structured manner. In terms of evaluation, the proposed methodology is time-efficient and complementary to various forms of empirical studies, such as user surveys, controlled experiments, observational studies, focus group discussions, and so on. In general, it reduces the amount of trial-and-error in the lifecycle of VA system development.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13677", "refList": ["10.1109/tvcg.2006.178", "10.1109/infvis.2000.885092", "10.1111/cgf.12920", "10.1057/ivs.2009.26", "10.1109/mcg.2017.3271463", "10.1007/978-3-319-10578-9\\_1", "10.1109/icdmw.2008.62", "10.1109/mcg.2017.51", "10.1145/3011141.3011207", "10.1109/tvcg.2013.134", "10.1080/10618600.1996.10474696", "10.1007/978-3-540-70956-5", "10.1109/visual.1990.146375", "10.1109/tvcg.2012.219", "10.1109/vl.1996.545307", "10.1057/ivs.2009.23", "10.1002/j.1538-7305.1948.tb00917.x", "10.1109/tvcg.2010.79", "10.1109/tvcg.2013.124", "10.1111/cgf.13211", "10.1007/978-3-642-40897-7\\_9", "10.1103/physrev.108.171", "10.1109/infvis.2004.59", "10.1109/iv.2008.36", "10.1111/cgf.13210", "10.1111/j.1467-8659.2008.01230.x", "10.1001/jama.293.10.1223", "10.1177/1473871611407399", "10.1109/visual.1995.480821", "10.1117/12.539227", "10.1109/tvcg.2015.2513410", "10.1109/vast.2011.6102463", "10.7749/citiescommunitiesterritories.dec2014.029.art01", "10.1109/mcg.2005.55", "10.1109/tvcg.2012.234", "10.1109/pacificvis.2012.6183556", "10.1103/physrev.106.620", "10.1109/infvis.1997.636792", "10.2307/2104491", "10.1145/2468356.2468677", "10.1145/3173574.3173611", "10.1109/tvcg.2018.2864838", "10.1111/cgf.13092", "10.1109/visual.2004.10", "10.1109/tvcg.2017.2744319", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2603178"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934264", "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;David S. Ebert;Arif Ghafoor", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Karimzadeh, Morteza; Ebert, David S.; Ghafoor, Arif, Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.", "keywords": "Summative evaluation,usefulness,evaluation process,taxonomy,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934264", "refList": ["10.1109/tvcg.2017.2744478", "10.1109/tvcg.2018.2865025", "10.1109/tvcg.2006.85", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2017.2745181", "10.1111/cgf.13677", "10.1109/tvcg.2018.2864844", "10.1109/tvcg.2013.126", "10.1109/tvcg.2018.2864811", "10.1109/infvis.2005.1532147", "10.1177/0956797613504966", "10.1145/2669557.2669579", "10.1109/mcg.2005.102", "10.1109/visual.2003.1250426", "10.1136/bmj.39489.470347.ad", "10.1109/tvcg.2017.2744080", "10.1109/mcg.2009.53", "10.1111/j.1467-8527.2005.00307.x", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2864886", "10.1109/tvcg.2018.2864843", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2018.2865051", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2018.2865026", "10.1007/978-3-540-71080-6\\_6", "10.1109/tvcg.2018.2865020", "10.1177/1473871611407399", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2018.2864526", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2864905", "10.1049/sej.1991.0040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2711030", "10.1109/tvcg.2011.279", "10.1109/vast.2017.8585505", "10.1147/jrd.2010.2042914", "10.1016/s0378-7206(98)00044-5", "10.1145/2993901.2993913", "10.1109/tvcg.2018.2865041", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744758", "10.1145/1168149.1168158", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2012.213", "10.1109/tvcg.2017.2744738", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2018.2864826", "10.1145/1377966.1377974", "10.1109/apec.2009.4802646", "10.1145/1168149.1168152", "10.1016/j.jss.2008.03.059", "10.1109/vast.2017.8585484", "10.1109/tvcg.2017.2744818", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2865042", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030388", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "year": "2020", "conferenceName": "SciVis", "authors": "Pepe Eulzer;Sabine Bauer;Francis Kilian;Kai Lawonn", "citationCount": "0", "affiliation": "Eulzer, P (Corresponding Author), Univ Jena, Jena, Germany. Eulzer, Pepe; Lawonn, Kai, Univ Jena, Jena, Germany. Bauer, Sabine, Univ Koblenz Landau, Koblenz, Germany. Kilian, Francis, Cath Clin Koblenz Montabaur, Dept Spine Surg, Koblenz, Germany.", "countries": "Germany", "abstract": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. $\\mathrm{By}$ linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. $\\mathrm{in}$ a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.", "keywords": "Medical visualization,bioinformatics,coordinated views,focus and context,biomechanical simulation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030388", "refList": ["10.1109/tvcg.2018.2864903", "10.1177/1473871613510429", "10.1093/ehjqcco/qcz052", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2018.2865076", "10.1055/s-0039-1687862", "10.1109/visual.1990.146375", "10.1109/tvcg.2017.2744198", "10.1016/j.ijmedinf.2014.10.001", "10.1109/tvcg.2013.124", "10.1016/j.jacc", "10.1111/cgf.13167", "10.17705/1thci.00055", "10.1136/bmjqs.2009.037895", "10.1109/tvcg.2013.238", "10.1109/tvcg.2018.2865240", "10.1186/1471-2261-6-34", "10.1109/tvcg.2019.2934264", "10.1109/tvcg.2013.200", "10.1109/tvcg.2011.209", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467091", "10.1136/bmjopen-2019-033208", "10.1109/beliv.2018.8634027", "10.1109/tvcg.2012.213", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2015.2467325", "10.1145/2133806.2133821", "10.1145/1806799.1806866", "10.1108/02635570610688869", "10.1002/hbm.20701", "10.1561/1100000039", "10.1145/3025453.3025645", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 5}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 61}, {"doi": "10.1109/tvcg.2018.2865018", "title": "TPFlow: Progressive Partition and Multidimensional Pattern Extraction for Large-Scale Spatio-Temporal Data Analysis", "year": "2018", "conferenceName": "VAST", "authors": "Dongyu Liu;Panpan Xu;Ren Liu", "citationCount": "6", "affiliation": "Liu, DY (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Dongyu, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Dongyu; Xu, Panpan; Ren, Liu, Bosch Res North Amer, Sunnyvale, CA USA.", "countries": "USA;China", "abstract": "Consider a multi-dimensional spatio-temporal (ST) dataset where each entry is a numerical measure defined by the corresponding temporal, spatial and other domain-specific dimensions. A typical approach to explore such data utilizes interactive visualizations with multiple coordinated views. Each view displays the aggregated measures along one or two dimensions. By brushing on the views, analysts can obtain detailed information. However, this approach often cannot provide sufficient guidance for analysts to identify patterns hidden within subsets of data. Without a priori hypotheses, analysts need to manually select and iterate through different slices to search for patterns, which can be a tedious and lengthy process. In this work, we model multidimensional ST data as tensors and propose a novel piecewise rank-one tensor decomposition algorithm which supports automatically slicing the data into homogeneous partitions and extracting the latent patterns in each partition for comparison and visual summarization. The algorithm optimizes a quantitative measure about how faithfully the extracted patterns visually represent the original data. Based on the algorithm we further propose a visual analytics framework that supports a top-down, progressive partitioning workflow for level-of-detail multidimensional ST data exploration. We demonstrate the general applicability and effectiveness of our technique on three datasets from different application domains: regional sales trend analysis, customer traffic analysis in department stores, and taxi trip analysis with origin-destination (OD) data. We further interview domain experts to verify the usability of the prototype.", "keywords": "Spatio-temporal data,tensor decomposition,interactive exploration,automatic pattern discoveries", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865018", "refList": ["10.1007/s13177-014-0099-7", "10.1109/tvcg.2013.226", "10.1111/j.1467-8659.2009.01664.x", "10.1007/s00371-013-0892-3", "10.1109/tvcg.2015.2468111", "10.2307/3151680", "10.1111/cgf.12129", "10.1145/2254556.2254659", "10.1109/tvcg.2017.2744419", "10.1145/2632048.2636073", "10.1109/2945.841121", "10.1109/tvcg.2014.2346449", "10.1109/tvcg.2007.70515", "10.1109/tsmc.2018.2871100", "10.1145/2629592", "10.1109/tvcg.2016.2616404", "10.1109/vl.1996.545307", "10.1109/infvis.2003.1249018", "10.1198/jcgs.2010.09051", "10.1007/978-3-319-71249-9\\_35", "10.1109/tvcg.2016.2598624", "10.1109/tvcg.2017.2744805", "10.1109/icde.2014.6816674", "10.1111/cgf.12888", "10.1109/tvcg.2006.161", "10.1109/tvcg.2016.2598862", "10.1016/s1045-926x(03)00046-6", "10.1089/cmb.2018.0139", "10.1109/tits.2017.2683539", "10.1109/tbdata.2016.2586447", "10.1137/07070111x", "10.1007/bf02310791", "10.1109/tits.2015.2436897", "10.1016/j.trc.2017.10.023", "10.1109/tvcg.2013.179", "10.1177/1473871616667632", "10.1137/s0895479899352045", "10.1109/jas.2017.7510538", "10.1111/j.1467-8659.2012.03117.x", "10.1007/bf01908075", "10.1002/widm.1", "10.2307/2669946", "10.1137/s0895479801387413", "10.1109/tvcg.2015.2467112", "10.1109/tvcg.2014.2346574", "10.1057/palgrave.ivs.9500184", "10.1109/infvis.1999.801851", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028889", "title": "A Visual Analytics Framework for Reviewing Multivariate Time-Series Data with Dimensionality Reduction", "year": "2020", "conferenceName": "VAST", "authors": "Takanori Fujiwara;Shilpika;Naohisa Sakamoto;Jorji Nonaka;Keiji Yamamoto;Kwan-Liu Ma", "citationCount": "0", "affiliation": "Fujiwara, T (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Fujiwara, Takanori; Shilpika; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA. Sakamoto, Naohisa, Kobe Univ, Kobe, Hyogo, Japan. Nonaka, Jorji; Yamamoto, Keiji, RIKEN R CCS, Kobe, Hyogo, Japan.", "countries": "Japan;USA", "abstract": "Data-driven problem solving in many real-world applications involves analysis of time-dependent multivariate data, for which dimensionality reduction (DR) methods are often used to uncover the intrinsic structure and features of the data. However, DR is usually applied to a subset of data that is either single-time-point multivariate or univariate time-series, resulting in the need to manually examine and correlate the DR results out of different data subsets. When the number of dimensions is large either in terms of the number of time points or attributes, this manual task becomes too tedious and infeasible. In this paper, we present MulTiDR, a new DR framework that enables processing of time-dependent multivariate data as a whole to provide a comprehensive overview of the data. With the framework, we employ DR in two steps. When treating the instances, time points, and attributes of the data as a 3D array, the first DR step reduces the three axes of the array to two, and the second DR step visualizes the data in a lower-dimensional space. In addition, by coupling with a contrastive learning method and interactive visualizations, our framework enhances analysts' ability to interpret DR results. We demonstrate the effectiveness of our framework with four case studies using real-world datasets.", "keywords": "Multivariate time-series,tensor,data cube,dimensionality reduction,interpretability,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028889", "refList": ["10.1016/j.neucom.2008.12.017", "10.1109/tvcg.2015.2467591", "10.1007/bf02289464", "10.1109/tvcg.2015.2468111", "10.1177/1350650119867242", "10.1109/pacificvis48177.2020.9280", "10.1109/tvcg.2017.2744419", "10.1109/tvcg.2011.185", "10.1109/bigdata.2015.7363807", "10.1145/2669557.2669559", "10.1016/j.patcog.2011.01.004", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1186/1475-925x-14-s2-s6", "10.1007/s12650-018-0530-2", "10.1109/tkde.2018.2878247", "10.1016/j.visinf.2018.04.010", "10.1002/1099-128x(200005/06)14:3", "10.1371/journal.pone.0107878", "10.1109/pacificvis.2017.8031601", "10.1016/j.jbi.2019.103291", "10.1109/daac49578.2019.00008", "10.1109/allerton.2019.8919886", "10.1007/978-3-319-13105-4\\_14", "10.4258/hir.2016.22.3.156", "10.1109/tvcg.2015.2467553", "10.1145/2245276.2245469", "10.1007/bf02294485", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2016.2598470", "10.1137/07070111x", "10.1109/tvcg.2019.2934433", "10.1109/tvcg.2016.2640960", "10.1007/bf02310791", "10.1109/tvcg.2016.2534558", "10.1109/access.2016.2529723", "10.1109/tvcg.2016.2598664", "10.2312/eurovisshort.20161164", "10.1109/tvcg.2018.2865018", "10.1111/cgf.12804", "10.1109/tvcg.2018.2846735", "10.1016/j.comnet.2017.06.013", "10.1146/annurev-statistics-041715-033624", "10.1109/pacificvis.2018.00026", "10.1109/tvcg.2015.2467851"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030425", "title": "Visual Analysis of Argumentation in Essays", "year": "2020", "conferenceName": "VAST", "authors": "Dora Kiesel;Patrick Riehmann;Henning Wachsmuth;Benno Stein;Bernd Fr\u00f6hlich", "citationCount": "0", "affiliation": "Kiesel, D (Corresponding Author), Bauhaus Univ Weimar, Weimar, Germany. Kiesel, Dora; Riehmann, Patrick; Stein, Benno; Froehlich, Bernd, Bauhaus Univ Weimar, Weimar, Germany. Wachsmuth, Henning, Paderborn Univ, Paderborn, Germany.", "countries": "Germany", "abstract": "This paper presents a visual analytics system for exploring, analyzing and comparing argument structures in essay corpora. We provide an overview of the corpus by a list of ArguLines which represent the argument units of each essay by a sequence of glyphs. Each glyph encodes the stance, the depth and the relative position of an argument unit. The overview can be ordered in various ways to reveal patterns and outliers. Subsets of essays can be selected and analyzed in detail using the Argument Unit Occurrence Tree which aggregates the argument structures using hierarchical histograms. This hierarchical view facilitates the estimation of statistics and trends concerning the progression of the argumentation in the essays. It also provides insights into the commonalities and differences between selected subsets. The text view is the necessary textual basis to verify conclusions from the other views and the annotation process. Linking the views and interaction techniques for visual filtering, studying the evolution of stance within a subset of essays and scrutinizing the order of argumentative units enable a deep analysis of essay corpora. Our expert reviews confirmed the utility of the system and revealed detailed and previously unknown information about the argumentation in our sample corpus.", "keywords": "Information Visualization,Text Analysis,User Interfaces,Visual Analytics,Argumentation Visualization,Glyph-based Techniques,Text and Document Data,Tree-based Visualization,Coordinated and Multiple Views,Close and Distant Reading", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030425", "refList": ["10.1109/tvcg.2014.2346575", "10.14778/2735479.2735485", "10.14778/3192965.3192971", "10.1111/cgf.12129", "10.1145/2206869.2206874", "10.1109/icde.2016.7498300", "10.14778/2831360.2831371", "10.1145/3035918.3056097", "10.1023/a:1009726021843", "10.1111/cgf.13678", "10.1145/3183713.3196905", "10.14778/3115404.3115418", "10.1109/tvcg.2012.180", "10.1109/icde.1999.754950", "10.14778/1453856.1453924", "10.1145/3209900.3209901", "10.1002/spe.2325", "10.1145/42201.42203", "10.1109/tvcg.2013.124", "10.1109/vast.2008.4677357", "10.1109/icde.2016.7498287", "10.1109/icde.2014.6816674", "10.14778/2732951.2732964", "10.1109/tvcg.2015.2467551", "10.1145/1084805.1084812", "10.1109/2.781635", "10.14778/2732279.2732280", "10.1109/icde.2015.7113427", "10.1109/tvcg.2015.2467091", "10.1007/s00778-017-0486-1", "10.1109/tvcg.2003.1196005", "10.1109/icde.2004.1320035", "10.1109/tvcg.2016.2607714", "10.14778/2732951.2732953", "10.1109/tvcg.2008.131", "10.1109/tvcg.2018.2865018", "10.1145/2133806.2133821", "10.1109/icde.2019.00035", "10.1109/tpds.2005.144", "10.14778/3236187.3236212", "10.1109/tvcg.2009.111", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/tvcg.2019.2934261", "title": "Ablate, Variate, and Contemplate: Visual Analytics for Discovering Neural Architectures", "year": "2019", "conferenceName": "VAST", "authors": "Dylan Cashman;Adam Perer;Remco Chang;Hendrik Strobelt", "citationCount": "1", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Perer, Adam, Carnegie Mellon Univ, Pittsburgh, PA 15213 USA. Strobelt, Hendrik, MIT IBM Watson AI Lab, Cambridge, MA USA.", "countries": "USA", "abstract": "The performance of deep learning models is dependent on the precise configuration of many layers and parameters. However, there are currently few systematic guidelines for how to configure a successful model. This means model builders often have to experiment with different configurations by manually programming different architectures (which is tedious and time consuming) or rely on purely automated approaches to generate and train the architectures (which is expensive). In this paper, we present Rapid Exploration of Model Architectures and Parameters, or REMAP, a visual analytics tool that allows a model builder to discover a deep learning model quickly via exploration and rapid experimentation of neural network architectures. In REMAP, the user explores the large and complex parameter space for neural network architectures using a combination of global inspection and local experimentation. Through a visual overview of a set of models, the user identifies interesting clusters of architectures. Based on their findings, the user can run ablation and variation experiments to identify the effects of adding, removing, or replacing layers in a given architecture and generate new models accordingly. They can also handcraft new models using a simple graphical interface. As a result, a model builder can build deep learning models quickly, efficiently, and without manual programming. We inform the design of REMAP through a design study with four deep learning model builders. Through a use case, we demonstrate that REMAP allows users to discover performant neural network architectures efficiently using visual exploration and user-defined semi-automated searches through the model space.", "keywords": "visual analytics,neural networks,parameter space exploration", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934261", "refList": ["10.1109/mcg.2018.2878902", "10.1111/cgf.12639", "10.1109/tvcg.2017.2744938", "10.1117/12.2007316", "10.1109/cvpr.2014.81", "10.1016/j.csda.2008.02.031", "10.1080/00994480.2000.10748487", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2013.125", "10.1109/cvpr.2015.7298594", "10.1111/j.1467-8659.2009.01475.x", "10.1109/tvcg.2012.65", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2017.2744805", "10.1145/2487575.2487629", "10.1109/tvcg.2018.2865044", "10.23915/distill.00010", "10.1109/72.279181", "10.1109/tvcg.2017.2744199", "10.1007/s13398-014-0173-7.2", "10.1109/tvcg.2018.2864504", "10.1109/vast.2012.6400490", "10.1007/978-3-319-10590-1\\_53", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2014.2346321", "10.1094/pdis-11-11-0999-pdn", "10.1109/ijcnn.2015.7280767", "10.1109/tvcg.2017.2744878", "10.5555/3326943.3327130", "10.1109/tvcg.2017.2744718", "10.1109/iccv.2015.169", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2014.223", "10.1109/cvpr.2016.90", "10.1109/vast.2010.5652443", "10.1111/cgf.13681", "10.1109/tvcg.2016.2598831", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028888", "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes", "year": "2020", "conferenceName": "VAST", "authors": "Yuxin Ma;Arlen Fan;Jingrui He;Arun Reddy Nelakurthi;Ross Maciejewski", "citationCount": "0", "affiliation": "Ma, YX (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Ma, Yuxin; Fan, Arlen; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. He, Jingrui, Univ Illinois, Champaign, IL USA. Nelakurthi, Arun Reddy, Samsung Res Amer, Mountain View, CA USA.", "countries": "USA", "abstract": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.", "keywords": "Transfer learning,deep learning,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028888", "refList": ["10.1109/tvcg.2014.2346578", "10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1109/tpami.2018.2868685", "10.1145/2702123.2702509", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2016.2598828", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2013.65", "10.1145/2976749.2978318", "10.1007/978-3-030-01424-7\\_27", "10.1109/tvcg.2019.2934261", "10.1007/s11704-016-6028-y", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/iccv.2015.279", "10.1109/mci.2018.2840738", "10.1109/tvcg.2019.2892483", "10.1109/vast.2018.8802509", "10.1109/tvcg.2013.124", "10.1186/s40537-016-0043-6", "10.1109/tvcg.2018.2864475", "10.1145/3200489", "10.1109/tvcg.2018.2865044", "10.1111/cgf.13210", "10.1109/tvcg.2018.2816223", "10.23915/distill.00007", "10.1109/tvcg.2017.2744199", "10.1109/tkde.2018.2876857", "10.1109/tvcg.2019.2934631", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2011.188", "10.1007/978-3-642-15561-1\\_16", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1109/tvcg.2017.2744718", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2016.2598541", "10.1109/tkde.2009.191", "10.1145/3065386", "10.1016/j.ins.2016.03.021", "10.1109/tvcg.2019.2903943", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/iccv.2017.74", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2012.207", "10.1111/cgf.13092", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2017.2754480", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934547", "title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data", "year": "2019", "conferenceName": "VAST", "authors": "Robert Kr\u00fcger;Johanna Beyer;Won-Dong Jang;Nam Wook Kim;Artem Sokolov;Peter K. Sorger;Hanspeter Pfister", "citationCount": "1", "affiliation": "Krueger, R (Corresponding Author), Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Krueger, R (Corresponding Author), Harvard Med Sch, Lab Syst Pharmacol, Boston, MA 02115 USA. Krueger, Robert; Beyer, Johanna; Jang, Won-Dong; Kim, Nam Wook; Pfister, Hanspeter, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Krueger, Robert; Sokolov, Artem; Sorger, Peter K., Harvard Med Sch, Lab Syst Pharmacol, Boston, MA 02115 USA.", "countries": "USA", "abstract": "Facetto is a scalable visual analytics application that is used to discover single-cell phenotypes in high-dimensional multi-channel microscopy images of human tumors and tissues. Such images represent the cutting edge of digital histology and promise to revolutionize how diseases such as cancer are studied, diagnosed, and treated. Highly multiplexed tissue images are complex, comprising 109 or more pixels, 60-plus channels, and millions of individual cells. This makes manual analysis challenging and error-prone. Existing automated approaches are also inadequate, in large part, because they are unable to effectively exploit the deep knowledge of human tissue biology available to anatomic pathologists. To overcome these challenges, Facetto enables a semi-automated analysis of cell types and states. It integrates unsupervised and supervised learning into the image and feature exploration process and offers tools for analytical provenance. Experts can cluster the data to discover new types of cancer and immune cells and use clustering results to train a convolutional neural network that classifies new cells accordingly. Likewise, the output of classifiers can be clustered to discover aggregate patterns and phenotype subsets. We also introduce a new hierarchical approach to keep track of analysis steps and data subsets created by users; this assists in the identification of cell types. Users can build phenotype trees and interact with the resulting hierarchical structures of both high-dimensional feature and image spaces. We report on use-cases in which domain scientists explore various large-scale fluorescence imaging datasets. We demonstrate how Facetto assists users in steering the clustering and classification process, inspecting analysis results, and gaining new scientific insights into cancer biology.", "keywords": "Clustering,Classification,Visual Analysis,Multiplex Tissue Imaging,Digital Pathology,Cancer Systems Biology", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934547", "refList": ["10.1145/1656274.1656280", "10.1111/cgf.12893", "10.1145/1142473.1142574", "10.1109/vahc.2017.8387544", "10.1111/j.2517-6161.1977.tb01600.x", "10.7554/elife.31657", "10.1109/tvcg.2013.125", "10.1186/1471-2105-16-s11-s5", "10.1109/isbi.2011.5872394", "10.1145/1268517.1268563", "10.1109/tvcg.2013.186", "10.1016/j.patrec.2009.09.011", "10.1001/jama.2016.17216", "10.1145/1656274.1656278", "10.1109/vahc.2017.8387545", "10.1145/1390156.1390183", "10.1109/tvcg.2017.2744805", "10.1109/hicss.2011.339", "10.1111/cgf.12613", "10.1007/bf01898350", "10.1109/vast.2014.7042495", "10.1109/tvcg.2015.2467551", "10.1016/j.cels.2016.03.008", "10.1038/nmeth.1896", "10.1109/tvcg.2012.277", "10.1016/s0893-6080(97)00002-6", "10.1057/palgrave.ivs.9500170", "10.1002/(sici)1097-4571(199307)44:6", "10.1109/tvcg.2007.70569", "10.1145/1830483.1830503", "10.1186/gb-2006-7-10-r100", "10.1109/tvcg.2012.213", "10.1109/tvcg.2012.258", "10.1109/tvcg.2017.2744158", "10.1109/sibgra.2002.1167123", "10.1007/978-3-319-24574-4\\_28", "10.1038/nmeth.4391", "10.1109/vast.2010.5652443", "10.1002/cpch.14", "10.1093/bioinformatics/bts288", "10.1109/tvcg.2016.2598468", "10.1109/tvcg.2016.2598587", "10.1080/00207179208934272"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030363", "title": "Improving the Usability of Virtual Reality Neuron Tracing with Topological Elements", "year": "2020", "conferenceName": "SciVis", "authors": "Torin McDonald;William Usher;Nathan Morrical;Attila Gyulassy;Steve Petruzza;Frederick Federer;Alessandra Angelucci;Valerio Pascucci", "citationCount": "0", "affiliation": "McDonald, T (Corresponding Author), Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA. McDonald, Torin; Usher, Will; Morrical, Nate; Gyulassy, Attila; Petruzza, Steve; Pascucci, Valerio, Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA. Petruzza, Steve, Utah State Univ, Logan, UT 84322 USA. Federer, Frederick; Angelucci, Alessandra, Univ Utah, Moran Ese Inst, Salt Lake City, UT 84112 USA.", "countries": "USA", "abstract": "Researchers in the field of connectomics are working to reconstruct a map of neural connections in the brain in order to understand at a fundamental level how the brain processes information. Constructing this wiring diagram is done by tracing neurons through high-resolution image stacks acquired with fluorescence microscopy imaging techniques. While a large number of automatic tracing algorithms have been proposed, these frequently rely on local features in the data and fail on noisy data or ambiguous cases, requiring time-consuming manual correction. As a result, manual and semi-automatic tracing methods remain the state-of-the-art for creating accurate neuron reconstructions. We propose a new semi-automatic method that uses topological features to guide users in tracing neurons and integrate this method within a virtual reality (VR) framework previously used for manual tracing. Our approach augments both visualization and interaction with topological elements, allowing rapid understanding and tracing of complex morphologies. In our pilot study, neuroscientists demonstrated a strong preference for using our tool over prior approaches, reported less fatigue during tracing, and commended the ability to better understand possible paths and alternatives. Quantitative evaluation of the traces reveals that users' tracing speed increased, while retaining similar accuracy compared to a fully manual approach.", "keywords": "Virtual Reality,Morse-Smale Complex,Semi-automatic Neuron Tracing", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030363", "refList": ["10.1038/nmeth.2869", "10.18637/jss.v028.c01", "10.1177/1473871611416549", "10.1016/j.celrep.2020.107523", "10.1602/neurorx.1.2.182", "10.1038/nprot.2014.191", "10.1136/jnnp-2011-300403", "10.1126/sciadv.aax5851", "10.2312/pe.vmv.vmv13.105-112", "10.1038/s43018-020-0031-9", "10.1109/tvcg.2013.213", "10.1093/jnci/92.8.613", "10.1109/52.329404", "10.1109/tvcg.2017.2785271", "10.1007/s11548-013-0820-z", "10.1109/tvcg.2019.2934547", "10.1002/cjp2.113", "10.1080/2162402x.2018.1507600", "10.1109/tvcg.2013.124", "10.1016/s0140-6736(14)60958-2", "10.1038/s41467-017-01689-9", "10.1016/j.cell.2018.07.010", "10.1007/978-3-319-24523-2", "10.1038/nrg3832", "10.2352/j.imagingsci.technol.2017.61.6.060404", "10.1109/tvcg.2018.2864907", "10.1111/cgf.13413", "10.1007/978-3-319-67979-2\\_4", "10.1145/2836034.2836040", "10.1038/nmeth.2563", "10.1016/0377-0427(87)90125-7", "10.1016/j.immuni.2016.04.014", "10.1002/cyto.a.22702", "10.12688/wellcomeopenres.15191.1", "10.1109/2945.981851", "10.1038/s43018-020-0026-6", "10.5281/zenodo", "10.1109/tvcg.2019.2931299", "10.1109/annes.1995.499469", "10.1145/2133806.2133821", "10.1002/hbm.20701", "10.1109/tvcg.2013.161", "10.1007/978-3-319-24523-210", "10.1126/scitranslmed.3004330", "10.1038/nmeth.4391", "10.1559/152304003100010929", "10.1126/science.280.5363.585", "10.1007/978-3-319-24523-2\\_10", "10.1109/tvcg.2016.2598587", "10.1038/s41597-019-0258-4"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030336", "title": "Visual cohort comparison for spatial single-cell omics-data", "year": "2020", "conferenceName": "VAST", "authors": "Antonios Somarakis;Marieke E. Ijsselsteijn;Sietse J. Luk;Boyd Kenkhuis;Noel F. C. C. de Miranda;Boudewijn P. F. Lelieveldt;Thomas H\u00f6llt", "citationCount": "0", "affiliation": "Hollt, T (Corresponding Author), Delft Univ Technol, Comp Graph \\& Visualizat Grp, Delft, Netherlands. Hollt, T (Corresponding Author), Leiden Univ, Med Ctr, Leiden Computat Biol Ctr, Leiden, Netherlands. Somarakis, Antonios; Lelieveldt, Boudewijn P. F., Leiden Univ, Med Ctr, Dept Radiol, Div Image Proc, Leiden, Netherlands. Ijsselsteijn, Marieke E.; de Miranda, Noel F. C. C., Leiden Univ, Med Ctr, Dept Pathol, Immunogen Grp, Leiden, Netherlands. Luk, Sietse J., Leiden Univ, Med Ctr, Hematol Dept, Leiden, Netherlands. Kenkhuis, Boyd, Leiden Univ, Med Ctr, Human Genet Dept, Leiden, Netherlands. Hollt, Thomas, Delft Univ Technol, Comp Graph \\& Visualizat Grp, Delft, Netherlands. Hollt, Thomas, Leiden Univ, Med Ctr, Leiden Computat Biol Ctr, Leiden, Netherlands.", "countries": "Netherlands", "abstract": "Spatially-resolved omics-data enable researchers to precisely distinguish cell types in tissue and explore their spatial interactions, enabling deep understanding of tissue functionality. To understand what causes or deteriorates a disease and identify related biomarkers, clinical researchers regularly perform large-scale cohort studies, requiring the comparison of such data at cellular level. In such studies, with little a-priori knowledge of what to expect in the data, explorative data analysis is a necessity. Here, we present an interactive visual analysis workflow for the comparison of cohorts of spatially-resolved omics-data. Our workflow allows the comparative analysis of two cohorts based on multiple levels-of-detail, from simple abundance of contained cell types over complex co-localization patterns to individual comparison of complete tissue images. As a result, the workflow enables the identification of cohort-differentiating features, as well as outlier samples at any stage of the workflow. During the development of the workflow, we continuously consulted with domain experts. To show the effectiveness of the workflow, we conducted multiple case studies with domain experts from different application areas and with different data modalities.", "keywords": "Visual analytics,Imaging Mass Cytometry,Vectra,spatially-resolved data,single-cell omics-data,Visual comparison", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030336", "refList": ["10.1038/nmeth.2869", "10.18637/jss.v028.c01", "10.1177/1473871611416549", "10.1016/j.celrep.2020.107523", "10.1602/neurorx.1.2.182", "10.1038/nprot.2014.191", "10.1136/jnnp-2011-300403", "10.1126/sciadv.aax5851", "10.2312/pe.vmv.vmv13.105-112", "10.1038/s43018-020-0031-9", "10.1109/tvcg.2013.213", "10.1093/jnci/92.8.613", "10.1109/52.329404", "10.1109/tvcg.2017.2785271", "10.1007/s11548-013-0820-z", "10.1109/tvcg.2019.2934547", "10.1002/cjp2.113", "10.1080/2162402x.2018.1507600", "10.1109/tvcg.2013.124", "10.1016/s0140-6736(14)60958-2", "10.1038/s41467-017-01689-9", "10.1016/j.cell.2018.07.010", "10.1007/978-3-319-24523-2", "10.1038/nrg3832", "10.2352/j.imagingsci.technol.2017.61.6.060404", "10.1109/tvcg.2018.2864907", "10.1111/cgf.13413", "10.1007/978-3-319-67979-2\\_4", "10.1145/2836034.2836040", "10.1038/nmeth.2563", "10.1101/2020.03.27.001834", "10.1016/0377-0427(87)90125-7", "10.1016/j.immuni.2016.04.014", "10.1002/cyto.a.22702", "10.12688/wellcomeopenres.15191.1", "10.1109/2945.981851", "10.1038/s43018-020-0026-6", "10.1038/s41586-019-1876-x", "10.1109/tvcg.2019.2931299", "10.1109/annes.1995.499469", "10.1145/2133806.2133821", "10.1109/tvcg.2013.161", "10.1007/978-3-319-24523-210", "10.1126/scitranslmed.3004330", "10.1038/nmeth.4391", "10.1111/cgf.14002", "10.1559/152304003100010929", "10.1126/science.280.5363.585", "10.1007/978-3-319-24523-2\\_10", "10.1109/tvcg.2016.2598587", "10.1038/s41597-019-0258-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/vast47406.2019.8986940", "title": "FDive: Learning Relevance Models Using Pattern-based Similarity Measures", "year": "2019", "conferenceName": "VAST", "authors": "Frederik L. Dennig;Tom Polk;Zudi Lin;Tobias Schreck;Hanspeter Pfister;Michael Behrisch", "citationCount": "0", "affiliation": "Dennig, FL (Corresponding Author), Univ Konstanz, Constance, Germany. Dennig, Frederik L.; Polk, Tom, Univ Konstanz, Constance, Germany. Lin, Zudi; Pfister, Hanspeter; Behrisch, Michael, Harvard Univ, Cambridge, MA 02138 USA. Schreck, Tobias, Graz Univ Technol, Graz, Austria.", "countries": "Germany;USA;Austria", "abstract": "The detection of interesting patterns in large high-dimensional datasets is difficult because of their dimensionality and pattern complexity. Therefore, analysts require automated support for the extraction of relevant patterns. In this paper, we present FDive, a visual active learning system that helps to create visually explorable relevance models, assisted by learning a pattern-based similarity. We use a small set of user-provided labels to rank similarity measures, consisting of feature descriptor and distance function combinations, by their ability to distinguish relevant from irrelevant data. Based on the best-ranked similarity measure, the system calculates an interactive Self-Organizing Map-based relevance model, which classifies data according to the cluster affiliation. It also automatically prompts further relevance feedback to improve its accuracy. Uncertain areas, especially near the decision boundaries, are highlighted and can be refined by the user. We evaluate our approach by comparison to state-of-the-art feature selection techniques and demonstrate the usefulness of our approach by a case study classifying electron microscopy images of brain cells. The results show that FDive enhances both the quality and understanding of relevance models and can thus lead to new insights for brain research.", "keywords": "Visual analytics,similarity measure selection,relevance feedback,active learning,self-organizing maps", "link": "http://dx.doi.org/10.1109/VAST47406.2019.8986940", "refList": ["10.1016/j.ijhcs.2006.07.005", "10.1109/tsmc.1978.4309999", "10.1109/igarss.2016.7729190", "10.1145/3301275.3302280", "10.1007/bf00337288", "10.1162/153244303322753616", "10.1186/1471-2105-13-s8-s4", "10.1109/vast.2014.7042480", "10.1006/jvci.1999.0413", "10.1109/tpami.2009.154", "10.1109/wiamis.2008.24", "10.1109/tvcg.2017.2744805", "10.1111/j.1467-8659.2011.01938.x", "10.1109/tvcg.2013.157", "10.1080/01969727308546046", "10.1109/tsmc.1973.4309314", "10.1016/j.jchromb.2012.05.020", "10.1109/tvcg.2014.2346481", "10.1007/978-3-319-14364-4\\_70", "10.2307/2287820", "10.1109/infvis.2005.1532142", "10.1109/tip.2002.801585", "10.1007/978-1-4614-7138-7\\_1", "10.1145/1390156.1390302", "10.1109/tvcg.2012.277", "10.3390/jimaging4080097", "10.1109/cbmi.2015.7153629", "10.1145/2836034.2836035", "10.1145/2362456.2362485", "10.1016/j.visinf.2019.03.002", "10.1109/tpami.1979.4766909", "10.1145/1459359.1459577", "10.1109/tkde.2009.191", "10.1109/icip.2001.959135", "10.1109/tvcg.2016.2598467", "10.1007/978-3-540-87481-2\\_21", "10.1109/cvpr.1997.609412", "10.2307/2685881", "10.1109/cvpr.2016.90", "10.1145/347090.347124", "10.1016/b978-044450270-4/50003-6", "10.1109/tvcg.2017.2744818", "10.1145/989863.989880", "10.1145/3185517", "10.1145/1282280.1282340", "10.1109/vast.2012.6400486", "10.1109/tpami.2006.68", "10.1109/vast.2011.6102453"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13681", "year": "2019", "title": "A User-based Visual Analytics Workflow for Exploratory Model Analysis", "conferenceName": "EuroVis", "authors": "Dylan Cashman;Shah Rukh Humayoun;Florian Heimerl;Kendall Park;Subhajit Das;John Thompson;Bahador Saket;Abigail Mosca;John T. Stasko;Alex Endert;Michael Gleicher;Remco Chang", "citationCount": "6", "affiliation": "Cashman, D; Humayoun, SR (Corresponding Author), Tufts Univ, Medford, MA 02155 USA.\nCashman, Dylan; Humayoun, Shah Rukh; Mosca, Abigail; Chang, Remco, Tufts Univ, Medford, MA 02155 USA.\nHeimerl, Florian; Park, Kendall; Gleicher, Michael, Georgia Tech, Atlanta, GA USA.\nDas, Subhajit; Thompson, John; Saket, Bahador; Stasko, John; Endert, Alex, Univ Wisconsin, Madison, WI USA.", "countries": "USA", "abstract": "Many visual analytics systems allow users to interact with machine learning models towards the goals of data exploration and insight generation on a given dataset. However, in some situations, insights may be less important than the production of an accurate predictive model for future use. In that case, users are more interested in generating of diverse and robust predictive models, verifying their performance on holdout data, and selecting the most suitable model for their usage scenario. In this paper, we consider the concept of Exploratory Model Analysis (EMA), which is defined as the process of discovering and selecting relevant models that can be used to make predictions on a data source. We delineate the differences between EMA and the well-known term exploratory data analysis in terms of the desired outcome of the analytic process: insights into the data or a set of deployable models. The contributions of this work are a visual analytics system workflow for EMA, a user study, and two use cases validating the effectiveness of the workflow. We found that our system workflow enabled users to generate complex models, to assess them for various qualities, and to select the most relevant model for their task.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13681", "refList": ["10.1109/tvcg.2017.2744683", "10.1111/cgf.12639", "10.1007/s11390-016-1663-1", "10.1109/tvcg.2017.2744938", "10.1117/12.2007316", "10.1109/tvcg.2017.2745178", "10.1145/2702123.2702509", "10.1080/02701367.1992.10608764", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2011.185", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2013.125", "10.1109/mcse.2007.55", "10.1007/978-3-540-70956-5", "10.1109/vl.1996.545307", "10.1901/jeab.1979.31-433", "10.1111/j.1467-8659.2009.01475.x", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2012.65", "10.1145/1835804.1835827", "10.1109/tvcg.2017.2745085", "10.1145/2939672.2939778", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2017.2744805", "10.1145/2487575.2487629", "10.1109/tvcg.2013.157", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2011.209", "10.1109/vast.2012.6400490", "10.1109/tvcg.2015.2513410", "10.1007/978-3-540-79347-2\\_3", "10.1109/tvcg.2014.2346431", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2012.277", "10.1109/infvis.1998.729560", "10.1109/infvis.2004.64", "10.1109/mcg.2006.70", "10.1109/tvcg.2012.260", "10.1109/tvcg.2014.2346660", "10.1145/1743546.1743567", "10.1111/cgf.13417", "10.1109/tvcg.2014.2346325", "10.1109/tvcg.2018.2864838", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2003.1207445", "10.1109/vast.2010.5652443", "10.1145/2641190.2641198", "10.1109/tvcg.2016.2598830", "10.1109/tvcg.2017.2744378", "10.1111/cgf.13324", "10.1109/mcg.2009.22", "10.1109/tvcg.2016.2599030", "10.1109/vast.2012.6400486", "10.1109/tvcg.2016.2598831", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934261", "title": "Ablate, Variate, and Contemplate: Visual Analytics for Discovering Neural Architectures", "year": "2019", "conferenceName": "VAST", "authors": "Dylan Cashman;Adam Perer;Remco Chang;Hendrik Strobelt", "citationCount": "1", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Perer, Adam, Carnegie Mellon Univ, Pittsburgh, PA 15213 USA. Strobelt, Hendrik, MIT IBM Watson AI Lab, Cambridge, MA USA.", "countries": "USA", "abstract": "The performance of deep learning models is dependent on the precise configuration of many layers and parameters. However, there are currently few systematic guidelines for how to configure a successful model. This means model builders often have to experiment with different configurations by manually programming different architectures (which is tedious and time consuming) or rely on purely automated approaches to generate and train the architectures (which is expensive). In this paper, we present Rapid Exploration of Model Architectures and Parameters, or REMAP, a visual analytics tool that allows a model builder to discover a deep learning model quickly via exploration and rapid experimentation of neural network architectures. In REMAP, the user explores the large and complex parameter space for neural network architectures using a combination of global inspection and local experimentation. Through a visual overview of a set of models, the user identifies interesting clusters of architectures. Based on their findings, the user can run ablation and variation experiments to identify the effects of adding, removing, or replacing layers in a given architecture and generate new models accordingly. They can also handcraft new models using a simple graphical interface. As a result, a model builder can build deep learning models quickly, efficiently, and without manual programming. We inform the design of REMAP through a design study with four deep learning model builders. Through a use case, we demonstrate that REMAP allows users to discover performant neural network architectures efficiently using visual exploration and user-defined semi-automated searches through the model space.", "keywords": "visual analytics,neural networks,parameter space exploration", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934261", "refList": ["10.1109/mcg.2018.2878902", "10.1111/cgf.12639", "10.1109/tvcg.2017.2744938", "10.1117/12.2007316", "10.1109/cvpr.2014.81", "10.1016/j.csda.2008.02.031", "10.1080/00994480.2000.10748487", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2013.125", "10.1109/cvpr.2015.7298594", "10.1111/j.1467-8659.2009.01475.x", "10.1109/tvcg.2012.65", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2017.2744805", "10.1145/2487575.2487629", "10.1109/tvcg.2018.2865044", "10.23915/distill.00010", "10.1109/72.279181", "10.1109/tvcg.2017.2744199", "10.1007/s13398-014-0173-7.2", "10.1109/tvcg.2018.2864504", "10.1109/vast.2012.6400490", "10.1007/978-3-319-10590-1\\_53", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2014.2346321", "10.1094/pdis-11-11-0999-pdn", "10.1109/ijcnn.2015.7280767", "10.1109/tvcg.2017.2744878", "10.5555/3326943.3327130", "10.1109/tvcg.2017.2744718", "10.1109/iccv.2015.169", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2014.223", "10.1109/cvpr.2016.90", "10.1109/vast.2010.5652443", "10.1111/cgf.13681", "10.1109/tvcg.2016.2598831", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028888", "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes", "year": "2020", "conferenceName": "VAST", "authors": "Yuxin Ma;Arlen Fan;Jingrui He;Arun Reddy Nelakurthi;Ross Maciejewski", "citationCount": "0", "affiliation": "Ma, YX (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Ma, Yuxin; Fan, Arlen; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. He, Jingrui, Univ Illinois, Champaign, IL USA. Nelakurthi, Arun Reddy, Samsung Res Amer, Mountain View, CA USA.", "countries": "USA", "abstract": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.", "keywords": "Transfer learning,deep learning,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028888", "refList": ["10.1109/tvcg.2014.2346578", "10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1109/tpami.2018.2868685", "10.1145/2702123.2702509", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2016.2598828", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2013.65", "10.1145/2976749.2978318", "10.1007/978-3-030-01424-7\\_27", "10.1109/tvcg.2019.2934261", "10.1007/s11704-016-6028-y", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/iccv.2015.279", "10.1109/mci.2018.2840738", "10.1109/tvcg.2019.2892483", "10.1109/vast.2018.8802509", "10.1109/tvcg.2013.124", "10.1186/s40537-016-0043-6", "10.1109/tvcg.2018.2864475", "10.1145/3200489", "10.1109/tvcg.2018.2865044", "10.1111/cgf.13210", "10.1109/tvcg.2018.2816223", "10.23915/distill.00007", "10.1109/tvcg.2017.2744199", "10.1109/tkde.2018.2876857", "10.1109/tvcg.2019.2934631", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2011.188", "10.1007/978-3-642-15561-1\\_16", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1109/tvcg.2017.2744718", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2016.2598541", "10.1109/tkde.2009.191", "10.1145/3065386", "10.1016/j.ins.2016.03.021", "10.1109/tvcg.2019.2903943", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/iccv.2017.74", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2012.207", "10.1111/cgf.13092", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2017.2754480", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 9}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13970", "year": "2020", "title": "QUESTO: Interactive Construction of Objective Functions for Classification Tasks", "conferenceName": "EuroVis", "authors": "Subhajit Das;Shenyu Xu;Michael Gleicher;Remco Chang;Alex Endert", "citationCount": "0", "affiliation": "Das, S (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA.\nDas, Subhajit; Xu, Shenyu; Endert, Alex, Georgia Inst Technol, Atlanta, GA 30332 USA.\nGleicher, Michael, Univ Wisconsin, Madison, WI USA.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.", "countries": "USA", "abstract": "Building effective classifiers requires providing the modeling algorithms with information about the training data and modeling goals in order to create a model that makes proper tradeoffs. Machine learning algorithms allow for flexible specification of such meta-information through the design of the objective functions that they solve. However, such objective functions are hard for users to specify as they are a specific mathematical formulation of their intents. In this paper, we present an approach that allows users to generate objective functions for classification problems through an interactive visual interface. Our approach adopts a semantic interaction design in that user interactions over data elements in the visualization are translated into objective function terms. The generated objective functions are solved by a machine learning solver that provides candidate models, which can be inspected by the user, and used to suggest refinements to the specifications. We demonstrate a visual analytics system QUESTO for users to manipulate objective functions to define domain-specific constraints. Through a user study we show that QUESTO helps users create various objective functions that satisfy their goals.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13970", "refList": ["10.1016/j.neucom.2017.01.105", "10.1371/journal.pone.0050474", "10.1109/tvcg.2016.2598839", "10.1007/978-3-642-21530-8\\_14", "10.1109/tvcg.2016.2598828", "10.5555/2969442.2969547", "10.1007/s00371-015-1132-9", "10.1145/2851581.2856492", "10.1126/scirobotics.aao6760", "10.1109/vast.2011.6102453", "10.1109/vast.2011.6102449", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2016.2598446", "10.1145/359784.360332", "10.1111/j.1467-8659.2009.01475.x", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2016.2598460", "10.1109/tevc.2015.2472283", "10.1145/2207676.2207741", "10.1109/tvcg.2017.2745085", "10.1145/2939672.2939778", "10.1145/1866029.1866038", "10.1145/2487575.2487629", "10.1145/3077257.3077259", "10.2312/eurova.20171123", "10.1145/2983924", "10.1109/mcg.2013.53", "10.1109/tvcg.2015.2467615", "10.1109/vast.2014.7042492", "10.1145/2675133.2675214", "10.1109/tvcg.2014.2346482", "10.1145/3180308.3180362", "10.1109/tvcg.2014.2346321", "10.24963/ijcai.2017/202", "10.1109/tvcg.2014.2346291", "10.1109/tbme.2012.2212278", "10.1007/s40708-016-0042-6", "10.1609/aimag.v35i4.2513", "10.1016/j.ijhcs.2009.03.004", "10.1109/tevc.2012.2225064", "10.1016/s0890-6955(02)00074-3", "10.1109/icmlde.2018.00014", "10.1111/cgf.13681", "10.1109/tvcg.2013.173", "10.1145/3025171.3025208", "10.1145/3025453.3026044", "10.1109/cec.2017.7969334", "10.1109/vast.2012.6400486", "10.1109/tvcg.2016.2598831", "10.1109/tcyb.2014.2310651"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13972", "year": "2020", "title": "Boxer: Interactive Comparison of Classifier Results", "conferenceName": "EuroVis", "authors": "Michael Gleicher;Aditya Barve;Xinyi Yu;Florian Heimerl", "citationCount": "0", "affiliation": "Gleicher, M (Corresponding Author), Univ Wisconsin, Madison, WI 53706 USA.\nGleicher, Michael; Barve, Aditya; Yu, Xinyi; Heimerl, Florian, Univ Wisconsin, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Machine learning practitioners often compare the results of different classifiers to help select, diagnose and tune models. We present Boxer, a system to enable such comparison. Our system facilitates interactive exploration of the experimental results obtained by applying multiple classifiers to a common set of model inputs. The approach focuses on allowing the user to identify interesting subsets of training and testing instances and comparing performance of the classifiers on these subsets. The system couples standard visual designs with set algebra interactions and comparative elements. This allows the user to compose and coordinate views to specify subsets and assess classifier performance on them. The flexibility of these compositions allow the user to address a wide range of scenarios in developing and assessing classifiers. We demonstrate Boxer in use cases including model selection, tuning, fairness assessment, and data quality diagnosis.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13972", "refList": ["10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2016.2598838", "10.1007/s10618-014-0368-8", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1145/3287560.3287589", "10.1109/vast.2017.8585721", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2009.128", "10.1109/tvcg.2017.2744018", "10.1080/00994480.2000.10748487", "10.5555/3305890.3306024", "10.1109/iccv.2015.329", "10.1109/tvcg.2013.125", "10.1089/big.2016.0007", "10.1109/memsys.2019.8870817", "10.1145/2939672.2939778", "10.1007/s11104-019-04156-0", "10.1371/journal.pone.0181142", "10.1145/3301275.3302324", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2018.2865044", "10.1023/a:1010933404324", "10.1145/2487575.2487579", "10.1109/tvcg.2013.157", "10.1145/2783258.2788613", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934631", "10.1016/s0304-3800(02)00064-9", "10.1007/s10115-013-0679-x", "10.1109/tvcg.2019.2934267", "10.1007/978-3-319-10590-1\\_53", "10.1109/vast.2017.8585720", "10.1016/0004-3702(80)90021-1", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2009.84", "10.1007/s11263-016-0911-8", "10.1111/cgf.12918", "10.1111/cgf.12373", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1080/00207176808905715", "10.1002/er.3827", "10.1109/tvcg.2014.2346660", "10.1111/cgf.13417", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2017.2744158", "10.1016/b978-0-12-815849-4.00004-9", "10.1097/ede.0b013e3181c30fb2", "10.1111/cgf.13681", "10.1016/j.ejor.2006.04.051", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2016.2598468", "10.9735/2229-3981", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 21}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 171}, {"doi": "10.1109/vast.2017.8585498", "title": "The Role of Explicit Knowledge: A Conceptual Model of Knowledge-Assisted Visual Analytics", "year": "2017", "conferenceName": "VAST", "authors": "Paolo Federico;Markus Wagner 0008;Alexander Rind;Albert Amor-Amoros;Silvia Miksch;Wolfgang Aigner", "citationCount": "3", "affiliation": "Federico, P (Corresponding Author), TU Wien, Vienna, Austria. Federico, Paolo; Wagner, Markus; Rind, Alexander; Amor-Amoros, Albert; Miksch, Silvia; Aigner, Wolfgang, TU Wien, Vienna, Austria. Wagner, Markus; Rind, Alexander; Aigner, Wolfgang, St Poelten Univ Appl Sci, Sankt Polten, Austria.", "countries": "Austria", "abstract": "Visual Analytics (VA) aims to combine the strengths of humans and computers for effective data analysis. In this endeavor, humans' tacit knowledge from prior experience is an important asset that can be leveraged by both human and computer to improve the analytic process. While VA environments are starting to include features to formalize, store, and utilize such knowledge, the mechanisms and degree in which these environments integrate explicit knowledge varies widely. Additionally, this important class of VA environments has never been elaborated on by existing work on VA theory. This paper proposes a conceptual model of Knowledge-assisted VA conceptually grounded on the visualization model by van Wijk. We apply the model to describe various examples of knowledge-assisted VA from the literature and elaborate on three of them in finer detail. Moreover, we illustrate the utilization of the model to compare different design alternatives and to evaluate existing approaches with respect to their use of knowledge. Finally, the model can inspire designers to generate novel VA environments using explicit knowledge effectively.", "keywords": "Automated analysis,tacit knowledge,explicit knowledge,visual analytics,information visualization,theory and model", "link": "http://dx.doi.org/10.1109/VAST.2017.8585498", "refList": ["10.1016/j.artmed.2006.03.001", "10.1057/ivs.2009.22", "10.1109/infvis.2000.885092", "10.2312/pe/eurovast/eurova11/009-012", "10.1109/tvcg.2016.2598839", "10.1109/tvcg.2014.2346575", "10.1111/cgf.13169", "10.1186/1471-2105-13-s8-s3", "10.1007/s00371-015-1132-9", "10.1109/tvcg.2013.146", "10.1109/tvcg.2016.2598471", "10.1016/j.cag.2009.06.004", "10.1145/2993901.2993915", "10.1111/cgf.12090", "10.1177/1473871611412817", "10.1145/2598153.2598172", "10.1016/j.cag.2009.06.006", "10.1109/tvcg.2016.2598460", "10.1016/j.autcon.2014.03.012", "10.1145/989863.989865", "10.1109/21.44068", "10.1057/palgrave.ivs.9500045", "10.2312/eurova.20151108", "10.1002/9781444303179.ch3", "10.1109/hicss.2016.183", "10.1177/0165551506070706", "10.1145/2494188.2494202", "10.1145/1556262.1556327", "10.1016/j.artmed.2006.04.002", "10.1007/978-3-540-71080-6\\_6", "10.1111/j.1467-8659.2008.01230.x", "10.1109/pacificvis.2011.5742371", "10.1109/vast.2014.7042530", "10.1109/mcg.2010.8", "10.1109/mcg.2015.25", "10.1109/vast.2012.6400555", "10.1109/tvcg.2014.2346481", "10.1145/2836034.2836040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2008.109", "10.1016/j.artmed.2005.10.003", "10.1109/mcg.2014.73", "10.1016/s0004-3702(96)00025-2", "10.1093/intqhc/mzm007", "10.1016/j.dss.2012.06.009", "10.1016/j.cose.2017.02.003", "10.1109/infvis.1998.729560", "10.1109/vast.2010.5654451", "10.1109/tvcg.2016.2598829", "10.1109/vast.2007.4389021", "10.1145/1562849.1562851", "10.1145/302979.303030", "10.1145/985692.985706", "10.1109/infvis.1997.636792", "10.1111/j.1467-8659.2012.03092.x", "10.1109/mcg.2009.6", "10.1057/ivs.2008.28", "10.1109/mcg.2005.91", "10.1016/j.artmed.2010.02.001", "10.1177/0272989x14565822", "10.1109/mcg.2010.15", "10.1007/s10844-014-0304-9", "10.2312/pe.eurovast.eurova13.043-047", "10.1109/mcg.2014.33", "10.1109/tvcg.2014.2346574", "10.1111/j.1467-8659.2009.01708.x", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934654", "title": "Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections", "year": "2019", "conferenceName": "VAST", "authors": "Mennatallah El-Assady;Rebecca Kehlbeck;Christopher Collins;Daniel A. Keim;Oliver Deussen", "citationCount": "3", "affiliation": "El-Assady, M (Corresponding Author), Univ Konstanz, Constance, Germany. El-Assady, M (Corresponding Author), Ontario Tech Univ, Oshawa, ON, Canada. El-Assady, Mennatallah; Kehlbeck, Rebecca; Keim, Daniel; Deussen, Oliver, Univ Konstanz, Constance, Germany. El-Assady, Mennatallah; Collins, Christopher, Ontario Tech Univ, Oshawa, ON, Canada.", "countries": "Canada;Germany", "abstract": "We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decision-making process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.", "keywords": "Topic Model Optimization,Word Embedding,Mixed-Initiative Refinement,Guided Visual Analytics,Semantic Mapping", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934654", "refList": ["10.1109/vast.2014.7042493", "10.1145/2133806.2133826", "10.1016/j.visinf.2018.09.003", "10.1007/978-3-319-67008-9\\_26", "10.1109/tvcg.2013.126", "10.1162/tacl\\_a\\_00140", "10.1007/s13202-018-0509-5", "10.1109/bdva.2018.8534018", "10.1108/eb026526", "10.1007/s10994-013-5413-0", "10.1145/564376.564421", "10.1016/j.visinf.2017.01.006", "10.3115/v1/p14-2050", "10.1007/bf00288933", "10.1109/tvcg.2013.212", "10.1145/2207676.2207741", "10.1109/tvcg.2013.162", "10.1109/tvcg.2016.2515592", "10.1109/tvcg.2017.2745080", "10.1109/mcg.2013.53", "10.1109/tvcg.2017.2744199", "10.1145/3091108", "10.18653/v1/p17-4009", "10.1162/jmlr.2003.3.4-5.951", "10.1109/vast.2017.8585498", "10.1109/tvcg.2017.2723397", "10.1109/tvcg.2018.2864769", "10.1007/s10618-005-0361-3", "10.3115/v1/d14-1167", "10.1007/bf01840357", "10.1162/jmlr.2003.3.4-5.993", "10.1145/2678025.2701370", "10.1016/j.ins.2016.06.040", "10.1109/tvcg.2017.2746018", "10.1109/vast.2011.6102461", "10.1111/cgf.13092", "10.3115/1117729.1117730", "10.1109/mcg.2015.91", "10.1145/2669557.2669572"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13959", "year": "2020", "title": "Knowledge-Assisted Comparative Assessment of Breast Cancer using Dynamic Contrast-Enhanced Magnetic Resonance Imaging", "conferenceName": "EuroVis", "authors": "Kai Nie;Pascal A. Baltzer;Bernhard Preim;Gabriel Mistelbauer", "citationCount": "0", "affiliation": "Nie, K (Corresponding Author), Otto von Guericke Univ, Dept Simulat \\& Graph, Magdeburg, Germany.\nNie, K.; Preim, B.; Mistelbauer, G., Otto von Guericke Univ, Dept Simulat \\& Graph, Magdeburg, Germany.\nBaltzer, P., Med Univ Vienna, Dept Biomed Imaging \\& Image Guided Therapy, Vienna, Austria.", "countries": "Germany;Austria", "abstract": "Breast perfusion data are dynamic medical image data that depict perfusion characteristics of the investigated tissue. These data consist of a series of static datasets that are acquired at different time points and aggregated into time intensity curves (TICs) for each voxel. The characteristics of these TICs provide important information about a lesion's composition, but their analysis is time-consuming due to their large number. Subsequently, these TICs are used to classify a lesion as benign or malignant. This lesion scoring is commonly done manually by physicians and may therefore be subject to bias. We propose an approach that addresses both of these problems by combining an automated lesion classification with a visual confirmatory analysis, especially for uncertain cases. Firstly, we cluster the TICs of a lesion using ordering points to identify the clustering structure (OPTICS) and then visualize these clusters. Together with their relative size, they are added to a library. We then model fuzzy inference rules by using the lesion's TIC clusters as antecedents and its score as consequent. Using a fuzzy scoring system, we can suggest a score for a new lesion. Secondly, to allow physicians to confirm the suggestion in uncertain cases, we display the TIC clusters together with their spatial distribution and allow them to compare two lesions side by side. With our knowledge-assisted comparative visual analysis, physicians can explore and classify breast lesions. The true positive prediction accuracy of our scoring system achieved 71.4\\% in one-fold cross-validation using 14 lesions.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13959", "refList": ["10.1016/j.patcog.2017.08.004", "10.1109/icinfa.2017.8078962", "10.1109/iccv.2013.222", "10.1002/jmri.1123", "10.1016/j.ejrad.2017.01.020", "10.1007/s00330-016-4612-z", "10.1002/widm.30", "10.1002/jmri.26721", "10.1109/tvcg.2008.95", "10.1007/s00330-015-4075-7", "10.1007/s00330-007-0762-3", "10.1148/radiology.213.3.r99dc01881", "10.1007/s10278-010-9298-1", "10.1016/j.datak.2006.01.013", "10.1016/j.cag.2010.05.016", "10.5121/mlaij.2016.3103", "10.1089/10665270360688057", "10.1117/1.jmi.5.1.014502", "10.1002/mp.12408", "10.1198/jcgs.2011.09224", "10.1109/cbms.2013.6627768", "10.1109/vast.2017.8585498", "10.1118/1.4937787", "10.1016/j.acra.2009.03.017", "10.1145/2503210.2503255", "10.1007/978-3-319-68548-9\\_44", "10.1109/tvcg.2007.70569", "10.1016/j.compmedimag.2007.02.007", "10.1109/tmi.2013.2281984", "10.1148/radiol.2442051620", "10.1016/j.jacr.2009.07.023", "10.1002/j.1538-7305.1957.tb01515.x", "10.1148/radiol.14121031", "10.3322/caac.21492", "10.1148/radiology.211.1.r99ap38101", "10.1016/j.compbiomed.2014.10.006", "10.1016/j.eswa.2016.01.004", "10.3238/arztebl.2018.0316", "10.1559/152304003100010929", "10.1016/s0002-9610(01)00726-7", "10.1016/j.ejrad.2020.108819", "10.1109/tmi.2012.2191302"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/tvcg.2018.2864526", "title": "Duet: Helping Data Analysis Novices Conduct Pairwise Comparisons by Minimal Specification", "year": "2018", "conferenceName": "VAST", "authors": "Po-Ming Law;Rahul C. Basole;Yanhong Wu", "citationCount": "6", "affiliation": "Law, PM (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA. Law, Po-Ming; Basole, Rahul C., Georgia Inst Technol, Atlanta, GA 30332 USA. Wu, Yanhong, Visa Res, Palo Alto, CA USA.", "countries": "USA", "abstract": "Data analysis novices often encounter barriers in executing low-level operations for pairwise comparisons. They may also run into barriers in interpreting the artifacts (e.g., visualizations) created as a result of the operations. We developed Duet, a visual analysis system designed to help data analysis novices conduct pairwise comparisons by addressing execution and interpretation barriers. To reduce the barriers in executing low-level operations during pairwise comparison, Duet employs minimal specification: when one object group (i.e. a group of records in a data table) is specified, Duet recommends object groups that are similar to or different from the specified one; when two object groups are specified, Duet recommends similar and different attributes between them. To lower the barriers in interpreting its recommendations, Duet explains the recommended groups and attributes using both visualizations and textual descriptions. We conducted a qualitative evaluation with eight participants to understand the effectiveness of Duet. The results suggest that minimal specification is easy to use and Duet's explanations are helpful for interpreting the recommendations despite some usability issues.", "keywords": "Pairwise comparison,novices,data analysis,automatic insight generation", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864526", "refList": ["10.1145/506443.506619", "10.1145/2501988.2502040", "10.1145/1842993.1843029", "10.1057/ivs.2008.27", "10.1177/0272989x10373805", "10.1109/tvcg.2015.2467195", "10.14778/2831360.2831371", "10.1145/3025171.3025227", "10.1145/1502650.1502695", "10.1145/2678025.2701399", "10.1145/2207676.2207738", "10.1109/tvcg.2017.2744684", "10.1007/978-3-540-24670-1\\_3", "10.2307/3001913", "10.1109/tvcg.2017.2745219", "10.1109/tvcg.2017.2744199", "10.1109/cvpr.2000.854761", "10.1145/2984511.2984588", "10.1145/2807442.2807459", "10.1145/3035918.3035922", "10.1109/tvcg.2011.188", "10.1145/2807442.2807478", "10.1109/tvcg.2010.164", "10.1109/vast.2011.6102435", "10.1145/2213836", "10.1109/tpami.2003.1195991", "10.1109/tvcg.2015.2467191", "10.1145/985692.985712", "10.1109/tvcg.2005.63", "10.1109/tvcg.2016.2598468", "10.1145/3025453.3025777"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934264", "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;David S. Ebert;Arif Ghafoor", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Karimzadeh, Morteza; Ebert, David S.; Ghafoor, Arif, Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.", "keywords": "Summative evaluation,usefulness,evaluation process,taxonomy,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934264", "refList": ["10.1109/tvcg.2017.2744478", "10.1109/tvcg.2018.2865025", "10.1109/tvcg.2006.85", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2017.2745181", "10.1111/cgf.13677", "10.1109/tvcg.2018.2864844", "10.1109/tvcg.2013.126", "10.1109/tvcg.2018.2864811", "10.1109/infvis.2005.1532147", "10.1177/0956797613504966", "10.1145/2669557.2669579", "10.1109/mcg.2005.102", "10.1109/visual.2003.1250426", "10.1136/bmj.39489.470347.ad", "10.1109/tvcg.2017.2744080", "10.1109/mcg.2009.53", "10.1111/j.1467-8527.2005.00307.x", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2864886", "10.1109/tvcg.2018.2864843", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2018.2865051", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2018.2865026", "10.1007/978-3-540-71080-6\\_6", "10.1109/tvcg.2018.2865020", "10.1177/1473871611407399", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2018.2864526", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2864905", "10.1049/sej.1991.0040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2711030", "10.1109/tvcg.2011.279", "10.1109/vast.2017.8585505", "10.1147/jrd.2010.2042914", "10.1016/s0378-7206(98)00044-5", "10.1145/2993901.2993913", "10.1109/tvcg.2018.2865041", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744758", "10.1145/1168149.1168158", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2012.213", "10.1109/tvcg.2017.2744738", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2018.2864826", "10.1145/1377966.1377974", "10.1109/apec.2009.4802646", "10.1145/1168149.1168152", "10.1016/j.jss.2008.03.059", "10.1109/vast.2017.8585484", "10.1109/tvcg.2017.2744818", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2865042", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030388", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "year": "2020", "conferenceName": "SciVis", "authors": "Pepe Eulzer;Sabine Bauer;Francis Kilian;Kai Lawonn", "citationCount": "0", "affiliation": "Eulzer, P (Corresponding Author), Univ Jena, Jena, Germany. Eulzer, Pepe; Lawonn, Kai, Univ Jena, Jena, Germany. Bauer, Sabine, Univ Koblenz Landau, Koblenz, Germany. Kilian, Francis, Cath Clin Koblenz Montabaur, Dept Spine Surg, Koblenz, Germany.", "countries": "Germany", "abstract": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. $\\mathrm{By}$ linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. $\\mathrm{in}$ a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.", "keywords": "Medical visualization,bioinformatics,coordinated views,focus and context,biomechanical simulation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030388", "refList": ["10.1109/tvcg.2018.2864903", "10.1177/1473871613510429", "10.1093/ehjqcco/qcz052", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2018.2865076", "10.1055/s-0039-1687862", "10.1109/visual.1990.146375", "10.1109/tvcg.2017.2744198", "10.1016/j.ijmedinf.2014.10.001", "10.1109/tvcg.2013.124", "10.1016/j.jacc", "10.1111/cgf.13167", "10.17705/1thci.00055", "10.1136/bmjqs.2009.037895", "10.1109/tvcg.2013.238", "10.1109/tvcg.2018.2865240", "10.1186/1471-2261-6-34", "10.1109/tvcg.2019.2934264", "10.1109/tvcg.2013.200", "10.1109/tvcg.2011.209", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467091", "10.1136/bmjopen-2019-033208", "10.1109/beliv.2018.8634027", "10.1109/tvcg.2012.213", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2015.2467325", "10.1145/2133806.2133821", "10.1145/1806799.1806866", "10.1108/02635570610688869", "10.1002/hbm.20701", "10.1561/1100000039", "10.1145/3025453.3025645", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2020.3029413", "title": "A Design Space of Vision Science Methods for Visualization Research", "year": "2020", "conferenceName": "InfoVis", "authors": "Madison A. Elliott;Christine Nothelfer;Cindy Xiong;Danielle Albers Szafir", "citationCount": "0", "affiliation": "Elliott, MA (Corresponding Author), Univ British Columbia, Vancouver, BC, Canada. Elliott, Madison A., Univ British Columbia, Vancouver, BC, Canada. Nothelfer, Christine, Northwestern Univ, Evanston, IL 60208 USA. Xiong, Cindy, Univ Massachusetts, Amherst, MA 01003 USA. Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "Canada;USA", "abstract": "A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.", "keywords": "Perception,human vision,empirical research,evaluation,HCI", "link": "http://dx.doi.org/10.1109/TVCG.2020.3029413", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1177/0886109909354981", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1093/bioinformatics/btq110", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1177/1744987107081254", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1177/174498710501000305", "10.1017/s1049096513001789", "10.1109/tvcg.2012.213", "10.1093/nar/gkz239", "10.1093/sysbio/sys062", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030405", "title": "Insights From Experiments With Rigor in an EvoBio Design Study", "year": "2020", "conferenceName": "InfoVis", "authors": "Jennifer Rogers;Austin H. Patton;Luke Harmon;Alexander Lex;Miriah D. Meyer", "citationCount": "0", "affiliation": "Rogers, J (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Rogers, Jen; Lex, Alexander; Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Patton, Austin H., Washington State Univ, Pullman, WA 99164 USA. Harmon, Luke, Univ Idaho, Moscow, ID 83843 USA.", "countries": "USA", "abstract": "Design study is an established approach of conducting problem-driven visualization research. The academic visualization community has produced a large body of work for reporting on design studies, informed by a handful of theoretical frameworks, and applied to a broad range of application areas. The result is an abundance of reported insights into visualization design, with an emphasis on novel visualization techniques and systems as the primary contribution of these studies. In recent work we proposed a new, interpretivist perspective on design study and six companion criteria for rigor that highlight the opportunities for researchers to contribute knowledge that extends beyond visualization idioms and software. In this work we conducted a year-long collaboration with evolutionary biologists to develop an interactive tool for visual exploration of multivariate datasets and phylogenetic trees. During this design study we experimented with methods to support three of the rigor criteria: ABUNDANT, REFLEXIVE, and TRANSPARENT. As a result we contribute two novel visualization techniques for the analysis of multivariate phylogenetic datasets, three methodological recommendations for conducting design studies drawn from reflections over our process of experimentation, and two writing devices for reporting interpretivist design study. We offer this work as an example for implementing the rigor criteria to produce a diverse range of knowledge contributions.", "keywords": "Methodologies,Application Motivated Visualization,Guidelines,Life Sciences Visualization,Health,Medicine,Biology,Bioinformatics,Genomics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030405", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1080/17493460802276893", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1109/tvcg.2012.213", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.1028", "year": "2020", "title": "A Radial Visualisation for Model Comparison and Feature Identification", "conferenceName": "PacificVis", "authors": "Jianlong Zhou;Weidong Huang;Fang Chen", "citationCount": "0", "affiliation": "Zhou, JL (Corresponding Author), Univ Technol Sydney, Data Sci Inst, Sydney, NSW, Australia.\nZhou, Jianlong; Chen, Fang, Univ Technol Sydney, Data Sci Inst, Sydney, NSW, Australia.\nHuang, Weidong, Univ Technol Sydney, Fac Transdisciplinary Innovat, Sydney, NSW, Australia.", "countries": "Australia", "abstract": "Machine Learning (ML) plays a key role in various intelligent systems, and building an effective ML model for a data set is a difficult task involving various steps including data cleaning, feature definition and extraction, ML algorithms development, model training and evaluation as well as others. One of the most important steps in the process is to compare generated substantial amounts of ML models to find the optimal one for the deployment. It is challenging to compare such models with dynamic number of features. This paper proposes a novel visualisation approach based on a radial net to compare ML models trained with a different number of features of a given data set while revealing implicit dependent relations. In the proposed approach, ML models and features are represented by lines and arcs respectively. The dependence of ML models with dynamic number of features is encoded into the structure of visualisation, where ML models and their dependent features are directly revealed from related line connections. ML model performance information is encoded with colour and line width in the innovative visualisation. Together with the structure of visualization, feature importance can be directly discerned to help to understand ML models.", "keywords": "Machine learning; performance; comparison; visualisation", "link": "https://doi.org/10.1109/PacificVis48177.2020.1028", "refList": ["10.1109/tvcg.2012.215", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2018.2864526", "10.1016/j.neucom.2011.10.038", "10.1063/1.3122936", "10.1145/2687924", "10.1109/tvcg.2009.23", "10.1016/j.compmedimag.2007.10.007", "10.1007/978-3-319-90403-0", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2018.2864499", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 13}, {"doi": "10.1109/tvcg.2019.2934547", "title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data", "year": "2019", "conferenceName": "VAST", "authors": "Robert Kr\u00fcger;Johanna Beyer;Won-Dong Jang;Nam Wook Kim;Artem Sokolov;Peter K. Sorger;Hanspeter Pfister", "citationCount": "1", "affiliation": "Krueger, R (Corresponding Author), Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Krueger, R (Corresponding Author), Harvard Med Sch, Lab Syst Pharmacol, Boston, MA 02115 USA. Krueger, Robert; Beyer, Johanna; Jang, Won-Dong; Kim, Nam Wook; Pfister, Hanspeter, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Krueger, Robert; Sokolov, Artem; Sorger, Peter K., Harvard Med Sch, Lab Syst Pharmacol, Boston, MA 02115 USA.", "countries": "USA", "abstract": "Facetto is a scalable visual analytics application that is used to discover single-cell phenotypes in high-dimensional multi-channel microscopy images of human tumors and tissues. Such images represent the cutting edge of digital histology and promise to revolutionize how diseases such as cancer are studied, diagnosed, and treated. Highly multiplexed tissue images are complex, comprising 109 or more pixels, 60-plus channels, and millions of individual cells. This makes manual analysis challenging and error-prone. Existing automated approaches are also inadequate, in large part, because they are unable to effectively exploit the deep knowledge of human tissue biology available to anatomic pathologists. To overcome these challenges, Facetto enables a semi-automated analysis of cell types and states. It integrates unsupervised and supervised learning into the image and feature exploration process and offers tools for analytical provenance. Experts can cluster the data to discover new types of cancer and immune cells and use clustering results to train a convolutional neural network that classifies new cells accordingly. Likewise, the output of classifiers can be clustered to discover aggregate patterns and phenotype subsets. We also introduce a new hierarchical approach to keep track of analysis steps and data subsets created by users; this assists in the identification of cell types. Users can build phenotype trees and interact with the resulting hierarchical structures of both high-dimensional feature and image spaces. We report on use-cases in which domain scientists explore various large-scale fluorescence imaging datasets. We demonstrate how Facetto assists users in steering the clustering and classification process, inspecting analysis results, and gaining new scientific insights into cancer biology.", "keywords": "Clustering,Classification,Visual Analysis,Multiplex Tissue Imaging,Digital Pathology,Cancer Systems Biology", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934547", "refList": ["10.1145/1656274.1656280", "10.1111/cgf.12893", "10.1145/1142473.1142574", "10.1109/vahc.2017.8387544", "10.1111/j.2517-6161.1977.tb01600.x", "10.7554/elife.31657", "10.1109/tvcg.2013.125", "10.1186/1471-2105-16-s11-s5", "10.1109/isbi.2011.5872394", "10.1145/1268517.1268563", "10.1109/tvcg.2013.186", "10.1016/j.patrec.2009.09.011", "10.1001/jama.2016.17216", "10.1145/1656274.1656278", "10.1109/vahc.2017.8387545", "10.1145/1390156.1390183", "10.1109/tvcg.2017.2744805", "10.1109/hicss.2011.339", "10.1111/cgf.12613", "10.1007/bf01898350", "10.1109/vast.2014.7042495", "10.1109/tvcg.2015.2467551", "10.1016/j.cels.2016.03.008", "10.1038/nmeth.1896", "10.1109/tvcg.2012.277", "10.1016/s0893-6080(97)00002-6", "10.1057/palgrave.ivs.9500170", "10.1002/(sici)1097-4571(199307)44:6", "10.1109/tvcg.2007.70569", "10.1145/1830483.1830503", "10.1186/gb-2006-7-10-r100", "10.1109/tvcg.2012.213", "10.1109/tvcg.2012.258", "10.1109/tvcg.2017.2744158", "10.1109/sibgra.2002.1167123", "10.1007/978-3-319-24574-4\\_28", "10.1038/nmeth.4391", "10.1109/vast.2010.5652443", "10.1002/cpch.14", "10.1093/bioinformatics/bts288", "10.1109/tvcg.2016.2598468", "10.1109/tvcg.2016.2598587", "10.1080/00207179208934272"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030363", "title": "Improving the Usability of Virtual Reality Neuron Tracing with Topological Elements", "year": "2020", "conferenceName": "SciVis", "authors": "Torin McDonald;William Usher;Nathan Morrical;Attila Gyulassy;Steve Petruzza;Frederick Federer;Alessandra Angelucci;Valerio Pascucci", "citationCount": "0", "affiliation": "McDonald, T (Corresponding Author), Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA. McDonald, Torin; Usher, Will; Morrical, Nate; Gyulassy, Attila; Petruzza, Steve; Pascucci, Valerio, Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA. Petruzza, Steve, Utah State Univ, Logan, UT 84322 USA. Federer, Frederick; Angelucci, Alessandra, Univ Utah, Moran Ese Inst, Salt Lake City, UT 84112 USA.", "countries": "USA", "abstract": "Researchers in the field of connectomics are working to reconstruct a map of neural connections in the brain in order to understand at a fundamental level how the brain processes information. Constructing this wiring diagram is done by tracing neurons through high-resolution image stacks acquired with fluorescence microscopy imaging techniques. While a large number of automatic tracing algorithms have been proposed, these frequently rely on local features in the data and fail on noisy data or ambiguous cases, requiring time-consuming manual correction. As a result, manual and semi-automatic tracing methods remain the state-of-the-art for creating accurate neuron reconstructions. We propose a new semi-automatic method that uses topological features to guide users in tracing neurons and integrate this method within a virtual reality (VR) framework previously used for manual tracing. Our approach augments both visualization and interaction with topological elements, allowing rapid understanding and tracing of complex morphologies. In our pilot study, neuroscientists demonstrated a strong preference for using our tool over prior approaches, reported less fatigue during tracing, and commended the ability to better understand possible paths and alternatives. Quantitative evaluation of the traces reveals that users' tracing speed increased, while retaining similar accuracy compared to a fully manual approach.", "keywords": "Virtual Reality,Morse-Smale Complex,Semi-automatic Neuron Tracing", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030363", "refList": ["10.1038/nmeth.2869", "10.18637/jss.v028.c01", "10.1177/1473871611416549", "10.1016/j.celrep.2020.107523", "10.1602/neurorx.1.2.182", "10.1038/nprot.2014.191", "10.1136/jnnp-2011-300403", "10.1126/sciadv.aax5851", "10.2312/pe.vmv.vmv13.105-112", "10.1038/s43018-020-0031-9", "10.1109/tvcg.2013.213", "10.1093/jnci/92.8.613", "10.1109/52.329404", "10.1109/tvcg.2017.2785271", "10.1007/s11548-013-0820-z", "10.1109/tvcg.2019.2934547", "10.1002/cjp2.113", "10.1080/2162402x.2018.1507600", "10.1109/tvcg.2013.124", "10.1016/s0140-6736(14)60958-2", "10.1038/s41467-017-01689-9", "10.1016/j.cell.2018.07.010", "10.1007/978-3-319-24523-2", "10.1038/nrg3832", "10.2352/j.imagingsci.technol.2017.61.6.060404", "10.1109/tvcg.2018.2864907", "10.1111/cgf.13413", "10.1007/978-3-319-67979-2\\_4", "10.1145/2836034.2836040", "10.1038/nmeth.2563", "10.1016/0377-0427(87)90125-7", "10.1016/j.immuni.2016.04.014", "10.1002/cyto.a.22702", "10.12688/wellcomeopenres.15191.1", "10.1109/2945.981851", "10.1038/s43018-020-0026-6", "10.5281/zenodo", "10.1109/tvcg.2019.2931299", "10.1109/annes.1995.499469", "10.1145/2133806.2133821", "10.1002/hbm.20701", "10.1109/tvcg.2013.161", "10.1007/978-3-319-24523-210", "10.1126/scitranslmed.3004330", "10.1038/nmeth.4391", "10.1559/152304003100010929", "10.1126/science.280.5363.585", "10.1007/978-3-319-24523-2\\_10", "10.1109/tvcg.2016.2598587", "10.1038/s41597-019-0258-4"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030336", "title": "Visual cohort comparison for spatial single-cell omics-data", "year": "2020", "conferenceName": "VAST", "authors": "Antonios Somarakis;Marieke E. Ijsselsteijn;Sietse J. Luk;Boyd Kenkhuis;Noel F. C. C. de Miranda;Boudewijn P. F. Lelieveldt;Thomas H\u00f6llt", "citationCount": "0", "affiliation": "Hollt, T (Corresponding Author), Delft Univ Technol, Comp Graph \\& Visualizat Grp, Delft, Netherlands. Hollt, T (Corresponding Author), Leiden Univ, Med Ctr, Leiden Computat Biol Ctr, Leiden, Netherlands. Somarakis, Antonios; Lelieveldt, Boudewijn P. F., Leiden Univ, Med Ctr, Dept Radiol, Div Image Proc, Leiden, Netherlands. Ijsselsteijn, Marieke E.; de Miranda, Noel F. C. C., Leiden Univ, Med Ctr, Dept Pathol, Immunogen Grp, Leiden, Netherlands. Luk, Sietse J., Leiden Univ, Med Ctr, Hematol Dept, Leiden, Netherlands. Kenkhuis, Boyd, Leiden Univ, Med Ctr, Human Genet Dept, Leiden, Netherlands. Hollt, Thomas, Delft Univ Technol, Comp Graph \\& Visualizat Grp, Delft, Netherlands. Hollt, Thomas, Leiden Univ, Med Ctr, Leiden Computat Biol Ctr, Leiden, Netherlands.", "countries": "Netherlands", "abstract": "Spatially-resolved omics-data enable researchers to precisely distinguish cell types in tissue and explore their spatial interactions, enabling deep understanding of tissue functionality. To understand what causes or deteriorates a disease and identify related biomarkers, clinical researchers regularly perform large-scale cohort studies, requiring the comparison of such data at cellular level. In such studies, with little a-priori knowledge of what to expect in the data, explorative data analysis is a necessity. Here, we present an interactive visual analysis workflow for the comparison of cohorts of spatially-resolved omics-data. Our workflow allows the comparative analysis of two cohorts based on multiple levels-of-detail, from simple abundance of contained cell types over complex co-localization patterns to individual comparison of complete tissue images. As a result, the workflow enables the identification of cohort-differentiating features, as well as outlier samples at any stage of the workflow. During the development of the workflow, we continuously consulted with domain experts. To show the effectiveness of the workflow, we conducted multiple case studies with domain experts from different application areas and with different data modalities.", "keywords": "Visual analytics,Imaging Mass Cytometry,Vectra,spatially-resolved data,single-cell omics-data,Visual comparison", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030336", "refList": ["10.1038/nmeth.2869", "10.18637/jss.v028.c01", "10.1177/1473871611416549", "10.1016/j.celrep.2020.107523", "10.1602/neurorx.1.2.182", "10.1038/nprot.2014.191", "10.1136/jnnp-2011-300403", "10.1126/sciadv.aax5851", "10.2312/pe.vmv.vmv13.105-112", "10.1038/s43018-020-0031-9", "10.1109/tvcg.2013.213", "10.1093/jnci/92.8.613", "10.1109/52.329404", "10.1109/tvcg.2017.2785271", "10.1007/s11548-013-0820-z", "10.1109/tvcg.2019.2934547", "10.1002/cjp2.113", "10.1080/2162402x.2018.1507600", "10.1109/tvcg.2013.124", "10.1016/s0140-6736(14)60958-2", "10.1038/s41467-017-01689-9", "10.1016/j.cell.2018.07.010", "10.1007/978-3-319-24523-2", "10.1038/nrg3832", "10.2352/j.imagingsci.technol.2017.61.6.060404", "10.1109/tvcg.2018.2864907", "10.1111/cgf.13413", "10.1007/978-3-319-67979-2\\_4", "10.1145/2836034.2836040", "10.1038/nmeth.2563", "10.1101/2020.03.27.001834", "10.1016/0377-0427(87)90125-7", "10.1016/j.immuni.2016.04.014", "10.1002/cyto.a.22702", "10.12688/wellcomeopenres.15191.1", "10.1109/2945.981851", "10.1038/s43018-020-0026-6", "10.1038/s41586-019-1876-x", "10.1109/tvcg.2019.2931299", "10.1109/annes.1995.499469", "10.1145/2133806.2133821", "10.1109/tvcg.2013.161", "10.1007/978-3-319-24523-210", "10.1126/scitranslmed.3004330", "10.1038/nmeth.4391", "10.1111/cgf.14002", "10.1559/152304003100010929", "10.1126/science.280.5363.585", "10.1007/978-3-319-24523-2\\_10", "10.1109/tvcg.2016.2598587", "10.1038/s41597-019-0258-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/tvcg.2019.2934658", "title": "LightGuider: Guiding Interactive Lighting Design using Suggestions, Provenance, and Quality Visualization", "year": "2019", "conferenceName": "VAST", "authors": "Andreas Walch;Michael Schw\u00e4rzler;Christian Luksch;Elmar Eisemann;Theresia Gschwandtner", "citationCount": "0", "affiliation": "Walch, A (Corresponding Author), VRVis Forschungs GmbH, Graz, Austria. Walch, Andreas; Luksch, Christian, VRVis Forschungs GmbH, Graz, Austria. Schwarzler, Michael; Eisemann, Elmar, Delft Univ Technol, Delft, Netherlands. Gschwandtner, Theresia, TU Wien, Vienna, Austria.", "countries": "Netherlands;Austria", "abstract": "LightGuider is a novel guidance-based approach to interactive lighting design, which typically consists of interleaved 3D modeling operations and light transport simulations. Rather than having designers use a trial-and-error approach to match their illumination constraints and aesthetic goals, LightGuider supports the process by simulating potential next modeling steps that can deliver the most significant improvements. LightGuider takes predefined quality criteria and the current focus of the designer into account to visualize suggestions for lighting-design improvements via a specialized provenance tree. This provenance tree integrates snapshot visualizations of how well a design meets the given quality criteria weighted by the designer's preferences. This integration facilitates the analysis of quality improvements over the course of a modeling workflow as well as the comparison of alternative design solutions. We evaluate our approach with three lighting designers to illustrate its usefulness.", "keywords": "guidance,3D modeling,lighting design,provenance,global illumination", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934658", "refList": ["10.1109/tvcg.2015.2468011", "10.1145/1243980.1243983", "10.1145/2629573", "10.1109/tvcg.2008.59", "10.1109/tvcg.2010.223", "10.1145/1345448.1345453", "10.1109/38.920623", "10.1145/1276377.1276444", "10.1109/tvcg.2013.126", "10.1007/s00371-015-1132-9", "10.1109/tvcg.2011.185", "10.1111/cgf.12924", "10.1145/1239451.1239505", "10.1109/tvcg.2013.147", "10.1145/985692.985765", "10.1287/opre.15.3.537", "10.1016/j.cag.2013.11.002", "10.1145/2702123.2702149", "10.1111/cgf.13101", "10.1109/tvcg.2015.2467551", "10.21037/jmai.2018.07.01", "10.1145/258734.258887", "10.1145/1084805.1084812", "10.1111/cgf.13452", "10.1109/tvcg.2014.2346321", "10.1006/ijhc.2000.0420", "10.1109/pg.2007.9", "10.1109/tvcg.2018.2865024", "10.1007/s00371-016-1257-5", "10.1111/cgf.12159", "10.1109/tvcg.2016.2598468", "10.1109/tvcg.2012.175"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2019.00026", "year": "2019", "title": "Visual Quality Guidance for Document Exploration with Focus+Context Techniques", "conferenceName": "PacificVis", "authors": "Qi Han;Dennis Thom;Markus John;Steffen Koch;Thomas Ertl;Florian Heimerl", "citationCount": "1", "affiliation": "Han, Q (Corresponding Author), Univ Stuttgart, Inst Visualisat \\& Interact Syst, Stuttgart, Germany.\nHan, Qi; Thom, Dennis; John, Markus; Koch, Steffen; Ertl, Thomas, Univ Stuttgart, Inst Visualisat \\& Interact Syst, Stuttgart, Germany.\nHeimerl, Florian, Univ Wisconsin, Dept Comp Sci, Madison, WI USA.", "countries": "Germany;USA", "abstract": "Magic lens based focus+context techniques are powerful means for exploring document spatializations. Typically, they only offer additional summarized or abstracted views on focused documents. As a consequence, users might miss important information that is either not shown in aggregated form or that never happens to get focused. In this work, we present the design process and user study results for improving a magic lens based document exploration approach with exemplary visual quality cues to guide users in steering the exploration and support them in interpreting the summarization results. We contribute a thorough analysis of potential sources of information loss involved in these techniques, which include the visual spatialization of text documents, user-steered exploration, and the visual summarization. With lessons learned from previous research, we highlight the various ways those information losses could hamper the exploration. Furthermore, we formally define measures for the aforementioned different types of information losses and bias. Finally, we present the visual cues to depict these quality measures that are seamlessly integrated into the exploration approach. These visual cues guide users during the exploration and reduce the risk of misinterpretation and accelerate insight generation. We conclude with the results of a controlled user study and discuss the benefits and challenges of integrating quality guidance in exploration techniques.", "keywords": "Document Visualization; Focus plus Context; Visual Guidance; Uncertainty Visualization; Document Spatialization; Text Mining; Visual Analytics", "link": "https://doi.org/10.1109/PacificVis.2019.00026", "refList": ["10.1177/1473871612460526", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2007.70443", "10.1109/tvcg.2015.2467591", "10.1109/vast.2016.7883507", "10.1145/1456650.1456652", "10.1109/vast.2009.5333443", "10.1109/tvcg.2017.2744138", "10.1016/j.neucom.2006.11.018", "10.1111/cgf.13181", "10.1109/vast.2011.6102456", "10.1109/tvcg.2016.2598445", "10.1177/1536867x0300300204", "10.1109/bigdata.2015.7363807", "10.1109/vast.2011.6102449", "10.1109/pacificvis.2015.7156366", "10.1111/cgf.12871", "10.1109/tvcg.2013.186", "10.1111/j.1467-8659.2010.01835.x", "10.1145/1842993.1843002", "10.1145/2207676.2207741", "10.1007/978-1-4471-6497-5\\_1", "10.1109/tvcg.2013.162", "10.1109/t-c.1969.222678", "10.1145/2911451.2911546", "10.1109/tvcg.2012.285", "10.1109/tvcg.2014.2346743", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2016.2598466", "10.1109/2945.981848", "10.1109/vast.2007.4389006", "10.1145/302979.303148", "10.1109/vast.2014.7042494", "10.1109/infvis.1995.528686", "10.1109/tvcg.2012.277", "10.1109/tvcg.2017.2723397", "10.1177/0963721413481473", "10.1109/visual.2000.885678", "10.1109/tvcg.2018.2865233", "10.1109/tvcg.2013.153", "10.1016/0169-7439(87)80084-9", "10.1111/cgf.12655", "10.1109/tvcg.2018.2846735", "10.3115/1117729.1117730", "10.1109/tvcg.2015.2467717", "10.1145/22339.22342", "10.1109/vast.2011.6102488", "10.1109/tvcg.2016.2598468", "10.1109/tvcg.2016.2599058", "10.1109/tvcg.2014.2346433", "10.1109/tvcg.2006.138"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13408", "year": "2018", "title": "Design Factors for Summary Visualization in Visual Analytics", "conferenceName": "EuroVis", "authors": "Alper Sarikaya;Michael Gleicher;Danielle Albers Szafir", "citationCount": "3", "affiliation": "Sarikaya, A (Corresponding Author), Microsoft Corp, Redmond, WA 98052 USA.\nSarikaya, A., Microsoft Corp, Redmond, WA 98052 USA.\nGleicher, M., Univ Wisconsin, Madison, WI 53706 USA.\nSzafir, D. A., Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Data summarization allows analysts to explore datasets that may be too complex or too large to visualize in detail. Designers face a number of design and implementation choices when using summarization in visual analytics systems. While these choices influence the utility of the resulting system, there are no clear guidelines for the use of these summarization techniques. In this paper, we codify summarization use in existing systems to identify key factors in the design of summary visualizations. We use quantitative content analysis to systematically survey examples of visual analytics systems and enumerate the use of these design factors in data summarization. Through this analysis, we expose the relationship between design considerations, strategies for data summarization in visualization systems, and how different summarization methods influence the analyses supported by systems. We use these results to synthesize common patterns in real-world use of summary visualizations and highlight open challenges and opportunities that these patterns offer for designing effective systems. This work provides a more principled understanding of design practices for summary visualization and offers insight into underutilized approaches.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13408", "refList": ["10.1057/palgrave.ivs.9500122", "10.1111/cgf.12386", "10.1109/tvcg.2006.85", "10.1177/1473871611416549", "10.1109/tvcg.2010.223", "10.1145/2207676.2208607", "10.1111/cgf.12377", "10.1109/tvcg.2013.65", "10.1109/tvcg.2014.2346260", "10.1109/mcg.2016.2", "10.1109/vl.1996.545307", "10.1145/108360.108361", "10.1002/9781118445112.stat06472", "10.1109/tvcg.2015.2467619", "10.1145/2207676.2207738", "10.1177/1473871615621602", "10.1109/infvis.2002.1173156", "10.1109/tvcg.2013.124", "10.1109/visual.1992.235203", "10.1109/tvcg.2007.70535", "10.1109/infvis.2005.1532136", "10.1109/tvcg.2006.161", "10.1109/2945.981847", "10.1167/16.5.11", "10.1109/tvcg.2013.120", "10.1109/tvcg.2009.84", "10.1109/tvcg.2017.2744198.9", "10.1109/tvcg.2012.110", "10.1145/2133416.2146416", "10.1109/infvis.1997.636792", "10.1109/69.553159", "10.1109/tvcg.2014.2346574", "10.1109/tvcg.2007.70515", "10.1016/j.cag.2007.01.030", "10.1109/tvcg.2016.2598468", "10.1109/tvcg.2014.2346433"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13709", "year": "2019", "title": "Designing Animated Transitions to Convey Aggregate Operations", "conferenceName": "EuroVis", "authors": "Younghoon Kim;Michael Correll;Jeffrey Heer", "citationCount": "1", "affiliation": "Kim, Y (Corresponding Author), Univ Washington, Seattle, WA 98195 USA.\nKim, Younghoon; Heer, Jeffrey, Univ Washington, Seattle, WA 98195 USA.\nCorrell, Michael, Tableau Res, Washington, DC USA.", "countries": "USA", "abstract": "Data can be aggregated in many ways before being visualized in charts, profoundly affecting what a chart conveys. Despite this importance, the type of aggregation is often communicated only via axis titles. In this paper, we investigate the use of animation to disambiguate different types of aggregation and communicate the meaning of aggregate operations. We present design rationales for animated transitions depicting aggregate operations and present the results of an experiment assessing the impact of these different transitions on identification tasks. We find that judiciously staged animated transitions can improve subjects' accuracy at identifying the aggregation performed, though sometimes with longer response times than with static transitions. Through an analysis of participants' rankings and qualitative responses, we find a consistent preference for animation over static transitions and highlight visual features subjects report relying on to make their judgments. We conclude by extending our animation designs to more complex charts of aggregated data such as box plots and bootstrapped confidence intervals.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13709", "refList": ["10.1111/cgf.13408", "10.1006/ijhc.1017", "10.1109/tvcg.2014.2346424", "10.1109/tvcg.2013.227", "10.1109/tvcg.2017.2785807", "10.1109/tvcg.2011.175", "10.1109/tvcg.2017.2750689", "10.1109/tvcg.2008.125", "10.1006/ijhc.2002.1017", "10.1109/tvcg.2011.185", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2018.2864884", "10.1006/s1045-926x(02)00028-9", "10.1109/infvis.1999.801854"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030396", "title": "What Makes a Data-GIF Understandable?", "year": "2020", "conferenceName": "InfoVis", "authors": "Xinhuan Shu;Aoyu Wu;Junxiu Tang;Benjamin Bach;Yingcai Wu;Huamin Qu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Shu, Xinhuan; Tang, Junxiu; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Shu, Xinhuan; Wu, Aoyu; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Shu, Xinhuan, Zhejiang Univ, Hangzhou, Peoples R China. Bach, Benjamin, Univ Edinburgh, Edinburgh, Midlothian, Scotland.", "countries": "Scotland;China", "abstract": "GIFs are enjoying increasing popularity on social media as a format for data-driven storytelling with visualization; simple visual messages are embedded in short animations that usually last less than 15 seconds and are played in automatic repetition. In this paper, we ask the question, \u201cWhat makes a data-GIF understandable?\u201d While other storytelling formats such as data videos, infographics, or data comics are relatively well studied, we have little knowledge about the design factors and principles for \u201cdata-GIFs\u201d. To close this gap, we provide results from semi-structured interviews and an online study with a total of 118 participants investigating the impact of design decisions on the understandability of data-GIFs. The study and our consequent analysis are informed by a systematic review and structured design space of 108 data-GIFs that we found online. Our results show the impact of design dimensions from our design space such as animation encoding, context preservation, or repetition on viewers understanding of the GIF's core message. The paper concludes with a list of suggestions for creating more effective Data-GIFs.", "keywords": "Data-GIFs,Data-driven Storytelling,Evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030396", "refList": ["10.1109/tvcg.2016.2598647", "10.1016/j.visinf.2020.07.001", "10.1177/1473871615594652", "10.1109/tvcg.2014.2346424", "10.1111/cgf.13195", "10.1109/vlsicircuits18222.2020.9162811", "10.1109/tvcg.2018.2864909", "10.1109/tvcg.2018.2864903", "10.1145/2702123.2702431", "10.1016/j.visinf.2019.12.002", "10.1145/3274349", "10.1111/cgf.13444", "10.1145/3206505.3206552", "10.1145/3290605.3300280", "10.1109/jstqe.2020.3021589", "10.1145/2647868.2656408", "10.1006/ijhc.1017", "10.1145/3290605.3300335", "10.1007/s12650-020-00689-0", "10.1145/2818048.2819936", "10.1111/cgf.13325", "10.1145/2858036.2858387", "10.1145/3027063.3053139", "10.1145/3290605.3300474", "10.1109/tvcg.2016.2598920", "10.1109/tvcg.2018.2864899", "10.1145/3290605.3300483", "10.1145/3173574.3173612", "10.1145/2909132.2909255", "10.1109/tvcg.2016.2598620", "10.1016/j.learninstruc.2007.09.013", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2008.125", "10.1145/3173574.3173909", "10.1109/tvcg.2019.2934397", "10.1111/cgf.13709", "10.1109/tvcg.2013.234", "10.1109/tvcg.2019.2934401", "10.1145/2858036.2858532", "10.1016/j.visinf.2020.08.001", "10.1109/tvcg.2010.179", "10.1109/cicc48029.2020.9075900"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14005", "year": "2020", "title": "Canis: A High-Level Language for Data-Driven Chart Animations", "conferenceName": "EuroVis", "authors": "T. Ge;Y. Zhao;B. Lee;D. Ren;B. Chen;Y. Wang", "citationCount": "0", "affiliation": "Wang, Y (Corresponding Author), Shandong Univ, Qingdao, Peoples R China.\nGe, T.; Zhao, Y.; Wang, Y., Shandong Univ, Qingdao, Peoples R China.\nLee, B., Microsoft Res, Redmond, WA USA.\nRen, D., Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.\nChen, B., Peking Univ, Beijing, Peoples R China.", "countries": "USA;China", "abstract": "In this paper, we introduce Canis, a high-level domain-specific language that enables declarative specifications of data-driven chart animations. By leveraging data-enriched SVG charts, its grammar of animations can be applied to the charts created by existing chart construction tools. With Canis, designers can select marks from the charts, partition the selected marks into mark units based on data attributes, and apply animation effects to the mark units, with the control of when the effects start. The Canis compiler automatically synthesizes the Lottie animation JSON files {[}Aira], which can be rendered natively across multiple platforms. To demonstrate Canis' expressiveness, we present a wide range of chart animations. We also evaluate its scalability by showing the effectiveness of our compiler in reducing the output specification size and comparing its performance on different platforms against D3.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14005", "refList": ["10.1109/tvcg.2016.2598647", "10.1198/jcgs.2009.07098", "10.1111/cgf.13709", "10.1109/tvcg.2010.78", "10.1109/tvcg.2014.2346424", "10.1145/3173574.3173697", "10.1111/cgf.13178", "10.1006/ijhc.1017", "10.1145/3025453.3025942", "10.1145/2702123.2702431", "10.1109/tvcg.2008.125", "10.1006/ijhc.2002.1017", "10.1109/tvcg.2011.185", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2016.2599030.2", "10.1145/2642918.2647411", "10.1109/tvcg.2009.174"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 9}, {"doi": "10.1111/cgf.13698", "year": "2019", "title": "Visual-Interactive Preprocessing of Multivariate Time Series Data", "conferenceName": "EuroVis", "authors": "J{\\\"{u}}rgen Bernard;Marco Hutter;Heiko Reinemuth;Hendrik Pfeifer;Christian Bors;J{\\\"{o}}rn Kohlhammer", "citationCount": "2", "affiliation": "Bernard, J (Corresponding Author), Tech Univ Darmstadt, Darmstadt, Germany.\nBernard, Juergen; Hutter, Marco; Reinemuth, Heiko; Pfeifer, Hendrik, Tech Univ Darmstadt, Darmstadt, Germany.\nBors, Christian, TU Wien, Vienna, Austria.\nKohlhammer, Joern, Fraunhofer IGD, Darmstadt, Germany.", "countries": "Germany;Austria", "abstract": "Pre-processing is a prerequisite to conduct effective and efficient downstream data analysis. Pre-processing pipelines often require multiple routines to address data quality challenges and to bring the data into a usable form. For both the construction and the refinement of pre-processing pipelines, human-in-the-loop approaches are highly beneficial. This particularly applies to multivariate time series, a complex data type with multiple values developing over time. Due to the high specificity of this domain, it has not been subject to in-depth research in visual analytics. We present a visual-interactive approach for preprocessing multivariate time series data with the following aspects. Our approach supports analysts to carry out six core analysis tasks related to pre-processing of multivariate time series. To support these tasks, we identify requirements to baseline toolkits that may help practitioners in their choice. We characterize the space of visualization designs for uncertainty-aware pre-processing and justify our decisions. Two usage scenarios demonstrate applicability of our approach, design choices, and uncertainty visualizations for the six analysis tasks. This work is one step towards strengthening the visual analytics support for data pre-processing in general and for uncertainty-aware pre-processing of multivariate time series in particular.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13698", "refList": ["10.1016/j.engappai.2010.09.007", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13190", "10.1177/1473871611416549", "10.1007/3-7908-1701-5\\_9", "10.1109/icdmw.2009.55", "10.3758/s13423-012-0247-5", "10.1109/pacificvis.2010.5429596", "10.1177/1473871611415994", "10.1117/12.2080001", "10.1145/2379776.2379788", "10.1145/3105971.3105984", "10.1016/j.patcog.2005.01.025", "10.1594/pangaea.150017", "10.1109/tvcg.2013.178", "10.1109/tvcg.2018.2859973", "10.1109/tvcg.2011.195", "10.1145/1126004.1126005", "10.1145/1656274.1656278", "10.1594/pangaea.804156", "10.1594/pangaea.787726", "10.1109/tvcg.2018.2865077", "10.1007/978-1-4471-6497-5\\_1", "10.2312/eurova.20151104", "10.1109/tvcg.2012.285", "10.1023/a:1024988512476", "10.1109/tvcg.2010.225", "10.1109/vast.2015.7347672", "10.1007/s00799-014-0134-y", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2018.2864907", "10.1007/s40430-018-1079-7", "10.1007/978-3-642-32498-7\\_5", "10.1111/cgf.13237", "10.1109/vast.2009.5332611", "10.1109/tvcg.2015.2467752", "10.1016/j.cviu.2006.08.002", "10.1109/tvcg.2014.2346321", "10.1145/1014052.1014104", "10.1007/978-3-540-71949-6", "10.2312/sca/sca10/001-010", "10.1145/3025453.3025738", "10.2312/eurova.20181112", "10.1109/pacificvis.2018.00034", "10.1007/978-0-85729-079-3\\_8", "10.1109/tvcg.2017.2779501", "10.1109/tvcg.2008.166", "10.1007/s10618-012-0285-7", "10.1109/tvcg.2013.222", "10.1109/tvcg.2018.2864889", "10.1109/tvcg.2016.2598592", "10.1109/vast.2010.5652530", "10.1145/2723372.2731081", "10.1109/tvcg.2018.2864914", "10.1109/infvis.2000.885098", "10.1016/j.neucom.2017.01.105", "10.2352/issn.2470-1173.2017.1.vda-387", "10.1109/tvcg.2015.2467851", "10.1109/tvcg.2016.2598468", "10.1109/tvcg.2016.2603178"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030364", "title": "Supporting the Problem-Solving Loop: Designing Highly Interactive Optimisation Systems", "year": "2020", "conferenceName": "VAST", "authors": "Jie Liu;Tim Dwyer;Guido Tack;Samuel Gratzl;Kim Marriott", "citationCount": "0", "affiliation": "Liu, J (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Liu, Jie; Dwyer, Tim; Tack, Guido; Gratzl, Samuel; Marriott, Kim, Monash Univ, Clayton, Vic, Australia.", "countries": "Australia", "abstract": "Efficient optimisation algorithms have become important tools for finding high-quality solutions to hard, real-world problems such as production scheduling, timetabling, or vehicle routing. These algorithms are typically \u201cblack boxes\u201d that work on mathematical models of the problem to solve. However, many problems are difficult to fully specify, and require a \u201chuman in the loop\u201d who collaborates with the algorithm by refining the model and guiding the search to produce acceptable solutions. Recently, the Problem-Solving Loop was introduced as a high-level model of such interactive optimisation. Here, we present and evaluate nine recommendations for the design of interactive visualisation tools supporting the Problem-Solving Loop. They range from the choice of visual representation for solutions and constraints to the use of a solution gallery to support exploration of alternate solutions. We first examined the applicability of the recommendations by investigating how well they had been supported in previous interactive optimisation tools. We then evaluated the recommendations in the context of the vehicle routing problem with time windows (VRPTW). To do so we built a sophisticated interactive visual system for solving VRPTW that was informed by the recommendations. Ten participants then used this system to solve a variety of routing problems. We report on participant comments and interaction patterns with the tool. These showed the tool was regarded as highly usable and the results generally supported the usefulness of the underlying recommendations.", "keywords": "Interactive optimisation,Interface design,Usability,Interactive systems and tools,Vehicle routing", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030364", "refList": ["10.1176/appi.ajp.2018.18070881", "10.1006/obhd.1997.2679", "10.1109/vlhcc.2018.8506576", "10.1177/0956797611417632", "10.1177/0956797619830649", "10.1177/0049124115610347", "10.1177/1948550618775108", "10.1145/3313831.3376533", "10.1511/2014.111.460", "10.1145/2858036.2858465", "10.1145/3173574.3173606", "10.1177/1745691616658637", "10.1038/483531a", "10.1145/3173574.3174053", "10.1037/pspp0000186", "10.1016/j.jclinepi.2015.05.029", "10.1126/science.aac4716", "10.1145/985692.985782", "10.1080/02699931.2018.1524747", "10.1073/pnas.1402786111", "10.1145/3290605.3300295", "10.1111/cgf.13698", "10.1145/3290605.3300709", "10.2307/2686111", "10.1214/17-ba1091", "10.1145/1449715.1449732", "10.2139/ssrn.2694998", "10.1145/3313831.3376777", "10.1177/2515245917747646", "10.1145/3290605.3300754", "10.1145/3025453.3025626", "10.1109/tvcg.2014.2346321", "10.1037/gpr0000123", "10.2139/ssrn.2535453", "10.1145/3025453.3025738", "10.1038/533452a", "10.1177/1948550617714584", "10.1007/s11222-013-9416-2", "10.3389/fpsyg.2016.01832", "10.1038/s41562-018-0506-1", "10.1007/s11222-016-9696-4", "10.1177/1367006918763132", "10.1109/tsmc.1981.4308636", "10.1038/nrd3439-c1", "10.1177/0956797617723726", "10.1111/j.1467-8659.2012.03116.x", "10.1146/annurev-psych-122216-011836", "10.1145/3290605.3300432"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.13730", "year": "2019", "title": "A Review of Guidance Approaches in Visual Data Analysis: A Multifocal Perspective", "conferenceName": "EuroVis", "authors": "Davide Ceneda;Theresia Gschwandtner;Silvia Miksch", "citationCount": "5", "affiliation": "Ceneda, D (Corresponding Author), TU Wien, Vienna, Austria.\nCeneda, D (Corresponding Author), TUWien, Fac Informat, Inst Visual Comp \\& Human Centered Technol, Favoritenstr 9-11-193, A-1040 Vienna, Austria.\nCeneda, Davide; Gschwandtner, Theresia; Miksch, Silvia, TU Wien, Vienna, Austria.\nCeneda, Davide, TUWien, Fac Informat, Inst Visual Comp \\& Human Centered Technol, Favoritenstr 9-11-193, A-1040 Vienna, Austria.", "countries": "Austria", "abstract": "Visual data analysis can be envisioned as a collaboration of the user and the computational system with the aim of completing a given task. Pursuing an effective system-user integration, in which the system actively helps the user to reach his/her analysis goal has been focus of visualization research for quite some time. However, this problem is still largely unsolved. As a result, users might be overwhelmed by powerful but complex visual analysis systems which also limits their ability to produce insightful results. In this context, guidance is a promising step towards enabling an effective mixed-initiative collaboration to promote the visual analysis. However, the way how guidance should be put into practice is still to be unravelled. Thus, we conducted a comprehensive literature research and provide an overview of how guidance is tackled by different approaches in visual analysis systems. We distinguish between guidance that is provided by the system to support the user, and guidance that is provided by the user to support the system. By identifying open problems, we highlight promising research directions and point to missing factors that are needed to enable the envisioned human-computer collaboration, and thus, promote a more effective visual data analysis.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13730", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2011.231", "10.1111/cgf.12385", "10.1145/1345448.1345453", "10.1016/j.visinf.2018.09.003", "10.1007/s00371-015-1132-9", "10.1111/j.1467-8659.2011.01957.x", "10.1145/2002353.2002355", "10.1145/2254556.2254659", "10.1109/tvcg.2011.218", "10.1109/tvcg.2012.195", "10.1109/apchi.1998.704487", "10.20380/gi2015.16", "10.1177/1473871611415994", "10.1016/0950-7051(95)98369-h", "10.1145/1502650.1502695", "10.1109/vl.1996.545307", "10.1111/j.1467-8659.2009.01475.x", "10.1145/2858036.2858529", "10.1007/978-3-642-23765-2\\_13", "10.1016/j.jvlc.2011.02.003", "10.1111/j.1467-8659.2012.03091.x", "10.1109/tvcg.2011.108", "10.1109/vast.2011.6102451", "10.1145/1124772.1124782", "10.1145/2207676.2207741", "10.1111/j.1467-8659.2012.03110.x", "10.1145/1378773.1378788", "10.1145/2702123.2702149", "10.1109/vast.2010.5652484", "10.1007/s11042-014-2408-1", "10.1109/infvis.2005.1532136", "10.1007/978-3-540-71080-6\\_6", "10.1109/iscas.2015.7168867", "10.1109/tvcg.2014.2346481", "10.1007/978-3-642-00304-2\\_1", "10.1109/vast.2009.5333023", "10.2307/2288400", "10.1016/s0950-7051(00)00101-5", "10.1145/846170.846172", "10.1109/vast.2010.5652885", "10.1109/vast.2014.7042477", "10.1109/infvis.2005.1532138", "10.1145/1562849.1562851", "10.1145/302979.303030", "10.1007/978-3-642-36763-2\\_42", "10.1111/cgf.13404", "10.1109/biovis.2012.6378590", "10.1016/s0962-1849(99)80009-2", "10.1109/vast.2009.5332586", "10.1007/978-3-642-41939-3\\_4", "10.1109/tvcg.2015.2467191", "10.14778/2536354.2536356", "10.1109/pacificvis.2011.5742382", "10.1145/1401890.1402026", "10.3390/ijgi4042159", "10.1109/visual.1997.663889", "10.1007/s10844-014-0304-9", "10.1145/347090.347124", "10.1109/vast.2010.5652443", "10.1109/infvis.2004.2", "10.1109/vast.2011.6102448", "10.1126/science.1160379", "10.1109/tvcg.2008.174", "10.1109/tvcg.2016.2598468", "10.1109/vast.2010.5652398", "10.1109/cse.2009.395"], "wos": 1, "children": [{"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1111/cgf.13670", "year": "2019", "title": "Follow The Clicks: Learning and Anticipating Mouse Interactions During Exploratory Data Analysis", "conferenceName": "EuroVis", "authors": "Alvitta Ottley;Roman Garnett;Ran Wan", "citationCount": "1", "affiliation": "Ottley, A (Corresponding Author), Washington Univ, Comp Sci \\& Engn, St Louis, MO 63110 USA.\nOttley, Alvitta; Garnett, Roman; Wan, Ran, Washington Univ, Comp Sci \\& Engn, St Louis, MO 63110 USA.", "countries": "USA", "abstract": "The goal of visual analytics is to create a symbiosis between human and computer by leveraging their unique strengths. While this model has demonstrated immense success, we are yet to realize the full potential of such a human-computer partnership. In a perfect collaborative mixed-initiative system, the computer must possess skills for learning and anticipating the users' needs. Addressing this gap, we propose a framework for inferring attention from passive observations of the user's click, thereby allowing accurate predictions of future events. We demonstrate this technique with a crime map and found that users' clicks can appear in our prediction set 92\\% - 97\\% of the time. Further analysis shows that we can achieve high prediction accuracy typically after three clicks. Altogether, we show that passive observations of interaction data can reveal valuable information that will allow the system to learn and anticipate future events.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13670", "refList": ["10.1057/ivs.2009.22", "10.1145/2882903.2882919", "10.1109/tvcg.2014.2346575", "10.1145/846183.846188", "10.1111/cgf.13405", "10.1109/tvcg.2016.2598471", "10.1145/3173574.3174168", "10.1145/948496.948514", "10.1109/mcg.2006.30", "10.1109/tvcg.2012.195", "10.1016/s0042-6989(99)00163-7", "10.1007/978-3-540-70956-5", "10.1145/643477.643478", "10.1038/35058500", "10.1023/a:1008935410038", "10.1002/9781118360491.ch10", "10.1007/978-94-009-3833-5\\_5", "10.1145/2702123.2702590", "10.1057/ivs.2008.31", "10.1145/1054972.1055012", "10.1109/tvcg.2015.2467551", "10.1145/636772.636798", "10.1109/tpami.2012.89", "10.1109/34.730558", "10.1094/pdis-11-11-0999-pdn", "10.1145/1476589.1476628", "10.1145/964442.964461", "10.1145/302979.303030", "10.1109/5254.796083", "10.1145/360402.360406", "10.1109/icarcv.2018.8581221", "10.1049/ip-f-2.1993.0015", "10.1109/compsac.2017.270", "10.1109/vast.2008.4677361", "10.1145/1979742.1979570", "10.1109/hicss.2005.286", "10.1145/2110192.2110202", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2016.2598468"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030383", "title": "II-20: Intelligent and pragmatic analytic categorization of image collections", "year": "2020", "conferenceName": "VAST", "authors": "Jan Zah\u00e1lka;Marcel Worring;Jarke J. van Wijk", "citationCount": "0", "affiliation": "Zahalka, J (Corresponding Author), Czech Tech Univ, Prague, Czech Republic. Zahalka, Jan, Czech Tech Univ, Prague, Czech Republic. Worring, Marcel, Univ Amsterdam, Amsterdam, Netherlands. van Wijk, Jarke J., Eindhoven Univ Technol, Eindhoven, Netherlands.", "countries": "Republic;Netherlands", "abstract": "In this paper, we introduce 11\u201320 (Image Insight 2020), a multimedia analytics approach for analytic categorization of image collections. Advanced visualizations for image collections exist, but they need tight integration with a machine model to support the task of analytic categorization. Directly employing computer vision and interactive learning techniques gravitates towards search. Analytic categorization, however, is not machine classification (the difference between the two is called the pragmatic gap): a human adds/redefines/deletes categories of relevance on the fly to build insight, whereas the machine classifier is rigid and non-adaptive. Analytic categorization that truly brings the user to insight requires a flexible machine model that allows dynamic sliding on the exploration-search axis, as well as semantic interactions: a human thinks about image data mostly in semantic terms. 11\u201320 brings three major contributions to multimedia analytics on image collections and towards closing the pragmatic gap. Firstly, a new machine model that closely follows the user's interactions and dynamically models her categories of relevance. II-20's machine model, in addition to matching and exceeding the state of the art's ability to produce relevant suggestions, allows the user to dynamically slide on the exploration-search axis without any additional input from her side. Secondly, the dynamic, 1-image-at-a-time Tetris metaphor that synergizes with the model. It allows a well-trained model to analyze the collection by itself with minimal interaction from the user and complements the classic grid metaphor. Thirdly, the fast-forward interaction, allowing the user to harness the model to quickly expand (\u201cfast-forward\u201d) the categories of relevance, expands the multimedia analytics semantic interaction dictionary. Automated experiments show that II-20's machine model outperforms the existing state of the art and also demonstrate the Tetris metaphor's analytic quality. User studies further confirm that II\u201320 is an intuitive, efficient, and effective multimedia analytics tool.", "keywords": "Multimedia analytics,image data,analytic categorization,pragmatic gap", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030383", "refList": ["10.1109/tvcg.2008.137", "10.1145/2882903.2882919", "10.1145/2556647.2556657", "10.1145/1142473.1142574", "10.1109/tvcg.2016.2598471", "10.1109/vast.2017.8585669", "10.1109/tvcg.2014.2346573", "10.1111/j.0956-7976.2005.00782.x", "10.1007/978-3-540-89965-5\\_27", "10.1109/visual.2019.8933611", "10.1109/vast.2010.5653598", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1109/tvcg.2012.271", "10.1109/mcg.2009.49", "10.1057/ivs.2008.31", "10.1111/cgf.12925", "10.1109/hicss.2016.183", "10.1109/tvcg.2016.2598466", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2018.2865117", "10.1109/vast47406.2019.8986948", "10.1016/s0950-7051(00)00101-5", "10.1109/tvcg.2016.2598594", "10.1109/tvcg.2006.101", "10.1111/cgf.12311", "10.1109/vast.2009.5333020", "10.1109/vast.2010.5652885", "10.1111/cgf.13670", "10.1109/wvl.1988.18020", "10.1145/2133806.2133821", "10.1109/vast.2012.6400486"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13726", "year": "2019", "title": "State-of-the-Art Report: Visual Computing in Radiation Therapy Planning", "conferenceName": "EuroVis", "authors": "Matthias Schlachter;Renata G. Raidou;Ludvig P. Muren;Bernhard Preim;P. M. Putora;Katja B{\\\"{u}}hler", "citationCount": "1", "affiliation": "Schlachter, M (Corresponding Author), VRVis Res Ctr, Vienna, Austria.\nSchlachter, M.; Buehler, K., VRVis Res Ctr, Vienna, Austria.\nRaidou, R. G., TU Wien, Vienna, Austria.\nMuren, L. P., Aarhus Univ Hosp, Dept Med Phys, Aarhus, Denmark.\nPreim, B., Univ Magdeburg, Magdeburg, Germany.\nPreim, B., Res Campus STIMULATE, Magdeburg, Germany.\nPutora, P. M., Kantonsspital St Gallen, Dept Radiat Oncol, St Gallen, Switzerland.\nPutora, P. M., Univ Bern, Dept Radiat Oncol, Bern, Switzerland.", "countries": "Germany;Switzerland;Austria;Denmark", "abstract": "Radiation therapy (RT) is one of the major curative approaches for cancer. It is a complex and risky treatment approach, which requires precise planning, prior to the administration of the treatment. Visual Computing (VC) is a fundamental component of RT planning, providing solutions in all parts of the process-from imaging to delivery. Despite the significant technological advancements of RT over the last decades, there are still many challenges to address. This survey provides an overview of the compound planning process of RT, and of the ways that VC has supported RT in all its facets. The RT planning process is described to enable a basic understanding in the involved data, users and workflow steps. A systematic categorization and an extensive analysis of existing literature in the joint VC/RT research is presented, covering the entire planning process. The survey concludes with a discussion on lessons learnt, current status, open challenges, and future directions in VC/RT research.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13726", "refList": ["10.1016/j.ijrobp.2014.01.046", "10.1016/j.ijrobp.2004.11.037", "10.1118/1.1935775", "10.21037/tlcr.2016.06.07", "10.1109/vast.2014.7042481", "10.1017/s1460396917000206", "10.1109/jproc.2002.804685", "10.1111/cgf.12193", "10.1016/j.radonc.2007.08.012", "10.1111/cgf.12927", "10.1016/s0360-3016(02)02866-3", "10.1097/pas.0000000000000530", "10.1088/0031-9155/46/3/304", "10.1145/1268517.1268563", "10.1088/0031-9155/53/12/r01", "10.1109/tvcg.2008.95", "10.1002/acm2.12209", "10.1016/s0167-8140(01)00444-3", "10.1118/1.4896821", "10.1016/j.ijrobp.2011.07.021", "10.1016/s0360-3016(00)00467-3", "10.1038/tp.2012.95", "10.1016/0360-3016(95)00272-3", "10.1111/cgf.13413", "10.1186/s13014-015-0569-3", "10.1097/01.rct.0000156396.13522.f2", "10.1016/j.apradiso.2004.05.049", "10.1118/1.2966347", "10.1146/annurev.bioeng.2.1.315", "10.1038/nrclinonc.2012.203", "10.1109/iembs.1997.756505", "10.1016/s0360-3016(99)00042-5", "10.1080/10408370091179244", "10.1109/ultsym.2001.991951", "10.1016/0360-3016(91)90168-4", "10.1118/1.1420400", "10.2214/ajr.12.8510", "10.3109/0284186x.2011.590524", "10.2214/ajr.09.2527", "10.1109/tmi.2017.2717800", "10.1016/j.adro.2017.04.005", "10.1146/annurev.bioeng.1.1.377", "10.1016/j.radonc.2009.12.034", "10.1016/j.ijrobp.2009.07.1754", "10.1109/tmi.2013.2265603", "10.1186/s13014-015-0404-x", "10.2312/vcbm.20161287", "10.1109/bmei.2013.6746924", "10.1016/j.ijrobp.2005.05.008", "10.1007/s10278-010-9321-6", "10.1016/j.ijom.2017.08.002", "10.4103/0971-6203.44472", "10.1016/s0360-3016(01)02805-x", "10.1002/jmri.21513", "10.1109/nssmic.1991.259172", "10.1007/s10278-016-9915-8", "10.1088/0031-9155/59/11/2687", "10.1118/1.4905050", "10.1287/opre.1070.0484", "10.1148/rg.271065078", "10.1102/1470-7330.2007.0013", "10.1016/j.ijrobp.2005.10.006", "10.1016/j.clon.2017.09.007", "10.3109/0284186x.2010.498433", "10.1109/vbc.1990.109295", "10.1016/j.cag.2010.05.016", "10.1111/cgf.12613", "10.1016/j.ijrobp.2008.05.011", "10.1109/iv.1997.626463", "10.1007/s10278-015-9839-8", "10.1088/0031-9155/38/6/001", "10.1016/j.cag.2013.10.015", "10.1007/s00066-004-1275-5", "10.1016/j.artmed.2008.11.003", "10.1016/j.ijrobp.2013.10.028", "10.1109/nssmic.2010.5874096", "10.1109/visual.1995.480795", "10.1111/1759-7714.12170", "10.1002/mp.12629", "10.1111/cgf.12165", "10.1118/1.2349696", "10.1148/radiol.2333032098", "10.1016/j.ejmp.2012.11.004", "10.1109/tvcg.2013.143", "10.14791/btrt.2015.3.1.8", "10.1118/1.2815943", "10.1111/cgf.12899", "10.1102/1470-7330.2006.9017", "10.1109/iv.2001.942055", "10.1016/j.media.2016.06.030", "10.1016/s0360-3016(03)00073-7", "10.1111/cgf.13306", "10.1016/j.radonc.2006.05.002", "10.1109/vahc.2017.8387544", "10.1109/titb.2006.875669", "10.1118/1.3521468", "10.1148/radiol.2431030580", "10.1097/mou.0b013e3282f19d01", "10.1016/s1053-4296(99)80051-4", "10.1016/j.suronc.2010.07.004", "10.1109/2945.597794", "10.1016/j.ijrobp.2007.01.056", "10.1088/0031-9155/52/13/n03", "10.1016/j.inffus.2013.12.002", "10.1016/s0958-3947(98)00014-4", "10.1088/0031-9155/44/11/201", "10.7785/tcrt.2012.500256", "10.1016/j.cmpb.2010.10.009", "10.1007/s00330-011-2377-y", "10.1118/1.4754659", "10.1109/icicic.2009.62", "10.1111/j.1467-8659.2011.01954.x", "10.1186/1748-717x-1-47", "10.1109/visual.1996.568110", "10.1007/978-3-540-85990-1\\_85", "10.2147/ott.s64161", "10.1109/tvcg.2007.70569", "10.1016/s0167-8140(02)00140-8", "10.1109/tns.2016.2616884", "10.3109/02841868709089981", "10.2312/vcbm.20161276", "10.1007/978-3-319-46723-8\\_12", "10.1109/iembs.2005.1615886", "10.1002/mp.13115", "10.1109/tvcg.2014.2346574", "10.1016/j.apradiso.2015.12.061", "10.1148/rg.313105139", "10.1120/jacmp.v8i3.2448", "10.1002/cncr.21324", "10.1016/j.jbi.2016.12.001", "10.1109/icmla.2008.126", "10.1007/978-3-642-21414-11", "10.1109/tmi.2016.2560942", "10.1016/0360-3016(96)00204-0", "10.1002/mp.12979", "10.1118/1.4828840", "10.1118/1.3253464", "10.1002/(sici)1522-2586(199909)10:3", "10.3109/0284186x.2013.814152", "10.1088/0031-9155/51/9/009", "10.1016/s1361-8415(00)00041-4", "10.1016/s0262-8856(03)00137-9", "10.1016/j.prro.2011.08.001", "10.1109/vast.2014.7042521", "10.1109/iv.2000.859783", "10.1080/02841860802266763", "10.1186/s40658-016-0151-6", "10.1016/j.radonc.2016.02.030", "10.1093/jicru/ndh001", "10.1109/medcom.2014.7005998", "10.1016/j.acra.2008.09.011", "10.1016/j.clon.2017.10.013", "10.1109/isbi.2012.6235553", "10.1016/j.radonc.2006.11.024", "10.1016/s1361-8415(01)80026-8", "10.2312/vcbm/vcbm08/093-100", "10.1016/s0167-8140(02)00015-4", "10.1109/cw.2017.56", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2016.2598468", "10.1016/s0360-3016(96)85024-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13972", "year": "2020", "title": "Boxer: Interactive Comparison of Classifier Results", "conferenceName": "EuroVis", "authors": "Michael Gleicher;Aditya Barve;Xinyi Yu;Florian Heimerl", "citationCount": "0", "affiliation": "Gleicher, M (Corresponding Author), Univ Wisconsin, Madison, WI 53706 USA.\nGleicher, Michael; Barve, Aditya; Yu, Xinyi; Heimerl, Florian, Univ Wisconsin, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Machine learning practitioners often compare the results of different classifiers to help select, diagnose and tune models. We present Boxer, a system to enable such comparison. Our system facilitates interactive exploration of the experimental results obtained by applying multiple classifiers to a common set of model inputs. The approach focuses on allowing the user to identify interesting subsets of training and testing instances and comparing performance of the classifiers on these subsets. The system couples standard visual designs with set algebra interactions and comparative elements. This allows the user to compose and coordinate views to specify subsets and assess classifier performance on them. The flexibility of these compositions allow the user to address a wide range of scenarios in developing and assessing classifiers. We demonstrate Boxer in use cases including model selection, tuning, fairness assessment, and data quality diagnosis.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13972", "refList": ["10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2016.2598838", "10.1007/s10618-014-0368-8", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1145/3287560.3287589", "10.1109/vast.2017.8585721", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2009.128", "10.1109/tvcg.2017.2744018", "10.1080/00994480.2000.10748487", "10.5555/3305890.3306024", "10.1109/iccv.2015.329", "10.1109/tvcg.2013.125", "10.1089/big.2016.0007", "10.1109/memsys.2019.8870817", "10.1145/2939672.2939778", "10.1007/s11104-019-04156-0", "10.1371/journal.pone.0181142", "10.1145/3301275.3302324", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2018.2865044", "10.1023/a:1010933404324", "10.1145/2487575.2487579", "10.1109/tvcg.2013.157", "10.1145/2783258.2788613", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934631", "10.1016/s0304-3800(02)00064-9", "10.1007/s10115-013-0679-x", "10.1109/tvcg.2019.2934267", "10.1007/978-3-319-10590-1\\_53", "10.1109/vast.2017.8585720", "10.1016/0004-3702(80)90021-1", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2009.84", "10.1007/s11263-016-0911-8", "10.1111/cgf.12918", "10.1111/cgf.12373", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1080/00207176808905715", "10.1002/er.3827", "10.1109/tvcg.2014.2346660", "10.1111/cgf.13417", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2017.2744158", "10.1016/b978-0-12-815849-4.00004-9", "10.1097/ede.0b013e3181c30fb2", "10.1111/cgf.13681", "10.1016/j.ejor.2006.04.051", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2016.2598468", "10.9735/2229-3981", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 239}, {"doi": "10.1109/tvcg.2017.2744184", "title": "Scatterplots: Tasks, Data, and Designs", "year": "2017", "conferenceName": "InfoVis", "authors": "Alper Sarikaya;Michael Gleicher", "citationCount": "17", "affiliation": "Sarikaya, A (Corresponding Author), Univ Wisconsin, Madison, WI 53706 USA. Sarikaya, Alper; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Traditional scatterplots fail to scale as the complexity and amount of data increases. In response, there exist many design options that modify or expand the traditional scatterplot design to meet these larger scales. This breadth of design options creates challenges for designers and practitioners who must select appropriate designs for particular analysis goals. In this paper, we help designers in making design choices for scatterplot visualizations. We survey the literature to catalog scatterplot-specific analysis tasks. We look at how data characteristics influence design decisions. We then survey scatterplot-like designs to understand the range of design options. Building upon these three organizations, we connect data characteristics, analysis tasks, and design choices in order to generate challenges, open questions, and example best practices for the effective design of scatterplots.", "keywords": "Scatterplots,task taxonomies,study of designs", "link": "http://dx.doi.org/10.1109/TVCG.2017.2744184", "refList": ["10.1057/palgrave.ivs.9500122", "10.2307/2288843", "10.1145/230562.230563", "10.1109/tvcg.2013.182", "10.1111/cgf.12386", "10.1109/vast.2016.7883507", "10.1109/tvcg.2008.119", "10.1109/vast.2009.5333895", "10.1109/tvcg.2016.2598839", "10.1111/j.1467-8659.2009.01467.x", "10.2307/2289444", "10.1109/tvcg.2013.65", "10.1109/tvcg.2013.187", "10.1109/tvcg.2011.185", "10.1109/tvcg.2011.229", "10.1111/cgf.12871", "10.1109/tvcg.2010.197", "10.1145/2669557.2669559", "10.1109/vast.2010.5652460", "10.1145/108360.108361", "10.1057/palgrave.ivs.9500179", "10.1145/1753326.1753714", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346983", "10.1145/22949.22950", "10.1145/1842993.1843002", "10.1177/1473871615621602", "10.1109/infvis.2002.1173156", "10.1109/tvcg.2013.124", "10.1109/tvcg.2009.122", "10.1109/tvcg.2007.70596", "10.1111/cgf.12641", "10.1109/vast.2014.7042511", "10.1109/tvcg.2011.167", "10.1109/infvis.2005.1532136", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2010.174", "10.1109/tvcg.2006.161", "10.1109/tvcg.2015.2467615", "10.1111/cgf.12877", "10.1111/j.1467-8659.2012.03125.x", "10.1109/infvis.2005.1532142", "10.3758/s13423-016-1174-7", "10.1109/tvcg.2013.150", "10.1109/tvcg.2013.120", "10.1109/tvcg.2015.2467618", "10.1002/jhbs.20078", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2009.84", "10.1109/tvcg.2006.163", "10.1109/tvcg.2011.223", "10.1145/1168149.1168168", "10.1057/ivs.2009.34", "10.1109/tvcg.2013.130", "10.1177/1473871616638892", "10.1109/pacificvis.2014.42", "10.1109/tvcg.2013.153", "10.1145/2702123.2702585", "10.1167/15.12.893", "10.1109/tvcg.2013.183", "10.1109/bigdata.2013.6691712", "10.2307/2288711", "10.1109/vast.2012.6400486", "10.1111/j.1467-8659.2009.01694.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2864912", "title": "Optimizing Color Assignment for Perception of Class Separability in Multiclass Scatterplots", "year": "2018", "conferenceName": "InfoVis", "authors": "Yunhai Wang;Xin Chen;Tong Ge;Chen Bao;Michael Sedlmair;Chi-Wing Fu;Oliver Deussen;Baoquan Chen", "citationCount": "7", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Jinan, Shandong, Peoples R China. Wang, Yunhai; Chen, Xin; Ge, Tong; Bao, Chen, Shandong Univ, Jinan, Shandong, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China. Sedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany. Deussen, Oliver, Konstanz Univ, Constance, Germany. Deussen, Oliver, SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Peoples R China.", "countries": "Germany;China", "abstract": "Appropriate choice of colors significantly aids viewers in understanding the structures in multiclass scatterplots and becomes more important with a growing number of data points and groups. An appropriate color mapping is also an important parameter for the creation of an aesthetically pleasing scatterplot. Currently, users of visualization software routinely rely on color mappings that have been pre-defined by the software. A default color mapping, however, cannot ensure an optimal perceptual separability between groups, and sometimes may even lead to a misinterpretation of the data. In this paper, we present an effective approach for color assignment based on a set of given colors that is designed to optimize the perception of scatterplots. Our approach takes into account the spatial relationships, density, degree of overlap between point clusters, and also the background color. For this purpose, we use a genetic algorithm that is able to efficiently find good color assignments. We implemented an interactive color assignment system with three extensions of the basic method that incorporates top K suggestions, user-defined color subsets, and classes of interest for the optimization. To demonstrate the effectiveness of our assignment technique, we conducted a numerical study and a controlled user study to compare our approach with default color assignments; our findings were verified by two expert studies. The results show that our approach is able to support users in distinguishing cluster numbers faster and more precisely than default assignment methods.", "keywords": "Color perception,visual design,scatterplots", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864912", "refList": ["10.1109/tvcg.2015.2489649", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2017.2701829", "10.1111/j.1467-8659.2009.01467.x", "10.1109/tvcg.2016.2598918", "10.2307/2683294", "10.1109/cvpr.2013.151", "10.1109/tvcg.2015.2467471", "10.1037/e588422013-001", "10.1016/j.csda.2008.11.033", "10.1109/visual.1996.568118", "10.1109/vast.2009.5332628", "10.1145/2669557.2669559", "10.1109/visual.1995.480803", "10.1109/pacificvis.2016.7465244", "10.1179/000870403235002042", "10.1109/tvcg.2016.2599214", "10.1111/j.1467-8659.2012.03125.x", "10.1111/cgf.12127", "10.1109/iv.2008.24", "10.1016/j.cag.2010.11.015", "10.1109/tvcg.2008.118", "10.1109/tvcg.2012.315", "10.1016/0377-0427(87)90125-7", "10.1109/tvcg.2017.2744184", "10.1109/cvpr.2012.6247743", "10.1111/j.1467-8659.2009.01359.x", "10.1049/cp:19951045", "10.1109/tvcg.2013.153", "10.1016/0042-6989(64)90037-9", "10.1002/col.20070", "10.1109/tvcg.2013.183", "10.1111/cgf.12499", "10.1145/1842993.1843034", "10.1111/cgf.12632", "10.1109/34.990132"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934541", "title": "A Recursive Subdivision Technique for Sampling Multi-class Scatterplots", "year": "2019", "conferenceName": "InfoVis", "authors": "Xin Chen;Tong Ge;Jian Zhang 0070;Baoquan Chen;Chi-Wing Fu;Oliver Deussen;Yunhai Wang", "citationCount": "5", "affiliation": "Chen, X (Corresponding Author), Shandong Univ, Jinan, Shandong, Peoples R China. Chen, Xin; Ge, Tong; Wang, Yunhai, Shandong Univ, Jinan, Shandong, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China. Fu, Chi-Wing, SIAT, Guangdong Prov Key Lab CV \\& VR Tech, Shenzhen, Guangdong, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Deussen, Oliver, Konstanz Univ, Constance, Germany. Deussen, Oliver, SIAT, Shenzhen VIsuCA Key Lab, Shenzhen, Guangdong, Peoples R China.", "countries": "Germany;China", "abstract": "We present a non-uniform recursive sampling technique for multi-class scatterplots, with the specific goal of faithfully presenting relative data and class densities, while preserving major outliers in the plots. Our technique is based on a customized binary kd-tree, in which leaf nodes are created by recursively subdividing the underlying multi-class density map. By backtracking, we merge leaf nodes until they encompass points of all classes for our subsequently applied outlier-aware multi-class sampling strategy. A quantitative evaluation shows that our approach can better preserve outliers and at the same time relative densities in multi-class scatterplots compared to the previous approaches, several case studies demonstrate the effectiveness of our approach in exploring complex and real world data.", "keywords": "Scatterplot,multi-class sampling,kd-tree,outlier,relative density", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934541", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2006.170", "10.1109/tvcg.2008.119", "10.1109/tvcg.2007.70580", "10.2307/2289444", "10.1145/1964921.1964943", "10.1109/tvcg.2013.65", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/itoec.2018.8740621", "10.1145/1778765.1778816", "10.1109/tvcg.2010.197", "10.1201/b17511", "10.1109/tvcg.2018.2864912", "10.1007/0-387-28695-0", "10.1109/5.726791", "10.1109/visual.1998.745301", "10.1016/j.physa.2011.12.004", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2004.1272729", "10.1145/335191.335388", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1109/iv.2002.1028760", "10.1145/1842993.1842999", "10.1109/tvcg.2018.2869149", "10.1038/nmeth.2490", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2010.176", "10.1145/2702123.2702585", "10.1057/ivs.2009.34"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14001", "year": "2020", "title": "Sunspot Plots: Model-based Structure Enhancement for Dense Scatter Plots", "conferenceName": "EuroVis", "authors": "Thomas Trautner;Fabian Bolte;Sergej Stoppel;Stefan Bruckner", "citationCount": "0", "affiliation": "Trautner, T (Corresponding Author), Univ Bergen, Bergen, Norway.\nTrautner, T.; Bolte, F.; Stoppel, S.; Bruckner, S., Univ Bergen, Bergen, Norway.", "countries": "Norway", "abstract": "Scatter plots are a powerful and well-established technique for visualizing the relationships between two variables as a collection of discrete points. However, especially when dealing with large and dense data, scatter plots often exhibit problems such as overplotting, making the data interpretation arduous. Density plots are able to overcome these limitations in highly populated regions, but fail to provide accurate information of individual data points. This is particularly problematic in sparse regions where the density estimate may not provide a good representation of the underlying data. In this paper, we present sunspot plots, a visualization technique that communicates dense data as a continuous data distribution, while preserving the discrete nature of data samples in sparsely populated areas. We furthermore demonstrate the advantages of our approach on typical failure cases of scatter plots within synthetic and real-world data sets and validate its effectiveness in a user study.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14001", "refList": ["10.1057/palgrave.ivs.9500122", "10.2312/eggh/hpg12/097-103", "10.1109/tvcg.2008.119", "10.1109/pacificvis.2011.5742387", "10.1038/331163a0", "10.1109/visual.2019.8933620", "10.2307/2683294", "10.1201/9781351072304", "10.2307/2289444", "10.1109/tvcg.2019.2934541", "10.1080/00949657508810123", "10.1109/tvcg.2013.65", "10.1109/infvis.1997.636789", "10.1109/2945.841121", "10.1109/mcse.2007.55", "10.1109/tvcg.2010.197", "10.1111/cgf.12871", "10.1109/infvis.2003.1249018", "10.1145/3173574.3173991", "10.1109/tvcg.2012.238", "10.1007/0-387-28695-0", "10.1109/pacificvis.2010.5429604", "10.18637/jss.v008.i03", "10.1109/tvcg.2007.70596", "10.1109/visual.1998.745301", "10.1007/0-387-37977-0\\_3", "10.1145/1556262.1556289", "10.2312/wiced.20161094", "10.1109/tvcg.2007.70535", "10.1145/1056808.1056914", "10.1109/tvcg.2003.1196007", "10.1109/iv.2004.1320190", "10.1111/cgf.12877", "10.1162/leon.2007.40.2.202a", "10.1109/tvcg.2017.2674978", "10.1057/ivs.2010.4", "10.1016/j.cag.2018.02.008", "10.1201/9781315140919", "10.1109/visual.2000.885677", "10.1002/jhbs.20078", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1109/visual.2001.964495", "10.1109/iv.2002.1028760", "10.1111/cgf.13684", "10.1109/hicss.2013.197", "10.1109/tvcg.2019.2903956", "10.1093/mnras/stt961", "10.1109/tvcg.2017.2668409", "10.1111/j.1467-8659.2009.01478.x", "10.1145/2702123.2702585", "10.2307/1418003", "10.2307/1390742", "10.1145/360825.360839", "10.1109/pacificvis.2009.4906843", "10.1057/ivs.2009.34", "10.2307/2288711"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934799", "title": "Data Sampling in Multi-view and Multi-class Scatterplots via Set Cover Optimization", "year": "2019", "conferenceName": "InfoVis", "authors": "Ruizhen Hu;Tingkai Sha;Oliver van Kaick;Oliver Deussen;Hui Huang 0004", "citationCount": "4", "affiliation": "Hu, RZ (Corresponding Author), Shenzhen Univ, Visual Comp Res Ctr, Shenzhen, Guangdong, Peoples R China. Hu, Ruizhen; Sha, Tingkai; Huang, Hui, Shenzhen Univ, Visual Comp Res Ctr, Shenzhen, Guangdong, Peoples R China. van Kaick, Oliver, Carleton Univ, Sch Comp Sci, Ottawa, ON, Canada. Deussen, Oliver, Konstanz Univ, Constance, Germany. Deussen, Oliver, SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Guangdong, Peoples R China.", "countries": "Canada;Germany;China", "abstract": "We present a method for data sampling in scatterplots by jointly optimizing point selection for different views or classes. Our method uses space-filling curves (Z-order curves) that partition a point set into subsets that, when covered each by one sample, provide a sampling or coreset with good approximation guarantees in relation to the original point set. For scatterplot matrices with multiple views, different views provide different space-filling curves, leading to different partitions of the given point set. For multi-class scatterplots, the focus on either per-class distribution or global distribution provides two different partitions of the given point set that need to be considered in the selection of the coreset. For both cases, we convert the coreset selection problem into an Exact Cover Problem (ECP), and demonstrate with quantitative and qualitative evaluations that an approximate solution that solves the ECP efficiently is able to provide high-quality samplings.", "keywords": "Sampling,Scatterplot,SPLOM,Exact Cover Problem", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934799", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2006.170", "10.1109/tvcg.2008.119", "10.1111/j.1467-8659.2009.01467.x", "10.2307/2289444", "10.1109/tvcg.2013.65", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/itoec.2018.8740621", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2864912", "10.1007/0-387-28695-0", "10.1109/visual.1998.745301", "10.1287/moor.4.3.233", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.2307/2284239", "10.1109/tvcg.2017.2674978", "10.1016/j.dss.2009.05.016", "10.1109/tvcg.2014.2346594", "10.1109/vds.2017.8573446", "10.1007/978-1-4684-2001-2\\_9", "10.1109/iv.2002.1028760", "10.1145/1842993.1842999", "10.1038/nmeth.2490", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2010.176", "10.1145/2702123.2702585", "10.1057/ivs.2009.34", "10.1179/000870403235002042"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934208", "title": "Evaluating Perceptual Bias During Geometric Scaling of Scatterplots", "year": "2019", "conferenceName": "VAST", "authors": "Yating Wei;Honghui Mei;Ying Zhao;Shuyue Zhou;Bingru Lin;Haojing Jiang;Wei Chen", "citationCount": "5", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou 310058, Zhejiang, Peoples R China. Zhao, Y (Corresponding Author), Cent South Univ, Sch Comp Sci \\& Engn, Changsha 410083, Hunan, Peoples R China. Wei, Yating; Mei, Honghui; Zhou, Shuyue; Lin, Bingru; Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou 310058, Zhejiang, Peoples R China. Zhao, Ying; Jiang, Haojing, Cent South Univ, Sch Comp Sci \\& Engn, Changsha 410083, Hunan, Peoples R China.", "countries": "China", "abstract": "Scatterplots are frequently scaled to fit display areas in multi-view and multi-device data analysis environments. A common method used for scaling is to enlarge or shrink the entire scatterplot together with the inside points synchronously and proportionally. This process is called geometric scaling. However, geometric scaling of scatterplots may cause a perceptual bias, that is, the perceived and physical values of visual features may be dissociated with respect to geometric scaling. For example, if a scatterplot is projected from a laptop to a large projector screen, then observers may feel that the scatterplot shown on the projector has fewer points than that viewed on the laptop. This paper presents an evaluation study on the perceptual bias of visual features in scatterplots caused by geometric scaling. The study focuses on three fundamental visual features (i.e., numerosity, correlation, and cluster separation) and three hypotheses that are formulated on the basis of our experience. We carefully design three controlled experiments by using well-prepared synthetic data and recruit participants to complete the experiments on the basis of their subjective experience. With a detailed analysis of the experimental results, we obtain a set of instructive findings. First, geometric scaling causes a bias that has a linear relationship with the scale ratio. Second, no significant difference exists between the biases measured from normally and uniformly distributed scatterplots. Third, changing the point radius can correct the bias to a certain extent. These findings can be used to inspire the design decisions of scatterplots in various scenarios.", "keywords": "Evaluation,scatterplot,geometric scaling,bias,perceptual consistency", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934208", "refList": ["10.2307/2288843", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2014.2346979", "10.1126/science.216.4550.1138", "10.1109/tvcg.2017.2744138", "10.1111/j.1467-8659.2009.01467.x", "10.1145/2491568.2491577", "10.1109/mwc.2018.1700325", "10.1145/2449396.2449439", "10.1109/tvcg.2011.127", "10.1109/tvcg.2011.229", "10.1167/15.5.4", "10.1073/pnas.1113195108", "10.1145/2702123.2702545", "10.1109/vast.2009.5332628", "10.1109/tvcg.2018.2800013", "10.1007/s12650-018-0530-2", "10.1016/s0042-6989(97)00340-4", "10.1109/vast.2010.5652460", "10.1109/mcg.2018.2879067", "10.1109/tvcg.2018.2864912", "10.1109/pacificvis.2010.5429604", "10.1109/tcst.2018.2819965", "10.1167/10.2.10", "10.1145/2470654.2481318", "10.1145/1842993.1843002", "10.1167/12.6.8", "10.1109/tvcg.2013.124", "10.1057/ivs.2008.13", "10.1109/tvcg.2007.70596", "10.1109/tvcg.2018.2865266", "10.1145/3173574.3173664", "10.1109/tvcg.2015.2467671", "10.1016/j.cag.2017.07.004", "10.1177/0956797613501520", "10.1109/tvcg.2018.2865020", "10.1111/j.1467-8659.2012.03125.x", "10.3758/bf03205986", "10.3758/s13423-016-1174-7", "10.1109/tvcg.2017.2680452", "10.1109/mc.2006.109", "10.1109/tvcg.2017.2744098", "10.1038/srep32810", "10.1016/j.visres.2013.06.006", "10.1002/jhbs.20078", "10.1109/tvcg.2006.163", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1016/j.cognition.2007.10.009", "10.1109/tvcg.2018.2865142", "10.3758/app.72.7.1839", "10.1109/tvcg.2018.2810918", "10.1016/j.jvlc.2017.10.001", "10.1145/2682623", "10.1109/tvcg.2018.2864884", "10.1145/3025453.3025984", "10.1109/tvcg.2006.184", "10.1109/tvcg.2013.153", "10.1016/j.jvlc.2018.08.003", "10.1109/tvcg.2016.2520921", "10.1111/cgf.13446", "10.1017/s0022381612000187", "10.1145/2702123.2702406", "10.1109/vast.2012.6400487", "10.1109/tvcg.2013.183", "10.1177/1473871611415997", "10.1145/2702123.2702585", "10.1145/2993901.2993903", "10.1109/tvcg.2013.120", "10.1111/cgf.12632", "10.1145/1385569.1385602", "10.1109/tvcg.2017.2754480", "10.1111/j.1467-8659.2009.01694.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030360", "title": "Gemini: A Grammar and Recommender System for Animated Transitions in Statistical Graphics", "year": "2020", "conferenceName": "InfoVis", "authors": "Younghoon Kim;Jeffrey Heer", "citationCount": "0", "affiliation": "Kim, Y (Corresponding Author), Univ Washington, Seattle, WA 98195 USA. Kim, Younghoon; Heer, Jeffrey, Univ Washington, Seattle, WA 98195 USA.", "countries": "USA", "abstract": "Animated transitions help viewers follow changes between related visualizations. Specifying effective animations demands significant effort: authors must select the elements and properties to animate, provide transition parameters, and coordinate the timing of stages. To facilitate this process, we present Gemini, a declarative grammar and recommendation system for animated transitions between single-view statistical graphics. Gemini specifications define transition \u201csteps\u201d in terms of high-level visual components (marks, axes, legends) and composition rules to synchronize and concatenate steps. With this grammar, Gemini can recommend animation designs to augment and accelerate designers' work. Gemini enumerates staged animation designs for given start and end states, and ranks those designs using a cost function informed by prior perceptual studies. To evaluate Gemini, we conduct both a formative study on Mechanical Turk to assess and tune our ranking function, and a summative study in which 8 experienced visualization developers implement animations in D3 that we then compare to Gemini's suggestions. We find that most designs (9/11) are exactly replicable in Gemini, with many (8/11) achievable via edits to suggestions, and that Gemini suggestions avoid multiple participant errors.", "keywords": "Animated transition,animation,transition,declarative grammar,automated design,charts", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030360", "refList": ["10.1109/tvcg.2015.2489649", "10.1109/tvcg.2016.2598918", "10.1109/tvcg.2015.2467471", "10.2312/eurovisshort.20151135", "10.1016/j.csda.2008.11.033", "10.1109/visual.1996.568118", "10.1016/0925-7721(92)90003-b", "10.1109/tvcg.2018.2864912", "10.1109/pacificvis.2016.7465244", "10.1145/2254556.2254572", "10.1109/tvcg.2016.2599214", "10.1111/j.1467-8659.2012.03125.x", "10.1111/cgf.12127", "10.1109/iv.2008.24", "10.1145/3025453.3026041", "10.1109/tvcg.2012.315", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2008.118", "10.1109/tvcg.2014.2346594", "10.1080/15230406.2016.1140074", "10.1111/j.1467-8659.2009.01359.x", "10.1126/science.185.4157.1124", "10.1109/tvcg.2010.162", "10.1109/tvcg.2009.113", "10.1109/pacificvis.2016.7465242", "10.1002/col.20070", "10.1126/science.220.4598.671", "10.1109/tvcg.2019.2961674", "10.1111/j.1467-8659.2008.01203.x", "10.1109/tvcg.2013.183", "10.1179/000870403235002042"], "wos": 1, "children": [], "len": 1}], "len": 33}, {"doi": "10.1109/tvcg.2018.2865141", "title": "A Declarative Rendering Model for Multiclass Density Maps", "year": "2018", "conferenceName": "InfoVis", "authors": "Jaemin Jo;Fr\u00e9d\u00e9ric Vernier;Pierre Dragicevic;Jean-Daniel Fekete", "citationCount": "9", "affiliation": "Jo, J (Corresponding Author), Seoul Natl Univ, Seoul, South Korea. Jo, Jaemin, Seoul Natl Univ, Seoul, South Korea. Vernier, Frederic, Univ Paris Saclay, Univ Paris Sud, CNRS, LIMSI, Paris, France. Dragicevic, Pierre; Fekete, Jean-Daniel, INRIA, Rocquencourt, France.", "countries": "Korea;France", "abstract": "Multiclass maps are scatterplots, multidimensional projections, or thematic geographic maps where data points have a categorical attribute in addition to two quantitative attributes. This categorical attribute is often rendered using shape or color, which does not scale when overplotting occurs. When the number of data points increases, multiclass maps must resort to data aggregation to remain readable. We present multiclass density maps: multiple 2D histograms computed for each of the category values. Multiclass density maps are meant as a building block to improve the expressiveness and scalability of multiclass map visualization. In this article, we first present a short survey of aggregated multiclass maps, mainly from cartography. We then introduce a declarative model-a simple yet expressive JSON grammar associated with visual semantics-that specifies a wide design space of visualizations for multiclass density maps. Our declarative model is expressive and can be efficiently implemented in visualization front-ends such as modern web browsers. Furthermore, it can be reconfigured dynamically to support data exploration tasks without recomputing the raw data. Finally, we demonstrate how our model can be used to reproduce examples from the past and support exploring data at scale.", "keywords": "Scalability,multiclass scatterplots,density maps,aggregation,declarative specification,visualization grammar", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865141", "refList": ["10.1057/palgrave.ivs.9500122", "10.2307/2980460", "10.2307/2683294", "10.1111/cgf.12129", "10.1109/tvcg.2013.65", "10.1109/tvcg.2011.185", "10.1111/cgf.12878", "10.1109/tvcg.2009.175", "10.1145/2669557.2669559", "10.1145/3173574.3173991", "10.1109/pacificvis.2016.7465244", "10.1109/infvis.2002.1173156", "10.1117/12.2041200", "10.1179/000870403235002042", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2013.130", "10.1145/2872427.2883041", "10.1145/2669557.2669578", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2013.179", "10.1109/tvcg.2011.197", "10.1145/1476589.1476628", "10.1559/152304000783547786", "10.1037/xhp0000255", "10.1080/10618600.1994.10474656", "10.1145/2702123.2702585", "10.3138/cart.52.4.2016-0007", "10.1057/ivs.2009.34", "10.1109/tvcg.2007.70623"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934807", "title": "A Comparison of Visualizations for Identifying Correlation over Space and Time", "year": "2019", "conferenceName": "InfoVis", "authors": "Vanessa Pe\u00f1a Araya;Emmanuel Pietriga;Anastasia Bezerianos", "citationCount": "0", "affiliation": "Pena-Araya, V (Corresponding Author), Univ Paris Saclay, Univ Paris Sud, INRIA, CNRS, Paris, France. Pena-Araya, Vanessa; Pietriga, Emmanuel; Bezerianos, Anastasia, Univ Paris Saclay, Univ Paris Sud, INRIA, CNRS, Paris, France.", "countries": "France", "abstract": "Observing the relationship between two or more variables over space and time is essential in many domains. For instance, looking, for different countries, at the evolution of both the life expectancy at birth and the fertility rate will give an overview of their demographics. The choice of visual representation for such multivariate data is key to enabling analysts to extract patterns and trends. Prior work has compared geo-temporal visualization techniques for a single thematic variable that evolves over space and time, or for two variables at a specific point in time. But how effective visualization techniques are at communicating correlation between two variables that evolve over space and time remains to be investigated. We report on a study comparing three techniques that are representative of different strategies to visualize geo-temporal multivariate data: either juxtaposing all locations for a given time step, or juxtaposing all time steps for a given location; and encoding thematic attributes either using symbols overlaid on top of map features, or using visual channels of the map features themselves. Participants performed a series of tasks that required them to identify if two variables were correlated over time and if there was a pattern in their evolution. Tasks varied in granularity for both dimensions: time (all time steps, a subrange of steps, one step only) and space (all locations, locations in a subregion, one location only). Our results show that a visualization's effectiveness depends strongly on the task to be carried out. Based on these findings we present a set of design guidelines about geo-temporal visualization techniques for communicating correlation.", "keywords": "geo-temporal data,bivariate maps,correlation,controlled study,bar chart,Dorling cartogram,small multiples", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934807", "refList": ["10.1111/j.1467-9671.2010.01194.x", "10.3390/ijgi2030817", "10.1109/tvcg.2018.2865141", "10.3390/ijgi7080288", "10.1080/00045608.2010.485449", "10.1109/pacificvis.2014.13", "10.1109/tvcg.2014.2346979", "10.1080/00045608.2015.1064510", "10.1111/cgf.12647", "10.1111/j.1467-8659.2009.01694.x", "10.1109/mcg.2003.1242376", "10.1080/09585192.2016.1278253", "10.1177/0956797613504966", "10.1109/tvcg.2011.185", "10.1006/ijhc.2002.1017", "10.1111/j.1467-8306.1994.tb01869.x", "10.1080/00045608.2011.577364", "10.1109/tvcg.2007.70623", "10.1111/cgf.12932", "10.1109/iv.2014.69", "10.1006/ijhc.1017", "10.1109/iv.2004.1320136", "10.1145/2801040.2801062", "10.1007/s00371-017-1461-y", "10.1111/j.1467-8659.2012.03093.x", "10.1109/tvcg.2013.66", "10.1007/s00779-018-1120-y", "10.1109/tvcg.2015.2467199", "10.1109/tvcg.2016.2642109", "10.1109/tvcg.2015.2467671", "10.1109/tvcg.2016.2598862", "10.1109/tvcg.2011.194", "10.3758/s13423-016-1174-7", "10.1109/tvcg.2017.2765330", "10.1109/iv.2004.1320137", "10.1038/nmeth.2659", "10.2312/cgvc.20181221", "10.1109/tvcg.2013.130", "10.3138/carto.42.4.349", "10.1111/j.1467-8306.2006.00514.x", "10.1117/12.912192", "10.1109/tvcg.2015.2467091", "10.1109/iv.2005.3", "10.1559/15230406384350", "10.1145/2909132.2909255", "10.1109/iv.2018.00056", "10.1109/tvcg.2008.125", "10.1109/2945.537309", "10.1109/tvcg.2018.2810918", "10.3390/ijgi6060180", "10.1145/3025453.3025801", "10.1080/14498596.2018.1440649", "10.1179/000870409x12525737905169", "10.2307/2284077", "10.1109/tvcg.2006.84", "10.1002/9780470979587.ch33", "10.1007/978-3-319-26633-6\\_13", "10.1136/bmj.316.7139.1236", "10.1145/2801040.2801061", "10.1037/0003-066x.60.2.170", "10.1145/989863.989940"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934811", "title": "Winglets: Visualizing Association with Uncertainty in Multi-class Scatterplots", "year": "2019", "conferenceName": "InfoVis", "authors": "Min Lu;Shuaiqi Wang;Joel Lanir;Noa Fish;Yang Yue;Daniel Cohen-Or;Hui Huang 0004", "citationCount": "1", "affiliation": "Huang, H (Corresponding Author), Shenzhen Univ, Shenzhen, Peoples R China. Lu, Min; Wang, Shuaiqi; Yue, Yang; Cohen-Or, Daniel; Huang, Hui, Shenzhen Univ, Shenzhen, Peoples R China. Lanir, Joel, Univ Haifa, Haifa, Israel. Fish, Noa, Tel Aviv Univ, Tel Aviv, Israel.", "countries": "Israel;China", "abstract": "This work proposes Winglets, an enhancement to the classic scatterplot to better perceptually pronounce multiple classes by improving the perception of association and uncertainty of points to their related cluster. Designed as a pair of dual-sided strokes belonging to a data point, Winglets leverage the Gestalt principle of Closure to shape the perception of the form of the clusters, rather than use an explicit divisive encoding. Through a subtle design of two dominant attributes, length and orientation, Winglets enable viewers to perform a mental completion of the clusters. A controlled user study was conducted to examine the efficiency of Winglets in perceiving the cluster association and the uncertainty of certain points. The results show Winglets form a more prominent association of points into clusters and improve the perception of associating uncertainty.", "keywords": "Scatterplot,Gestalt laws,Association,Uncertainty", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934811", "refList": ["10.1109/tvcg.2018.2865141", "10.1214/aoms/1177728190", "10.1111/j.1467-8659.2009.01467.x", "10.2307/2289444", "10.1109/tvcg.2013.65", "10.1037/a0029334", "10.1109/vast.2009.5332628", "10.1007/bf00410640", "10.1109/vast.2010.5652460", "10.1145/2590349", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2013.20", "10.1002/9781118575574", "10.2312/conf/eg2013/stars/039-063", "10.1109/tvcg.2009.122", "10.1109/tvcg.2017.2750689", "10.1023/a:1024454423670", "10.1515/secm-2017-0114", "10.1016/0377-0427(87)90125-7", "10.1016/0010-0285(92)90014-s", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1016/b978-044481862-1.50075-3", "10.1007/s10851-006-9176-0", "10.1145/1842993.1842999", "10.1145/37402.37422", "10.1037/0033-295x.115.1.131", "10.1109/tvcg.2013.153", "10.1007/978-3-642-04898-2\\_455", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1559/152304003100010929", "10.1057/ivs.2009.34"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030373", "title": "Multi-Perspective, Simultaneous Embedding", "year": "2020", "conferenceName": "InfoVis", "authors": "Md. Iqbal Hossain;Vahan Huroyan;Stephen G. Kobourov;Raymundo Navarrete", "citationCount": "0", "affiliation": "Hossain, MI (Corresponding Author), Univ Arizona, Dept Comp Sci, Tucson, AZ 85721 USA. Hossain, Md Iqbal; Kobourov, Stephen, Univ Arizona, Dept Comp Sci, Tucson, AZ 85721 USA. Huroyan, Vahan; Navarrete, Raymundo, Univ Arizona, Dept Math, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "We describe MPSE: a Multi-Perspective Simultaneous Embedding method for visualizing high-dimensional data, based on multiple pairwise distances between the data points. Specifically, MPSE computes positions for the points in 3D and provides different views into the data by means of 2D projections (planes) that preserve each of the given distance matrices. We consider two versions of the problem: fixed projections and variable projections. MPSE with fixed projections takes as input a set of pairwise distance matrices defined on the data points, along with the same number of projections and embeds the points in 3D so that the pairwise distances are preserved in the given projections. MPSE with variable projections takes as input a set of pairwise distance matrices and embeds the points in 3D while also computing the appropriate projections that preserve the pairwise distances. The proposed approach can be useful in multiple scenarios: from creating simultaneous embedding of multiple graphs on the same set of vertices, to reconstructing a 3D object from multiple 2D snapshots, to analyzing data from multiple points of view. We provide a functional prototype of MPSE that is based on an adaptive and stochastic generalization of multi-dimensional scaling to multiple distances and multiple variable projections. We provide an extensive quantitative evaluation with datasets of different sizes and using different number of projections, as well as several examples that illustrate the quality of the resulting solutions.", "keywords": "Graph visualization,Dimensionality reduction,Multidimensional scaling,Mental map preservation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030373", "refList": ["10.1016/j.cag.2014.01.006", "10.1109/tvcg.2007.70443", "10.1007/978-3-642-04843-2\\_36", "10.1111/cgf.12639", "10.1109/tvcg.2019.2944182", "10.1109/tvcg.2019.2934811", "10.1007/978-1-4757-2261-1", "10.1145/3173574.3174209", "10.1016/j.neucom.2006.11.018", "10.1109/tpami.2013.57", "10.2307/2334381", "10.1109/tpami.2012.88", "10.1109/tvcg.2017.2661309", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1111/j.1467-8659.2010.01835.x", "10.1109/vl.1996.545307", "10.1109/iv.2009.25", "10.1109/msp.2010.939739", "10.1063/1.3067728", "10.2312/pe/eurovast/eurovast10/013-018", "10.1109/vast.2010.5652460", "10.25365/thesis.51534", "10.21105/joss.00861", "10.1145/1007730.1007731", "10.1109/tvcg.2013.20", "10.1109/tvcg.2011.220", "10.1037/h0071325", "10.1109/vast.2012.6400488", "10.1109/tvcg.2007.70535", "10.1198/jasa.2009.0111", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2017.2698041", "10.1109/tvcg.2015.2467717", "10.1111/j", "10.1016/j.neucom.2014.07.071", "10.1126/science.290.5500.2323", "10.1109/tvcg.2013.153", "10.2312/pe/eurovast", "10.1109/tvcg.2018.2846735", "10.1007/bf02289530", "10.1111/j.1745-3984.2003.tb01108.x", "10.1177/1473871615600010", "10.5555/3053814.3053816", "10.1109/tvcg.2018.2865194"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2020.3030430", "title": "Competing Models: Inferring Exploration Patterns and Information Relevance via Bayesian Model Selection", "year": "2020", "conferenceName": "VAST", "authors": "Shayan Monadjemi;Roman Garnett;Alvitta Ottley", "citationCount": "0", "affiliation": "Monadjemi, S (Corresponding Author), Washington Univ, St Louis, MO 14263 USA. Monadjemi, Shayan; Garnett, Roman; Ottley, Alvitta, Washington Univ, St Louis, MO 14263 USA.", "countries": "USA", "abstract": "Analyzing interaction data provides an opportunity to learn about users, uncover their underlying goals, and create intelligent visualization systems. The first step for intelligent response in visualizations is to enable computers to infer user goals and strategies through observing their interactions with a system. Researchers have proposed multiple techniques to model users, however, their frameworks often depend on the visualization design, interaction space, and dataset. Due to these dependencies, many techniques do not provide a general algorithmic solution to user exploration modeling. In this paper, we construct a series of models based on the dataset and pose user exploration modeling as a Bayesian model selection problem where we maintain a belief over numerous competing models that could explain user interactions. Each of these competing models represent an exploration strategy the user could adopt during a session. The goal of our technique is to make high-level and in-depth inferences about the user by observing their low-level interactions. Although our proposed idea is applicable to various probabilistic model spaces, we demonstrate a specific instance of encoding exploration patterns as competing models to infer information relevance. We validate our technique's ability to infer exploration bias, predict future interactions, and summarize an analytic session using user study datasets. Our results indicate that depending on the application, our method outperforms established baselines for bias detection and future interaction prediction. Finally, we discuss future research directions based on our proposed modeling paradigm and suggest how practitioners can use this method to build intelligent visualization systems that understand users' goals and adapt to improve the exploration process.", "keywords": "User Interaction Modeling,Bayesian Machine Learning", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030430", "refList": ["10.1109/tvcg.2018.2865141", "10.1145/355588.365140", "10.1007/978-0-387-98141-3\\_1", "10.1145/2882903.2882919", "10.1111/cgf.13708", "10.1145/3183713.3183738", "10.1111/cgf.12129", "10.1109/tvcg.2014.2346452", "10.1109/tvcg.2013.65", "10.1109/tvcg.2011.185", "10.1109/infvis.2003.1249018", "10.1109/tvcg.2017.2671341", "10.1109/infvis.2002.1173156", "10.1109/38.974515", "10.1109/tvcg.2016.2598624", "10.1109/infvis.2003.1249019", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2006.161", "10.1109/ldav.2015.7348077", "10.1109/iv.2004.1320190", "10.1145/3139958.3140037", "10.1109/vlhcc.2005.11", "10.1145/98524.98564", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2009.84", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2013.179", "10.1016/s0097-8493(02)00283-2", "10.14778/2428536.2428538", "10.1109/icde.2014.6816720", "10.1109/tvcg.2017.2785807", "10.1198/jcgs.2009.07098", "10.1109/tvcg.2017.2754480"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2018.2865266", "title": "Image-Based Aspect Ratio Selection", "year": "2018", "conferenceName": "InfoVis", "authors": "Yunhai Wang;Zeyu Wang 0005;Chi-Wing Fu;Hansj\u00f6rg Schmauder;Oliver Deussen;Daniel Weiskopf", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Jinan, Shandong, Peoples R China. Wang, Yunhai; Wang, Zeyu, Shandong Univ, Jinan, Shandong, Peoples R China. Wang, Zeyu; Deussen, Oliver, SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Peoples R China. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China. Deussen, Oliver, Konstanz Univ, Constance, Germany. Schmauder, Hansjorq; Weiskopf, Daniel, Univ Stuttgart, VISUS, Stuttgart, Germany.", "countries": "Germany;China", "abstract": "Selecting a good aspect ratio is crucial for effective 2D diagrams. There are several aspect ratio selection methods for function plots and line charts, but only few can handle general, discrete diagrams such as 2D scatter plots. However, these methods either lack a perceptual foundation or heavily rely on intermediate isoline representations, which depend on choosing the right isovalues and are time-consuming to compute. This paper introduces a general image-based approach for selecting aspect ratios for a wide variety of 2D diagrams, ranging from scatter plots and density function plots to line charts. Our approach is derived from Federer's co-area formula and a line integral representation that enable us to directly construct image-based versions of existing selection methods using density fields. In contrast to previous methods, our approach bypasses isoline computation, so it is faster to compute, while following the perceptual foundation to select aspect ratios. Furthermore, this approach is complemented by an anisotropic kernel density estimation to construct density fields, allowing us to more faithfully characterize data patterns, such as the subgroups in scatterplots or dense regions in time series. We demonstrate the effectiveness of our approach by quantitatively comparing to previous methods and revisiting a prior user study. Finally, we present extensions for ROI banking, multi-scale banking, and the application to image data.", "keywords": "Aspect ratio,image-based method,Federer's co-area formula,density field,anisotropic kernel density estimation", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865266", "refList": ["10.2307/2288843", "10.1109/tvcg.2012.196", "10.1109/tvcg.2010.146", "10.1109/tvcg.2008.119", "10.1167/3.9.11", "10.1163/156856895x00098", "10.1109/tvcg.2017.2653106", "10.1109/tvcg.2013.187", "10.1145/2094114.2094126", "10.1109/vast.2009.5332628", "10.1111/j.1467-8659.2011.01912.x", "10.1145/22949.22950", "10.1111/cgf.12117", "10.1109/tvcg.2011.167", "10.1016/j.visres.2008.03.006", "10.1109/tvcg.2017.2787113", "10.1109/tvcg.2006.168", "10.2307/1390686", "10.1090/s0002-9947-1959-0110078-1", "10.1111/j.1467-8659.2012.03125.x", "10.1137/0802003", "10.1109/infvis.2005.1532142", "10.1109/tvcg.2012.118", "10.1109/tvcg.2009.131", "10.1109/tvcg.2006.163", "10.1109/tvcg.2017.2744184", "10.1111/j.1467-8659.2012.03122.x", "10.1109/tvcg.2008.160", "10.1145/1073204.1073241", "10.1038/358600a0"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934208", "title": "Evaluating Perceptual Bias During Geometric Scaling of Scatterplots", "year": "2019", "conferenceName": "VAST", "authors": "Yating Wei;Honghui Mei;Ying Zhao;Shuyue Zhou;Bingru Lin;Haojing Jiang;Wei Chen", "citationCount": "5", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou 310058, Zhejiang, Peoples R China. Zhao, Y (Corresponding Author), Cent South Univ, Sch Comp Sci \\& Engn, Changsha 410083, Hunan, Peoples R China. Wei, Yating; Mei, Honghui; Zhou, Shuyue; Lin, Bingru; Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou 310058, Zhejiang, Peoples R China. Zhao, Ying; Jiang, Haojing, Cent South Univ, Sch Comp Sci \\& Engn, Changsha 410083, Hunan, Peoples R China.", "countries": "China", "abstract": "Scatterplots are frequently scaled to fit display areas in multi-view and multi-device data analysis environments. A common method used for scaling is to enlarge or shrink the entire scatterplot together with the inside points synchronously and proportionally. This process is called geometric scaling. However, geometric scaling of scatterplots may cause a perceptual bias, that is, the perceived and physical values of visual features may be dissociated with respect to geometric scaling. For example, if a scatterplot is projected from a laptop to a large projector screen, then observers may feel that the scatterplot shown on the projector has fewer points than that viewed on the laptop. This paper presents an evaluation study on the perceptual bias of visual features in scatterplots caused by geometric scaling. The study focuses on three fundamental visual features (i.e., numerosity, correlation, and cluster separation) and three hypotheses that are formulated on the basis of our experience. We carefully design three controlled experiments by using well-prepared synthetic data and recruit participants to complete the experiments on the basis of their subjective experience. With a detailed analysis of the experimental results, we obtain a set of instructive findings. First, geometric scaling causes a bias that has a linear relationship with the scale ratio. Second, no significant difference exists between the biases measured from normally and uniformly distributed scatterplots. Third, changing the point radius can correct the bias to a certain extent. These findings can be used to inspire the design decisions of scatterplots in various scenarios.", "keywords": "Evaluation,scatterplot,geometric scaling,bias,perceptual consistency", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934208", "refList": ["10.2307/2288843", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2014.2346979", "10.1126/science.216.4550.1138", "10.1109/tvcg.2017.2744138", "10.1111/j.1467-8659.2009.01467.x", "10.1145/2491568.2491577", "10.1109/mwc.2018.1700325", "10.1145/2449396.2449439", "10.1109/tvcg.2011.127", "10.1109/tvcg.2011.229", "10.1167/15.5.4", "10.1073/pnas.1113195108", "10.1145/2702123.2702545", "10.1109/vast.2009.5332628", "10.1109/tvcg.2018.2800013", "10.1007/s12650-018-0530-2", "10.1016/s0042-6989(97)00340-4", "10.1109/vast.2010.5652460", "10.1109/mcg.2018.2879067", "10.1109/tvcg.2018.2864912", "10.1109/pacificvis.2010.5429604", "10.1109/tcst.2018.2819965", "10.1167/10.2.10", "10.1145/2470654.2481318", "10.1145/1842993.1843002", "10.1167/12.6.8", "10.1109/tvcg.2013.124", "10.1057/ivs.2008.13", "10.1109/tvcg.2007.70596", "10.1109/tvcg.2018.2865266", "10.1145/3173574.3173664", "10.1109/tvcg.2015.2467671", "10.1016/j.cag.2017.07.004", "10.1177/0956797613501520", "10.1109/tvcg.2018.2865020", "10.1111/j.1467-8659.2012.03125.x", "10.3758/bf03205986", "10.3758/s13423-016-1174-7", "10.1109/tvcg.2017.2680452", "10.1109/mc.2006.109", "10.1109/tvcg.2017.2744098", "10.1038/srep32810", "10.1016/j.visres.2013.06.006", "10.1002/jhbs.20078", "10.1109/tvcg.2006.163", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1016/j.cognition.2007.10.009", "10.1109/tvcg.2018.2865142", "10.3758/app.72.7.1839", "10.1109/tvcg.2018.2810918", "10.1016/j.jvlc.2017.10.001", "10.1145/2682623", "10.1109/tvcg.2018.2864884", "10.1145/3025453.3025984", "10.1109/tvcg.2006.184", "10.1109/tvcg.2013.153", "10.1016/j.jvlc.2018.08.003", "10.1109/tvcg.2016.2520921", "10.1111/cgf.13446", "10.1017/s0022381612000187", "10.1145/2702123.2702406", "10.1109/vast.2012.6400487", "10.1109/tvcg.2013.183", "10.1177/1473871611415997", "10.1145/2702123.2702585", "10.1145/2993901.2993903", "10.1109/tvcg.2013.120", "10.1111/cgf.12632", "10.1145/1385569.1385602", "10.1109/tvcg.2017.2754480", "10.1111/j.1467-8659.2009.01694.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 7}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934541", "title": "A Recursive Subdivision Technique for Sampling Multi-class Scatterplots", "year": "2019", "conferenceName": "InfoVis", "authors": "Xin Chen;Tong Ge;Jian Zhang 0070;Baoquan Chen;Chi-Wing Fu;Oliver Deussen;Yunhai Wang", "citationCount": "5", "affiliation": "Chen, X (Corresponding Author), Shandong Univ, Jinan, Shandong, Peoples R China. Chen, Xin; Ge, Tong; Wang, Yunhai, Shandong Univ, Jinan, Shandong, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China. Fu, Chi-Wing, SIAT, Guangdong Prov Key Lab CV \\& VR Tech, Shenzhen, Guangdong, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Deussen, Oliver, Konstanz Univ, Constance, Germany. Deussen, Oliver, SIAT, Shenzhen VIsuCA Key Lab, Shenzhen, Guangdong, Peoples R China.", "countries": "Germany;China", "abstract": "We present a non-uniform recursive sampling technique for multi-class scatterplots, with the specific goal of faithfully presenting relative data and class densities, while preserving major outliers in the plots. Our technique is based on a customized binary kd-tree, in which leaf nodes are created by recursively subdividing the underlying multi-class density map. By backtracking, we merge leaf nodes until they encompass points of all classes for our subsequently applied outlier-aware multi-class sampling strategy. A quantitative evaluation shows that our approach can better preserve outliers and at the same time relative densities in multi-class scatterplots compared to the previous approaches, several case studies demonstrate the effectiveness of our approach in exploring complex and real world data.", "keywords": "Scatterplot,multi-class sampling,kd-tree,outlier,relative density", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934541", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2006.170", "10.1109/tvcg.2008.119", "10.1109/tvcg.2007.70580", "10.2307/2289444", "10.1145/1964921.1964943", "10.1109/tvcg.2013.65", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/itoec.2018.8740621", "10.1145/1778765.1778816", "10.1109/tvcg.2010.197", "10.1201/b17511", "10.1109/tvcg.2018.2864912", "10.1007/0-387-28695-0", "10.1109/5.726791", "10.1109/visual.1998.745301", "10.1016/j.physa.2011.12.004", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2004.1272729", "10.1145/335191.335388", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1109/iv.2002.1028760", "10.1145/1842993.1842999", "10.1109/tvcg.2018.2869149", "10.1038/nmeth.2490", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2010.176", "10.1145/2702123.2702585", "10.1057/ivs.2009.34"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14001", "year": "2020", "title": "Sunspot Plots: Model-based Structure Enhancement for Dense Scatter Plots", "conferenceName": "EuroVis", "authors": "Thomas Trautner;Fabian Bolte;Sergej Stoppel;Stefan Bruckner", "citationCount": "0", "affiliation": "Trautner, T (Corresponding Author), Univ Bergen, Bergen, Norway.\nTrautner, T.; Bolte, F.; Stoppel, S.; Bruckner, S., Univ Bergen, Bergen, Norway.", "countries": "Norway", "abstract": "Scatter plots are a powerful and well-established technique for visualizing the relationships between two variables as a collection of discrete points. However, especially when dealing with large and dense data, scatter plots often exhibit problems such as overplotting, making the data interpretation arduous. Density plots are able to overcome these limitations in highly populated regions, but fail to provide accurate information of individual data points. This is particularly problematic in sparse regions where the density estimate may not provide a good representation of the underlying data. In this paper, we present sunspot plots, a visualization technique that communicates dense data as a continuous data distribution, while preserving the discrete nature of data samples in sparsely populated areas. We furthermore demonstrate the advantages of our approach on typical failure cases of scatter plots within synthetic and real-world data sets and validate its effectiveness in a user study.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14001", "refList": ["10.1057/palgrave.ivs.9500122", "10.2312/eggh/hpg12/097-103", "10.1109/tvcg.2008.119", "10.1109/pacificvis.2011.5742387", "10.1038/331163a0", "10.1109/visual.2019.8933620", "10.2307/2683294", "10.1201/9781351072304", "10.2307/2289444", "10.1109/tvcg.2019.2934541", "10.1080/00949657508810123", "10.1109/tvcg.2013.65", "10.1109/infvis.1997.636789", "10.1109/2945.841121", "10.1109/mcse.2007.55", "10.1109/tvcg.2010.197", "10.1111/cgf.12871", "10.1109/infvis.2003.1249018", "10.1145/3173574.3173991", "10.1109/tvcg.2012.238", "10.1007/0-387-28695-0", "10.1109/pacificvis.2010.5429604", "10.18637/jss.v008.i03", "10.1109/tvcg.2007.70596", "10.1109/visual.1998.745301", "10.1007/0-387-37977-0\\_3", "10.1145/1556262.1556289", "10.2312/wiced.20161094", "10.1109/tvcg.2007.70535", "10.1145/1056808.1056914", "10.1109/tvcg.2003.1196007", "10.1109/iv.2004.1320190", "10.1111/cgf.12877", "10.1162/leon.2007.40.2.202a", "10.1109/tvcg.2017.2674978", "10.1057/ivs.2010.4", "10.1016/j.cag.2018.02.008", "10.1201/9781315140919", "10.1109/visual.2000.885677", "10.1002/jhbs.20078", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1109/visual.2001.964495", "10.1109/iv.2002.1028760", "10.1111/cgf.13684", "10.1109/hicss.2013.197", "10.1109/tvcg.2019.2903956", "10.1093/mnras/stt961", "10.1109/tvcg.2017.2668409", "10.1111/j.1467-8659.2009.01478.x", "10.1145/2702123.2702585", "10.2307/1418003", "10.2307/1390742", "10.1145/360825.360839", "10.1109/pacificvis.2009.4906843", "10.1057/ivs.2009.34", "10.2307/2288711"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934208", "title": "Evaluating Perceptual Bias During Geometric Scaling of Scatterplots", "year": "2019", "conferenceName": "VAST", "authors": "Yating Wei;Honghui Mei;Ying Zhao;Shuyue Zhou;Bingru Lin;Haojing Jiang;Wei Chen", "citationCount": "5", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou 310058, Zhejiang, Peoples R China. Zhao, Y (Corresponding Author), Cent South Univ, Sch Comp Sci \\& Engn, Changsha 410083, Hunan, Peoples R China. Wei, Yating; Mei, Honghui; Zhou, Shuyue; Lin, Bingru; Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou 310058, Zhejiang, Peoples R China. Zhao, Ying; Jiang, Haojing, Cent South Univ, Sch Comp Sci \\& Engn, Changsha 410083, Hunan, Peoples R China.", "countries": "China", "abstract": "Scatterplots are frequently scaled to fit display areas in multi-view and multi-device data analysis environments. A common method used for scaling is to enlarge or shrink the entire scatterplot together with the inside points synchronously and proportionally. This process is called geometric scaling. However, geometric scaling of scatterplots may cause a perceptual bias, that is, the perceived and physical values of visual features may be dissociated with respect to geometric scaling. For example, if a scatterplot is projected from a laptop to a large projector screen, then observers may feel that the scatterplot shown on the projector has fewer points than that viewed on the laptop. This paper presents an evaluation study on the perceptual bias of visual features in scatterplots caused by geometric scaling. The study focuses on three fundamental visual features (i.e., numerosity, correlation, and cluster separation) and three hypotheses that are formulated on the basis of our experience. We carefully design three controlled experiments by using well-prepared synthetic data and recruit participants to complete the experiments on the basis of their subjective experience. With a detailed analysis of the experimental results, we obtain a set of instructive findings. First, geometric scaling causes a bias that has a linear relationship with the scale ratio. Second, no significant difference exists between the biases measured from normally and uniformly distributed scatterplots. Third, changing the point radius can correct the bias to a certain extent. These findings can be used to inspire the design decisions of scatterplots in various scenarios.", "keywords": "Evaluation,scatterplot,geometric scaling,bias,perceptual consistency", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934208", "refList": ["10.2307/2288843", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2014.2346979", "10.1126/science.216.4550.1138", "10.1109/tvcg.2017.2744138", "10.1111/j.1467-8659.2009.01467.x", "10.1145/2491568.2491577", "10.1109/mwc.2018.1700325", "10.1145/2449396.2449439", "10.1109/tvcg.2011.127", "10.1109/tvcg.2011.229", "10.1167/15.5.4", "10.1073/pnas.1113195108", "10.1145/2702123.2702545", "10.1109/vast.2009.5332628", "10.1109/tvcg.2018.2800013", "10.1007/s12650-018-0530-2", "10.1016/s0042-6989(97)00340-4", "10.1109/vast.2010.5652460", "10.1109/mcg.2018.2879067", "10.1109/tvcg.2018.2864912", "10.1109/pacificvis.2010.5429604", "10.1109/tcst.2018.2819965", "10.1167/10.2.10", "10.1145/2470654.2481318", "10.1145/1842993.1843002", "10.1167/12.6.8", "10.1109/tvcg.2013.124", "10.1057/ivs.2008.13", "10.1109/tvcg.2007.70596", "10.1109/tvcg.2018.2865266", "10.1145/3173574.3173664", "10.1109/tvcg.2015.2467671", "10.1016/j.cag.2017.07.004", "10.1177/0956797613501520", "10.1109/tvcg.2018.2865020", "10.1111/j.1467-8659.2012.03125.x", "10.3758/bf03205986", "10.3758/s13423-016-1174-7", "10.1109/tvcg.2017.2680452", "10.1109/mc.2006.109", "10.1109/tvcg.2017.2744098", "10.1038/srep32810", "10.1016/j.visres.2013.06.006", "10.1002/jhbs.20078", "10.1109/tvcg.2006.163", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1016/j.cognition.2007.10.009", "10.1109/tvcg.2018.2865142", "10.3758/app.72.7.1839", "10.1109/tvcg.2018.2810918", "10.1016/j.jvlc.2017.10.001", "10.1145/2682623", "10.1109/tvcg.2018.2864884", "10.1145/3025453.3025984", "10.1109/tvcg.2006.184", "10.1109/tvcg.2013.153", "10.1016/j.jvlc.2018.08.003", "10.1109/tvcg.2016.2520921", "10.1111/cgf.13446", "10.1017/s0022381612000187", "10.1145/2702123.2702406", "10.1109/vast.2012.6400487", "10.1109/tvcg.2013.183", "10.1177/1473871611415997", "10.1145/2702123.2702585", "10.1145/2993901.2993903", "10.1109/tvcg.2013.120", "10.1111/cgf.12632", "10.1145/1385569.1385602", "10.1109/tvcg.2017.2754480", "10.1111/j.1467-8659.2009.01694.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934811", "title": "Winglets: Visualizing Association with Uncertainty in Multi-class Scatterplots", "year": "2019", "conferenceName": "InfoVis", "authors": "Min Lu;Shuaiqi Wang;Joel Lanir;Noa Fish;Yang Yue;Daniel Cohen-Or;Hui Huang 0004", "citationCount": "1", "affiliation": "Huang, H (Corresponding Author), Shenzhen Univ, Shenzhen, Peoples R China. Lu, Min; Wang, Shuaiqi; Yue, Yang; Cohen-Or, Daniel; Huang, Hui, Shenzhen Univ, Shenzhen, Peoples R China. Lanir, Joel, Univ Haifa, Haifa, Israel. Fish, Noa, Tel Aviv Univ, Tel Aviv, Israel.", "countries": "Israel;China", "abstract": "This work proposes Winglets, an enhancement to the classic scatterplot to better perceptually pronounce multiple classes by improving the perception of association and uncertainty of points to their related cluster. Designed as a pair of dual-sided strokes belonging to a data point, Winglets leverage the Gestalt principle of Closure to shape the perception of the form of the clusters, rather than use an explicit divisive encoding. Through a subtle design of two dominant attributes, length and orientation, Winglets enable viewers to perform a mental completion of the clusters. A controlled user study was conducted to examine the efficiency of Winglets in perceiving the cluster association and the uncertainty of certain points. The results show Winglets form a more prominent association of points into clusters and improve the perception of associating uncertainty.", "keywords": "Scatterplot,Gestalt laws,Association,Uncertainty", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934811", "refList": ["10.1109/tvcg.2018.2865141", "10.1214/aoms/1177728190", "10.1111/j.1467-8659.2009.01467.x", "10.2307/2289444", "10.1109/tvcg.2013.65", "10.1037/a0029334", "10.1109/vast.2009.5332628", "10.1007/bf00410640", "10.1109/vast.2010.5652460", "10.1145/2590349", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2013.20", "10.1002/9781118575574", "10.2312/conf/eg2013/stars/039-063", "10.1109/tvcg.2009.122", "10.1109/tvcg.2017.2750689", "10.1023/a:1024454423670", "10.1515/secm-2017-0114", "10.1016/0377-0427(87)90125-7", "10.1016/0010-0285(92)90014-s", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1016/b978-044481862-1.50075-3", "10.1007/s10851-006-9176-0", "10.1145/1842993.1842999", "10.1145/37402.37422", "10.1037/0033-295x.115.1.131", "10.1109/tvcg.2013.153", "10.1007/978-3-642-04898-2\\_455", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1559/152304003100010929", "10.1057/ivs.2009.34"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030373", "title": "Multi-Perspective, Simultaneous Embedding", "year": "2020", "conferenceName": "InfoVis", "authors": "Md. Iqbal Hossain;Vahan Huroyan;Stephen G. Kobourov;Raymundo Navarrete", "citationCount": "0", "affiliation": "Hossain, MI (Corresponding Author), Univ Arizona, Dept Comp Sci, Tucson, AZ 85721 USA. Hossain, Md Iqbal; Kobourov, Stephen, Univ Arizona, Dept Comp Sci, Tucson, AZ 85721 USA. Huroyan, Vahan; Navarrete, Raymundo, Univ Arizona, Dept Math, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "We describe MPSE: a Multi-Perspective Simultaneous Embedding method for visualizing high-dimensional data, based on multiple pairwise distances between the data points. Specifically, MPSE computes positions for the points in 3D and provides different views into the data by means of 2D projections (planes) that preserve each of the given distance matrices. We consider two versions of the problem: fixed projections and variable projections. MPSE with fixed projections takes as input a set of pairwise distance matrices defined on the data points, along with the same number of projections and embeds the points in 3D so that the pairwise distances are preserved in the given projections. MPSE with variable projections takes as input a set of pairwise distance matrices and embeds the points in 3D while also computing the appropriate projections that preserve the pairwise distances. The proposed approach can be useful in multiple scenarios: from creating simultaneous embedding of multiple graphs on the same set of vertices, to reconstructing a 3D object from multiple 2D snapshots, to analyzing data from multiple points of view. We provide a functional prototype of MPSE that is based on an adaptive and stochastic generalization of multi-dimensional scaling to multiple distances and multiple variable projections. We provide an extensive quantitative evaluation with datasets of different sizes and using different number of projections, as well as several examples that illustrate the quality of the resulting solutions.", "keywords": "Graph visualization,Dimensionality reduction,Multidimensional scaling,Mental map preservation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030373", "refList": ["10.1016/j.cag.2014.01.006", "10.1109/tvcg.2007.70443", "10.1007/978-3-642-04843-2\\_36", "10.1111/cgf.12639", "10.1109/tvcg.2019.2944182", "10.1109/tvcg.2019.2934811", "10.1007/978-1-4757-2261-1", "10.1145/3173574.3174209", "10.1016/j.neucom.2006.11.018", "10.1109/tpami.2013.57", "10.2307/2334381", "10.1109/tpami.2012.88", "10.1109/tvcg.2017.2661309", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1111/j.1467-8659.2010.01835.x", "10.1109/vl.1996.545307", "10.1109/iv.2009.25", "10.1109/msp.2010.939739", "10.1063/1.3067728", "10.2312/pe/eurovast/eurovast10/013-018", "10.1109/vast.2010.5652460", "10.25365/thesis.51534", "10.21105/joss.00861", "10.1145/1007730.1007731", "10.1109/tvcg.2013.20", "10.1109/tvcg.2011.220", "10.1037/h0071325", "10.1109/vast.2012.6400488", "10.1109/tvcg.2007.70535", "10.1198/jasa.2009.0111", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2017.2698041", "10.1109/tvcg.2015.2467717", "10.1111/j", "10.1016/j.neucom.2014.07.071", "10.1126/science.290.5500.2323", "10.1109/tvcg.2013.153", "10.2312/pe/eurovast", "10.1109/tvcg.2018.2846735", "10.1007/bf02289530", "10.1111/j.1745-3984.2003.tb01108.x", "10.1177/1473871615600010", "10.5555/3053814.3053816", "10.1109/tvcg.2018.2865194"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030430", "title": "Competing Models: Inferring Exploration Patterns and Information Relevance via Bayesian Model Selection", "year": "2020", "conferenceName": "VAST", "authors": "Shayan Monadjemi;Roman Garnett;Alvitta Ottley", "citationCount": "0", "affiliation": "Monadjemi, S (Corresponding Author), Washington Univ, St Louis, MO 14263 USA. Monadjemi, Shayan; Garnett, Roman; Ottley, Alvitta, Washington Univ, St Louis, MO 14263 USA.", "countries": "USA", "abstract": "Analyzing interaction data provides an opportunity to learn about users, uncover their underlying goals, and create intelligent visualization systems. The first step for intelligent response in visualizations is to enable computers to infer user goals and strategies through observing their interactions with a system. Researchers have proposed multiple techniques to model users, however, their frameworks often depend on the visualization design, interaction space, and dataset. Due to these dependencies, many techniques do not provide a general algorithmic solution to user exploration modeling. In this paper, we construct a series of models based on the dataset and pose user exploration modeling as a Bayesian model selection problem where we maintain a belief over numerous competing models that could explain user interactions. Each of these competing models represent an exploration strategy the user could adopt during a session. The goal of our technique is to make high-level and in-depth inferences about the user by observing their low-level interactions. Although our proposed idea is applicable to various probabilistic model spaces, we demonstrate a specific instance of encoding exploration patterns as competing models to infer information relevance. We validate our technique's ability to infer exploration bias, predict future interactions, and summarize an analytic session using user study datasets. Our results indicate that depending on the application, our method outperforms established baselines for bias detection and future interaction prediction. Finally, we discuss future research directions based on our proposed modeling paradigm and suggest how practitioners can use this method to build intelligent visualization systems that understand users' goals and adapt to improve the exploration process.", "keywords": "User Interaction Modeling,Bayesian Machine Learning", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030430", "refList": ["10.1109/tvcg.2018.2865141", "10.1145/355588.365140", "10.1007/978-0-387-98141-3\\_1", "10.1145/2882903.2882919", "10.1111/cgf.13708", "10.1145/3183713.3183738", "10.1111/cgf.12129", "10.1109/tvcg.2014.2346452", "10.1109/tvcg.2013.65", "10.1109/tvcg.2011.185", "10.1109/infvis.2003.1249018", "10.1109/tvcg.2017.2671341", "10.1109/infvis.2002.1173156", "10.1109/38.974515", "10.1109/tvcg.2016.2598624", "10.1109/infvis.2003.1249019", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2006.161", "10.1109/ldav.2015.7348077", "10.1109/iv.2004.1320190", "10.1145/3139958.3140037", "10.1109/vlhcc.2005.11", "10.1145/98524.98564", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2009.84", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2013.179", "10.1016/s0097-8493(02)00283-2", "10.14778/2428536.2428538", "10.1109/icde.2014.6816720", "10.1109/tvcg.2017.2785807", "10.1198/jcgs.2009.07098", "10.1109/tvcg.2017.2754480"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030394", "title": "Direct Volume Rendering with Nonparametric Models of Uncertainty", "year": "2020", "conferenceName": "SciVis", "authors": "Tushar M. Athawale;Bo Ma 0002;Elham Sakhaee;Christopher R. Johnson;Alireza Entezari", "citationCount": "0", "affiliation": "Athawale, TM (Corresponding Author), Univ Utah, Sci Comp \\& Imaging SCI Inst, Salt Lake City, UT 84112 USA. Athawale, Tushar M.; Johnson, Chris R., Univ Utah, Sci Comp \\& Imaging SCI Inst, Salt Lake City, UT 84112 USA. Ma, Bo; Sakhaee, Elham; Entezari, Alireza, Univ Florida, Dept CISE, Gainesville, FL 32611 USA.", "countries": "USA", "abstract": "We present a nonparametric statistical framework for the quantification, analysis, and propagation of data uncertainty in direct volume rendering (DVR). The state-of-the-art statistical DVR framework allows for preserving the transfer function (TF) of the ground truth function when visualizing uncertain data; however, the existing framework is restricted to parametric models of uncertainty. In this paper, we address the limitations of the existing DVR framework by extending the DVR framework for nonparametric distributions. We exploit the quantile interpolation technique to derive probability distributions representing uncertainty in viewing-ray sample intensities in closed form, which allows for accurate and efficient computation. We evaluate our proposed nonparametric statistical models through qualitative and quantitative comparisons with the mean-field and parametric statistical models, such as uniform and Gaussian, as well as Gaussian mixtures. In addition, we present an extension of the state-of-the-art rendering parametric framework to 2D TFs for improved DVR classifications. We show the applicability of our uncertainty quantification framework to ensemble, downsampled, and bivariate versions of scalar field datasets.", "keywords": "Volumes,uncertainty,nonparametric,2D transfer function", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030394", "refList": ["10.1109/icdm.2012.80", "10.1145/1401890.1401904", "10.1109/tvcg.2017.2745139", "10.18128/d030.v6.0", "10.1111/cgf.12142", "10.1109/tvcg.2009.114", "10.1111/j.1467-8659.2009.01677.x", "10.1109/mcse.2007.55", "10.1109/icde.2012.16", "10.1198/106186008x320465", "10.1145/1835804.1835868", "10.1559/1523040054738936", "10.1109/tvcg.2019.2934432", "10.1109/34.1000236", "10.1109/infvis.2005.1532136", "10.1109/tvcg.2012.128", "10.1109/pacificvis.2017.8031573", "10.1109/tvcg.2016.2598920", "10.1109/tvcg.2018.2865021", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1109/sp.2009.22", "10.1109/tvcg.2017.2744184", "10.1145/773153.773173", "10.1145/2660267.2660348", "10.1109/icdmw.2009.93", "10.1016/j.jtrangeo.2015.09.001", "10.1109/icde.2010.5447831", "10.1007/11681878\\_14", "10.1111/cgf.13409", "10.1007/s13278-014-0205-5", "10.1109/tvcg.2011.163", "10.1145/3035918.3035940", "10.1109/sp.2008.33", "10.1145/2882903.2882931"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2019.00018", "year": "2019", "title": "Scatterplot Summarization by Constructing Fast and Robust Principal Graphs from Skeletons", "conferenceName": "PacificVis", "authors": "Jos{\\'{e}} Matute;Marcel Fischer;Alexandru C. Telea;Lars Linsen", "citationCount": "1", "affiliation": "Matute, J (Corresponding Author), Univ Munster, Munster, Germany.\nMatute, Jose; Fischer, Marcel; Linsen, Lars, Univ Munster, Munster, Germany.\nTelea, Alexandru C., Univ Groningen, Groningen, Netherlands.", "countries": "Germany;Netherlands", "abstract": "Principal curves are a long-standing and well-known method for summarizing large scatterplots. They are defined as self-consistent curves (or curve sets in the more general case) that locally pass through the middle of the scatterplot data. However, computing principal curves that capture well complex scatterplot topologies and are robust to noise is hard and/or slow for large scatterplots. We present a fast and robust approach for computing principal graphs (a generalization of principal curves for more complex topologies) inspired by the similarity to medial descriptors (curves locally centered in a shape). Compared to state-of-the-art methods for computing principal graphs, we outperform these in terms of computational scalability and robustness to noise and resolution. We also demonstrate the advantages of our method over other scatterplot summarization approaches.", "keywords": "", "link": "https://doi.org/10.1109/PacificVis.2019.00018", "refList": ["10.1016/j.cag.2014.01.006", "10.1016/s0167-8655(02)00032-6", "10.1006/jmva.2000.1917", "10.1111/cgf.12386", "10.1109/vast.2008.4677367", "10.1109/tpami.2004.1261076", "10.1111/cgf.12865", "10.1109/tvcg.2010.213", "10.1109/tvcg.2014.2346455", "10.1109/tvcg.2011.233", "10.1111/j.1467-8659.2009.01680.x", "10.1109/tvcg.2016.2515611", "10.1109/tit.1982.1056489", "10.1109/34.899944", "10.1007/bf01889678", "10.1137/s0036144599352836", "10.1145/2858036.2858155", "10.1002/jhbs.20078", "10.1198/jcgs.2011.09224", "10.1109/tvcg.2017.2744184", "10.1057/ivs.2010.2", "10.1111/j.1467-8659.2012.03107.x", "10.2307/2289936", "10.1109/pacificvis.2014.42", "10.1109/tpami.2008.21", "10.1111/cgf.13446", "10.2307/2290446", "10.1109/34.982884", "10.1109/tvcg.2006.94", "10.2307/2288711", "10.1117/12.304651", "10.5201/ipol.2013.87", "10.1109/mlsp.2008.4685520"], "wos": 1, "children": [{"doi": "10.1109/pacificvis48177.2020.8199", "year": "2020", "title": "Efficient Morphing of Shape-preserving Star Coordinates", "conferenceName": "PacificVis", "authors": "Vladimir Molchanov;Sagad Hamid;Lars Linsen", "citationCount": "0", "affiliation": "Molchanov, V (Corresponding Author), Westfalische Wilhelms Univ Munster, Munster, Germany.\nMolchanov, Vladimir; Hamid, Sagad; Linsen, Lars, Westfalische Wilhelms Univ Munster, Munster, Germany.", "countries": "Germany", "abstract": "Data tours follow an exploratory multi-dimensional data visualization concept that provides animations of projections of the multidimensional data to a 2D visual space. To create an animation, a sequence of key projections is provided and morphings between each pair of consecutive key projections are computed, which then can be stitched together to form the data tour. The morphings should be smooth so that a user can easily follow the transformations, and their computations shall be fast to allow for their integration into an interactive visual exploration process. Moreover, if the key projections are chosen to satisfy additional conditions, it is desirable that these conditions are maintained during morphing. Shape preservation is such a desirable condition, as it avoids shape distortions that may otherwise be caused by a projection. We develop a novel efficient morphing algorithms for computing shape-preserving data tours, i.e., data tours constructed for a sequence of shape-preserving linear projections. We propose a stepping strategy for the morphing to avoid discontinuities in the evolution of the projections, where we represent the linear projections using a star-coordinates system. Our algorithms are less computationally involved, produce smoother morphings, and require less user-defined parameter settings than existing state-of-the-art approaches.", "keywords": "Human-centered computing; Visualization; Visualization techniques", "link": "https://doi.org/10.1109/PacificVis48177.2020.8199", "refList": ["10.2312/eurovisshort.20141152", "10.1109/tvcg.2013.182", "10.1109/tvcg.2008.153", "10.1109/tvcg.2015.2467591", "10.4135/9781412985130", "10.1111/cgf.12878", "10.1016/j.cag.2016.08.007", "10.1080/10618600.1995.10474674", "10.1109/infvis.2003.1249004", "10.1002/9781118445112.stat06472", "10.1109/visual.1997.663916", "10.1111/cgf.12117", "10.1137/0906011", "10.1109/tvcg.2018.2865118", "10.1126/science.290.5500.2319", "10.1111/cgf.12845", "10.1111/j.1467-8659.2012.03125.x", "10.1109/tvcg.2017.2705189", "10.1002/0471725293", "10.1111/cgf.12876", "10.1111/cgf.13404", "10.2307/2289161", "10.1111/cgf.13446", "10.1198/106186008x318440", "10.2307/1390747", "10.1109/tvcg.2012.35", "10.1109/tvcg.2015.2467132", "10.1109/tvcg.2006.94", "10.1109/t-c.1974.224051", "10.1109/pacificvis.2019.00018", "10.1109/tvcg.2017.2744339", "10.1109/tsmcb.2005.850151"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.13446", "year": "2018", "title": "Quality Metrics for Information Visualization", "conferenceName": "EuroVis", "authors": "Michael Behrisch;Michael Blumenschein;Nam Wook Kim;Lin Shao;Mennatallah El{-}Assady;Johannes Fuchs;Daniel Seebacher;Alexandra Diehl;Ulrik Brandes;Hanspeter Pfister;Tobias Schreck;Daniel Weiskopf;Daniel A. Keim", "citationCount": "25", "affiliation": "Behrisch, M (Corresponding Author), Harvard Univ, Cambridge, MA 02138 USA.\nBehrisch, M.; Kim, N. W.; Pfister, H., Harvard Univ, Cambridge, MA 02138 USA.\nBlumenschein, M.; El-Assady, M.; Fuchs, J.; Seebacher, D.; Diehl, A.; Keim, D. A., Univ Konstanz, Constance, Germany.\nShao, L.; Schreck, T., Graz Univ Technol, Graz, Austria.\nBrandes, U., Swiss Fed Inst Technol, Zurich, Switzerland.\nWeiskopf, D., Univ Stuttgart, Stuttgart, Germany.", "countries": "Switzerland;Germany;USA;Austria", "abstract": "The visualization community has developed to date many intuitions and understandings of how to judge the quality of views in visualizing data. The computation of a visualization's quality and usefulness ranges from measuring clutter and overlap, up to the existence and perception of specific (visual) patterns. This survey attempts to report, categorize and unify the diverse understandings and aims to establish a common vocabulary that will enable a wide audience to understand their differences and subtleties. For this purpose, we present a commonly applicable quality metric formalization that should detail and relate all constituting parts of a quality metric. We organize our corpus of reviewed research papers along the data types established in the information visualization community: multi- and high-dimensional, relational, sequential, geospatial and text data. For each data type, we select the visualization subdomains in which quality metrics are an active research field and report their findings, reason on the underlying concepts, describe goals and outline the constraints and requirements. One central goal of this survey is to provide guidance on future research opportunities for the field and outline how different visualization communities could benefit from each other by applying or transferring knowledge to their respective subdomain. Additionally, we aim to motivate the visualization community to compare computed measures to the perception of humans.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13446", "refList": ["10.2307/276978", "10.1057/ivs.2009.10", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2014.2346420", "10.1016/j.cag.2018.01.010", "10.1109/tvcg.2017.2743959", "10.1109/tvcg.2011.127", "10.1109/tvcg.2015.2467759", "10.1109/tvcg.2013.187", "10.1109/tvcg.2007.70594", "10.1186/1471-2105-9-155", "10.1145/2702123.2702545", "10.1145/1645953.1646023", "10.1093/bioinformatics/btm312", "10.1109/pacificvis.2016.7465245", "10.1145/3038462.3038463", "10.1007/978-3-540-70956-5", "10.1109/tvcg.2017.2743939", "10.1111/cgf.12932", "10.1111/j.1467-8659.2012.03106.x", "10.1145/1133265.1133318", "10.1109/tvcg.2010.186", "10.1109/iv.2013.101", "10.1109/tvcg.2016.2598590", "10.2312/conf/eg2013/stars/039-063", "10.3758/bf03201236", "10.1111/cgf.12935", "10.1145/1056808.1056914", "10.1007/bf01898350", "10.2307/1390686", "10.1109/2945.981848", "10.1109/hicss.2008.422", "10.1109/tvcg.2011.201", "10.1109/pacificvis.2011.5742390", "10.2307/2288400", "10.1111/cgf.12872", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2006.163", "10.1109/tvcg.2013.150", "10.1109/tvcg.2009.171", "10.1109/tvcg.2017.2723397", "10.1117/12.2079841", "10.1109/pacificvis.2009.4906838", "10.1109/infvis.2004.1", "10.1109/tvcg.2008.166", "10.1287/opre.20.5.993", "10.7155/jgaa.00370", "10.1111/j.1467-8659.2008.01240.x", "10.1109/inf0vis.2005.14", "10.1109/tvcg.2009.23", "10.1109/infvis.1998.729559", "10.1177/0165551506078083", "10.1109/t-c.1974.224051", "10.1109/tvcg.2017.2744339", "10.1109/tvcg.2014.2346433", "10.1145/989863.989940", "10.1109/tvcg.2015.2509990", "10.2307/2288843", "10.1016/j.neucom.2017.01.105", "10.1073/pnas.43.10.923", "10.1111/cgf.12647", "10.1016/j.rse.2017.06.031", "10.1109/infvis.2005.1532145", "10.1109/2945.841121", "10.1109/tvcg.2011.229", "10.2312/eurova.20141140", "10.1111/j.2044-8317.1974.tb00534.x", "10.1109/pacificvis.2015.7156366", "10.1109/vast.2009.5332628", "10.1109/vast.2014.7042480", "10.1145/2993901.2993907", "10.1177/0951692899011001004", "10.1007/978-3-642-03655-2\\_43", "10.1198/106186008x320465", "10.1109/visual.1997.663916", "10.1111/cgf.12125", "10.1109/infvis.2003.1249009", "10.1109/icdm.2003.1250978", "10.1109/tvcg.2017.2745919", "10.1109/tvcg.2012.128", "10.1109/vast.2010.5652450", "10.1109/tvcg.2006.161", "10.1145/571647.571649", "10.1109/tvcg.2011.193", "10.1111/cgf.12633", "10.1109/tvcg.2017.2674978", "10.1109/vast.2010.5652433", "10.1109/tvcg.2017.2745978", "10.1515/itit-2014-1070", "10.1145/1374489.1374501", "10.1109/tvcg.2009.153", "10.2312/eurovisshort.20171128", "10.2312/eurovisshort.20151130", "10.1016/0925-7721(94)00014-x", "10.1057/palgrave.ivs.95000/3", "10.1145/302979.303030", "10.1002/widm.1071", "10.1109/pacificvis.2014.42", "10.1109/tvcg.2016.2598467", "10.1111/j.1467-8659.2009.01667.x", "10.2307/2287708", "10.1177/1473871613477091", "10.1145/967900.968153", "10.1109/tvcg.2010.162", "10.1145/568522.568523", "10.1016/j.cag.2004.03.022", "10.1109/tvcg.2015.2466992", "10.1145/2993901.2993903", "10.1147/jrd.2015.2411412", "10.1177/154193120504900508", "10.1109/tvcg.2006.138", "10.1111/cgf.13168", "10.1111/cgf.12380", "10.2312/conf/eg2013/stars/095-116", "10.1109/tvcg.2017.2745859", "10.1109/tvcg.2017.2701829", "10.1109/tvcg.2007.70529", "10.1007/bf01199431", "10.1117/12.697548", "10.1109/tvcg.2017.2653106", "10.1207/s15327906mbr2701\\_4", "10.1109/vl.1996.545307", "10.1109/tvcg.2017.2745140", "10.1111/j.1467-8659.2012.03069.x", "10.1111/j.1467-8659.2011.01961.x", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.124", "10.1057/ivs.2008.13", "10.2312/vissym/eurovis06/195-202", "10.2312/vissym/eurovis07/163-170", "10.1109/tvcg.2015.2467324", "10.1109/tvcg.2011.167", "10.1109/pacificvis.2014.40", "10.1109/pacificvis.2012.6183570", "10.1111/j.1467-8659.2012.03125.x", "10.1075/idj.20.1.02bei", "10.1111/1467-8306.00061", "10.1145/2858036.2858155", "10.1109/tvcg.2012.108", "10.2312/vmv.20171261", "10.1109/tvcg.2015.2467531", "10.1145/1379092.1379130", "10.1109/tpami.1979.4766909", "10.1080/13875860903039172", "10.1109/mcg.2006.70", "10.1109/tvcg.2010.242", "10.1109/vast.2007.4389004", "10.1109/tvcg.2015.2467191", "10.2312/cgvc.20171276", "10.1016/0169-7439(87)80084-9", "10.2312/pe/eurovisshort/eurovisshort2012/097-101", "10.1016/j.neucom.2014.07.073", "10.2307/2685263", "10.1145/996546.996554", "10.1111/j.1467-8659.2009.01666.x", "10.1117/12.2083444", "10.1109/tvcg.2014.2346572", "10.1007/978-0-387-39940-9\\_262", "10.1007/bf00988593", "10.1145/1054972.1055078", "10.1111/j.1467-8659.2009.01467.x", "10.1111/cgf.13181", "10.1145/502512.502530", "10.1109/pacificvis.2010.5429600", "10.1111/j.1467-8659.2011.01923.x", "10.1109/mcg.2004.41", "10.1109/tvcg.2015.2467971", "10.1111/j.1467-8659.2011.01919.x", "10.1002/sam.10071", "10.1109/tvcg.2010.184", "10.1007/978-3-642-27848-8\\_648-1", "10.1109/tvcg.2010.132", "10.1145/22949.22950", "10.1145/1842993.1843002", "10.1109/infvis.2000.885096", "10.1016/j.ins.2015.04.017", "10.1109/iv.2009.43", "10.1016/j.visinf.2017.11.001", "10.1109/tvcg.2011.239", "10.1109/infovis.2005.40", "10.1111/cgf.12641", "10.1109/pacificvis.2016.7465262", "10.1145/2470654.2466443", "10.1007/s10844-011-0157-4", "10.1111/cgf.12919", "10.1016/j.jvlc.2016.07.003", "10.1109/tvcg.2016.2549018", "10.1057/palgrave.ivs.9500166", "10.3406/colan.1981.1409.1", "10.1093/bioinformatics/bti141", "10.1111/j.1467-8306.2004.09401004.x", "10.1145/1168149.1168168", "10.1109/tvcg.2017.2744184", "10.1117/12.2079420", "10.1109/iv.2005.62", "10.1145/102377.115768", "10.1109/tvcg.2014.2346677", "10.1109/tvcg.2014.2346426", "10.1111/j.1467-8659.2012.03107.x", "10.5220/0006097400400051", "10.2991/978-94-6239-186-4", "10.2307/2284077", "10.1109/iv.2008.89", "10.1016/j.jvlc.2015.12.001", "10.1109/pacificvis.2013.6596147", "10.1145/1242572.1242826", "10.1016/j.cag.2007.01.030", "10.1111/cgf.12632", "10.2312/eurovisstar.20151113", "10.1559/152304009788188808"], "wos": 1, "children": [{"doi": "10.1109/vast.2018.8802486", "title": "SMARTexplore: Simplifying High-Dimensional Data Analysis through a Table-Based Visual Analytics Approach", "year": "2018", "conferenceName": "VAST", "authors": "Michael Blumenschein;Michael Behrisch;Stefanie Schmid;Simon Butscher;Deborah Wahl;Karoline Villinger;Britta Renner;Harald Reiterer;Daniel A. Keim", "citationCount": "0", "affiliation": "Blumenschein, M (Corresponding Author), Univ Konstanz, Constance, Germany. Blumenschein, Michael; Schmid, Stefanie; Butscher, Simon; Wahl, Deborah R.; Villinger, Karoline; Renner, Britta; Reiterer, Harald; Keim, Daniel A., Univ Konstanz, Constance, Germany. Behrisch, Michael, Harvard Univ, Cambridge, MA 02138 USA.", "countries": "Germany;USA", "abstract": "We present SMARTEXPLORE, a novel visual analytics technique that simplifies the identification and understanding of clusters, correlations, and complex patterns in high-dimensional data. The analysis is integrated into an interactive table-based visualization that maintains a consistent and familiar representation throughout the analysis. The visualization is tightly coupled with pattern matching, subspace analysis, reordering, and layout algorithms. To increase the analyst's trust in the revealed patterns, SMARTEXPLORE automatically selects and computes statistical measures based on dimension and data properties. While existing approaches to analyzing high-dimensional data (e.g., planar projections and Parallel coordinates) have proven effective, they typically have steep learning curves for non-visualization experts. Our evaluation, based on three expert case studies, confirms that non-visualization experts successfully reveal patterns in high-dimensional data when using SMARTEXPLORE.", "keywords": "High-dimensional data,visual exploration,pattern-driven analysis,tabular visualization,subspace,aggregation", "link": "http://dx.doi.org/10.1109/VAST.2018.8802486", "refList": ["10.1177/1473871612460526", "10.1109/tvcg.2015.2489649", "10.1111/j.1467-8659.2008.01241.x", "10.1007/978-3-319-25087-8\\_29", "10.1007/b98835", "10.13140/rg.2.2.16570.90567", "10.1109/tvcg.2014.2346260", "10.1109/vast.2009.5332628", "10.2307/2528823", "10.1109/tvcg.2010.184", "10.1145/1133265.1133318", "10.1057/palgrave.ivs.9500072", "10.1111/j.1467-8659.2012.03110.x", "10.1145/1007730.1007731", "10.1057/palgrave.ivs.9500086", "10.1111/cgf.12935", "10.1109/tvcg.2014.2346279", "10.1007/bf01898350", "10.1109/tvcg.2013.173", "10.1109/tvcg.2017.2672987", "10.1109/tvcg.2014.2346248", "10.1109/infvis.2004.46", "10.1109/vast.2010.5652433", "10.1109/vast.2009.5332611", "10.1109/tvcg.2015.2467553", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2013.150", "10.1109/tvcg.2016.2640960", "10.1111/cgf.12630", "10.1007/978-1-4757-1904-8", "10.1109/tvcg.2011.188", "10.1109/tvcg.2010.138", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2017.2743978", "10.1111/cgf.12879", "10.1109/iv.2008.33", "10.1111/cgf.13446", "10.1007/s00371-018-1483-0", "10.1109/infvis.1998.729559", "10.1145/2669557.2669572", "10.1111/j.1467-8659.2008.01239.x"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934541", "title": "A Recursive Subdivision Technique for Sampling Multi-class Scatterplots", "year": "2019", "conferenceName": "InfoVis", "authors": "Xin Chen;Tong Ge;Jian Zhang 0070;Baoquan Chen;Chi-Wing Fu;Oliver Deussen;Yunhai Wang", "citationCount": "5", "affiliation": "Chen, X (Corresponding Author), Shandong Univ, Jinan, Shandong, Peoples R China. Chen, Xin; Ge, Tong; Wang, Yunhai, Shandong Univ, Jinan, Shandong, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China. Fu, Chi-Wing, SIAT, Guangdong Prov Key Lab CV \\& VR Tech, Shenzhen, Guangdong, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Deussen, Oliver, Konstanz Univ, Constance, Germany. Deussen, Oliver, SIAT, Shenzhen VIsuCA Key Lab, Shenzhen, Guangdong, Peoples R China.", "countries": "Germany;China", "abstract": "We present a non-uniform recursive sampling technique for multi-class scatterplots, with the specific goal of faithfully presenting relative data and class densities, while preserving major outliers in the plots. Our technique is based on a customized binary kd-tree, in which leaf nodes are created by recursively subdividing the underlying multi-class density map. By backtracking, we merge leaf nodes until they encompass points of all classes for our subsequently applied outlier-aware multi-class sampling strategy. A quantitative evaluation shows that our approach can better preserve outliers and at the same time relative densities in multi-class scatterplots compared to the previous approaches, several case studies demonstrate the effectiveness of our approach in exploring complex and real world data.", "keywords": "Scatterplot,multi-class sampling,kd-tree,outlier,relative density", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934541", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2006.170", "10.1109/tvcg.2008.119", "10.1109/tvcg.2007.70580", "10.2307/2289444", "10.1145/1964921.1964943", "10.1109/tvcg.2013.65", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/itoec.2018.8740621", "10.1145/1778765.1778816", "10.1109/tvcg.2010.197", "10.1201/b17511", "10.1109/tvcg.2018.2864912", "10.1007/0-387-28695-0", "10.1109/5.726791", "10.1109/visual.1998.745301", "10.1016/j.physa.2011.12.004", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2004.1272729", "10.1145/335191.335388", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1109/iv.2002.1028760", "10.1145/1842993.1842999", "10.1109/tvcg.2018.2869149", "10.1038/nmeth.2490", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2010.176", "10.1145/2702123.2702585", "10.1057/ivs.2009.34"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14001", "year": "2020", "title": "Sunspot Plots: Model-based Structure Enhancement for Dense Scatter Plots", "conferenceName": "EuroVis", "authors": "Thomas Trautner;Fabian Bolte;Sergej Stoppel;Stefan Bruckner", "citationCount": "0", "affiliation": "Trautner, T (Corresponding Author), Univ Bergen, Bergen, Norway.\nTrautner, T.; Bolte, F.; Stoppel, S.; Bruckner, S., Univ Bergen, Bergen, Norway.", "countries": "Norway", "abstract": "Scatter plots are a powerful and well-established technique for visualizing the relationships between two variables as a collection of discrete points. However, especially when dealing with large and dense data, scatter plots often exhibit problems such as overplotting, making the data interpretation arduous. Density plots are able to overcome these limitations in highly populated regions, but fail to provide accurate information of individual data points. This is particularly problematic in sparse regions where the density estimate may not provide a good representation of the underlying data. In this paper, we present sunspot plots, a visualization technique that communicates dense data as a continuous data distribution, while preserving the discrete nature of data samples in sparsely populated areas. We furthermore demonstrate the advantages of our approach on typical failure cases of scatter plots within synthetic and real-world data sets and validate its effectiveness in a user study.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14001", "refList": ["10.1057/palgrave.ivs.9500122", "10.2312/eggh/hpg12/097-103", "10.1109/tvcg.2008.119", "10.1109/pacificvis.2011.5742387", "10.1038/331163a0", "10.1109/visual.2019.8933620", "10.2307/2683294", "10.1201/9781351072304", "10.2307/2289444", "10.1109/tvcg.2019.2934541", "10.1080/00949657508810123", "10.1109/tvcg.2013.65", "10.1109/infvis.1997.636789", "10.1109/2945.841121", "10.1109/mcse.2007.55", "10.1109/tvcg.2010.197", "10.1111/cgf.12871", "10.1109/infvis.2003.1249018", "10.1145/3173574.3173991", "10.1109/tvcg.2012.238", "10.1007/0-387-28695-0", "10.1109/pacificvis.2010.5429604", "10.18637/jss.v008.i03", "10.1109/tvcg.2007.70596", "10.1109/visual.1998.745301", "10.1007/0-387-37977-0\\_3", "10.1145/1556262.1556289", "10.2312/wiced.20161094", "10.1109/tvcg.2007.70535", "10.1145/1056808.1056914", "10.1109/tvcg.2003.1196007", "10.1109/iv.2004.1320190", "10.1111/cgf.12877", "10.1162/leon.2007.40.2.202a", "10.1109/tvcg.2017.2674978", "10.1057/ivs.2010.4", "10.1016/j.cag.2018.02.008", "10.1201/9781315140919", "10.1109/visual.2000.885677", "10.1002/jhbs.20078", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1109/visual.2001.964495", "10.1109/iv.2002.1028760", "10.1111/cgf.13684", "10.1109/hicss.2013.197", "10.1109/tvcg.2019.2903956", "10.1093/mnras/stt961", "10.1109/tvcg.2017.2668409", "10.1111/j.1467-8659.2009.01478.x", "10.1145/2702123.2702585", "10.2307/1418003", "10.2307/1390742", "10.1145/360825.360839", "10.1109/pacificvis.2009.4906843", "10.1057/ivs.2009.34", "10.2307/2288711"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934284", "title": "Color Crafting: Automating the Construction of Designer Quality Color Ramps", "year": "2019", "conferenceName": "InfoVis", "authors": "Stephen Smart;Keke Wu;Danielle Albers Szafir", "citationCount": "3", "affiliation": "Smart, S (Corresponding Author), Univ Colorado, Boulder, CO 80309 USA. Smart, Stephen; Wu, Keke; Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Visualizations often encode numeric data using sequential and diverging color ramps. Effective ramps use colors that are sufficiently discriminable, align well with the data, and are aesthetically pleasing. Designers rely on years of experience to create high-quality color ramps. However, it is challenging for novice visualization developers that lack this experience to craft effective ramps as most guidelines for constructing ramps are loosely defined qualitative heuristics that are often difficult to apply. Our goal is to enable visualization developers to readily create effective color encodings using a single seed color. We do this using an algorithmic approach that models designer practices by analyzing patterns in the structure of designer-crafted color ramps. We construct these models from a corpus of 222 expert-designed color ramps, and use the results to automatically generate ramps that mimic designer practices. We evaluate our approach through an empirical study comparing the outputs of our approach with designer-crafted color ramps. Our models produce ramps that support accurate and aesthetically pleasing visualizations at least as well as designer ramps and that outperform conventional mathematical approaches.", "keywords": "Visualization,Aesthetics in Visualization,Color Perception,Visual Design,Design Mining", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934284", "refList": ["10.1145/3009924", "10.1109/tvcg.2015.2489649", "10.1109/tvcg.2017.2744359", "10.1002/(sici)1098-1098(199622)7:2", "10.1016/s0734-189x(83)80046-2", "10.1109/tvcg.2016.2599106", "10.1364/josaa.29.000313", "10.1109/tvcg.2016.2598918", "10.2307/2683294", "10.1109/tvcg.2017.2653106", "10.1016/j.ijhcs.2010.05.006", "10.1016/0146-664x(81)90006-x", "10.1007/978-3-642-10520-3\\_9", "10.1109/38.135886", "10.1146/annurev-psych-120710-100504", "10.1016/j.csda.2008.11.033", "10.1145/2461912.2461988", "10.1016/j.cub.2007.06.022", "10.1016/s0146-664x(79)80040-4", "10.1109/mcg.2004.1297012", "10.1109/iv.2009.94", "10.1109/tpami.2010.184", "10.3758/s13414-010-0027-0", "10.1145/22949.22950", "10.1109/visual.1995.480803", "10.1109/tvcg.2014.2346277", "10.2307/2684111", "10.1109/tvcg.2018.2865240", "10.1111/cgf.12633", "10.1109/38.7760", "10.1016/j.jspi.2015.04.007", "10.1109/iv.2008.24", "10.1145/2939502.2939506", "10.1111/cgf.12127", "10.1016/j.cag.2010.11.015", "10.1145/3025453.3026041", "10.1109/tvcg.2012.315", "10.2307/2288400", "10.1511/2005.5.436", "10.1016/s0097-8493(96)00072-6", "10.1109/mcg.2018.011461525", "10.1145/2470654.2466420", "10.1109/tvcg.2012.279", "10.1109/tvcg.2014.2346978", "10.1109/tvcg.2017.2743978", "10.1145/3406601.3406602", "10.1117/12.2084548", "10.1109/tvcg.2018.2865147", "10.1109/tvcg.2015.2467191", "10.1111/cgf.13446", "10.1109/tvcg.2017.2744320", "10.1111/j.1467-8659.2008.01203.x", "10.1145/2702613.2702975", "10.1007/978-3-319-26633-6\\_13", "10.1145/2207676.2208547", "10.1109/tvcg.2008.174", "10.1179/000870403235002042"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028891", "title": "A Structured Review of Data Management Technology for Interactive Visualization and Analysis", "year": "2020", "conferenceName": "InfoVis", "authors": "Leilani Battle;Carlos Scheidegger", "citationCount": "0", "affiliation": "Battle, L (Corresponding Author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA. Battle, Leilani, Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA. Scheidegger, Carlos, Univ Arizona, Dept Comp Sci, HDC Lab, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "In the last two decades, interactive visualization and analysis have become a central tool in data-driven decision making. Concurrently to the contributions in data visualization, research in data management has produced technology that directly benefits interactive analysis. Here, we contribute a systematic review of 30 years of work in this adjacent field, and highlight techniques and principles we believe to be underappreciated in visualization work. We structure our review along two axes. First, we use task taxonomies from the visualization literature to structure the space of interactions in usual systems. Second, we created a categorization of data management work that strikes a balance between specificity and generality. Concretely, we contribute a characterization of 131 research papers along these two axes. We find that five notions in data management venues fit interactive visualization systems well: materialized views, approximate query processing, user modeling and query prediction, muiti-query optimization, lineage techniques, and indexing techniques. In addition, we find a preponderance of work in materialized views and approximate query processing, most targeting a limited subset of the interaction tasks in the taxonomy we used. This suggests natural avenues of future research both in visualization and data management. Our categorization both changes how we visualization researchers design and build our systems, and highlights where future work is necessary.", "keywords": "", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028891", "refList": ["10.1109/tvcg.2012.233", "10.1016/s0022-5371(74)80015-0", "10.1109/tvcg.2017.2744359", "10.1037/0096-3445.136.4.623", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2012.196", "10.1037/a0029856", "10.1109/tvcg.2014.2346979", "10.1037/h0030300", "10.1109/tvcg.2016.2598918", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2018.2864909", "10.1111/cgf.13079", "10.3389/fpsyg.2012.00355", "10.1145/2858036.2858465", "10.1080/01621459.1989.10478821", "10.1037/0278-7393.24.3.732", "10.1109/tvcg.2011.127", "10.1145/2858036.2858063", "10.4249/scholarpedia.3325", "10.4324/9781410611949", "10.1111/cgf.13444", "10.1145/2993901.2993909", "10.1037/0033-295x.96.2.267", "10.1006/ijhc.1017", "10.1086/405615", "10.1109/tvcg.2019.2934801", "10.1038/17953", "10.1037/xhp0000314", "10.1109/tvcg.2019.2934400", "10.1145/2470654.2470723", "10.1037/0096-1523.16.2.332", "10.1167/16.5.11", "10.3758/s13423-016-1174-7", "10.3758/bf03207704", "10.1146/annurev.psych.55.090902.141415", "10.2307/2288400", "10.3758/bf03204258", "10.1109/tvcg.2011.279", "10.1109/vissoft.2014.36", "10.3758/s13423-011-0055-3", "10.1145/3025453.3025922", "10.1109/tvcg.2019.2934284", "10.3758/bf03210498", "10.3758/bf03200774", "10.2307/1419876", "10.1038/s41562-017-0058", "10.1109/tvcg.2010.237", "10.1109/pacificvis.2012.6183556", "10.1109/infvis.1997.636792", "10.1093/acprof:oso/9780198523192.003.0005", "10.1073/pnas.1117465109", "10.1109/tvcg.2013.234", "10.1038/nn.3655", "10.1111/cgf.12379", "10.1146/annurev-psych-010416-044232", "10.1111/cgf.13695", "10.1037/0033-295x.107.3.500", "10.1109/tvcg.2013.183", "10.1146/annurev.psych.53.100901.135125", "10.1037//0022-3514.79.6.995", "10.1559/152304003100010929", "10.1109/tvcg.2018.2865147", "10.1037/0096-1523.18.3.849", "10.1111/j.1467-8659.2009.01694.x", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934799", "title": "Data Sampling in Multi-view and Multi-class Scatterplots via Set Cover Optimization", "year": "2019", "conferenceName": "InfoVis", "authors": "Ruizhen Hu;Tingkai Sha;Oliver van Kaick;Oliver Deussen;Hui Huang 0004", "citationCount": "4", "affiliation": "Hu, RZ (Corresponding Author), Shenzhen Univ, Visual Comp Res Ctr, Shenzhen, Guangdong, Peoples R China. Hu, Ruizhen; Sha, Tingkai; Huang, Hui, Shenzhen Univ, Visual Comp Res Ctr, Shenzhen, Guangdong, Peoples R China. van Kaick, Oliver, Carleton Univ, Sch Comp Sci, Ottawa, ON, Canada. Deussen, Oliver, Konstanz Univ, Constance, Germany. Deussen, Oliver, SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Guangdong, Peoples R China.", "countries": "Canada;Germany;China", "abstract": "We present a method for data sampling in scatterplots by jointly optimizing point selection for different views or classes. Our method uses space-filling curves (Z-order curves) that partition a point set into subsets that, when covered each by one sample, provide a sampling or coreset with good approximation guarantees in relation to the original point set. For scatterplot matrices with multiple views, different views provide different space-filling curves, leading to different partitions of the given point set. For multi-class scatterplots, the focus on either per-class distribution or global distribution provides two different partitions of the given point set that need to be considered in the selection of the coreset. For both cases, we convert the coreset selection problem into an Exact Cover Problem (ECP), and demonstrate with quantitative and qualitative evaluations that an approximate solution that solves the ECP efficiently is able to provide high-quality samplings.", "keywords": "Sampling,Scatterplot,SPLOM,Exact Cover Problem", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934799", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2006.170", "10.1109/tvcg.2008.119", "10.1111/j.1467-8659.2009.01467.x", "10.2307/2289444", "10.1109/tvcg.2013.65", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/itoec.2018.8740621", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2864912", "10.1007/0-387-28695-0", "10.1109/visual.1998.745301", "10.1287/moor.4.3.233", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.2307/2284239", "10.1109/tvcg.2017.2674978", "10.1016/j.dss.2009.05.016", "10.1109/tvcg.2014.2346594", "10.1109/vds.2017.8573446", "10.1007/978-1-4684-2001-2\\_9", "10.1109/iv.2002.1028760", "10.1145/1842993.1842999", "10.1038/nmeth.2490", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2010.176", "10.1145/2702123.2702585", "10.1057/ivs.2009.34", "10.1179/000870403235002042"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934432", "title": "Discriminability Tests for Visualization Effectiveness and Scalability", "year": "2019", "conferenceName": "InfoVis", "authors": "Rafael Veras;Christopher Collins", "citationCount": "1", "affiliation": "Veras, R (Corresponding Author), Ontario Tech Univ, Oshawa, ON, Canada. Veras, Rafael; Collins, Christopher, Ontario Tech Univ, Oshawa, ON, Canada.", "countries": "Canada", "abstract": "The scalability of a particular visualization approach is limited by the ability for people to discern differences between plots made with different datasets. Ideally, when the data changes, the visualization changes in perceptible ways. This relation breaks down when there is a mismatch between the encoding and the character of the dataset being viewed. Unfortunately, visualizations are often designed and evaluated without fully exploring how they will respond to a wide variety of datasets. We explore the use of an image similarity measure, the Multi-Scale Structural Similarity Index (MS-SSIM), for testing the discriminability of a data visualization across a variety of datasets. MS-SSIM is able to capture the similarity of two visualizations across multiple scales, including low level granular changes and high level patterns. Significant data changes that are not captured by the MS-SSIM indicate visualizations of low discriminability and effectiveness. The measure's utility is demonstrated with two empirical studies. In the first, we compare human similarity judgments and MS-SSIM scores for a collection of scatterplots. In the second, we compute the discriminability values for a set of basic visualizations and compare them with empirical measurements of effectiveness. In both cases, the analyses show that the computational measure is able to approximate empirical results. Our approach can be used to rank competing encodings on their discriminability and to aid in selecting visualizations for a particular type of data distribution.", "keywords": "Scalability,Discriminability,Simulation,Perception", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934432", "refList": ["10.1109/tvcg.2012.233", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2012.230", "10.1111/cgf.12647", "10.1109/tvcg.2016.2598918", "10.1109/tvcg.2007.70529", "10.1145/2858036.2858435", "10.1109/vast.2009.5332628", "10.1145/3025453.3025912", "10.1145/1133265.1133318", "10.1109/tip.2010.2092435", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2865264", "10.1175/jtech-d-11-00103.1", "10.1145/1190036.1190039", "10.1111/cgf.12127", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1109/mcg.2014.18", "10.1145/3173574.3174172", "10.1109/tvcg.2009.153", "10.1057/palgrave.ivs.9500070", "10.1109/tvcg.2018.2790961", "10.1109/tvcg.2014.2346978", "10.1109/tvcg.2018.2810918", "10.1111/cgf.13409", "10.1109/tip.2003.819861", "10.1145/2642918.2647411", "10.1080/15230406.2016.1140074", "10.1109/jstsp.2009.2015374", "10.1109/tvcg.2010.237", "10.1109/mcg.2009.6", "10.1111/j.1467-8659.2009.01667.x", "10.1109/tvcg.2010.161", "10.1111/cgf.13446", "10.1109/tvcg.2014.2346325", "10.1109/tvcg.2009.111", "10.1147/jrd.2015.2411412"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030394", "title": "Direct Volume Rendering with Nonparametric Models of Uncertainty", "year": "2020", "conferenceName": "SciVis", "authors": "Tushar M. Athawale;Bo Ma 0002;Elham Sakhaee;Christopher R. Johnson;Alireza Entezari", "citationCount": "0", "affiliation": "Athawale, TM (Corresponding Author), Univ Utah, Sci Comp \\& Imaging SCI Inst, Salt Lake City, UT 84112 USA. Athawale, Tushar M.; Johnson, Chris R., Univ Utah, Sci Comp \\& Imaging SCI Inst, Salt Lake City, UT 84112 USA. Ma, Bo; Sakhaee, Elham; Entezari, Alireza, Univ Florida, Dept CISE, Gainesville, FL 32611 USA.", "countries": "USA", "abstract": "We present a nonparametric statistical framework for the quantification, analysis, and propagation of data uncertainty in direct volume rendering (DVR). The state-of-the-art statistical DVR framework allows for preserving the transfer function (TF) of the ground truth function when visualizing uncertain data; however, the existing framework is restricted to parametric models of uncertainty. In this paper, we address the limitations of the existing DVR framework by extending the DVR framework for nonparametric distributions. We exploit the quantile interpolation technique to derive probability distributions representing uncertainty in viewing-ray sample intensities in closed form, which allows for accurate and efficient computation. We evaluate our proposed nonparametric statistical models through qualitative and quantitative comparisons with the mean-field and parametric statistical models, such as uniform and Gaussian, as well as Gaussian mixtures. In addition, we present an extension of the state-of-the-art rendering parametric framework to 2D TFs for improved DVR classifications. We show the applicability of our uncertainty quantification framework to ensemble, downsampled, and bivariate versions of scalar field datasets.", "keywords": "Volumes,uncertainty,nonparametric,2D transfer function", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030394", "refList": ["10.1109/icdm.2012.80", "10.1145/1401890.1401904", "10.1109/tvcg.2017.2745139", "10.18128/d030.v6.0", "10.1111/cgf.12142", "10.1109/tvcg.2009.114", "10.1111/j.1467-8659.2009.01677.x", "10.1109/mcse.2007.55", "10.1109/icde.2012.16", "10.1198/106186008x320465", "10.1145/1835804.1835868", "10.1559/1523040054738936", "10.1109/tvcg.2019.2934432", "10.1109/34.1000236", "10.1109/infvis.2005.1532136", "10.1109/tvcg.2012.128", "10.1109/pacificvis.2017.8031573", "10.1109/tvcg.2016.2598920", "10.1109/tvcg.2018.2865021", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1109/sp.2009.22", "10.1109/tvcg.2017.2744184", "10.1145/773153.773173", "10.1145/2660267.2660348", "10.1109/icdmw.2009.93", "10.1016/j.jtrangeo.2015.09.001", "10.1109/icde.2010.5447831", "10.1007/11681878\\_14", "10.1111/cgf.13409", "10.1007/s13278-014-0205-5", "10.1109/tvcg.2011.163", "10.1145/3035918.3035940", "10.1109/sp.2008.33", "10.1145/2882903.2882931"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934208", "title": "Evaluating Perceptual Bias During Geometric Scaling of Scatterplots", "year": "2019", "conferenceName": "VAST", "authors": "Yating Wei;Honghui Mei;Ying Zhao;Shuyue Zhou;Bingru Lin;Haojing Jiang;Wei Chen", "citationCount": "5", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou 310058, Zhejiang, Peoples R China. Zhao, Y (Corresponding Author), Cent South Univ, Sch Comp Sci \\& Engn, Changsha 410083, Hunan, Peoples R China. Wei, Yating; Mei, Honghui; Zhou, Shuyue; Lin, Bingru; Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou 310058, Zhejiang, Peoples R China. Zhao, Ying; Jiang, Haojing, Cent South Univ, Sch Comp Sci \\& Engn, Changsha 410083, Hunan, Peoples R China.", "countries": "China", "abstract": "Scatterplots are frequently scaled to fit display areas in multi-view and multi-device data analysis environments. A common method used for scaling is to enlarge or shrink the entire scatterplot together with the inside points synchronously and proportionally. This process is called geometric scaling. However, geometric scaling of scatterplots may cause a perceptual bias, that is, the perceived and physical values of visual features may be dissociated with respect to geometric scaling. For example, if a scatterplot is projected from a laptop to a large projector screen, then observers may feel that the scatterplot shown on the projector has fewer points than that viewed on the laptop. This paper presents an evaluation study on the perceptual bias of visual features in scatterplots caused by geometric scaling. The study focuses on three fundamental visual features (i.e., numerosity, correlation, and cluster separation) and three hypotheses that are formulated on the basis of our experience. We carefully design three controlled experiments by using well-prepared synthetic data and recruit participants to complete the experiments on the basis of their subjective experience. With a detailed analysis of the experimental results, we obtain a set of instructive findings. First, geometric scaling causes a bias that has a linear relationship with the scale ratio. Second, no significant difference exists between the biases measured from normally and uniformly distributed scatterplots. Third, changing the point radius can correct the bias to a certain extent. These findings can be used to inspire the design decisions of scatterplots in various scenarios.", "keywords": "Evaluation,scatterplot,geometric scaling,bias,perceptual consistency", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934208", "refList": ["10.2307/2288843", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2014.2346979", "10.1126/science.216.4550.1138", "10.1109/tvcg.2017.2744138", "10.1111/j.1467-8659.2009.01467.x", "10.1145/2491568.2491577", "10.1109/mwc.2018.1700325", "10.1145/2449396.2449439", "10.1109/tvcg.2011.127", "10.1109/tvcg.2011.229", "10.1167/15.5.4", "10.1073/pnas.1113195108", "10.1145/2702123.2702545", "10.1109/vast.2009.5332628", "10.1109/tvcg.2018.2800013", "10.1007/s12650-018-0530-2", "10.1016/s0042-6989(97)00340-4", "10.1109/vast.2010.5652460", "10.1109/mcg.2018.2879067", "10.1109/tvcg.2018.2864912", "10.1109/pacificvis.2010.5429604", "10.1109/tcst.2018.2819965", "10.1167/10.2.10", "10.1145/2470654.2481318", "10.1145/1842993.1843002", "10.1167/12.6.8", "10.1109/tvcg.2013.124", "10.1057/ivs.2008.13", "10.1109/tvcg.2007.70596", "10.1109/tvcg.2018.2865266", "10.1145/3173574.3173664", "10.1109/tvcg.2015.2467671", "10.1016/j.cag.2017.07.004", "10.1177/0956797613501520", "10.1109/tvcg.2018.2865020", "10.1111/j.1467-8659.2012.03125.x", "10.3758/bf03205986", "10.3758/s13423-016-1174-7", "10.1109/tvcg.2017.2680452", "10.1109/mc.2006.109", "10.1109/tvcg.2017.2744098", "10.1038/srep32810", "10.1016/j.visres.2013.06.006", "10.1002/jhbs.20078", "10.1109/tvcg.2006.163", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1016/j.cognition.2007.10.009", "10.1109/tvcg.2018.2865142", "10.3758/app.72.7.1839", "10.1109/tvcg.2018.2810918", "10.1016/j.jvlc.2017.10.001", "10.1145/2682623", "10.1109/tvcg.2018.2864884", "10.1145/3025453.3025984", "10.1109/tvcg.2006.184", "10.1109/tvcg.2013.153", "10.1016/j.jvlc.2018.08.003", "10.1109/tvcg.2016.2520921", "10.1111/cgf.13446", "10.1017/s0022381612000187", "10.1145/2702123.2702406", "10.1109/vast.2012.6400487", "10.1109/tvcg.2013.183", "10.1177/1473871611415997", "10.1145/2702123.2702585", "10.1145/2993901.2993903", "10.1109/tvcg.2013.120", "10.1111/cgf.12632", "10.1145/1385569.1385602", "10.1109/tvcg.2017.2754480", "10.1111/j.1467-8659.2009.01694.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934300", "title": "GUIRO: User-Guided Matrix Reordering", "year": "2019", "conferenceName": "VAST", "authors": "Michael Behrisch;Tobias Schreck;Hanspeter Pfister", "citationCount": "1", "affiliation": "Behrisch, M (Corresponding Author), Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Behrisch, Michael; Pfister, Hanspeter, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Schreck, Tobias, Graz Univ Technol, Graz, Austria.", "countries": "USA;Austria", "abstract": "Matrix representations are one of the main established and empirically proven to be effective visualization techniques for relational (or network) data. However, matrices\u2014similar to node-link diagrams\u2014are most effective if their layout reveals the underlying data topology. Given the many developed algorithms, a practical problem arises: \u201cWhich matrix reordering algorithm should I choose for my dataset at hand?\u201d To make matters worse, different reordering algorithms applied to the same dataset may let significantly different visual matrix patterns emerge. This leads to the question of trustworthiness and explainability of these fully automated, often heuristic, black-box processes. We present GUIRO, a Visual Analytics system that helps novices, network analysts, and algorithm designers to open the black-box. Users can investigate the usefulness and expressiveness of 70 accessible matrix reordering algorithms. For network analysts, we introduce a novel model space representation and two interaction techniques for a user-guided reordering of rows or columns, and especially groups thereof (submatrix reordering). These novel techniques contribute to the understanding of the global and local dataset topology. We support algorithm designers by giving them access to 16 reordering quality metrics and visual exploration means for comparing reordering implementations on a row/column permutation level. We evaluated GUIRO in a guided explorative user study with 12 subjects, a case study demonstrating its usefulness in a real-world scenario, and through an expert study gathering feedback on our design decisions. We found that our proposed methods help even inexperienced users to understand matrix patterns and allow a user-guided steering of reordering algorithms. GUIRO helps to increase the transparency of matrix reordering algorithms, thus helping a broad range of users to get a better insight into the complex reordering process, in turn supporting data and reordering algorithm insights.", "keywords": "Visual Analytics,matrix,black-box algorithms,seriation,ordering,sorting,steerable algorithm,interaction,2D projection", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934300", "refList": ["10.1109/tvcg.2007.70582", "10.2307/276978", "10.1109/tvcg.2008.61", "10.3390/su10040973", "10.1101/121889", "10.1145/1345448.1345453", "10.1186/s12879-014-0695-9", "10.1186/1471-2105-9-155", "10.1145/1124772.1124891", "10.1109/tvcg.2010.159", "10.1111/j.2044-8317.1974.tb00534.x", "10.1109/tpami.2015.2470671", "10.1198/000313005x22770", "10.1109/tvcg.2012.219", "10.1002/sam.10071", "10.1016/j.ejor.2016.08.066", "10.1007/s00265-003-0651-y", "10.1109/biovis.2013.6664342", "10.3115/v1/p14-1062", "10.3389/fpsyg.2017.01349", "10.1109/tpami.2004.1265866", "10.1057/palgrave.ivs.9500086", "10.1007/s004260000031", "10.1111/cgf.12935", "10.1145/800195.805928", "10.1109/tvcg.2014.2346279", "10.2312/eurovisstar.20141174", "10.1109/tvcg.2012.256", "10.1093/bioinformatics/17.suppl\\_1.s22", "10.1109/infvis.2004.46", "10.1007/978-3-319-06793-3\\_5", "10.1109/tvcg.2006.147", "10.1109/tvcg.2015.2468078", "10.1093/bioinformatics/bti141", "10.1145/1168149.1168168", "10.1109/tvcg.2017.2745978", "10.1177/1473871613513228", "10.1109/mcg.2014.62", "10.1109/tvcg.2006.166", "10.1145/2470654.2470724", "10.1177/154193120605000909", "10.1109/sibgrapi.2007.21", "10.1145/3065386", "10.1109/tvcg.2006.160", "10.1287/opre.20.5.993", "10.1111/cgf.13446", "10.1057/palgrave.ivs.9500092", "10.1145/568522.568523", "10.1109/mcg.2013.66", "10.1109/tvcg.2018.2865940", "10.1109/tvcg.2012.208"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934796", "title": "Improving the Robustness of Scagnostics", "year": "2019", "conferenceName": "InfoVis", "authors": "Yunhai Wang;Zeyu Wang 0005;Tingting Liu;Michael Correll;Zhanglin Cheng;Oliver Deussen;Michael Sedlmair", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Jinan, Peoples R China. Wang, Yunhai; Wang, Zeyu; Liu, Tingting, Shandong Univ, Jinan, Peoples R China. Wang, Zeyu; Cheng, Zhanglin; Deussen, Oliver, SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Peoples R China. Correll, Michael, Tableau Res, Seattle, WA USA. Deussen, Oliver, Konstanz Univ, Constance, Germany. Sedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "In this paper, we examine the robustness of scagnostics through a series of theoretical and empirical studies. First, we investigate the sensitivity of scagnostics by employing perturbing operations on more than 60M synthetic and real-world scatterplots. We found that two scagnostic measures, Outlying and Clumpy, are overly sensitive to data binning. To understand how these measures align with human judgments of visual features, we conducted a study with 24 participants, which reveals that i) humans are not sensitive to small perturbations of the data that cause large changes in both measures, and ii) the perception of clumpiness heavily depends on per-cluster topologies and structures. Motivated by these results, we propose Robust Scagnostics (RScag) by combining adaptive binning with a hierarchy-based form of scagnostics. An analysis shows that RScag improves on the robustness of original scagnostics, aligns better with human judgments, and is equally fast as the traditional scagnostic measures.", "keywords": "Scagnostics,scatterplots,sensitivity analysis,Robust Scagnostics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934796", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2346979", "10.1111/j.1467-8659.2009.01467.x", "10.1109/vast.2008.4677368", "10.2307/2289444", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/vast.2009.5332628", "10.1111/insr.12095\\_11", "10.1109/tvcg.2010.184", "10.1109/tvcg.2015.2467323", "10.1109/vast.2010.5652460", "10.1111/j.1467-8659.2012.03069.x", "10.1145/1842993.1843002", "10.1198/106186008x320465", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.20", "10.1111/cgf.12641", "10.1109/tvcg.2012.128", "10.1109/tvcg.2015.2467671", "10.1057/palgrave.ivs.9500091", "10.1007/978-1-4612-4400-4", "10.1111/cgf.13176", "10.1002/0470870958", "10.1109/tvcg.2018.2864907", "10.1111/j.1467-8659.2012.03125.x", "10.1109/vast.2012.6400490", "10.1109/infvis.2005.1532142", "10.1109/vast.2010.5652433", "10.1109/vast.2009.5332611", "10.1201/9781315140919", "10.1145/2858036.2858155", "10.1109/ldav.2013.6675164", "10.1515/itit-2014-1070", "10.1111/cgf.13684", "10.1109/tpami.1979.4766909", "10.2307/2289936", "10.1109/pacificvis.2014.42", "10.1109/tvcg.2016.2598467", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2006.94", "10.1109/tvcg.2014.2346572", "10.1111/cgf.12632", "10.1109/tvcg.2017.2744339"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/vast47406.2019.8986917", "title": "VIANA: Visual Interactive Annotation of Argumentation", "year": "2019", "conferenceName": "VAST", "authors": "Fabian Sperrle;Rita Sevastjanova;Rebecca Kehlbeck;Mennatallah El-Assady", "citationCount": "0", "affiliation": "Sperrle, F (Corresponding Author), Univ Konstanz, Constance, Germany. Sperrle, Fabian; Sevastjanova, Rita; Kehlbeck, Rebecca; El-Assady, Mennatallah, Univ Konstanz, Constance, Germany.", "countries": "Germany", "abstract": "Argumentation Mining addresses the challenging tasks of identifying boundaries of argumentative text fragments and extracting their relationships. Fully automated solutions do not reach satisfactory accuracy due to their insufficient incorporation of semantics and domain knowledge. Therefore, experts currently rely on time-consuming manual annotations. In this paper, we present a visual analytics system that augments the manual annotation process by automatically suggesting which text fragments to annotate next. The accuracy of those suggestions is improved over time by incorporating linguistic knowledge and language modeling to learn a measure of argument similarity from user interactions. Based on a long-term collaboration with domain experts, we identify and model five high-level analysis tasks. We enable close reading and note-taking, annotation of arguments, argument reconstruction, extraction of argument relations, and exploration of argument graphs. To avoid context switches, we transition between all views through seamless morphing, visually anchoring all text- and graph-based layers. We evaluate our system with a two-stage expert user study based on a corpus of presidential debates. The results show that experts prefer our system over existing solutions due to the speedup provided by the automatic suggestions and the tight integration between text and graph views.", "keywords": "Argumentation annotation,machine learning,user interaction,layered interfaces,semantic transitions", "link": "http://dx.doi.org/10.1109/VAST47406.2019.8986917", "refList": ["10.1007/978-3-642-40624-9\\_1", "10.3233/978-1-61499-436-7-185", "10.1145/371920.372071", "10.3233/978-1-61499-906-5-4", "10.1109/tvcg.2015.2467759", "10.18653/v1/d15-1050", "10.2307/2529310", "10.1109/mic.2003.1167344", "10.1145/312624.312682", "10.1145/2850417", "10.18653/v1/p17-2039", "10.1016/j.eswa.2016.02.013", "10.1007/978-0-387-85820-3\\_3", "10.1007/s11412-009-9080-x", "10.1109/tvcg.2008.127", "10.1145/2207676.2207741", "10.1109/cit.2012.217", "10.3233/aac-170022", "10.1109/tvcg.2017.2745080", "10.1007/978-3-319-44039-2\\_6", "10.1145/3290605.3300233", "10.1007/978-94-017-0431-1", "10.3233/978-1-61499-906-5-313", "10.1142/s0218213004001922", "10.1109/tvcg.2012.262", "10.1177/001316446002000104", "10.1109/tvcg.2018.2834341", "10.3233/978-1-61499-436-7-463", "10.1145/1772690.1772773", "10.1109/tvcg.2014.2346677", "10.1145/2523813", "10.1162/153244303322533223", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2015.2467531", "10.1109/tvcg.2018.2864769", "10.1007/978-3-319-90092-6\\_14", "10.1137/1.9781611972801.19", "10.1109/vast.2012.6400485", "10.1007/s10579-013-9215-6", "10.3115/v1/d14-1162", "10.1111/cgf.13446", "10.1109/tvcg.2006.156", "10.1111/cgf.13092", "10.1145/2645710.2645759", "10.1109/bigdata.2017.8258140", "10.1145/2669557.2669572", "10.2312/eurovisstar.20151113"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030376", "title": "Insight Beyond Numbers: The Impact of Qualitative Factors on Visual Data Analysis", "year": "2020", "conferenceName": "VAST", "authors": "Benjamin Karer;Hans Hagen;Dirk J. Lehmann", "citationCount": "0", "affiliation": "Karer, B (Corresponding Author), Fed Criminal Police Off Germany, Wiesbaden, Germany. Karer, Benjamin, Fed Criminal Police Off Germany, Wiesbaden, Germany. Hagen, Hans, TU Kaiserslautern, Kaiserslautern, Germany. Lehmann, Dirk J., Ostfalia Univ Appl Sci, Wolfenbuttel, Germany. Lehmann, Dirk J., IAV GmbH, Berlin, Germany.", "countries": "Germany", "abstract": "As of today, data analysis focuses primarily on the findings to be made inside the data and concentrates less on how those findings relate to the domain of investigation. Contemporary visualization as a field of research shows a strong tendency to adopt this data-centrism. Despite their decisive influence on the analysis result, qualitative aspects of the analysis process such as the structure, soundness, and complexity of the applied reasoning strategy are rarely discussed explicitly. We argue that if the purpose of visualization is the provision of domain insight rather than the depiction of data analysis results, a holistic perspective requires a qualitative component to to be added to the discussion of quantitative and human factors. To support this point, we demonstrate how considerations of qualitative factors in visual analysis can be applied to obtain explanations and possible solutions for a number of practical limitations inherent to the data-centric perspective on analysis. Based on this discussion of what we call qualitative visual analysis, we develop an inside-outside principle of nested levels of context that can serve as a conceptual basis for the development of visualization systems that optimally support the emergence of insight during analysis.", "keywords": "Visualization,Reasoning,Qualitative Aspects", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030376", "refList": ["10.1057/ivs.2009.22", "10.1109/tvcg.2015.2467732", "10.1109/infvis.2000.885092", "10.1109/vast.2014.7042482", "10.1145/1498700.1498704", "10.1109/tvcg.2009.108", "10.1109/tvcg.2018.2829750", "10.1037/0033-295x.111.4.1036", "10.1109/tvcg.2012.199", "10.1109/38.267476", "10.1007/978-3-540-70956-5\\_2", "10.1109/tvcg.2015.2467613", "10.1109/tvcg.2015.2467195", "10.1109/tvcg.2014.2346419", "10.1109/vast.2017.8585669", "10.1109/tvcg.2010.132", "10.1145/22949.22950", "10.1109/tvcg.2014.2346984", "10.1017/cbo9780511816772", "10.1109/mcg.2003.1231171", "10.1111/coin.12227", "10.1109/tvcg.2018.2865138", "10.1109/38.788803", "10.1109/tvcg.2018.2864849", "10.1109/mcg.2012.120", "10.1109/tvcg.2007.70535", "10.1162/neco.2008.12-06-420", "10.1109/tvcg.2018.2865240", "10.1089/tmj.2010.0114", "10.1109/beliv.2018.8634267", "10.1109/tvcg.2014.2346481", "10.1207/s15327809jls0402\\_2", "10.1109/tvcg.2006.80", "10.1145/2858036.2858280", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2011.52", "10.1016/j.cag.2014.03.002", "10.1109/tvcg.2015.2513410", "10.1111/j.1756-8765.2011.01150.x", "10.1109/infvis.2005.1532142", "10.1002/spe.4380211102", "10.1186/s41235-018-0120-9", "10.1073/pnas.1807180116", "10.1109/tvcg.2012.133", "10.1109/tvcg.2012.273", "10.1111/j.1467-8659.2011.01928.x", "10.1145/353485.353486", "10.1109/tvcg.2015.2462356", "10.1109/mcg.2019.2923483", "10.1111/cgf.13264", "10.1057/palgrave.ivs.9500070", "10.1145/1168149.1168158", "10.1109/mcg.2019.2961716", "10.1016/s0167-739x(96)00029-5", "10.1111/j.1467-8659.2009.01667.x", "10.1177/1473871611415989", "10.1109/tvcg.2013.234", "10.1109/iv.2012.33", "10.1111/cgf.13446", "10.1057/ivs.2008.28", "10.1111/cgf.12379", "10.1037/0033-295x.112.1.159", "10.1109/tvcg.2010.161", "10.1109/tvcg.2017.2744319", "10.1145/989863.989880", "10.1007/s11390-016-1663-1", "10.1177/1473871615609787", "10.1016/j.cola.2019.100911"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030395", "title": "Towards Modeling Visualization Processes as Dynamic Bayesian Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Christian Heine 0002", "citationCount": "0", "affiliation": "Heine, C (Corresponding Author), Univ Leipzig, Leipzig, Germany. Heine, Christian, Univ Leipzig, Leipzig, Germany.", "countries": "Germany", "abstract": "Visualization designs typically need to be evaluated with user studies, because their suitability for a particular task is hard to predict. What the field of visualization is currently lacking are theories and models that can be used to explain why certain designs work and others do not. This paper outlines a general framework for modeling visualization processes that can serve as the first step towards such a theory. It surveys related research in mathematical and computational psychology and argues for the use of dynamic Bayesian networks to describe these time-dependent, probabilistic processes. It is discussed how these models could be used to aid in design evaluation. The development of concrete models will be a long process. Thus, the paper outlines a research program sketching how to develop prototypes and their extensions from existing models, controlled experiments, and observational studies.", "keywords": "Visualization,model building,perception,cognition,dynamic Bayesian networks", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030395", "refList": ["10.1057/ivs.2009.22", "10.1109/tvcg.2015.2467732", "10.1109/infvis.2000.885092", "10.1109/vast.2014.7042482", "10.1145/1498700.1498704", "10.1109/tvcg.2009.108", "10.1109/tvcg.2018.2829750", "10.1037/0033-295x.111.4.1036", "10.1109/tvcg.2012.199", "10.1111/cgf.13899", "10.1109/38.267476", "10.1007/978-3-540-70956-5\\_2", "10.1109/tvcg.2015.2467613", "10.1109/tvcg.2015.2467195", "10.1109/tvcg.2014.2346419", "10.1145/3290605.3300562", "10.1109/vl.1996.545307", "10.1109/vast.2017.8585669", "10.1109/infvis.2003.1249004", "10.1109/tvcg.2010.132", "10.1145/22949.22950", "10.1109/tvcg.2014.2346984", "10.1145/3290605.3300418", "10.1017/cbo9780511816772", "10.1109/mcg.2003.1231171", "10.1111/coin.12227", "10.1109/tvcg.2018.2865138", "10.1109/38.788803", "10.1109/tvcg.2018.2864849", "10.1109/mcg.2012.120", "10.1109/tvcg.2007.70535", "10.1162/neco.2008.12-06-420", "10.1109/tvcg.2018.2865240", "10.1089/tmj.2010.0114", "10.1109/beliv.2018.8634267", "10.1109/tvcg.2014.2346481", "10.1207/s15327809jls0402\\_2", "10.1109/tvcg.2006.80", "10.1145/2858036.2858280", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2011.52", "10.1016/j.cag.2014.03.002", "10.1109/tvcg.2015.2513410", "10.1111/j.1756-8765.2011.01150.x", "10.1109/infvis.2005.1532142", "10.1002/spe.4380211102", "10.1186/s41235-018-0120-9", "10.1073/pnas.1807180116", "10.1109/tvcg.2012.133", "10.1109/tvcg.2012.273", "10.1111/j.1467-8659.2011.01928.x", "10.1145/353485.353486", "10.1109/tvcg.2015.2462356", "10.1109/mcg.2019.2923483", "10.1111/cgf.13264", "10.1057/palgrave.ivs.9500070", "10.1145/1168149.1168158", "10.1109/mcg.2019.2961716", "10.1016/s0167-739x(96)00029-5", "10.1111/j.1467-8659.2009.01667.x", "10.1177/1473871611415989", "10.1109/tvcg.2013.234", "10.1109/iv.2012.33", "10.1111/cgf.13446", "10.1057/ivs.2008.28", "10.1111/cgf.12379", "10.1037/0033-295x.112.1.159", "10.1017/cb09781139033916.005", "10.1109/tvcg.2010.161", "10.1109/tvcg.2017.2744319", "10.1145/989863.989880", "10.1007/s11390-016-1663-1", "10.1177/1473871615609787", "10.1016/j.cola.2019.100911", "10.1057/palgrave.ivs.9500025"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.8199", "year": "2020", "title": "Efficient Morphing of Shape-preserving Star Coordinates", "conferenceName": "PacificVis", "authors": "Vladimir Molchanov;Sagad Hamid;Lars Linsen", "citationCount": "0", "affiliation": "Molchanov, V (Corresponding Author), Westfalische Wilhelms Univ Munster, Munster, Germany.\nMolchanov, Vladimir; Hamid, Sagad; Linsen, Lars, Westfalische Wilhelms Univ Munster, Munster, Germany.", "countries": "Germany", "abstract": "Data tours follow an exploratory multi-dimensional data visualization concept that provides animations of projections of the multidimensional data to a 2D visual space. To create an animation, a sequence of key projections is provided and morphings between each pair of consecutive key projections are computed, which then can be stitched together to form the data tour. The morphings should be smooth so that a user can easily follow the transformations, and their computations shall be fast to allow for their integration into an interactive visual exploration process. Moreover, if the key projections are chosen to satisfy additional conditions, it is desirable that these conditions are maintained during morphing. Shape preservation is such a desirable condition, as it avoids shape distortions that may otherwise be caused by a projection. We develop a novel efficient morphing algorithms for computing shape-preserving data tours, i.e., data tours constructed for a sequence of shape-preserving linear projections. We propose a stepping strategy for the morphing to avoid discontinuities in the evolution of the projections, where we represent the linear projections using a star-coordinates system. Our algorithms are less computationally involved, produce smoother morphings, and require less user-defined parameter settings than existing state-of-the-art approaches.", "keywords": "Human-centered computing; Visualization; Visualization techniques", "link": "https://doi.org/10.1109/PacificVis48177.2020.8199", "refList": ["10.2312/eurovisshort.20141152", "10.1109/tvcg.2013.182", "10.1109/tvcg.2008.153", "10.1109/tvcg.2015.2467591", "10.4135/9781412985130", "10.1111/cgf.12878", "10.1016/j.cag.2016.08.007", "10.1080/10618600.1995.10474674", "10.1109/infvis.2003.1249004", "10.1002/9781118445112.stat06472", "10.1109/visual.1997.663916", "10.1111/cgf.12117", "10.1137/0906011", "10.1109/tvcg.2018.2865118", "10.1126/science.290.5500.2319", "10.1111/cgf.12845", "10.1111/j.1467-8659.2012.03125.x", "10.1109/tvcg.2017.2705189", "10.1002/0471725293", "10.1111/cgf.12876", "10.1111/cgf.13404", "10.2307/2289161", "10.1111/cgf.13446", "10.1198/106186008x318440", "10.2307/1390747", "10.1109/tvcg.2012.35", "10.1109/tvcg.2015.2467132", "10.1109/tvcg.2006.94", "10.1109/t-c.1974.224051", "10.1109/pacificvis.2019.00018", "10.1109/tvcg.2017.2744339", "10.1109/tsmcb.2005.850151"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2019.00018", "year": "2019", "title": "Scatterplot Summarization by Constructing Fast and Robust Principal Graphs from Skeletons", "conferenceName": "PacificVis", "authors": "Jos{\\'{e}} Matute;Marcel Fischer;Alexandru C. Telea;Lars Linsen", "citationCount": "1", "affiliation": "Matute, J (Corresponding Author), Univ Munster, Munster, Germany.\nMatute, Jose; Fischer, Marcel; Linsen, Lars, Univ Munster, Munster, Germany.\nTelea, Alexandru C., Univ Groningen, Groningen, Netherlands.", "countries": "Germany;Netherlands", "abstract": "Principal curves are a long-standing and well-known method for summarizing large scatterplots. They are defined as self-consistent curves (or curve sets in the more general case) that locally pass through the middle of the scatterplot data. However, computing principal curves that capture well complex scatterplot topologies and are robust to noise is hard and/or slow for large scatterplots. We present a fast and robust approach for computing principal graphs (a generalization of principal curves for more complex topologies) inspired by the similarity to medial descriptors (curves locally centered in a shape). Compared to state-of-the-art methods for computing principal graphs, we outperform these in terms of computational scalability and robustness to noise and resolution. We also demonstrate the advantages of our method over other scatterplot summarization approaches.", "keywords": "", "link": "https://doi.org/10.1109/PacificVis.2019.00018", "refList": ["10.1016/j.cag.2014.01.006", "10.1016/s0167-8655(02)00032-6", "10.1006/jmva.2000.1917", "10.1111/cgf.12386", "10.1109/vast.2008.4677367", "10.1109/tpami.2004.1261076", "10.1111/cgf.12865", "10.1109/tvcg.2010.213", "10.1109/tvcg.2014.2346455", "10.1109/tvcg.2011.233", "10.1111/j.1467-8659.2009.01680.x", "10.1109/tvcg.2016.2515611", "10.1109/tit.1982.1056489", "10.1109/34.899944", "10.1007/bf01889678", "10.1137/s0036144599352836", "10.1145/2858036.2858155", "10.1002/jhbs.20078", "10.1198/jcgs.2011.09224", "10.1109/tvcg.2017.2744184", "10.1057/ivs.2010.2", "10.1111/j.1467-8659.2012.03107.x", "10.2307/2289936", "10.1109/pacificvis.2014.42", "10.1109/tpami.2008.21", "10.1111/cgf.13446", "10.2307/2290446", "10.1109/34.982884", "10.1109/tvcg.2006.94", "10.2307/2288711", "10.1117/12.304651", "10.5201/ipol.2013.87", "10.1109/mlsp.2008.4685520"], "wos": 1, "children": [{"doi": "10.1109/pacificvis48177.2020.8199", "year": "2020", "title": "Efficient Morphing of Shape-preserving Star Coordinates", "conferenceName": "PacificVis", "authors": "Vladimir Molchanov;Sagad Hamid;Lars Linsen", "citationCount": "0", "affiliation": "Molchanov, V (Corresponding Author), Westfalische Wilhelms Univ Munster, Munster, Germany.\nMolchanov, Vladimir; Hamid, Sagad; Linsen, Lars, Westfalische Wilhelms Univ Munster, Munster, Germany.", "countries": "Germany", "abstract": "Data tours follow an exploratory multi-dimensional data visualization concept that provides animations of projections of the multidimensional data to a 2D visual space. To create an animation, a sequence of key projections is provided and morphings between each pair of consecutive key projections are computed, which then can be stitched together to form the data tour. The morphings should be smooth so that a user can easily follow the transformations, and their computations shall be fast to allow for their integration into an interactive visual exploration process. Moreover, if the key projections are chosen to satisfy additional conditions, it is desirable that these conditions are maintained during morphing. Shape preservation is such a desirable condition, as it avoids shape distortions that may otherwise be caused by a projection. We develop a novel efficient morphing algorithms for computing shape-preserving data tours, i.e., data tours constructed for a sequence of shape-preserving linear projections. We propose a stepping strategy for the morphing to avoid discontinuities in the evolution of the projections, where we represent the linear projections using a star-coordinates system. Our algorithms are less computationally involved, produce smoother morphings, and require less user-defined parameter settings than existing state-of-the-art approaches.", "keywords": "Human-centered computing; Visualization; Visualization techniques", "link": "https://doi.org/10.1109/PacificVis48177.2020.8199", "refList": ["10.2312/eurovisshort.20141152", "10.1109/tvcg.2013.182", "10.1109/tvcg.2008.153", "10.1109/tvcg.2015.2467591", "10.4135/9781412985130", "10.1111/cgf.12878", "10.1016/j.cag.2016.08.007", "10.1080/10618600.1995.10474674", "10.1109/infvis.2003.1249004", "10.1002/9781118445112.stat06472", "10.1109/visual.1997.663916", "10.1111/cgf.12117", "10.1137/0906011", "10.1109/tvcg.2018.2865118", "10.1126/science.290.5500.2319", "10.1111/cgf.12845", "10.1111/j.1467-8659.2012.03125.x", "10.1109/tvcg.2017.2705189", "10.1002/0471725293", "10.1111/cgf.12876", "10.1111/cgf.13404", "10.2307/2289161", "10.1111/cgf.13446", "10.1198/106186008x318440", "10.2307/1390747", "10.1109/tvcg.2012.35", "10.1109/tvcg.2015.2467132", "10.1109/tvcg.2006.94", "10.1109/t-c.1974.224051", "10.1109/pacificvis.2019.00018", "10.1109/tvcg.2017.2744339", "10.1109/tsmcb.2005.850151"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.13684", "year": "2019", "title": "ClustMe: A Visual Quality Measure for Ranking Monochrome Scatterplots based on Cluster Patterns", "conferenceName": "EuroVis", "authors": "Mostafa M. Abbas;Micha{\\\"{e}}l Aupetit;Michael Sedlmair;Halima Bensmail", "citationCount": "4", "affiliation": "Abbas, MM (Corresponding Author), HBKU, QCRI, Doha, Qatar.\nAbbas, Mostafa M.; Aupetit, Michael; Bensmail, Halima, HBKU, QCRI, Doha, Qatar.\nSedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany.", "countries": "Qatar;Germany", "abstract": "We propose ClustMe, a new visual quality measure to rank monochrome scatterplots based on cluster patterns. ClustMe is based on data collected from a human-subjects study, in which 34 participants judged synthetically generated cluster patterns in 1000 scatterplots. We generated these patterns by carefully varying the free parameters of a simple Gaussian Mixture Model with two components, and asked the participants to count the number of clusters they could see (1 or more than 1). Based on the results, we form ClustMe by selecting the model that best predicts these human judgments among 7 different state-of-the-art merging techniques (Demp). To quantitatively evaluate ClustMe, we conducted a second study, in which 31 human subjects ranked 435 pairs of scatterplots of real and synthetic data in terms of cluster patterns complexity. We use this data to compare ClustMe's performance to 4 other state-of-the-art clustering measures, including the well-known Clumpiness scagnostics. We found that of all measures, ClustMe is in strongest agreement with the human rankings.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13684", "refList": ["10.1109/tvcg.2014.2330617", "10.1109/tvcg.2014.2346979", "10.1109/tvcg.2017.2744138", "10.1109/tvcg.2017.2701829", "10.2307/2529310", "10.1109/tvcg.2011.229", "10.1109/vast.2011.6102437", "10.1068/p030033", "10.1016/j.jvcir.2011.01.005", "10.1109/tnn.2005.845141", "10.1145/2669557.2669559", "10.1167/8.7.6", "10.1145/2993901.2993907", "10.1145/2803140.2803143", "10.1145/1842993.1843002", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.124", "10.1057/ivs.2008.13", "10.1109/vast.2012.6400488", "10.1111/j.1467-9574.2008.00412.x", "10.1214/aos/1176344136", "10.1109/tvcg.2015.2467671", "10.3138/y308-2422-8615-1233", "10.1111/j.1467-8659.2012.03125.x", "10.1177/001316446002000104", "10.1145/2858036.2858155", "10.1007/s11634-010-0058-3", "10.1214/009053605000000417", "10.1109/tvcg.2009.153", "10.1023/a:1018510926151", "10.1109/tvcg.2014.2346978", "10.1007/s10816-016-9307-x", "10.1177/0301006615602599", "10.1109/pacificvis.2014.42", "10.1162/neco.1991.3.2.246", "10.1007/3-540-44491-2\\_3", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2018.2846735", "10.2307/2291604", "10.1109/inf0vis.2005.14", "10.1198/016214502760047131", "10.1007/s00180-008-0119-7", "10.1109/t-c.1974.224051", "10.1111/cgf.12632", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934796", "title": "Improving the Robustness of Scagnostics", "year": "2019", "conferenceName": "InfoVis", "authors": "Yunhai Wang;Zeyu Wang 0005;Tingting Liu;Michael Correll;Zhanglin Cheng;Oliver Deussen;Michael Sedlmair", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Jinan, Peoples R China. Wang, Yunhai; Wang, Zeyu; Liu, Tingting, Shandong Univ, Jinan, Peoples R China. Wang, Zeyu; Cheng, Zhanglin; Deussen, Oliver, SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Peoples R China. Correll, Michael, Tableau Res, Seattle, WA USA. Deussen, Oliver, Konstanz Univ, Constance, Germany. Sedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "In this paper, we examine the robustness of scagnostics through a series of theoretical and empirical studies. First, we investigate the sensitivity of scagnostics by employing perturbing operations on more than 60M synthetic and real-world scatterplots. We found that two scagnostic measures, Outlying and Clumpy, are overly sensitive to data binning. To understand how these measures align with human judgments of visual features, we conducted a study with 24 participants, which reveals that i) humans are not sensitive to small perturbations of the data that cause large changes in both measures, and ii) the perception of clumpiness heavily depends on per-cluster topologies and structures. Motivated by these results, we propose Robust Scagnostics (RScag) by combining adaptive binning with a hierarchy-based form of scagnostics. An analysis shows that RScag improves on the robustness of original scagnostics, aligns better with human judgments, and is equally fast as the traditional scagnostic measures.", "keywords": "Scagnostics,scatterplots,sensitivity analysis,Robust Scagnostics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934796", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2346979", "10.1111/j.1467-8659.2009.01467.x", "10.1109/vast.2008.4677368", "10.2307/2289444", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/vast.2009.5332628", "10.1111/insr.12095\\_11", "10.1109/tvcg.2010.184", "10.1109/tvcg.2015.2467323", "10.1109/vast.2010.5652460", "10.1111/j.1467-8659.2012.03069.x", "10.1145/1842993.1843002", "10.1198/106186008x320465", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.20", "10.1111/cgf.12641", "10.1109/tvcg.2012.128", "10.1109/tvcg.2015.2467671", "10.1057/palgrave.ivs.9500091", "10.1007/978-1-4612-4400-4", "10.1111/cgf.13176", "10.1002/0470870958", "10.1109/tvcg.2018.2864907", "10.1111/j.1467-8659.2012.03125.x", "10.1109/vast.2012.6400490", "10.1109/infvis.2005.1532142", "10.1109/vast.2010.5652433", "10.1109/vast.2009.5332611", "10.1201/9781315140919", "10.1145/2858036.2858155", "10.1109/ldav.2013.6675164", "10.1515/itit-2014-1070", "10.1111/cgf.13684", "10.1109/tpami.1979.4766909", "10.2307/2289936", "10.1109/pacificvis.2014.42", "10.1109/tvcg.2016.2598467", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2006.94", "10.1109/tvcg.2014.2346572", "10.1111/cgf.12632", "10.1109/tvcg.2017.2744339"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.14001", "year": "2020", "title": "Sunspot Plots: Model-based Structure Enhancement for Dense Scatter Plots", "conferenceName": "EuroVis", "authors": "Thomas Trautner;Fabian Bolte;Sergej Stoppel;Stefan Bruckner", "citationCount": "0", "affiliation": "Trautner, T (Corresponding Author), Univ Bergen, Bergen, Norway.\nTrautner, T.; Bolte, F.; Stoppel, S.; Bruckner, S., Univ Bergen, Bergen, Norway.", "countries": "Norway", "abstract": "Scatter plots are a powerful and well-established technique for visualizing the relationships between two variables as a collection of discrete points. However, especially when dealing with large and dense data, scatter plots often exhibit problems such as overplotting, making the data interpretation arduous. Density plots are able to overcome these limitations in highly populated regions, but fail to provide accurate information of individual data points. This is particularly problematic in sparse regions where the density estimate may not provide a good representation of the underlying data. In this paper, we present sunspot plots, a visualization technique that communicates dense data as a continuous data distribution, while preserving the discrete nature of data samples in sparsely populated areas. We furthermore demonstrate the advantages of our approach on typical failure cases of scatter plots within synthetic and real-world data sets and validate its effectiveness in a user study.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14001", "refList": ["10.1057/palgrave.ivs.9500122", "10.2312/eggh/hpg12/097-103", "10.1109/tvcg.2008.119", "10.1109/pacificvis.2011.5742387", "10.1038/331163a0", "10.1109/visual.2019.8933620", "10.2307/2683294", "10.1201/9781351072304", "10.2307/2289444", "10.1109/tvcg.2019.2934541", "10.1080/00949657508810123", "10.1109/tvcg.2013.65", "10.1109/infvis.1997.636789", "10.1109/2945.841121", "10.1109/mcse.2007.55", "10.1109/tvcg.2010.197", "10.1111/cgf.12871", "10.1109/infvis.2003.1249018", "10.1145/3173574.3173991", "10.1109/tvcg.2012.238", "10.1007/0-387-28695-0", "10.1109/pacificvis.2010.5429604", "10.18637/jss.v008.i03", "10.1109/tvcg.2007.70596", "10.1109/visual.1998.745301", "10.1007/0-387-37977-0\\_3", "10.1145/1556262.1556289", "10.2312/wiced.20161094", "10.1109/tvcg.2007.70535", "10.1145/1056808.1056914", "10.1109/tvcg.2003.1196007", "10.1109/iv.2004.1320190", "10.1111/cgf.12877", "10.1162/leon.2007.40.2.202a", "10.1109/tvcg.2017.2674978", "10.1057/ivs.2010.4", "10.1016/j.cag.2018.02.008", "10.1201/9781315140919", "10.1109/visual.2000.885677", "10.1002/jhbs.20078", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1109/visual.2001.964495", "10.1109/iv.2002.1028760", "10.1111/cgf.13684", "10.1109/hicss.2013.197", "10.1109/tvcg.2019.2903956", "10.1093/mnras/stt961", "10.1109/tvcg.2017.2668409", "10.1111/j.1467-8659.2009.01478.x", "10.1145/2702123.2702585", "10.2307/1418003", "10.2307/1390742", "10.1145/360825.360839", "10.1109/pacificvis.2009.4906843", "10.1057/ivs.2009.34", "10.2307/2288711"], "wos": 1, "children": [], "len": 1}], "len": 13}, {"doi": "10.1111/cgf.14000", "year": "2020", "title": "Evaluating Reordering Strategies for Cluster Identification in Parallel Coordinates", "conferenceName": "EuroVis", "authors": "Michael Blumenschein;Xuan Zhang;David Pomerenke;Daniel A. Keim;Johannes Fuchs", "citationCount": "0", "affiliation": "Blumenschein, M (Corresponding Author), Univ Konstanz, Constance, Germany.\nBlumenschein, Michael; Pomerenke, David; Keim, Daniel A.; Fuchs, Johannes, Univ Konstanz, Constance, Germany.\nZhang, Xuan, Rhein Westfal TH Aachen, Aachen, Germany.", "countries": "Germany", "abstract": "The ability to perceive patterns in parallel coordinates plots (PCPs) is heavily influenced by the ordering of the dimensions. While the community has proposed over 30 automatic ordering strategies, we still lack empirical guidance for choosing an appropriate strategy for a given task. In this paper, we first propose a classification of tasks and patterns and analyze which PCP reordering strategies help in detecting them. Based on our classification, we then conduct an empirical user study with 31 participants to evaluate reordering strategies for cluster identification tasks. We particularly measure time, identification quality, and the users' confidence for two different strategies using both synthetic and real-world datasets. Our results show that, somewhat unexpectedly, participants tend to focus on dissimilar rather than similar dimension pairs when detecting clusters, and are more confident in their answers. This is especially true when increasing the amount of clutter in the data. As a result of these findings, we propose a new reordering strategy based on the dissimilarity of neighboring dimension pairs.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14000", "refList": ["10.1111/j.1467-8659.2008.01241.x", "10.2312/conf/eg2013/stars/095-116", "10.1109/tvcg.2014.2346979", "10.1117/12.838819", "10.2307/2290001", "10.1111/cgf.12638", "10.1007/978-3-642-30217-6\\_42", "10.1109/sc.2005.47", "10.1002/wics.145", "10.1109/tvcg.2011.229", "10.1109/vast.2009.5332628", "10.1109/vast.2017.8585613", "10.1145/2993901.2993907", "10.1007/bf00410640", "10.1109/tvcg.2010.184", "10.11591/ijece.v5i6", "10.1111/j.1467-8659.2011.01961.x", "10.1109/visual.1997.663916", "10.1111/j.1467-8659.2012.03129.x", "10.1109/visual.1999.809866", "10.1057/ivs.2008.13", "10.1016/j.visinf.2017.11.001", "10.2307/2282967", "10.1007/978-0-387-68628-8\\_10", "10.5888/pcd9.120082", "10.1109/tvcg.2007.70535", "10.1109/vast.2010.5652450", "10.1007/bf01898350", "10.1016/0010-0285(91)90009-d", "10.1007/978-3-642-24958-7\\_12", "10.1109/infvis.2005.1532142", "10.1057/palgrave.ivs.9500166", "10.1111/j.1467-8659.2008.01239.x", "10.1109/mcse.2015.55", "10.2312/pe/eurovast/eurova12/007-011.7", "10.1109/tvcg.2009.153", "10.1198/jcgs.2010.09136", "10.1145/2463676.2463696", "10.1109/infvis.2005.1532138", "10.1109/tvcg.2010.242", "10.5220/0006097400400051", "10.1109/visual.2019.8933706", "10.1109/atc.2015.7388338", "10.2312/pe/eurovast/eurova12/007-011", "10.5120/5044-7370", "10.1111/cgf.13446", "10.1109/icmla.2012.148", "10.1016/j.jvlc.2015.12.001", "10.1109/tvcg.2015.2466992", "10.1111/j.1467-8659.2009.01666.x", "10.5169/seals-266450", "10.1109/infvis.1998.729559", "10.1016/j.jvlc.2017.10.003", "10.1109/visual.1995.485139", "10.1007/978-0-387-39940-9\\_262", "10.1109/tvcg.2006.138"], "wos": 1, "children": [], "len": 1}], "len": 71}, {"doi": "10.1111/cgf.13682", "year": "2019", "title": "Toward Understanding Representation Methods in Visualization Recommendations through Scatterplot Construction Tasks", "conferenceName": "EuroVis", "authors": "Sehi L'Yi;Youli Chang;DongHwa Shin;Jinwook Seo", "citationCount": "0", "affiliation": "Seo, J (Corresponding Author), Seoul Natl Univ, Seoul, South Korea.\nL'Yi, Sehi; Chang, Youli; Shin, DongHwa; Seo, Jinwook, Seoul Natl Univ, Seoul, South Korea.", "countries": "Korea", "abstract": "Most visualization recommendation systems predominantly rely on graphical previews to describe alternative visual encodings. However, since InfoVis novices are not familiar with visual representations (e.g., interpretation barriers {[}GTS10]), novices might have difficulty understanding and choosing recommended visual encodings. As an initial step toward understanding effective representation methods for visualization recommendations, we investigate the effectiveness of three representation methods (i.e., previews, animated transitions, and textual descriptions) under scatterplot construction tasks. Our results show how different representations individually and cooperatively help users understand and choose recommended visualizations, for example, by supporting their expect-and-confirm process. Based on our study results, we discuss design implications for visualization recommendation interfaces.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13682", "refList": ["10.1177/1473871611416549", "10.1111/cgf.12106", "10.1145/2047196.2047247", "10.1007/s00371-015-1132-9", "10.1145/2858036.2858435", "10.1057/ivs.2008.27", "10.1109/tvcg.2007.70594", "10.2312/pe.eurovisshort.eurovisshort2013.019-023", "10.1007/978-3-642-23768-3\\_23", "10.14778/2831360.2831371", "10.1145/1502650.1502695", "10.1109/pacificvis.2017.8031599", "10.1145/3025453.3025942", "10.1145/2858036.2858101", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2014.2346292", "10.1109/tvcg.2016.2598920", "10.1057/palgrave/ivs/9500002", "10.1109/tvcg.2017.2744184", "10.1111/cgf.13409", "10.1109/tvcg.2007.70539", "10.1109/2945.981851", "10.1109/tvcg.2010.164", "10.1145/3025453.3025752", "10.1109/pacificvis.2014.42", "10.1145/2213836", "10.1111/cgf.12635", "10.1109/tvcg.2015.2467191", "10.1057/palgrave.ivs.9500091", "10.1109/tvcg.2015.2413786"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13673", "year": "2019", "title": "Multiple Views: different meanings and collocated words", "conferenceName": "EuroVis", "authors": "Jonathan C. Roberts;Hayder Al{-}Maneea;Peter W. S. Butcher;Robert Lew;Geraint Rees;Nirwan Sharma;Ana Frankenberg{-}Garcia", "citationCount": "1", "affiliation": "Roberts, JC (Corresponding Author), Bangor Univ, Bangor, Gwynedd, Wales.\nRoberts, J. C.; Al-maneea, H.; Butcher, P. W. S.; Sharma, N., Bangor Univ, Bangor, Gwynedd, Wales.\nAl-maneea, H., Basrah Univ, Basrah, Iraq.\nLew, R.; Rees, G., Adam Mickiewicz Univ, Poznan, Poland.\nFrankenberg-Garcia, A., Univ Surrey, Guildford, Surrey, England.", "countries": "Poland;Wales;England;Iraq", "abstract": "We report on an in-depth corpus linguistic study on multiple views' terminology and word collocation. We take a broad interpretation of these terms, and explore the meaning and diversity of their use in visualisation literature. First we explore senses of the term multiple views' (e.g., multiple views' can mean juxtaposition, many viewport projections or several alternative opinions). Second, we investigate term popularity and frequency of occurrences, investigating usage of multiple' and view' (e.g., multiple views, multiple visualisations, multiple sets). Third, we investigate word collocations and terms that have a similar sense (e.g., multiple views, side-by-side, small multiples). We built and used several corpora, including a 6-million-word corpus of all IEEE Visualisation conference articles published in IEEE Transactions on Visualisation and Computer Graphics 2012 to 2017. We draw on our substantial experience from early work in coordinated and multiple views, and with collocation analysis develop several lists of terms. This research provides insight into term use, a reference for novice and expert authors in visualisation, and contributes a taxonomy of multiple view' terms.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13673", "refList": ["10.1109/tvcg.2017.2744359", "10.1109/38.31462", "10.1109/cmv.2003.1215002", "10.2307/2289444", "10.1016/j.ijhcs.2011.02.007", "10.1017/s0022112091210654", "10.1016/b978-008044531-1/50426-7", "10.1109/cmv.2003.1215001", "10.1109/tvcg.2013.134", "10.1117/12.378894", "10.1037/0096-1523.21.6.1494", "10.1057/palgrave.ivs.9500086", "10.1109/iv.2011.42", "10.1109/tvcg.2017.2744199", "10.4324/9780203810088", "10.1145/1321440.1321580", "10.1109/tvcg.2015.2467271", "10.1016/j.learninstruc.2006.03.001", "10.1145/345513.345282", "10.1007/s40607-014-0009-9", "10.1109/tvcg.2017.2743859", "10.1109/infvis.1997.636761", "10.1109/tvcg.2013.219", "10.1007/978-3-319-55627-7", "10.1109/tvcg.2009.111", "10.1109/iv.2008.21", "10.1109/infvis.2002.1173157", "10.1109/mcg.2014.82", "10.1145/3143699.3143717", "10.1109/tvcg.2012.226", "10.1145/345513.345271", "10.2478/jazcas-2018-0006", "10.1145/2556288.2556969", "10.1109/visual.1998.745282", "10.1145/1276377.1276427", "10.1109/infvis.2001.963283", "10.1007/978-1-4471-6497-5\\_1", "10.1109/visual.1994.346302", "10.1057/palgrave.ivs.9500068", "10.1109/tvcg.2006.160", "10.1109/visual.1990.146374", "10.1109/tvcg.2016.2614803", "10.1177/1473871611416549", "10.1109/tvcg.2017.2745878", "10.1109/tvcg.2006.69", "10.1109/tpami.1983.4767367", "10.1109/visual.1991.175794", "10.1109/tvcg.2017.2744159", "10.1109/tvcg.2017.2744198", "10.1109/32.328995", "10.1016/j.jeap.2018.07.003", "10.1016/j.geomorph.2012.08.021", "10.1109/tvcg.2014.2346920", "10.1109/2.917550", "10.1017/s0958344018000150", "10.1109/tvcg.2009.94", "10.1179/2051819613z.0000000003", "10.1109/vast.2008.4677370", "10.1117/12.309533", "10.1109/tvcg.2014.2346747", "10.1075/ijcl.13.4.06ray", "10.1007/s12650-015-0323-9", "10.1109/tvcg.2006.178", "10.1109/tvcg.2016.2615308", "10.2307/1269768", "10.2307/4132312", "10.1109/tvcg.2018.2864903", "10.1109/mis.2006.100", "10.1109/iv.1998.694193", "10.1007/s11192-015-1830-0", "10.1109/tvcg.2017.2744080", "10.1111/j.1467-9280.1997.tb00442.x", "10.1109/cmv.2007.20", "10.1109/infvis.2003.1249006", "10.1109/tvcg.2017.2745941", "10.1016/s1364-6613(99)01332-7", "10.1145/989863.989893", "10.1109/tse.1985.232211", "10.1109/tvcg.2017.2744184", "10.1109/infvis.2004.12", "10.1075/ijcl.17.3.04har", "10.1109/tvcg.2010.179", "10.1016/j.jss.2006.05.024", "10.1109/cmv.2003.1215005", "10.1109/tvcg.2017.2744358", "10.1145/2992154.2996365", "10.1109/tvcg.2016.2598827"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030338", "title": "Composition and Configuration Patterns in Multiple-View Visualizations", "year": "2020", "conferenceName": "InfoVis", "authors": "Xi Chen;Wei Zeng 0004;Yanna Lin;Hayder Al-Maneea;Jonathan Roberts 0002;Remco Chang", "citationCount": "0", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Xi; Zeng, Wei; Lin, Yanna, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. AI-maneea, Hayder Mahdi; Roberts, Jonathan, Bangor Univ, Bangor, Gwynedd, Wales. Chang, Remco, Tufts Univ, Medford, MA 02155 USA.", "countries": "USA;Wales;China", "abstract": "Multiple-view visualization (MV) is a layout design technique often employed to help users see a large number of data attributes and values in a single cohesive representation. Because of its generalizability, the MV design has been widely adopted by the visualization community to help users examine and interact with large, complex, and high-dimensional data. However, although ubiquitous, there has been little work to categorize and analyze MVs in order to better understand its design space. As a result, there has been little to no guideline in how to use the MV design effectively. In this paper, we present an in-depth study of how MVs are designed in practice. We focus on two fundamental measures of multiple-view patterns: composition, which quantifies what view types and how many are there; and configuration, which characterizes spatial arrangement of view layouts in the display space. We build a new dataset containing 360 images of MVs collected from IEEE VIS, EuroVis, and PacificVis publications 2011 to 2019, and make fine-grained annotations of view types and layouts for these visualization images. From this data we conduct composition and configuration analyses using quantitative metrics of term frequency and layout topology. We identify common practices around MVs, including relationship of view types, popular view layouts, and correlation between view types and layouts. We combine the findings into a MV recommendation system, providing interactive tools to explore the design space, and support example-based design.", "keywords": "Multiple views,design pattern,quantitative analysis,example-based design", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030338", "refList": ["10.1109/tvcg.2018.2865235", "10.1109/tvcg.2016.2615308", "10.1145/2642918.2647398", "10.1177/1473871611416549", "10.1109/tvcg.2019.2934810", "10.1109/tvcg.2014.48", "10.1109/tvcg.2018.2864903", "10.1145/345513.345271", "10.1109/iv.1998.694193", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2017.2744019", "10.1109/tvcg.2011.185", "10.1109/tvcg.2015.2467194", "10.1109/visual.1991.175815", "10.14778/2831360.2831371", "10.1109/mcg.2019.2924636", "10.1111/cgf.13673", "10.1111/cgf.12131", "10.1109/tvcg.2017.2744198", "10.1145/108360.108361", "10.1109/vl.1996.545307", "10.1109/tvcg.2017.2745140", "10.1109/cmv.2007.20", "10.1145/198366.198376", "10.1145/2508363.2508405", "10.1111/cgf.12114", "10.1109/icde.2018.00019", "10.1109/tvcg.2017.2787113", "10.1109/tvcg.2018.2865240", "10.1111/cgf.13380", "10.1111/cgf.12902", "10.1109/vast.2015.7347628", "10.2307/2288400", "10.1109/infvis.2004.12", "10.1145/102377.115768", "10.1109/tvcg.2009.179", "10.1109/2945.981851", "10.1109/tvcg.2007.70521", "10.1109/pacificvis.2012.6183556", "10.1145/2213836", "10.1109/tvcg.2013.234", "10.1109/iv.2008.87"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.14001", "year": "2020", "title": "Sunspot Plots: Model-based Structure Enhancement for Dense Scatter Plots", "conferenceName": "EuroVis", "authors": "Thomas Trautner;Fabian Bolte;Sergej Stoppel;Stefan Bruckner", "citationCount": "0", "affiliation": "Trautner, T (Corresponding Author), Univ Bergen, Bergen, Norway.\nTrautner, T.; Bolte, F.; Stoppel, S.; Bruckner, S., Univ Bergen, Bergen, Norway.", "countries": "Norway", "abstract": "Scatter plots are a powerful and well-established technique for visualizing the relationships between two variables as a collection of discrete points. However, especially when dealing with large and dense data, scatter plots often exhibit problems such as overplotting, making the data interpretation arduous. Density plots are able to overcome these limitations in highly populated regions, but fail to provide accurate information of individual data points. This is particularly problematic in sparse regions where the density estimate may not provide a good representation of the underlying data. In this paper, we present sunspot plots, a visualization technique that communicates dense data as a continuous data distribution, while preserving the discrete nature of data samples in sparsely populated areas. We furthermore demonstrate the advantages of our approach on typical failure cases of scatter plots within synthetic and real-world data sets and validate its effectiveness in a user study.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14001", "refList": ["10.1057/palgrave.ivs.9500122", "10.2312/eggh/hpg12/097-103", "10.1109/tvcg.2008.119", "10.1109/pacificvis.2011.5742387", "10.1038/331163a0", "10.1109/visual.2019.8933620", "10.2307/2683294", "10.1201/9781351072304", "10.2307/2289444", "10.1109/tvcg.2019.2934541", "10.1080/00949657508810123", "10.1109/tvcg.2013.65", "10.1109/infvis.1997.636789", "10.1109/2945.841121", "10.1109/mcse.2007.55", "10.1109/tvcg.2010.197", "10.1111/cgf.12871", "10.1109/infvis.2003.1249018", "10.1145/3173574.3173991", "10.1109/tvcg.2012.238", "10.1007/0-387-28695-0", "10.1109/pacificvis.2010.5429604", "10.18637/jss.v008.i03", "10.1109/tvcg.2007.70596", "10.1109/visual.1998.745301", "10.1007/0-387-37977-0\\_3", "10.1145/1556262.1556289", "10.2312/wiced.20161094", "10.1109/tvcg.2007.70535", "10.1145/1056808.1056914", "10.1109/tvcg.2003.1196007", "10.1109/iv.2004.1320190", "10.1111/cgf.12877", "10.1162/leon.2007.40.2.202a", "10.1109/tvcg.2017.2674978", "10.1057/ivs.2010.4", "10.1016/j.cag.2018.02.008", "10.1201/9781315140919", "10.1109/visual.2000.885677", "10.1002/jhbs.20078", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1109/visual.2001.964495", "10.1109/iv.2002.1028760", "10.1111/cgf.13684", "10.1109/hicss.2013.197", "10.1109/tvcg.2019.2903956", "10.1093/mnras/stt961", "10.1109/tvcg.2017.2668409", "10.1111/j.1467-8659.2009.01478.x", "10.1145/2702123.2702585", "10.2307/1418003", "10.2307/1390742", "10.1145/360825.360839", "10.1109/pacificvis.2009.4906843", "10.1057/ivs.2009.34", "10.2307/2288711"], "wos": 1, "children": [], "len": 1}], "len": 177}, {"doi": "10.1109/tvcg.2019.2934796", "title": "Improving the Robustness of Scagnostics", "year": "2019", "conferenceName": "InfoVis", "authors": "Yunhai Wang;Zeyu Wang 0005;Tingting Liu;Michael Correll;Zhanglin Cheng;Oliver Deussen;Michael Sedlmair", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Jinan, Peoples R China. Wang, Yunhai; Wang, Zeyu; Liu, Tingting, Shandong Univ, Jinan, Peoples R China. Wang, Zeyu; Cheng, Zhanglin; Deussen, Oliver, SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Peoples R China. Correll, Michael, Tableau Res, Seattle, WA USA. Deussen, Oliver, Konstanz Univ, Constance, Germany. Sedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "In this paper, we examine the robustness of scagnostics through a series of theoretical and empirical studies. First, we investigate the sensitivity of scagnostics by employing perturbing operations on more than 60M synthetic and real-world scatterplots. We found that two scagnostic measures, Outlying and Clumpy, are overly sensitive to data binning. To understand how these measures align with human judgments of visual features, we conducted a study with 24 participants, which reveals that i) humans are not sensitive to small perturbations of the data that cause large changes in both measures, and ii) the perception of clumpiness heavily depends on per-cluster topologies and structures. Motivated by these results, we propose Robust Scagnostics (RScag) by combining adaptive binning with a hierarchy-based form of scagnostics. An analysis shows that RScag improves on the robustness of original scagnostics, aligns better with human judgments, and is equally fast as the traditional scagnostic measures.", "keywords": "Scagnostics,scatterplots,sensitivity analysis,Robust Scagnostics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934796", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2346979", "10.1111/j.1467-8659.2009.01467.x", "10.1109/vast.2008.4677368", "10.2307/2289444", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/vast.2009.5332628", "10.1111/insr.12095\\_11", "10.1109/tvcg.2010.184", "10.1109/tvcg.2015.2467323", "10.1109/vast.2010.5652460", "10.1111/j.1467-8659.2012.03069.x", "10.1145/1842993.1843002", "10.1198/106186008x320465", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.20", "10.1111/cgf.12641", "10.1109/tvcg.2012.128", "10.1109/tvcg.2015.2467671", "10.1057/palgrave.ivs.9500091", "10.1007/978-1-4612-4400-4", "10.1111/cgf.13176", "10.1002/0470870958", "10.1109/tvcg.2018.2864907", "10.1111/j.1467-8659.2012.03125.x", "10.1109/vast.2012.6400490", "10.1109/infvis.2005.1532142", "10.1109/vast.2010.5652433", "10.1109/vast.2009.5332611", "10.1201/9781315140919", "10.1145/2858036.2858155", "10.1109/ldav.2013.6675164", "10.1515/itit-2014-1070", "10.1111/cgf.13684", "10.1109/tpami.1979.4766909", "10.2307/2289936", "10.1109/pacificvis.2014.42", "10.1109/tvcg.2016.2598467", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2006.94", "10.1109/tvcg.2014.2346572", "10.1111/cgf.12632", "10.1109/tvcg.2017.2744339"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13446", "year": "2018", "title": "Quality Metrics for Information Visualization", "conferenceName": "EuroVis", "authors": "Michael Behrisch;Michael Blumenschein;Nam Wook Kim;Lin Shao;Mennatallah El{-}Assady;Johannes Fuchs;Daniel Seebacher;Alexandra Diehl;Ulrik Brandes;Hanspeter Pfister;Tobias Schreck;Daniel Weiskopf;Daniel A. Keim", "citationCount": "25", "affiliation": "Behrisch, M (Corresponding Author), Harvard Univ, Cambridge, MA 02138 USA.\nBehrisch, M.; Kim, N. W.; Pfister, H., Harvard Univ, Cambridge, MA 02138 USA.\nBlumenschein, M.; El-Assady, M.; Fuchs, J.; Seebacher, D.; Diehl, A.; Keim, D. A., Univ Konstanz, Constance, Germany.\nShao, L.; Schreck, T., Graz Univ Technol, Graz, Austria.\nBrandes, U., Swiss Fed Inst Technol, Zurich, Switzerland.\nWeiskopf, D., Univ Stuttgart, Stuttgart, Germany.", "countries": "Switzerland;Germany;USA;Austria", "abstract": "The visualization community has developed to date many intuitions and understandings of how to judge the quality of views in visualizing data. The computation of a visualization's quality and usefulness ranges from measuring clutter and overlap, up to the existence and perception of specific (visual) patterns. This survey attempts to report, categorize and unify the diverse understandings and aims to establish a common vocabulary that will enable a wide audience to understand their differences and subtleties. For this purpose, we present a commonly applicable quality metric formalization that should detail and relate all constituting parts of a quality metric. We organize our corpus of reviewed research papers along the data types established in the information visualization community: multi- and high-dimensional, relational, sequential, geospatial and text data. For each data type, we select the visualization subdomains in which quality metrics are an active research field and report their findings, reason on the underlying concepts, describe goals and outline the constraints and requirements. One central goal of this survey is to provide guidance on future research opportunities for the field and outline how different visualization communities could benefit from each other by applying or transferring knowledge to their respective subdomain. Additionally, we aim to motivate the visualization community to compare computed measures to the perception of humans.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13446", "refList": ["10.2307/276978", "10.1057/ivs.2009.10", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2014.2346420", "10.1016/j.cag.2018.01.010", "10.1109/tvcg.2017.2743959", "10.1109/tvcg.2011.127", "10.1109/tvcg.2015.2467759", "10.1109/tvcg.2013.187", "10.1109/tvcg.2007.70594", "10.1186/1471-2105-9-155", "10.1145/2702123.2702545", "10.1145/1645953.1646023", "10.1093/bioinformatics/btm312", "10.1109/pacificvis.2016.7465245", "10.1145/3038462.3038463", "10.1007/978-3-540-70956-5", "10.1109/tvcg.2017.2743939", "10.1111/cgf.12932", "10.1111/j.1467-8659.2012.03106.x", "10.1145/1133265.1133318", "10.1109/tvcg.2010.186", "10.1109/iv.2013.101", "10.1109/tvcg.2016.2598590", "10.2312/conf/eg2013/stars/039-063", "10.3758/bf03201236", "10.1111/cgf.12935", "10.1145/1056808.1056914", "10.1007/bf01898350", "10.2307/1390686", "10.1109/2945.981848", "10.1109/hicss.2008.422", "10.1109/tvcg.2011.201", "10.1109/pacificvis.2011.5742390", "10.2307/2288400", "10.1111/cgf.12872", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2006.163", "10.1109/tvcg.2013.150", "10.1109/tvcg.2009.171", "10.1109/tvcg.2017.2723397", "10.1117/12.2079841", "10.1109/pacificvis.2009.4906838", "10.1109/infvis.2004.1", "10.1109/tvcg.2008.166", "10.1287/opre.20.5.993", "10.7155/jgaa.00370", "10.1111/j.1467-8659.2008.01240.x", "10.1109/inf0vis.2005.14", "10.1109/tvcg.2009.23", "10.1109/infvis.1998.729559", "10.1177/0165551506078083", "10.1109/t-c.1974.224051", "10.1109/tvcg.2017.2744339", "10.1109/tvcg.2014.2346433", "10.1145/989863.989940", "10.1109/tvcg.2015.2509990", "10.2307/2288843", "10.1016/j.neucom.2017.01.105", "10.1073/pnas.43.10.923", "10.1111/cgf.12647", "10.1016/j.rse.2017.06.031", "10.1109/infvis.2005.1532145", "10.1109/2945.841121", "10.1109/tvcg.2011.229", "10.2312/eurova.20141140", "10.1111/j.2044-8317.1974.tb00534.x", "10.1109/pacificvis.2015.7156366", "10.1109/vast.2009.5332628", "10.1109/vast.2014.7042480", "10.1145/2993901.2993907", "10.1177/0951692899011001004", "10.1007/978-3-642-03655-2\\_43", "10.1198/106186008x320465", "10.1109/visual.1997.663916", "10.1111/cgf.12125", "10.1109/infvis.2003.1249009", "10.1109/icdm.2003.1250978", "10.1109/tvcg.2017.2745919", "10.1109/tvcg.2012.128", "10.1109/vast.2010.5652450", "10.1109/tvcg.2006.161", "10.1145/571647.571649", "10.1109/tvcg.2011.193", "10.1111/cgf.12633", "10.1109/tvcg.2017.2674978", "10.1109/vast.2010.5652433", "10.1109/tvcg.2017.2745978", "10.1515/itit-2014-1070", "10.1145/1374489.1374501", "10.1109/tvcg.2009.153", "10.2312/eurovisshort.20171128", "10.2312/eurovisshort.20151130", "10.1016/0925-7721(94)00014-x", "10.1057/palgrave.ivs.95000/3", "10.1145/302979.303030", "10.1002/widm.1071", "10.1109/pacificvis.2014.42", "10.1109/tvcg.2016.2598467", "10.1111/j.1467-8659.2009.01667.x", "10.2307/2287708", "10.1177/1473871613477091", "10.1145/967900.968153", "10.1109/tvcg.2010.162", "10.1145/568522.568523", "10.1016/j.cag.2004.03.022", "10.1109/tvcg.2015.2466992", "10.1145/2993901.2993903", "10.1147/jrd.2015.2411412", "10.1177/154193120504900508", "10.1109/tvcg.2006.138", "10.1111/cgf.13168", "10.1111/cgf.12380", "10.2312/conf/eg2013/stars/095-116", "10.1109/tvcg.2017.2745859", "10.1109/tvcg.2017.2701829", "10.1109/tvcg.2007.70529", "10.1007/bf01199431", "10.1117/12.697548", "10.1109/tvcg.2017.2653106", "10.1207/s15327906mbr2701\\_4", "10.1109/vl.1996.545307", "10.1109/tvcg.2017.2745140", "10.1111/j.1467-8659.2012.03069.x", "10.1111/j.1467-8659.2011.01961.x", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.124", "10.1057/ivs.2008.13", "10.2312/vissym/eurovis06/195-202", "10.2312/vissym/eurovis07/163-170", "10.1109/tvcg.2015.2467324", "10.1109/tvcg.2011.167", "10.1109/pacificvis.2014.40", "10.1109/pacificvis.2012.6183570", "10.1111/j.1467-8659.2012.03125.x", "10.1075/idj.20.1.02bei", "10.1111/1467-8306.00061", "10.1145/2858036.2858155", "10.1109/tvcg.2012.108", "10.2312/vmv.20171261", "10.1109/tvcg.2015.2467531", "10.1145/1379092.1379130", "10.1109/tpami.1979.4766909", "10.1080/13875860903039172", "10.1109/mcg.2006.70", "10.1109/tvcg.2010.242", "10.1109/vast.2007.4389004", "10.1109/tvcg.2015.2467191", "10.2312/cgvc.20171276", "10.1016/0169-7439(87)80084-9", "10.2312/pe/eurovisshort/eurovisshort2012/097-101", "10.1016/j.neucom.2014.07.073", "10.2307/2685263", "10.1145/996546.996554", "10.1111/j.1467-8659.2009.01666.x", "10.1117/12.2083444", "10.1109/tvcg.2014.2346572", "10.1007/978-0-387-39940-9\\_262", "10.1007/bf00988593", "10.1145/1054972.1055078", "10.1111/j.1467-8659.2009.01467.x", "10.1111/cgf.13181", "10.1145/502512.502530", "10.1109/pacificvis.2010.5429600", "10.1111/j.1467-8659.2011.01923.x", "10.1109/mcg.2004.41", "10.1109/tvcg.2015.2467971", "10.1111/j.1467-8659.2011.01919.x", "10.1002/sam.10071", "10.1109/tvcg.2010.184", "10.1007/978-3-642-27848-8\\_648-1", "10.1109/tvcg.2010.132", "10.1145/22949.22950", "10.1145/1842993.1843002", "10.1109/infvis.2000.885096", "10.1016/j.ins.2015.04.017", "10.1109/iv.2009.43", "10.1016/j.visinf.2017.11.001", "10.1109/tvcg.2011.239", "10.1109/infovis.2005.40", "10.1111/cgf.12641", "10.1109/pacificvis.2016.7465262", "10.1145/2470654.2466443", "10.1007/s10844-011-0157-4", "10.1111/cgf.12919", "10.1016/j.jvlc.2016.07.003", "10.1109/tvcg.2016.2549018", "10.1057/palgrave.ivs.9500166", "10.3406/colan.1981.1409.1", "10.1093/bioinformatics/bti141", "10.1111/j.1467-8306.2004.09401004.x", "10.1145/1168149.1168168", "10.1109/tvcg.2017.2744184", "10.1117/12.2079420", "10.1109/iv.2005.62", "10.1145/102377.115768", "10.1109/tvcg.2014.2346677", "10.1109/tvcg.2014.2346426", "10.1111/j.1467-8659.2012.03107.x", "10.5220/0006097400400051", "10.2991/978-94-6239-186-4", "10.2307/2284077", "10.1109/iv.2008.89", "10.1016/j.jvlc.2015.12.001", "10.1109/pacificvis.2013.6596147", "10.1145/1242572.1242826", "10.1016/j.cag.2007.01.030", "10.1111/cgf.12632", "10.2312/eurovisstar.20151113", "10.1559/152304009788188808"], "wos": 1, "children": [{"doi": "10.1109/vast.2018.8802486", "title": "SMARTexplore: Simplifying High-Dimensional Data Analysis through a Table-Based Visual Analytics Approach", "year": "2018", "conferenceName": "VAST", "authors": "Michael Blumenschein;Michael Behrisch;Stefanie Schmid;Simon Butscher;Deborah Wahl;Karoline Villinger;Britta Renner;Harald Reiterer;Daniel A. Keim", "citationCount": "0", "affiliation": "Blumenschein, M (Corresponding Author), Univ Konstanz, Constance, Germany. Blumenschein, Michael; Schmid, Stefanie; Butscher, Simon; Wahl, Deborah R.; Villinger, Karoline; Renner, Britta; Reiterer, Harald; Keim, Daniel A., Univ Konstanz, Constance, Germany. Behrisch, Michael, Harvard Univ, Cambridge, MA 02138 USA.", "countries": "Germany;USA", "abstract": "We present SMARTEXPLORE, a novel visual analytics technique that simplifies the identification and understanding of clusters, correlations, and complex patterns in high-dimensional data. The analysis is integrated into an interactive table-based visualization that maintains a consistent and familiar representation throughout the analysis. The visualization is tightly coupled with pattern matching, subspace analysis, reordering, and layout algorithms. To increase the analyst's trust in the revealed patterns, SMARTEXPLORE automatically selects and computes statistical measures based on dimension and data properties. While existing approaches to analyzing high-dimensional data (e.g., planar projections and Parallel coordinates) have proven effective, they typically have steep learning curves for non-visualization experts. Our evaluation, based on three expert case studies, confirms that non-visualization experts successfully reveal patterns in high-dimensional data when using SMARTEXPLORE.", "keywords": "High-dimensional data,visual exploration,pattern-driven analysis,tabular visualization,subspace,aggregation", "link": "http://dx.doi.org/10.1109/VAST.2018.8802486", "refList": ["10.1177/1473871612460526", "10.1109/tvcg.2015.2489649", "10.1111/j.1467-8659.2008.01241.x", "10.1007/978-3-319-25087-8\\_29", "10.1007/b98835", "10.13140/rg.2.2.16570.90567", "10.1109/tvcg.2014.2346260", "10.1109/vast.2009.5332628", "10.2307/2528823", "10.1109/tvcg.2010.184", "10.1145/1133265.1133318", "10.1057/palgrave.ivs.9500072", "10.1111/j.1467-8659.2012.03110.x", "10.1145/1007730.1007731", "10.1057/palgrave.ivs.9500086", "10.1111/cgf.12935", "10.1109/tvcg.2014.2346279", "10.1007/bf01898350", "10.1109/tvcg.2013.173", "10.1109/tvcg.2017.2672987", "10.1109/tvcg.2014.2346248", "10.1109/infvis.2004.46", "10.1109/vast.2010.5652433", "10.1109/vast.2009.5332611", "10.1109/tvcg.2015.2467553", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2013.150", "10.1109/tvcg.2016.2640960", "10.1111/cgf.12630", "10.1007/978-1-4757-1904-8", "10.1109/tvcg.2011.188", "10.1109/tvcg.2010.138", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2017.2743978", "10.1111/cgf.12879", "10.1109/iv.2008.33", "10.1111/cgf.13446", "10.1007/s00371-018-1483-0", "10.1109/infvis.1998.729559", "10.1145/2669557.2669572", "10.1111/j.1467-8659.2008.01239.x"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934541", "title": "A Recursive Subdivision Technique for Sampling Multi-class Scatterplots", "year": "2019", "conferenceName": "InfoVis", "authors": "Xin Chen;Tong Ge;Jian Zhang 0070;Baoquan Chen;Chi-Wing Fu;Oliver Deussen;Yunhai Wang", "citationCount": "5", "affiliation": "Chen, X (Corresponding Author), Shandong Univ, Jinan, Shandong, Peoples R China. Chen, Xin; Ge, Tong; Wang, Yunhai, Shandong Univ, Jinan, Shandong, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China. Fu, Chi-Wing, SIAT, Guangdong Prov Key Lab CV \\& VR Tech, Shenzhen, Guangdong, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Deussen, Oliver, Konstanz Univ, Constance, Germany. Deussen, Oliver, SIAT, Shenzhen VIsuCA Key Lab, Shenzhen, Guangdong, Peoples R China.", "countries": "Germany;China", "abstract": "We present a non-uniform recursive sampling technique for multi-class scatterplots, with the specific goal of faithfully presenting relative data and class densities, while preserving major outliers in the plots. Our technique is based on a customized binary kd-tree, in which leaf nodes are created by recursively subdividing the underlying multi-class density map. By backtracking, we merge leaf nodes until they encompass points of all classes for our subsequently applied outlier-aware multi-class sampling strategy. A quantitative evaluation shows that our approach can better preserve outliers and at the same time relative densities in multi-class scatterplots compared to the previous approaches, several case studies demonstrate the effectiveness of our approach in exploring complex and real world data.", "keywords": "Scatterplot,multi-class sampling,kd-tree,outlier,relative density", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934541", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2006.170", "10.1109/tvcg.2008.119", "10.1109/tvcg.2007.70580", "10.2307/2289444", "10.1145/1964921.1964943", "10.1109/tvcg.2013.65", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/itoec.2018.8740621", "10.1145/1778765.1778816", "10.1109/tvcg.2010.197", "10.1201/b17511", "10.1109/tvcg.2018.2864912", "10.1007/0-387-28695-0", "10.1109/5.726791", "10.1109/visual.1998.745301", "10.1016/j.physa.2011.12.004", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2004.1272729", "10.1145/335191.335388", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1109/iv.2002.1028760", "10.1145/1842993.1842999", "10.1109/tvcg.2018.2869149", "10.1038/nmeth.2490", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2010.176", "10.1145/2702123.2702585", "10.1057/ivs.2009.34"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14001", "year": "2020", "title": "Sunspot Plots: Model-based Structure Enhancement for Dense Scatter Plots", "conferenceName": "EuroVis", "authors": "Thomas Trautner;Fabian Bolte;Sergej Stoppel;Stefan Bruckner", "citationCount": "0", "affiliation": "Trautner, T (Corresponding Author), Univ Bergen, Bergen, Norway.\nTrautner, T.; Bolte, F.; Stoppel, S.; Bruckner, S., Univ Bergen, Bergen, Norway.", "countries": "Norway", "abstract": "Scatter plots are a powerful and well-established technique for visualizing the relationships between two variables as a collection of discrete points. However, especially when dealing with large and dense data, scatter plots often exhibit problems such as overplotting, making the data interpretation arduous. Density plots are able to overcome these limitations in highly populated regions, but fail to provide accurate information of individual data points. This is particularly problematic in sparse regions where the density estimate may not provide a good representation of the underlying data. In this paper, we present sunspot plots, a visualization technique that communicates dense data as a continuous data distribution, while preserving the discrete nature of data samples in sparsely populated areas. We furthermore demonstrate the advantages of our approach on typical failure cases of scatter plots within synthetic and real-world data sets and validate its effectiveness in a user study.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14001", "refList": ["10.1057/palgrave.ivs.9500122", "10.2312/eggh/hpg12/097-103", "10.1109/tvcg.2008.119", "10.1109/pacificvis.2011.5742387", "10.1038/331163a0", "10.1109/visual.2019.8933620", "10.2307/2683294", "10.1201/9781351072304", "10.2307/2289444", "10.1109/tvcg.2019.2934541", "10.1080/00949657508810123", "10.1109/tvcg.2013.65", "10.1109/infvis.1997.636789", "10.1109/2945.841121", "10.1109/mcse.2007.55", "10.1109/tvcg.2010.197", "10.1111/cgf.12871", "10.1109/infvis.2003.1249018", "10.1145/3173574.3173991", "10.1109/tvcg.2012.238", "10.1007/0-387-28695-0", "10.1109/pacificvis.2010.5429604", "10.18637/jss.v008.i03", "10.1109/tvcg.2007.70596", "10.1109/visual.1998.745301", "10.1007/0-387-37977-0\\_3", "10.1145/1556262.1556289", "10.2312/wiced.20161094", "10.1109/tvcg.2007.70535", "10.1145/1056808.1056914", "10.1109/tvcg.2003.1196007", "10.1109/iv.2004.1320190", "10.1111/cgf.12877", "10.1162/leon.2007.40.2.202a", "10.1109/tvcg.2017.2674978", "10.1057/ivs.2010.4", "10.1016/j.cag.2018.02.008", "10.1201/9781315140919", "10.1109/visual.2000.885677", "10.1002/jhbs.20078", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1109/visual.2001.964495", "10.1109/iv.2002.1028760", "10.1111/cgf.13684", "10.1109/hicss.2013.197", "10.1109/tvcg.2019.2903956", "10.1093/mnras/stt961", "10.1109/tvcg.2017.2668409", "10.1111/j.1467-8659.2009.01478.x", "10.1145/2702123.2702585", "10.2307/1418003", "10.2307/1390742", "10.1145/360825.360839", "10.1109/pacificvis.2009.4906843", "10.1057/ivs.2009.34", "10.2307/2288711"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934284", "title": "Color Crafting: Automating the Construction of Designer Quality Color Ramps", "year": "2019", "conferenceName": "InfoVis", "authors": "Stephen Smart;Keke Wu;Danielle Albers Szafir", "citationCount": "3", "affiliation": "Smart, S (Corresponding Author), Univ Colorado, Boulder, CO 80309 USA. Smart, Stephen; Wu, Keke; Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Visualizations often encode numeric data using sequential and diverging color ramps. Effective ramps use colors that are sufficiently discriminable, align well with the data, and are aesthetically pleasing. Designers rely on years of experience to create high-quality color ramps. However, it is challenging for novice visualization developers that lack this experience to craft effective ramps as most guidelines for constructing ramps are loosely defined qualitative heuristics that are often difficult to apply. Our goal is to enable visualization developers to readily create effective color encodings using a single seed color. We do this using an algorithmic approach that models designer practices by analyzing patterns in the structure of designer-crafted color ramps. We construct these models from a corpus of 222 expert-designed color ramps, and use the results to automatically generate ramps that mimic designer practices. We evaluate our approach through an empirical study comparing the outputs of our approach with designer-crafted color ramps. Our models produce ramps that support accurate and aesthetically pleasing visualizations at least as well as designer ramps and that outperform conventional mathematical approaches.", "keywords": "Visualization,Aesthetics in Visualization,Color Perception,Visual Design,Design Mining", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934284", "refList": ["10.1145/3009924", "10.1109/tvcg.2015.2489649", "10.1109/tvcg.2017.2744359", "10.1002/(sici)1098-1098(199622)7:2", "10.1016/s0734-189x(83)80046-2", "10.1109/tvcg.2016.2599106", "10.1364/josaa.29.000313", "10.1109/tvcg.2016.2598918", "10.2307/2683294", "10.1109/tvcg.2017.2653106", "10.1016/j.ijhcs.2010.05.006", "10.1016/0146-664x(81)90006-x", "10.1007/978-3-642-10520-3\\_9", "10.1109/38.135886", "10.1146/annurev-psych-120710-100504", "10.1016/j.csda.2008.11.033", "10.1145/2461912.2461988", "10.1016/j.cub.2007.06.022", "10.1016/s0146-664x(79)80040-4", "10.1109/mcg.2004.1297012", "10.1109/iv.2009.94", "10.1109/tpami.2010.184", "10.3758/s13414-010-0027-0", "10.1145/22949.22950", "10.1109/visual.1995.480803", "10.1109/tvcg.2014.2346277", "10.2307/2684111", "10.1109/tvcg.2018.2865240", "10.1111/cgf.12633", "10.1109/38.7760", "10.1016/j.jspi.2015.04.007", "10.1109/iv.2008.24", "10.1145/2939502.2939506", "10.1111/cgf.12127", "10.1016/j.cag.2010.11.015", "10.1145/3025453.3026041", "10.1109/tvcg.2012.315", "10.2307/2288400", "10.1511/2005.5.436", "10.1016/s0097-8493(96)00072-6", "10.1109/mcg.2018.011461525", "10.1145/2470654.2466420", "10.1109/tvcg.2012.279", "10.1109/tvcg.2014.2346978", "10.1109/tvcg.2017.2743978", "10.1145/3406601.3406602", "10.1117/12.2084548", "10.1109/tvcg.2018.2865147", "10.1109/tvcg.2015.2467191", "10.1111/cgf.13446", "10.1109/tvcg.2017.2744320", "10.1111/j.1467-8659.2008.01203.x", "10.1145/2702613.2702975", "10.1007/978-3-319-26633-6\\_13", "10.1145/2207676.2208547", "10.1109/tvcg.2008.174", "10.1179/000870403235002042"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028891", "title": "A Structured Review of Data Management Technology for Interactive Visualization and Analysis", "year": "2020", "conferenceName": "InfoVis", "authors": "Leilani Battle;Carlos Scheidegger", "citationCount": "0", "affiliation": "Battle, L (Corresponding Author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA. Battle, Leilani, Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA. Scheidegger, Carlos, Univ Arizona, Dept Comp Sci, HDC Lab, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "In the last two decades, interactive visualization and analysis have become a central tool in data-driven decision making. Concurrently to the contributions in data visualization, research in data management has produced technology that directly benefits interactive analysis. Here, we contribute a systematic review of 30 years of work in this adjacent field, and highlight techniques and principles we believe to be underappreciated in visualization work. We structure our review along two axes. First, we use task taxonomies from the visualization literature to structure the space of interactions in usual systems. Second, we created a categorization of data management work that strikes a balance between specificity and generality. Concretely, we contribute a characterization of 131 research papers along these two axes. We find that five notions in data management venues fit interactive visualization systems well: materialized views, approximate query processing, user modeling and query prediction, muiti-query optimization, lineage techniques, and indexing techniques. In addition, we find a preponderance of work in materialized views and approximate query processing, most targeting a limited subset of the interaction tasks in the taxonomy we used. This suggests natural avenues of future research both in visualization and data management. Our categorization both changes how we visualization researchers design and build our systems, and highlights where future work is necessary.", "keywords": "", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028891", "refList": ["10.1109/tvcg.2012.233", "10.1016/s0022-5371(74)80015-0", "10.1109/tvcg.2017.2744359", "10.1037/0096-3445.136.4.623", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2012.196", "10.1037/a0029856", "10.1109/tvcg.2014.2346979", "10.1037/h0030300", "10.1109/tvcg.2016.2598918", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2018.2864909", "10.1111/cgf.13079", "10.3389/fpsyg.2012.00355", "10.1145/2858036.2858465", "10.1080/01621459.1989.10478821", "10.1037/0278-7393.24.3.732", "10.1109/tvcg.2011.127", "10.1145/2858036.2858063", "10.4249/scholarpedia.3325", "10.4324/9781410611949", "10.1111/cgf.13444", "10.1145/2993901.2993909", "10.1037/0033-295x.96.2.267", "10.1006/ijhc.1017", "10.1086/405615", "10.1109/tvcg.2019.2934801", "10.1038/17953", "10.1037/xhp0000314", "10.1109/tvcg.2019.2934400", "10.1145/2470654.2470723", "10.1037/0096-1523.16.2.332", "10.1167/16.5.11", "10.3758/s13423-016-1174-7", "10.3758/bf03207704", "10.1146/annurev.psych.55.090902.141415", "10.2307/2288400", "10.3758/bf03204258", "10.1109/tvcg.2011.279", "10.1109/vissoft.2014.36", "10.3758/s13423-011-0055-3", "10.1145/3025453.3025922", "10.1109/tvcg.2019.2934284", "10.3758/bf03210498", "10.3758/bf03200774", "10.2307/1419876", "10.1038/s41562-017-0058", "10.1109/tvcg.2010.237", "10.1109/pacificvis.2012.6183556", "10.1109/infvis.1997.636792", "10.1093/acprof:oso/9780198523192.003.0005", "10.1073/pnas.1117465109", "10.1109/tvcg.2013.234", "10.1038/nn.3655", "10.1111/cgf.12379", "10.1146/annurev-psych-010416-044232", "10.1111/cgf.13695", "10.1037/0033-295x.107.3.500", "10.1109/tvcg.2013.183", "10.1146/annurev.psych.53.100901.135125", "10.1037//0022-3514.79.6.995", "10.1559/152304003100010929", "10.1109/tvcg.2018.2865147", "10.1037/0096-1523.18.3.849", "10.1111/j.1467-8659.2009.01694.x", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934799", "title": "Data Sampling in Multi-view and Multi-class Scatterplots via Set Cover Optimization", "year": "2019", "conferenceName": "InfoVis", "authors": "Ruizhen Hu;Tingkai Sha;Oliver van Kaick;Oliver Deussen;Hui Huang 0004", "citationCount": "4", "affiliation": "Hu, RZ (Corresponding Author), Shenzhen Univ, Visual Comp Res Ctr, Shenzhen, Guangdong, Peoples R China. Hu, Ruizhen; Sha, Tingkai; Huang, Hui, Shenzhen Univ, Visual Comp Res Ctr, Shenzhen, Guangdong, Peoples R China. van Kaick, Oliver, Carleton Univ, Sch Comp Sci, Ottawa, ON, Canada. Deussen, Oliver, Konstanz Univ, Constance, Germany. Deussen, Oliver, SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Guangdong, Peoples R China.", "countries": "Canada;Germany;China", "abstract": "We present a method for data sampling in scatterplots by jointly optimizing point selection for different views or classes. Our method uses space-filling curves (Z-order curves) that partition a point set into subsets that, when covered each by one sample, provide a sampling or coreset with good approximation guarantees in relation to the original point set. For scatterplot matrices with multiple views, different views provide different space-filling curves, leading to different partitions of the given point set. For multi-class scatterplots, the focus on either per-class distribution or global distribution provides two different partitions of the given point set that need to be considered in the selection of the coreset. For both cases, we convert the coreset selection problem into an Exact Cover Problem (ECP), and demonstrate with quantitative and qualitative evaluations that an approximate solution that solves the ECP efficiently is able to provide high-quality samplings.", "keywords": "Sampling,Scatterplot,SPLOM,Exact Cover Problem", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934799", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2006.170", "10.1109/tvcg.2008.119", "10.1111/j.1467-8659.2009.01467.x", "10.2307/2289444", "10.1109/tvcg.2013.65", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/itoec.2018.8740621", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2864912", "10.1007/0-387-28695-0", "10.1109/visual.1998.745301", "10.1287/moor.4.3.233", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.2307/2284239", "10.1109/tvcg.2017.2674978", "10.1016/j.dss.2009.05.016", "10.1109/tvcg.2014.2346594", "10.1109/vds.2017.8573446", "10.1007/978-1-4684-2001-2\\_9", "10.1109/iv.2002.1028760", "10.1145/1842993.1842999", "10.1038/nmeth.2490", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2010.176", "10.1145/2702123.2702585", "10.1057/ivs.2009.34", "10.1179/000870403235002042"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934432", "title": "Discriminability Tests for Visualization Effectiveness and Scalability", "year": "2019", "conferenceName": "InfoVis", "authors": "Rafael Veras;Christopher Collins", "citationCount": "1", "affiliation": "Veras, R (Corresponding Author), Ontario Tech Univ, Oshawa, ON, Canada. Veras, Rafael; Collins, Christopher, Ontario Tech Univ, Oshawa, ON, Canada.", "countries": "Canada", "abstract": "The scalability of a particular visualization approach is limited by the ability for people to discern differences between plots made with different datasets. Ideally, when the data changes, the visualization changes in perceptible ways. This relation breaks down when there is a mismatch between the encoding and the character of the dataset being viewed. Unfortunately, visualizations are often designed and evaluated without fully exploring how they will respond to a wide variety of datasets. We explore the use of an image similarity measure, the Multi-Scale Structural Similarity Index (MS-SSIM), for testing the discriminability of a data visualization across a variety of datasets. MS-SSIM is able to capture the similarity of two visualizations across multiple scales, including low level granular changes and high level patterns. Significant data changes that are not captured by the MS-SSIM indicate visualizations of low discriminability and effectiveness. The measure's utility is demonstrated with two empirical studies. In the first, we compare human similarity judgments and MS-SSIM scores for a collection of scatterplots. In the second, we compute the discriminability values for a set of basic visualizations and compare them with empirical measurements of effectiveness. In both cases, the analyses show that the computational measure is able to approximate empirical results. Our approach can be used to rank competing encodings on their discriminability and to aid in selecting visualizations for a particular type of data distribution.", "keywords": "Scalability,Discriminability,Simulation,Perception", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934432", "refList": ["10.1109/tvcg.2012.233", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2012.230", "10.1111/cgf.12647", "10.1109/tvcg.2016.2598918", "10.1109/tvcg.2007.70529", "10.1145/2858036.2858435", "10.1109/vast.2009.5332628", "10.1145/3025453.3025912", "10.1145/1133265.1133318", "10.1109/tip.2010.2092435", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2865264", "10.1175/jtech-d-11-00103.1", "10.1145/1190036.1190039", "10.1111/cgf.12127", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1109/mcg.2014.18", "10.1145/3173574.3174172", "10.1109/tvcg.2009.153", "10.1057/palgrave.ivs.9500070", "10.1109/tvcg.2018.2790961", "10.1109/tvcg.2014.2346978", "10.1109/tvcg.2018.2810918", "10.1111/cgf.13409", "10.1109/tip.2003.819861", "10.1145/2642918.2647411", "10.1080/15230406.2016.1140074", "10.1109/jstsp.2009.2015374", "10.1109/tvcg.2010.237", "10.1109/mcg.2009.6", "10.1111/j.1467-8659.2009.01667.x", "10.1109/tvcg.2010.161", "10.1111/cgf.13446", "10.1109/tvcg.2014.2346325", "10.1109/tvcg.2009.111", "10.1147/jrd.2015.2411412"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030394", "title": "Direct Volume Rendering with Nonparametric Models of Uncertainty", "year": "2020", "conferenceName": "SciVis", "authors": "Tushar M. Athawale;Bo Ma 0002;Elham Sakhaee;Christopher R. Johnson;Alireza Entezari", "citationCount": "0", "affiliation": "Athawale, TM (Corresponding Author), Univ Utah, Sci Comp \\& Imaging SCI Inst, Salt Lake City, UT 84112 USA. Athawale, Tushar M.; Johnson, Chris R., Univ Utah, Sci Comp \\& Imaging SCI Inst, Salt Lake City, UT 84112 USA. Ma, Bo; Sakhaee, Elham; Entezari, Alireza, Univ Florida, Dept CISE, Gainesville, FL 32611 USA.", "countries": "USA", "abstract": "We present a nonparametric statistical framework for the quantification, analysis, and propagation of data uncertainty in direct volume rendering (DVR). The state-of-the-art statistical DVR framework allows for preserving the transfer function (TF) of the ground truth function when visualizing uncertain data; however, the existing framework is restricted to parametric models of uncertainty. In this paper, we address the limitations of the existing DVR framework by extending the DVR framework for nonparametric distributions. We exploit the quantile interpolation technique to derive probability distributions representing uncertainty in viewing-ray sample intensities in closed form, which allows for accurate and efficient computation. We evaluate our proposed nonparametric statistical models through qualitative and quantitative comparisons with the mean-field and parametric statistical models, such as uniform and Gaussian, as well as Gaussian mixtures. In addition, we present an extension of the state-of-the-art rendering parametric framework to 2D TFs for improved DVR classifications. We show the applicability of our uncertainty quantification framework to ensemble, downsampled, and bivariate versions of scalar field datasets.", "keywords": "Volumes,uncertainty,nonparametric,2D transfer function", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030394", "refList": ["10.1109/icdm.2012.80", "10.1145/1401890.1401904", "10.1109/tvcg.2017.2745139", "10.18128/d030.v6.0", "10.1111/cgf.12142", "10.1109/tvcg.2009.114", "10.1111/j.1467-8659.2009.01677.x", "10.1109/mcse.2007.55", "10.1109/icde.2012.16", "10.1198/106186008x320465", "10.1145/1835804.1835868", "10.1559/1523040054738936", "10.1109/tvcg.2019.2934432", "10.1109/34.1000236", "10.1109/infvis.2005.1532136", "10.1109/tvcg.2012.128", "10.1109/pacificvis.2017.8031573", "10.1109/tvcg.2016.2598920", "10.1109/tvcg.2018.2865021", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1109/sp.2009.22", "10.1109/tvcg.2017.2744184", "10.1145/773153.773173", "10.1145/2660267.2660348", "10.1109/icdmw.2009.93", "10.1016/j.jtrangeo.2015.09.001", "10.1109/icde.2010.5447831", "10.1007/11681878\\_14", "10.1111/cgf.13409", "10.1007/s13278-014-0205-5", "10.1109/tvcg.2011.163", "10.1145/3035918.3035940", "10.1109/sp.2008.33", "10.1145/2882903.2882931"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934208", "title": "Evaluating Perceptual Bias During Geometric Scaling of Scatterplots", "year": "2019", "conferenceName": "VAST", "authors": "Yating Wei;Honghui Mei;Ying Zhao;Shuyue Zhou;Bingru Lin;Haojing Jiang;Wei Chen", "citationCount": "5", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou 310058, Zhejiang, Peoples R China. Zhao, Y (Corresponding Author), Cent South Univ, Sch Comp Sci \\& Engn, Changsha 410083, Hunan, Peoples R China. Wei, Yating; Mei, Honghui; Zhou, Shuyue; Lin, Bingru; Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou 310058, Zhejiang, Peoples R China. Zhao, Ying; Jiang, Haojing, Cent South Univ, Sch Comp Sci \\& Engn, Changsha 410083, Hunan, Peoples R China.", "countries": "China", "abstract": "Scatterplots are frequently scaled to fit display areas in multi-view and multi-device data analysis environments. A common method used for scaling is to enlarge or shrink the entire scatterplot together with the inside points synchronously and proportionally. This process is called geometric scaling. However, geometric scaling of scatterplots may cause a perceptual bias, that is, the perceived and physical values of visual features may be dissociated with respect to geometric scaling. For example, if a scatterplot is projected from a laptop to a large projector screen, then observers may feel that the scatterplot shown on the projector has fewer points than that viewed on the laptop. This paper presents an evaluation study on the perceptual bias of visual features in scatterplots caused by geometric scaling. The study focuses on three fundamental visual features (i.e., numerosity, correlation, and cluster separation) and three hypotheses that are formulated on the basis of our experience. We carefully design three controlled experiments by using well-prepared synthetic data and recruit participants to complete the experiments on the basis of their subjective experience. With a detailed analysis of the experimental results, we obtain a set of instructive findings. First, geometric scaling causes a bias that has a linear relationship with the scale ratio. Second, no significant difference exists between the biases measured from normally and uniformly distributed scatterplots. Third, changing the point radius can correct the bias to a certain extent. These findings can be used to inspire the design decisions of scatterplots in various scenarios.", "keywords": "Evaluation,scatterplot,geometric scaling,bias,perceptual consistency", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934208", "refList": ["10.2307/2288843", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2014.2346979", "10.1126/science.216.4550.1138", "10.1109/tvcg.2017.2744138", "10.1111/j.1467-8659.2009.01467.x", "10.1145/2491568.2491577", "10.1109/mwc.2018.1700325", "10.1145/2449396.2449439", "10.1109/tvcg.2011.127", "10.1109/tvcg.2011.229", "10.1167/15.5.4", "10.1073/pnas.1113195108", "10.1145/2702123.2702545", "10.1109/vast.2009.5332628", "10.1109/tvcg.2018.2800013", "10.1007/s12650-018-0530-2", "10.1016/s0042-6989(97)00340-4", "10.1109/vast.2010.5652460", "10.1109/mcg.2018.2879067", "10.1109/tvcg.2018.2864912", "10.1109/pacificvis.2010.5429604", "10.1109/tcst.2018.2819965", "10.1167/10.2.10", "10.1145/2470654.2481318", "10.1145/1842993.1843002", "10.1167/12.6.8", "10.1109/tvcg.2013.124", "10.1057/ivs.2008.13", "10.1109/tvcg.2007.70596", "10.1109/tvcg.2018.2865266", "10.1145/3173574.3173664", "10.1109/tvcg.2015.2467671", "10.1016/j.cag.2017.07.004", "10.1177/0956797613501520", "10.1109/tvcg.2018.2865020", "10.1111/j.1467-8659.2012.03125.x", "10.3758/bf03205986", "10.3758/s13423-016-1174-7", "10.1109/tvcg.2017.2680452", "10.1109/mc.2006.109", "10.1109/tvcg.2017.2744098", "10.1038/srep32810", "10.1016/j.visres.2013.06.006", "10.1002/jhbs.20078", "10.1109/tvcg.2006.163", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1016/j.cognition.2007.10.009", "10.1109/tvcg.2018.2865142", "10.3758/app.72.7.1839", "10.1109/tvcg.2018.2810918", "10.1016/j.jvlc.2017.10.001", "10.1145/2682623", "10.1109/tvcg.2018.2864884", "10.1145/3025453.3025984", "10.1109/tvcg.2006.184", "10.1109/tvcg.2013.153", "10.1016/j.jvlc.2018.08.003", "10.1109/tvcg.2016.2520921", "10.1111/cgf.13446", "10.1017/s0022381612000187", "10.1145/2702123.2702406", "10.1109/vast.2012.6400487", "10.1109/tvcg.2013.183", "10.1177/1473871611415997", "10.1145/2702123.2702585", "10.1145/2993901.2993903", "10.1109/tvcg.2013.120", "10.1111/cgf.12632", "10.1145/1385569.1385602", "10.1109/tvcg.2017.2754480", "10.1111/j.1467-8659.2009.01694.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934300", "title": "GUIRO: User-Guided Matrix Reordering", "year": "2019", "conferenceName": "VAST", "authors": "Michael Behrisch;Tobias Schreck;Hanspeter Pfister", "citationCount": "1", "affiliation": "Behrisch, M (Corresponding Author), Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Behrisch, Michael; Pfister, Hanspeter, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Schreck, Tobias, Graz Univ Technol, Graz, Austria.", "countries": "USA;Austria", "abstract": "Matrix representations are one of the main established and empirically proven to be effective visualization techniques for relational (or network) data. However, matrices\u2014similar to node-link diagrams\u2014are most effective if their layout reveals the underlying data topology. Given the many developed algorithms, a practical problem arises: \u201cWhich matrix reordering algorithm should I choose for my dataset at hand?\u201d To make matters worse, different reordering algorithms applied to the same dataset may let significantly different visual matrix patterns emerge. This leads to the question of trustworthiness and explainability of these fully automated, often heuristic, black-box processes. We present GUIRO, a Visual Analytics system that helps novices, network analysts, and algorithm designers to open the black-box. Users can investigate the usefulness and expressiveness of 70 accessible matrix reordering algorithms. For network analysts, we introduce a novel model space representation and two interaction techniques for a user-guided reordering of rows or columns, and especially groups thereof (submatrix reordering). These novel techniques contribute to the understanding of the global and local dataset topology. We support algorithm designers by giving them access to 16 reordering quality metrics and visual exploration means for comparing reordering implementations on a row/column permutation level. We evaluated GUIRO in a guided explorative user study with 12 subjects, a case study demonstrating its usefulness in a real-world scenario, and through an expert study gathering feedback on our design decisions. We found that our proposed methods help even inexperienced users to understand matrix patterns and allow a user-guided steering of reordering algorithms. GUIRO helps to increase the transparency of matrix reordering algorithms, thus helping a broad range of users to get a better insight into the complex reordering process, in turn supporting data and reordering algorithm insights.", "keywords": "Visual Analytics,matrix,black-box algorithms,seriation,ordering,sorting,steerable algorithm,interaction,2D projection", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934300", "refList": ["10.1109/tvcg.2007.70582", "10.2307/276978", "10.1109/tvcg.2008.61", "10.3390/su10040973", "10.1101/121889", "10.1145/1345448.1345453", "10.1186/s12879-014-0695-9", "10.1186/1471-2105-9-155", "10.1145/1124772.1124891", "10.1109/tvcg.2010.159", "10.1111/j.2044-8317.1974.tb00534.x", "10.1109/tpami.2015.2470671", "10.1198/000313005x22770", "10.1109/tvcg.2012.219", "10.1002/sam.10071", "10.1016/j.ejor.2016.08.066", "10.1007/s00265-003-0651-y", "10.1109/biovis.2013.6664342", "10.3115/v1/p14-1062", "10.3389/fpsyg.2017.01349", "10.1109/tpami.2004.1265866", "10.1057/palgrave.ivs.9500086", "10.1007/s004260000031", "10.1111/cgf.12935", "10.1145/800195.805928", "10.1109/tvcg.2014.2346279", "10.2312/eurovisstar.20141174", "10.1109/tvcg.2012.256", "10.1093/bioinformatics/17.suppl\\_1.s22", "10.1109/infvis.2004.46", "10.1007/978-3-319-06793-3\\_5", "10.1109/tvcg.2006.147", "10.1109/tvcg.2015.2468078", "10.1093/bioinformatics/bti141", "10.1145/1168149.1168168", "10.1109/tvcg.2017.2745978", "10.1177/1473871613513228", "10.1109/mcg.2014.62", "10.1109/tvcg.2006.166", "10.1145/2470654.2470724", "10.1177/154193120605000909", "10.1109/sibgrapi.2007.21", "10.1145/3065386", "10.1109/tvcg.2006.160", "10.1287/opre.20.5.993", "10.1111/cgf.13446", "10.1057/palgrave.ivs.9500092", "10.1145/568522.568523", "10.1109/mcg.2013.66", "10.1109/tvcg.2018.2865940", "10.1109/tvcg.2012.208"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934796", "title": "Improving the Robustness of Scagnostics", "year": "2019", "conferenceName": "InfoVis", "authors": "Yunhai Wang;Zeyu Wang 0005;Tingting Liu;Michael Correll;Zhanglin Cheng;Oliver Deussen;Michael Sedlmair", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Jinan, Peoples R China. Wang, Yunhai; Wang, Zeyu; Liu, Tingting, Shandong Univ, Jinan, Peoples R China. Wang, Zeyu; Cheng, Zhanglin; Deussen, Oliver, SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Peoples R China. Correll, Michael, Tableau Res, Seattle, WA USA. Deussen, Oliver, Konstanz Univ, Constance, Germany. Sedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "In this paper, we examine the robustness of scagnostics through a series of theoretical and empirical studies. First, we investigate the sensitivity of scagnostics by employing perturbing operations on more than 60M synthetic and real-world scatterplots. We found that two scagnostic measures, Outlying and Clumpy, are overly sensitive to data binning. To understand how these measures align with human judgments of visual features, we conducted a study with 24 participants, which reveals that i) humans are not sensitive to small perturbations of the data that cause large changes in both measures, and ii) the perception of clumpiness heavily depends on per-cluster topologies and structures. Motivated by these results, we propose Robust Scagnostics (RScag) by combining adaptive binning with a hierarchy-based form of scagnostics. An analysis shows that RScag improves on the robustness of original scagnostics, aligns better with human judgments, and is equally fast as the traditional scagnostic measures.", "keywords": "Scagnostics,scatterplots,sensitivity analysis,Robust Scagnostics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934796", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2346979", "10.1111/j.1467-8659.2009.01467.x", "10.1109/vast.2008.4677368", "10.2307/2289444", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/vast.2009.5332628", "10.1111/insr.12095\\_11", "10.1109/tvcg.2010.184", "10.1109/tvcg.2015.2467323", "10.1109/vast.2010.5652460", "10.1111/j.1467-8659.2012.03069.x", "10.1145/1842993.1843002", "10.1198/106186008x320465", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.20", "10.1111/cgf.12641", "10.1109/tvcg.2012.128", "10.1109/tvcg.2015.2467671", "10.1057/palgrave.ivs.9500091", "10.1007/978-1-4612-4400-4", "10.1111/cgf.13176", "10.1002/0470870958", "10.1109/tvcg.2018.2864907", "10.1111/j.1467-8659.2012.03125.x", "10.1109/vast.2012.6400490", "10.1109/infvis.2005.1532142", "10.1109/vast.2010.5652433", "10.1109/vast.2009.5332611", "10.1201/9781315140919", "10.1145/2858036.2858155", "10.1109/ldav.2013.6675164", "10.1515/itit-2014-1070", "10.1111/cgf.13684", "10.1109/tpami.1979.4766909", "10.2307/2289936", "10.1109/pacificvis.2014.42", "10.1109/tvcg.2016.2598467", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2006.94", "10.1109/tvcg.2014.2346572", "10.1111/cgf.12632", "10.1109/tvcg.2017.2744339"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/vast47406.2019.8986917", "title": "VIANA: Visual Interactive Annotation of Argumentation", "year": "2019", "conferenceName": "VAST", "authors": "Fabian Sperrle;Rita Sevastjanova;Rebecca Kehlbeck;Mennatallah El-Assady", "citationCount": "0", "affiliation": "Sperrle, F (Corresponding Author), Univ Konstanz, Constance, Germany. Sperrle, Fabian; Sevastjanova, Rita; Kehlbeck, Rebecca; El-Assady, Mennatallah, Univ Konstanz, Constance, Germany.", "countries": "Germany", "abstract": "Argumentation Mining addresses the challenging tasks of identifying boundaries of argumentative text fragments and extracting their relationships. Fully automated solutions do not reach satisfactory accuracy due to their insufficient incorporation of semantics and domain knowledge. Therefore, experts currently rely on time-consuming manual annotations. In this paper, we present a visual analytics system that augments the manual annotation process by automatically suggesting which text fragments to annotate next. The accuracy of those suggestions is improved over time by incorporating linguistic knowledge and language modeling to learn a measure of argument similarity from user interactions. Based on a long-term collaboration with domain experts, we identify and model five high-level analysis tasks. We enable close reading and note-taking, annotation of arguments, argument reconstruction, extraction of argument relations, and exploration of argument graphs. To avoid context switches, we transition between all views through seamless morphing, visually anchoring all text- and graph-based layers. We evaluate our system with a two-stage expert user study based on a corpus of presidential debates. The results show that experts prefer our system over existing solutions due to the speedup provided by the automatic suggestions and the tight integration between text and graph views.", "keywords": "Argumentation annotation,machine learning,user interaction,layered interfaces,semantic transitions", "link": "http://dx.doi.org/10.1109/VAST47406.2019.8986917", "refList": ["10.1007/978-3-642-40624-9\\_1", "10.3233/978-1-61499-436-7-185", "10.1145/371920.372071", "10.3233/978-1-61499-906-5-4", "10.1109/tvcg.2015.2467759", "10.18653/v1/d15-1050", "10.2307/2529310", "10.1109/mic.2003.1167344", "10.1145/312624.312682", "10.1145/2850417", "10.18653/v1/p17-2039", "10.1016/j.eswa.2016.02.013", "10.1007/978-0-387-85820-3\\_3", "10.1007/s11412-009-9080-x", "10.1109/tvcg.2008.127", "10.1145/2207676.2207741", "10.1109/cit.2012.217", "10.3233/aac-170022", "10.1109/tvcg.2017.2745080", "10.1007/978-3-319-44039-2\\_6", "10.1145/3290605.3300233", "10.1007/978-94-017-0431-1", "10.3233/978-1-61499-906-5-313", "10.1142/s0218213004001922", "10.1109/tvcg.2012.262", "10.1177/001316446002000104", "10.1109/tvcg.2018.2834341", "10.3233/978-1-61499-436-7-463", "10.1145/1772690.1772773", "10.1109/tvcg.2014.2346677", "10.1145/2523813", "10.1162/153244303322533223", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2015.2467531", "10.1109/tvcg.2018.2864769", "10.1007/978-3-319-90092-6\\_14", "10.1137/1.9781611972801.19", "10.1109/vast.2012.6400485", "10.1007/s10579-013-9215-6", "10.3115/v1/d14-1162", "10.1111/cgf.13446", "10.1109/tvcg.2006.156", "10.1111/cgf.13092", "10.1145/2645710.2645759", "10.1109/bigdata.2017.8258140", "10.1145/2669557.2669572", "10.2312/eurovisstar.20151113"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030376", "title": "Insight Beyond Numbers: The Impact of Qualitative Factors on Visual Data Analysis", "year": "2020", "conferenceName": "VAST", "authors": "Benjamin Karer;Hans Hagen;Dirk J. Lehmann", "citationCount": "0", "affiliation": "Karer, B (Corresponding Author), Fed Criminal Police Off Germany, Wiesbaden, Germany. Karer, Benjamin, Fed Criminal Police Off Germany, Wiesbaden, Germany. Hagen, Hans, TU Kaiserslautern, Kaiserslautern, Germany. Lehmann, Dirk J., Ostfalia Univ Appl Sci, Wolfenbuttel, Germany. Lehmann, Dirk J., IAV GmbH, Berlin, Germany.", "countries": "Germany", "abstract": "As of today, data analysis focuses primarily on the findings to be made inside the data and concentrates less on how those findings relate to the domain of investigation. Contemporary visualization as a field of research shows a strong tendency to adopt this data-centrism. Despite their decisive influence on the analysis result, qualitative aspects of the analysis process such as the structure, soundness, and complexity of the applied reasoning strategy are rarely discussed explicitly. We argue that if the purpose of visualization is the provision of domain insight rather than the depiction of data analysis results, a holistic perspective requires a qualitative component to to be added to the discussion of quantitative and human factors. To support this point, we demonstrate how considerations of qualitative factors in visual analysis can be applied to obtain explanations and possible solutions for a number of practical limitations inherent to the data-centric perspective on analysis. Based on this discussion of what we call qualitative visual analysis, we develop an inside-outside principle of nested levels of context that can serve as a conceptual basis for the development of visualization systems that optimally support the emergence of insight during analysis.", "keywords": "Visualization,Reasoning,Qualitative Aspects", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030376", "refList": ["10.1057/ivs.2009.22", "10.1109/tvcg.2015.2467732", "10.1109/infvis.2000.885092", "10.1109/vast.2014.7042482", "10.1145/1498700.1498704", "10.1109/tvcg.2009.108", "10.1109/tvcg.2018.2829750", "10.1037/0033-295x.111.4.1036", "10.1109/tvcg.2012.199", "10.1109/38.267476", "10.1007/978-3-540-70956-5\\_2", "10.1109/tvcg.2015.2467613", "10.1109/tvcg.2015.2467195", "10.1109/tvcg.2014.2346419", "10.1109/vast.2017.8585669", "10.1109/tvcg.2010.132", "10.1145/22949.22950", "10.1109/tvcg.2014.2346984", "10.1017/cbo9780511816772", "10.1109/mcg.2003.1231171", "10.1111/coin.12227", "10.1109/tvcg.2018.2865138", "10.1109/38.788803", "10.1109/tvcg.2018.2864849", "10.1109/mcg.2012.120", "10.1109/tvcg.2007.70535", "10.1162/neco.2008.12-06-420", "10.1109/tvcg.2018.2865240", "10.1089/tmj.2010.0114", "10.1109/beliv.2018.8634267", "10.1109/tvcg.2014.2346481", "10.1207/s15327809jls0402\\_2", "10.1109/tvcg.2006.80", "10.1145/2858036.2858280", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2011.52", "10.1016/j.cag.2014.03.002", "10.1109/tvcg.2015.2513410", "10.1111/j.1756-8765.2011.01150.x", "10.1109/infvis.2005.1532142", "10.1002/spe.4380211102", "10.1186/s41235-018-0120-9", "10.1073/pnas.1807180116", "10.1109/tvcg.2012.133", "10.1109/tvcg.2012.273", "10.1111/j.1467-8659.2011.01928.x", "10.1145/353485.353486", "10.1109/tvcg.2015.2462356", "10.1109/mcg.2019.2923483", "10.1111/cgf.13264", "10.1057/palgrave.ivs.9500070", "10.1145/1168149.1168158", "10.1109/mcg.2019.2961716", "10.1016/s0167-739x(96)00029-5", "10.1111/j.1467-8659.2009.01667.x", "10.1177/1473871611415989", "10.1109/tvcg.2013.234", "10.1109/iv.2012.33", "10.1111/cgf.13446", "10.1057/ivs.2008.28", "10.1111/cgf.12379", "10.1037/0033-295x.112.1.159", "10.1109/tvcg.2010.161", "10.1109/tvcg.2017.2744319", "10.1145/989863.989880", "10.1007/s11390-016-1663-1", "10.1177/1473871615609787", "10.1016/j.cola.2019.100911"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030395", "title": "Towards Modeling Visualization Processes as Dynamic Bayesian Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Christian Heine 0002", "citationCount": "0", "affiliation": "Heine, C (Corresponding Author), Univ Leipzig, Leipzig, Germany. Heine, Christian, Univ Leipzig, Leipzig, Germany.", "countries": "Germany", "abstract": "Visualization designs typically need to be evaluated with user studies, because their suitability for a particular task is hard to predict. What the field of visualization is currently lacking are theories and models that can be used to explain why certain designs work and others do not. This paper outlines a general framework for modeling visualization processes that can serve as the first step towards such a theory. It surveys related research in mathematical and computational psychology and argues for the use of dynamic Bayesian networks to describe these time-dependent, probabilistic processes. It is discussed how these models could be used to aid in design evaluation. The development of concrete models will be a long process. Thus, the paper outlines a research program sketching how to develop prototypes and their extensions from existing models, controlled experiments, and observational studies.", "keywords": "Visualization,model building,perception,cognition,dynamic Bayesian networks", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030395", "refList": ["10.1057/ivs.2009.22", "10.1109/tvcg.2015.2467732", "10.1109/infvis.2000.885092", "10.1109/vast.2014.7042482", "10.1145/1498700.1498704", "10.1109/tvcg.2009.108", "10.1109/tvcg.2018.2829750", "10.1037/0033-295x.111.4.1036", "10.1109/tvcg.2012.199", "10.1111/cgf.13899", "10.1109/38.267476", "10.1007/978-3-540-70956-5\\_2", "10.1109/tvcg.2015.2467613", "10.1109/tvcg.2015.2467195", "10.1109/tvcg.2014.2346419", "10.1145/3290605.3300562", "10.1109/vl.1996.545307", "10.1109/vast.2017.8585669", "10.1109/infvis.2003.1249004", "10.1109/tvcg.2010.132", "10.1145/22949.22950", "10.1109/tvcg.2014.2346984", "10.1145/3290605.3300418", "10.1017/cbo9780511816772", "10.1109/mcg.2003.1231171", "10.1111/coin.12227", "10.1109/tvcg.2018.2865138", "10.1109/38.788803", "10.1109/tvcg.2018.2864849", "10.1109/mcg.2012.120", "10.1109/tvcg.2007.70535", "10.1162/neco.2008.12-06-420", "10.1109/tvcg.2018.2865240", "10.1089/tmj.2010.0114", "10.1109/beliv.2018.8634267", "10.1109/tvcg.2014.2346481", "10.1207/s15327809jls0402\\_2", "10.1109/tvcg.2006.80", "10.1145/2858036.2858280", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2011.52", "10.1016/j.cag.2014.03.002", "10.1109/tvcg.2015.2513410", "10.1111/j.1756-8765.2011.01150.x", "10.1109/infvis.2005.1532142", "10.1002/spe.4380211102", "10.1186/s41235-018-0120-9", "10.1073/pnas.1807180116", "10.1109/tvcg.2012.133", "10.1109/tvcg.2012.273", "10.1111/j.1467-8659.2011.01928.x", "10.1145/353485.353486", "10.1109/tvcg.2015.2462356", "10.1109/mcg.2019.2923483", "10.1111/cgf.13264", "10.1057/palgrave.ivs.9500070", "10.1145/1168149.1168158", "10.1109/mcg.2019.2961716", "10.1016/s0167-739x(96)00029-5", "10.1111/j.1467-8659.2009.01667.x", "10.1177/1473871611415989", "10.1109/tvcg.2013.234", "10.1109/iv.2012.33", "10.1111/cgf.13446", "10.1057/ivs.2008.28", "10.1111/cgf.12379", "10.1037/0033-295x.112.1.159", "10.1017/cb09781139033916.005", "10.1109/tvcg.2010.161", "10.1109/tvcg.2017.2744319", "10.1145/989863.989880", "10.1007/s11390-016-1663-1", "10.1177/1473871615609787", "10.1016/j.cola.2019.100911", "10.1057/palgrave.ivs.9500025"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.8199", "year": "2020", "title": "Efficient Morphing of Shape-preserving Star Coordinates", "conferenceName": "PacificVis", "authors": "Vladimir Molchanov;Sagad Hamid;Lars Linsen", "citationCount": "0", "affiliation": "Molchanov, V (Corresponding Author), Westfalische Wilhelms Univ Munster, Munster, Germany.\nMolchanov, Vladimir; Hamid, Sagad; Linsen, Lars, Westfalische Wilhelms Univ Munster, Munster, Germany.", "countries": "Germany", "abstract": "Data tours follow an exploratory multi-dimensional data visualization concept that provides animations of projections of the multidimensional data to a 2D visual space. To create an animation, a sequence of key projections is provided and morphings between each pair of consecutive key projections are computed, which then can be stitched together to form the data tour. The morphings should be smooth so that a user can easily follow the transformations, and their computations shall be fast to allow for their integration into an interactive visual exploration process. Moreover, if the key projections are chosen to satisfy additional conditions, it is desirable that these conditions are maintained during morphing. Shape preservation is such a desirable condition, as it avoids shape distortions that may otherwise be caused by a projection. We develop a novel efficient morphing algorithms for computing shape-preserving data tours, i.e., data tours constructed for a sequence of shape-preserving linear projections. We propose a stepping strategy for the morphing to avoid discontinuities in the evolution of the projections, where we represent the linear projections using a star-coordinates system. Our algorithms are less computationally involved, produce smoother morphings, and require less user-defined parameter settings than existing state-of-the-art approaches.", "keywords": "Human-centered computing; Visualization; Visualization techniques", "link": "https://doi.org/10.1109/PacificVis48177.2020.8199", "refList": ["10.2312/eurovisshort.20141152", "10.1109/tvcg.2013.182", "10.1109/tvcg.2008.153", "10.1109/tvcg.2015.2467591", "10.4135/9781412985130", "10.1111/cgf.12878", "10.1016/j.cag.2016.08.007", "10.1080/10618600.1995.10474674", "10.1109/infvis.2003.1249004", "10.1002/9781118445112.stat06472", "10.1109/visual.1997.663916", "10.1111/cgf.12117", "10.1137/0906011", "10.1109/tvcg.2018.2865118", "10.1126/science.290.5500.2319", "10.1111/cgf.12845", "10.1111/j.1467-8659.2012.03125.x", "10.1109/tvcg.2017.2705189", "10.1002/0471725293", "10.1111/cgf.12876", "10.1111/cgf.13404", "10.2307/2289161", "10.1111/cgf.13446", "10.1198/106186008x318440", "10.2307/1390747", "10.1109/tvcg.2012.35", "10.1109/tvcg.2015.2467132", "10.1109/tvcg.2006.94", "10.1109/t-c.1974.224051", "10.1109/pacificvis.2019.00018", "10.1109/tvcg.2017.2744339", "10.1109/tsmcb.2005.850151"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2019.00018", "year": "2019", "title": "Scatterplot Summarization by Constructing Fast and Robust Principal Graphs from Skeletons", "conferenceName": "PacificVis", "authors": "Jos{\\'{e}} Matute;Marcel Fischer;Alexandru C. Telea;Lars Linsen", "citationCount": "1", "affiliation": "Matute, J (Corresponding Author), Univ Munster, Munster, Germany.\nMatute, Jose; Fischer, Marcel; Linsen, Lars, Univ Munster, Munster, Germany.\nTelea, Alexandru C., Univ Groningen, Groningen, Netherlands.", "countries": "Germany;Netherlands", "abstract": "Principal curves are a long-standing and well-known method for summarizing large scatterplots. They are defined as self-consistent curves (or curve sets in the more general case) that locally pass through the middle of the scatterplot data. However, computing principal curves that capture well complex scatterplot topologies and are robust to noise is hard and/or slow for large scatterplots. We present a fast and robust approach for computing principal graphs (a generalization of principal curves for more complex topologies) inspired by the similarity to medial descriptors (curves locally centered in a shape). Compared to state-of-the-art methods for computing principal graphs, we outperform these in terms of computational scalability and robustness to noise and resolution. We also demonstrate the advantages of our method over other scatterplot summarization approaches.", "keywords": "", "link": "https://doi.org/10.1109/PacificVis.2019.00018", "refList": ["10.1016/j.cag.2014.01.006", "10.1016/s0167-8655(02)00032-6", "10.1006/jmva.2000.1917", "10.1111/cgf.12386", "10.1109/vast.2008.4677367", "10.1109/tpami.2004.1261076", "10.1111/cgf.12865", "10.1109/tvcg.2010.213", "10.1109/tvcg.2014.2346455", "10.1109/tvcg.2011.233", "10.1111/j.1467-8659.2009.01680.x", "10.1109/tvcg.2016.2515611", "10.1109/tit.1982.1056489", "10.1109/34.899944", "10.1007/bf01889678", "10.1137/s0036144599352836", "10.1145/2858036.2858155", "10.1002/jhbs.20078", "10.1198/jcgs.2011.09224", "10.1109/tvcg.2017.2744184", "10.1057/ivs.2010.2", "10.1111/j.1467-8659.2012.03107.x", "10.2307/2289936", "10.1109/pacificvis.2014.42", "10.1109/tpami.2008.21", "10.1111/cgf.13446", "10.2307/2290446", "10.1109/34.982884", "10.1109/tvcg.2006.94", "10.2307/2288711", "10.1117/12.304651", "10.5201/ipol.2013.87", "10.1109/mlsp.2008.4685520"], "wos": 1, "children": [{"doi": "10.1109/pacificvis48177.2020.8199", "year": "2020", "title": "Efficient Morphing of Shape-preserving Star Coordinates", "conferenceName": "PacificVis", "authors": "Vladimir Molchanov;Sagad Hamid;Lars Linsen", "citationCount": "0", "affiliation": "Molchanov, V (Corresponding Author), Westfalische Wilhelms Univ Munster, Munster, Germany.\nMolchanov, Vladimir; Hamid, Sagad; Linsen, Lars, Westfalische Wilhelms Univ Munster, Munster, Germany.", "countries": "Germany", "abstract": "Data tours follow an exploratory multi-dimensional data visualization concept that provides animations of projections of the multidimensional data to a 2D visual space. To create an animation, a sequence of key projections is provided and morphings between each pair of consecutive key projections are computed, which then can be stitched together to form the data tour. The morphings should be smooth so that a user can easily follow the transformations, and their computations shall be fast to allow for their integration into an interactive visual exploration process. Moreover, if the key projections are chosen to satisfy additional conditions, it is desirable that these conditions are maintained during morphing. Shape preservation is such a desirable condition, as it avoids shape distortions that may otherwise be caused by a projection. We develop a novel efficient morphing algorithms for computing shape-preserving data tours, i.e., data tours constructed for a sequence of shape-preserving linear projections. We propose a stepping strategy for the morphing to avoid discontinuities in the evolution of the projections, where we represent the linear projections using a star-coordinates system. Our algorithms are less computationally involved, produce smoother morphings, and require less user-defined parameter settings than existing state-of-the-art approaches.", "keywords": "Human-centered computing; Visualization; Visualization techniques", "link": "https://doi.org/10.1109/PacificVis48177.2020.8199", "refList": ["10.2312/eurovisshort.20141152", "10.1109/tvcg.2013.182", "10.1109/tvcg.2008.153", "10.1109/tvcg.2015.2467591", "10.4135/9781412985130", "10.1111/cgf.12878", "10.1016/j.cag.2016.08.007", "10.1080/10618600.1995.10474674", "10.1109/infvis.2003.1249004", "10.1002/9781118445112.stat06472", "10.1109/visual.1997.663916", "10.1111/cgf.12117", "10.1137/0906011", "10.1109/tvcg.2018.2865118", "10.1126/science.290.5500.2319", "10.1111/cgf.12845", "10.1111/j.1467-8659.2012.03125.x", "10.1109/tvcg.2017.2705189", "10.1002/0471725293", "10.1111/cgf.12876", "10.1111/cgf.13404", "10.2307/2289161", "10.1111/cgf.13446", "10.1198/106186008x318440", "10.2307/1390747", "10.1109/tvcg.2012.35", "10.1109/tvcg.2015.2467132", "10.1109/tvcg.2006.94", "10.1109/t-c.1974.224051", "10.1109/pacificvis.2019.00018", "10.1109/tvcg.2017.2744339", "10.1109/tsmcb.2005.850151"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.13684", "year": "2019", "title": "ClustMe: A Visual Quality Measure for Ranking Monochrome Scatterplots based on Cluster Patterns", "conferenceName": "EuroVis", "authors": "Mostafa M. Abbas;Micha{\\\"{e}}l Aupetit;Michael Sedlmair;Halima Bensmail", "citationCount": "4", "affiliation": "Abbas, MM (Corresponding Author), HBKU, QCRI, Doha, Qatar.\nAbbas, Mostafa M.; Aupetit, Michael; Bensmail, Halima, HBKU, QCRI, Doha, Qatar.\nSedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany.", "countries": "Qatar;Germany", "abstract": "We propose ClustMe, a new visual quality measure to rank monochrome scatterplots based on cluster patterns. ClustMe is based on data collected from a human-subjects study, in which 34 participants judged synthetically generated cluster patterns in 1000 scatterplots. We generated these patterns by carefully varying the free parameters of a simple Gaussian Mixture Model with two components, and asked the participants to count the number of clusters they could see (1 or more than 1). Based on the results, we form ClustMe by selecting the model that best predicts these human judgments among 7 different state-of-the-art merging techniques (Demp). To quantitatively evaluate ClustMe, we conducted a second study, in which 31 human subjects ranked 435 pairs of scatterplots of real and synthetic data in terms of cluster patterns complexity. We use this data to compare ClustMe's performance to 4 other state-of-the-art clustering measures, including the well-known Clumpiness scagnostics. We found that of all measures, ClustMe is in strongest agreement with the human rankings.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13684", "refList": ["10.1109/tvcg.2014.2330617", "10.1109/tvcg.2014.2346979", "10.1109/tvcg.2017.2744138", "10.1109/tvcg.2017.2701829", "10.2307/2529310", "10.1109/tvcg.2011.229", "10.1109/vast.2011.6102437", "10.1068/p030033", "10.1016/j.jvcir.2011.01.005", "10.1109/tnn.2005.845141", "10.1145/2669557.2669559", "10.1167/8.7.6", "10.1145/2993901.2993907", "10.1145/2803140.2803143", "10.1145/1842993.1843002", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.124", "10.1057/ivs.2008.13", "10.1109/vast.2012.6400488", "10.1111/j.1467-9574.2008.00412.x", "10.1214/aos/1176344136", "10.1109/tvcg.2015.2467671", "10.3138/y308-2422-8615-1233", "10.1111/j.1467-8659.2012.03125.x", "10.1177/001316446002000104", "10.1145/2858036.2858155", "10.1007/s11634-010-0058-3", "10.1214/009053605000000417", "10.1109/tvcg.2009.153", "10.1023/a:1018510926151", "10.1109/tvcg.2014.2346978", "10.1007/s10816-016-9307-x", "10.1177/0301006615602599", "10.1109/pacificvis.2014.42", "10.1162/neco.1991.3.2.246", "10.1007/3-540-44491-2\\_3", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2018.2846735", "10.2307/2291604", "10.1109/inf0vis.2005.14", "10.1198/016214502760047131", "10.1007/s00180-008-0119-7", "10.1109/t-c.1974.224051", "10.1111/cgf.12632", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934796", "title": "Improving the Robustness of Scagnostics", "year": "2019", "conferenceName": "InfoVis", "authors": "Yunhai Wang;Zeyu Wang 0005;Tingting Liu;Michael Correll;Zhanglin Cheng;Oliver Deussen;Michael Sedlmair", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Jinan, Peoples R China. Wang, Yunhai; Wang, Zeyu; Liu, Tingting, Shandong Univ, Jinan, Peoples R China. Wang, Zeyu; Cheng, Zhanglin; Deussen, Oliver, SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Peoples R China. Correll, Michael, Tableau Res, Seattle, WA USA. Deussen, Oliver, Konstanz Univ, Constance, Germany. Sedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "In this paper, we examine the robustness of scagnostics through a series of theoretical and empirical studies. First, we investigate the sensitivity of scagnostics by employing perturbing operations on more than 60M synthetic and real-world scatterplots. We found that two scagnostic measures, Outlying and Clumpy, are overly sensitive to data binning. To understand how these measures align with human judgments of visual features, we conducted a study with 24 participants, which reveals that i) humans are not sensitive to small perturbations of the data that cause large changes in both measures, and ii) the perception of clumpiness heavily depends on per-cluster topologies and structures. Motivated by these results, we propose Robust Scagnostics (RScag) by combining adaptive binning with a hierarchy-based form of scagnostics. An analysis shows that RScag improves on the robustness of original scagnostics, aligns better with human judgments, and is equally fast as the traditional scagnostic measures.", "keywords": "Scagnostics,scatterplots,sensitivity analysis,Robust Scagnostics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934796", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2346979", "10.1111/j.1467-8659.2009.01467.x", "10.1109/vast.2008.4677368", "10.2307/2289444", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/vast.2009.5332628", "10.1111/insr.12095\\_11", "10.1109/tvcg.2010.184", "10.1109/tvcg.2015.2467323", "10.1109/vast.2010.5652460", "10.1111/j.1467-8659.2012.03069.x", "10.1145/1842993.1843002", "10.1198/106186008x320465", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.20", "10.1111/cgf.12641", "10.1109/tvcg.2012.128", "10.1109/tvcg.2015.2467671", "10.1057/palgrave.ivs.9500091", "10.1007/978-1-4612-4400-4", "10.1111/cgf.13176", "10.1002/0470870958", "10.1109/tvcg.2018.2864907", "10.1111/j.1467-8659.2012.03125.x", "10.1109/vast.2012.6400490", "10.1109/infvis.2005.1532142", "10.1109/vast.2010.5652433", "10.1109/vast.2009.5332611", "10.1201/9781315140919", "10.1145/2858036.2858155", "10.1109/ldav.2013.6675164", "10.1515/itit-2014-1070", "10.1111/cgf.13684", "10.1109/tpami.1979.4766909", "10.2307/2289936", "10.1109/pacificvis.2014.42", "10.1109/tvcg.2016.2598467", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2006.94", "10.1109/tvcg.2014.2346572", "10.1111/cgf.12632", "10.1109/tvcg.2017.2744339"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.14001", "year": "2020", "title": "Sunspot Plots: Model-based Structure Enhancement for Dense Scatter Plots", "conferenceName": "EuroVis", "authors": "Thomas Trautner;Fabian Bolte;Sergej Stoppel;Stefan Bruckner", "citationCount": "0", "affiliation": "Trautner, T (Corresponding Author), Univ Bergen, Bergen, Norway.\nTrautner, T.; Bolte, F.; Stoppel, S.; Bruckner, S., Univ Bergen, Bergen, Norway.", "countries": "Norway", "abstract": "Scatter plots are a powerful and well-established technique for visualizing the relationships between two variables as a collection of discrete points. However, especially when dealing with large and dense data, scatter plots often exhibit problems such as overplotting, making the data interpretation arduous. Density plots are able to overcome these limitations in highly populated regions, but fail to provide accurate information of individual data points. This is particularly problematic in sparse regions where the density estimate may not provide a good representation of the underlying data. In this paper, we present sunspot plots, a visualization technique that communicates dense data as a continuous data distribution, while preserving the discrete nature of data samples in sparsely populated areas. We furthermore demonstrate the advantages of our approach on typical failure cases of scatter plots within synthetic and real-world data sets and validate its effectiveness in a user study.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14001", "refList": ["10.1057/palgrave.ivs.9500122", "10.2312/eggh/hpg12/097-103", "10.1109/tvcg.2008.119", "10.1109/pacificvis.2011.5742387", "10.1038/331163a0", "10.1109/visual.2019.8933620", "10.2307/2683294", "10.1201/9781351072304", "10.2307/2289444", "10.1109/tvcg.2019.2934541", "10.1080/00949657508810123", "10.1109/tvcg.2013.65", "10.1109/infvis.1997.636789", "10.1109/2945.841121", "10.1109/mcse.2007.55", "10.1109/tvcg.2010.197", "10.1111/cgf.12871", "10.1109/infvis.2003.1249018", "10.1145/3173574.3173991", "10.1109/tvcg.2012.238", "10.1007/0-387-28695-0", "10.1109/pacificvis.2010.5429604", "10.18637/jss.v008.i03", "10.1109/tvcg.2007.70596", "10.1109/visual.1998.745301", "10.1007/0-387-37977-0\\_3", "10.1145/1556262.1556289", "10.2312/wiced.20161094", "10.1109/tvcg.2007.70535", "10.1145/1056808.1056914", "10.1109/tvcg.2003.1196007", "10.1109/iv.2004.1320190", "10.1111/cgf.12877", "10.1162/leon.2007.40.2.202a", "10.1109/tvcg.2017.2674978", "10.1057/ivs.2010.4", "10.1016/j.cag.2018.02.008", "10.1201/9781315140919", "10.1109/visual.2000.885677", "10.1002/jhbs.20078", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1109/visual.2001.964495", "10.1109/iv.2002.1028760", "10.1111/cgf.13684", "10.1109/hicss.2013.197", "10.1109/tvcg.2019.2903956", "10.1093/mnras/stt961", "10.1109/tvcg.2017.2668409", "10.1111/j.1467-8659.2009.01478.x", "10.1145/2702123.2702585", "10.2307/1418003", "10.2307/1390742", "10.1145/360825.360839", "10.1109/pacificvis.2009.4906843", "10.1057/ivs.2009.34", "10.2307/2288711"], "wos": 1, "children": [], "len": 1}], "len": 13}, {"doi": "10.1111/cgf.14000", "year": "2020", "title": "Evaluating Reordering Strategies for Cluster Identification in Parallel Coordinates", "conferenceName": "EuroVis", "authors": "Michael Blumenschein;Xuan Zhang;David Pomerenke;Daniel A. Keim;Johannes Fuchs", "citationCount": "0", "affiliation": "Blumenschein, M (Corresponding Author), Univ Konstanz, Constance, Germany.\nBlumenschein, Michael; Pomerenke, David; Keim, Daniel A.; Fuchs, Johannes, Univ Konstanz, Constance, Germany.\nZhang, Xuan, Rhein Westfal TH Aachen, Aachen, Germany.", "countries": "Germany", "abstract": "The ability to perceive patterns in parallel coordinates plots (PCPs) is heavily influenced by the ordering of the dimensions. While the community has proposed over 30 automatic ordering strategies, we still lack empirical guidance for choosing an appropriate strategy for a given task. In this paper, we first propose a classification of tasks and patterns and analyze which PCP reordering strategies help in detecting them. Based on our classification, we then conduct an empirical user study with 31 participants to evaluate reordering strategies for cluster identification tasks. We particularly measure time, identification quality, and the users' confidence for two different strategies using both synthetic and real-world datasets. Our results show that, somewhat unexpectedly, participants tend to focus on dissimilar rather than similar dimension pairs when detecting clusters, and are more confident in their answers. This is especially true when increasing the amount of clutter in the data. As a result of these findings, we propose a new reordering strategy based on the dissimilarity of neighboring dimension pairs.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14000", "refList": ["10.1111/j.1467-8659.2008.01241.x", "10.2312/conf/eg2013/stars/095-116", "10.1109/tvcg.2014.2346979", "10.1117/12.838819", "10.2307/2290001", "10.1111/cgf.12638", "10.1007/978-3-642-30217-6\\_42", "10.1109/sc.2005.47", "10.1002/wics.145", "10.1109/tvcg.2011.229", "10.1109/vast.2009.5332628", "10.1109/vast.2017.8585613", "10.1145/2993901.2993907", "10.1007/bf00410640", "10.1109/tvcg.2010.184", "10.11591/ijece.v5i6", "10.1111/j.1467-8659.2011.01961.x", "10.1109/visual.1997.663916", "10.1111/j.1467-8659.2012.03129.x", "10.1109/visual.1999.809866", "10.1057/ivs.2008.13", "10.1016/j.visinf.2017.11.001", "10.2307/2282967", "10.1007/978-0-387-68628-8\\_10", "10.5888/pcd9.120082", "10.1109/tvcg.2007.70535", "10.1109/vast.2010.5652450", "10.1007/bf01898350", "10.1016/0010-0285(91)90009-d", "10.1007/978-3-642-24958-7\\_12", "10.1109/infvis.2005.1532142", "10.1057/palgrave.ivs.9500166", "10.1111/j.1467-8659.2008.01239.x", "10.1109/mcse.2015.55", "10.2312/pe/eurovast/eurova12/007-011.7", "10.1109/tvcg.2009.153", "10.1198/jcgs.2010.09136", "10.1145/2463676.2463696", "10.1109/infvis.2005.1532138", "10.1109/tvcg.2010.242", "10.5220/0006097400400051", "10.1109/visual.2019.8933706", "10.1109/atc.2015.7388338", "10.2312/pe/eurovast/eurova12/007-011", "10.5120/5044-7370", "10.1111/cgf.13446", "10.1109/icmla.2012.148", "10.1016/j.jvlc.2015.12.001", "10.1109/tvcg.2015.2466992", "10.1111/j.1467-8659.2009.01666.x", "10.5169/seals-266450", "10.1109/infvis.1998.729559", "10.1016/j.jvlc.2017.10.003", "10.1109/visual.1995.485139", "10.1007/978-0-387-39940-9\\_262", "10.1109/tvcg.2006.138"], "wos": 1, "children": [], "len": 1}], "len": 71}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13196", "year": "2017", "title": "Adaptable Radial Axes Plots for Improved Multivariate Data Visualization", "conferenceName": "EuroVis", "authors": "Manuel Rubio{-}S{\\'{a}}nchez;Alberto S{\\'{a}}nchez;D. J. Lehmann", "citationCount": "8", "affiliation": "Rubio-Sanchez, M (Corresponding Author), Univ Rey Juan Carlos, Madrid, Spain.\nRubio-Sanchez, M.; Sanchez, A.; Lehmann, D. J., Univ Rey Juan Carlos, Madrid, Spain.\nSanchez, A., Ctr Computat Simulat, Madrid, Spain.\nLehmann, D. J., Univ Magdeburg, Magdeburg, Germany.", "countries": "Spain;Germany", "abstract": "Radial axes plots are multivariate visualization techniques that extend scatterplots in order to represent high-dimensional data as points on an observable display. Well-known methods include star coordinates or principal component biplots, which represent data attributes as vectors that define axes, and produce linear dimensionality reduction mappings. In this paper we propose a hybrid approach that bridges the gap between star coordinates and principal component biplots, which we denominate adaptable radial axes plots. It is based on solving convex optimization problems where users can: (a) update the axis vectors interactively, as in star coordinates, while producing mappings that enable to estimate attribute values optimally through labeled axes, similarly to principal component biplots; (b) use different norms in order to explore additional nonlinear mappings of the data; and (c) include weights and constraints in the optimization problems for sorting the data along one axis. The result is a flexible technique that complements, extends, and enhances current radial methods for data analysis.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13196", "refList": ["10.1109/tvcg.2013.182", "10.1111/j.1467-8659.2011.01958.x", "10.1057/palgrave.ivs.9500099", "10.2307/2334381", "10.1109/tvcg.2014.2346258", "10.1007/s00357-016-9213-7", "10.1109/pacificvis.2015.7156391", "10.1145/22949.22950", "10.1109/tvcg.2015.2467324", "10.1111/cgf.12641", "10.1109/infvis.2005.1532136", "10.1109/pacificvis.2014.58", "10.2307/2288400", "10.1145/1400214.1400234", "10.1007/978-1-84800-155-8\\_7", "10.1111/cgf.12116", "10.1145/1054972.1055031", "10.1109/tvcg.2009.23", "10.1007/bf02288367", "10.1109/tvcg.2010.209"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865118", "title": "Shape-preserving Star Coordinates", "year": "2018", "conferenceName": "InfoVis", "authors": "Vladimir Molchanov;Lars Linsen", "citationCount": "1", "affiliation": "Molchanov, V (Corresponding Author), Westfalische Wilhelms Univ Munster, Munster, Germany. Molchanov, Vladimir; Linsen, Lars, Westfalische Wilhelms Univ Munster, Munster, Germany.", "countries": "Germany", "abstract": "Dimensionality reduction is commonly applied to multidimensional data to reduce the complexity of their analysis. In visual analysis systems, projections embed multidimensional data into 2D or 3D spaces for graphical representation. To facilitate a robust and accurate analysis, essential characteristics of the multidimensional data shall be preserved when projecting. Orthographic star coordinates is a state-of-the-art linear projection method that avoids distortion of multidimensional clusters by restricting interactive exploration to orthographic projections. However, existing numerical methods for computing orthographic star coordinates have a number of limitations when putting them into practice. We overcome these limitations by proposing the novel concept of shape-preserving star coordinates where shape preservation is assured using a superset of orthographic projections. Our scheme is explicit, exact, simple, fast, parameter-free, and stable. To maintain a valid shape-preserving star-coordinates configuration during user interaction with one of the star-coordinates axes, we derive an algorithm that only requires us to modify the configuration of one additional compensatory axis. Different design goals can be targeted by using different strategies for selecting the compensatory axis. We propose and discuss four strategies including a strategy that approximates orthographic star coordinates very well and a data-driven strategy. We further present shape-preserving morphing strategies between two shape-preserving configurations, which can be adapted for the generation of data tours. We apply our concept to multiple data analysis scenarios to document its applicability and validate its desired properties.", "keywords": "Star coordinates,multidimensional data projection,multivariate data visualization", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865118", "refList": ["10.1007/978-1-4612-4214", "10.2312/eurovisshort.20141152", "10.1109/tvcg.2013.182", "10.1057/palgrave.ivs.9500099", "10.2307/2334381", "10.1109/tvcg.2014.2346258", "10.1016/j.cag.2016.08.007", "10.1111/cgf.13196", "10.1080/10618600.1995.10474674", "10.1111/cgf.12117", "10.1137/0906011", "10.1109/tvcg.2015.2467324", "10.3390/pr5040075", "10.2307/2289161", "10.1109/tvcg.2015.2467132", "10.1109/t-c.1974.224051", "10.1177/1473871612439357", "10.2312/vcbm.20151204", "10.1109/tvcg.2010.209"], "wos": 1, "children": [{"doi": "10.1109/pacificvis48177.2020.8199", "year": "2020", "title": "Efficient Morphing of Shape-preserving Star Coordinates", "conferenceName": "PacificVis", "authors": "Vladimir Molchanov;Sagad Hamid;Lars Linsen", "citationCount": "0", "affiliation": "Molchanov, V (Corresponding Author), Westfalische Wilhelms Univ Munster, Munster, Germany.\nMolchanov, Vladimir; Hamid, Sagad; Linsen, Lars, Westfalische Wilhelms Univ Munster, Munster, Germany.", "countries": "Germany", "abstract": "Data tours follow an exploratory multi-dimensional data visualization concept that provides animations of projections of the multidimensional data to a 2D visual space. To create an animation, a sequence of key projections is provided and morphings between each pair of consecutive key projections are computed, which then can be stitched together to form the data tour. The morphings should be smooth so that a user can easily follow the transformations, and their computations shall be fast to allow for their integration into an interactive visual exploration process. Moreover, if the key projections are chosen to satisfy additional conditions, it is desirable that these conditions are maintained during morphing. Shape preservation is such a desirable condition, as it avoids shape distortions that may otherwise be caused by a projection. We develop a novel efficient morphing algorithms for computing shape-preserving data tours, i.e., data tours constructed for a sequence of shape-preserving linear projections. We propose a stepping strategy for the morphing to avoid discontinuities in the evolution of the projections, where we represent the linear projections using a star-coordinates system. Our algorithms are less computationally involved, produce smoother morphings, and require less user-defined parameter settings than existing state-of-the-art approaches.", "keywords": "Human-centered computing; Visualization; Visualization techniques", "link": "https://doi.org/10.1109/PacificVis48177.2020.8199", "refList": ["10.2312/eurovisshort.20141152", "10.1109/tvcg.2013.182", "10.1109/tvcg.2008.153", "10.1109/tvcg.2015.2467591", "10.4135/9781412985130", "10.1111/cgf.12878", "10.1016/j.cag.2016.08.007", "10.1080/10618600.1995.10474674", "10.1109/infvis.2003.1249004", "10.1002/9781118445112.stat06472", "10.1109/visual.1997.663916", "10.1111/cgf.12117", "10.1137/0906011", "10.1109/tvcg.2018.2865118", "10.1126/science.290.5500.2319", "10.1111/cgf.12845", "10.1111/j.1467-8659.2012.03125.x", "10.1109/tvcg.2017.2705189", "10.1002/0471725293", "10.1111/cgf.12876", "10.1111/cgf.13404", "10.2307/2289161", "10.1111/cgf.13446", "10.1198/106186008x318440", "10.2307/1390747", "10.1109/tvcg.2012.35", "10.1109/tvcg.2015.2467132", "10.1109/tvcg.2006.94", "10.1109/t-c.1974.224051", "10.1109/pacificvis.2019.00018", "10.1109/tvcg.2017.2744339", "10.1109/tsmcb.2005.850151"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 5}], "len": 721}, "index": 1299, "embedding": [-2.1502761840820312, 4.6623053550720215, -1.0366979837417603, -2.298758029937744, -0.15161657333374023, 0.16650904715061188, -0.6933266520500183, -0.5719693899154663, 11.387539863586426, 13.573355674743652, -1.647068977355957, 6.473385810852051, 16.11638069152832, -1.6603052616119385, 18.342531204223633, -0.5381391644477844, 6.224165439605713, 0.06749609857797623, 5.268621444702148, -1.0093307495117188, -0.06630208343267441, 2.5494437217712402, 2.864936590194702, 10.213628768920898, -2.388123035430908, 22.88459014892578, 2.0432748794555664, 4.684713363647461, -0.4862351715564728, 9.707422256469727, 2.83003830909729, 5.695966720581055], "projection": [2.409740447998047, 12.283040046691895], "size": 361, "height": 8, "width": 114}