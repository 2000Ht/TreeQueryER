{"data": {"doi": "10.1111/cgf.13195", "year": "2017", "title": "Visual Narrative Flow: Exploring Factors Shaping Data Visualization Story Reading Experiences", "conferenceName": "EuroVis", "authors": "Sean McKenna;Nathalie Henry Riche;Bongshin Lee;Jeremy Boy;Miriah Meyer", "citationCount": "16", "affiliation": "McKenna, S (Corresponding Author), Microsoft Res, Redmond, WA 98052 USA.\nMcKenna, S (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA.\nMcKenna, S.; Riche, N. Henry; Lee, B., Microsoft Res, Redmond, WA 98052 USA.\nMcKenna, S.; Meyer, M., Univ Utah, Salt Lake City, UT 84112 USA.\nBoy, J., United Nations Global Pulse, New York, NY USA.", "countries": "USA", "abstract": "Many factors can shape the flow of visual data-driven stories, and thereby the way readers experience those stories. Through the analysis of 80 existing stories found on popular websites, we systematically investigate and identify seven characteristics of these stories, which we name flow-factors, and we illustrate how they feed into the broader concept of visual narrative flow. These flow-factors are navigation input, level of control, navigation progress, story layout, role of visualization, story progression, and navigation feedback. We also describe a series of studies we conducted, which shed initial light on how different visual narrative flows impact the reading experience. We report on two exploratory studies, in which we gathered reactions and preferences of readers for stepper- vs. scroller-driven flows. We then report on a crowdsourced study with 240 participants, in which we explore the effect of the combination of different flow-factors on readers' engagement. Our results indicate that visuals and navigation feedback (e.g., static vs. animated transitions) have an impact on readers' engagement, while level of control (e.g., discrete vs. continuous) may not.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13195", "refList": ["10.1109/tvcg.2016.2598647", "10.1145/2702123.2702452", "10.1145/2702123.2702431", "10.1145/1056808.1057068", "10.1006/ijhc.2002.1017", "10.1177/1473871611413180", "10.1111/cgf.12392", "10.1006/ijhc.1017", "10.1145/989863.989865", "10.1145/2858036.2858387", "10.18637/jss.v067.i01", "10.1109/tvcg.2013.119", "10.1201/9781498710411", "10.1145/2909132.2909255", "10.1109/tvcg.2007.70539", "10.1002/asi.21229", "10.1002/hbm.20701", "10.1109/tvcg.2010.179", "10.1145/2993901.2993903"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865232", "title": "Narvis: Authoring Narrative Slideshows for Introducing Data Visualization Designs", "year": "2018", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhen Li;Siwei Fu;Weiwei Cui;Huamin Qu", "citationCount": "3", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Li, Zhen; Fu, Siwei; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Cui, Weiwei, Microsoft Res Asia, Beijing, Peoples R China.", "countries": "China", "abstract": "Visual designs can be complex in modern data visualization systems, which poses special challenges for explaining them to the non-experts. However, few if any presentation tools are tailored for this purpose. In this study, we present Narvis, a slideshow authoring tool designed for introducing data visualizations to non-experts. Narvis targets two types of end users: teachers, experts in data visualization who produce tutorials for explaining a data visualization, and students, non-experts who try to understand visualization designs through tutorials. We present an analysis of requirements through close discussions with the two types of end users. The resulting considerations guide the design and implementation of Narvis. Additionally, to help teachers better organize their introduction slideshows, we specify a data visualization as a hierarchical combination of components, which are automatically detected and extracted by Narvis. The teachers craft an introduction slideshow through first organizing these components, and then explaining them sequentially. A series of templates are provided for adding annotations and animations to improve efficiency during the authoring process. We evaluate Narvis through a qualitative analysis of the authoring experience, and a preliminary evaluation of the generated slideshows.", "keywords": "Education,Narrative Visualization,Authoring Tools", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865232", "refList": ["10.1016/s0042-6989(00)00168-1", "10.1109/tvcg.2016.2598647", "10.1111/cgf.13195", "10.1145/2702123.2702452", "10.1145/2702123.2702431", "10.1016/j.ipm.2015.02.003", "10.1145/2047196.2047247", "10.1145/2858036.2858435", "10.1109/mcg.2017.51", "10.1109/vast.2007.4388992", "10.1111/cgf.12392", "10.1038/35058500", "10.1109/tvcg.2011.239", "10.1109/tvcg.2010.183", "10.1109/mcg.2015.99", "10.1109/tvcg.2013.119", "10.1145/2598784.2598806", "10.1111/cogs.12016", "10.1109/tvcg.2016.2598876", "10.1145/2598510.2598566", "10.2307/2288400", "10.1109/tvcg.2015.2467196", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2015.2467531", "10.1145/2642918.2647411", "10.1109/tvcg.2009.174", "10.1109/tvcg.2013.191", "10.1109/pacificvis.2012.6183556", "10.1007/s12528-011-9042-y", "10.1109/tvcg.2015.2413786"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934398", "title": "DataShot: Automatic Generation of Fact Sheets from Tabular Data", "year": "2019", "conferenceName": "InfoVis", "authors": "Yun Wang 0012;Zhida Sun;Haidong Zhang;Weiwei Cui;Ke Xu;Xiaojuan Ma;Dongmei Zhang", "citationCount": "3", "affiliation": "Wang, Y (Corresponding Author), Microsoft Res Asia, Beijing, Peoples R China. Wang, Yun; Sun, Zhida; Zhang, Haidong; Cui, Weiwei; Xu, Ke; Zhang, Dongmei, Microsoft Res Asia, Beijing, Peoples R China. Sun, Zhida; Xu, Ke; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Dept Comp Sci \\& Engn, Hong Kong, Peoples R China.", "countries": "China", "abstract": "Fact sheets with vivid graphical design and intriguing statistical insights are prevalent for presenting raw data. They help audiences understand data-related facts effectively and make a deep impression. However, designing a fact sheet requires both data and design expertise and is a laborious and time-consuming process. One needs to not only understand the data in depth but also produce intricate graphical representations. To assist in the design process, we present DataShot which, to the best of our knowledge, is the first automated system that creates fact sheets automatically from tabular data. First, we conduct a qualitative analysis of 245 infographic examples to explore general infographic design space at both the sheet and element levels. We identify common infographic structures, sheet layouts, fact types, and visualization styles during the study. Based on these findings, we propose a fact sheet generation pipeline, consisting of fact extraction, fact composition, and presentation synthesis, for the auto-generation workflow. To validate our system, we present use cases with three real-world datasets. We conduct an in-lab user study to understand the usage of our system. Our evaluation results show that DataShot can efficiently generate satisfactory fact sheets to support further customization and data presentation.", "keywords": "Fact sheet,infographic,visualization,and automated design", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934398", "refList": ["10.1109/tvcg.2016.2598647", "10.1109/tvcg.2018.2865158", "10.1111/cgf.13195", "10.1145/2851141.2851145", "10.1109/tvcg.2018.2829750", "10.1145/2556288.2557228", "10.1111/cgf.12392", "10.1145/3025171.3025227", "10.1109/vl.1996.545307", "10.1109/tvcg.2017.2744198", "10.14778/2733004.2733035", "10.1109/pacificvis.2017.8031599", "10.1109/pacificvis.2009.4906837", "10.1145/22949.22950", "10.1137/1.9781611972788.22", "10.1145/2702123.2702149", "10.1109/infvis.2005.1532136", "10.1109/mcg.2015.99", "10.1145/1183614.1183688", "10.1109/tvcg.2015.2467321", "10.1109/tvcg.2013.119", "10.1109/tvcg.2016.2598876", "10.1145/2470654.2481374", "10.1145/3025453.3025866", "10.1145/2901790.2901817", "10.1145/3035918.3035922", "10.1111/cgf.12391", "10.1145/3173574.3173612", "10.1109/tvcg.2016.2598620", "10.1109/tvcg.2014.2346291", "10.1145/3173574.3173909", "10.1109/mc.2013.36", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2013.234", "10.1177/1473871618806555", "10.1145/3173574.3173697", "10.1109/tvcg.2010.179", "10.1016/j.ijhcs.2018.01.004", "10.1109/tvcg.2016.2614803"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030403", "title": "Calliope: Automatic Visual Data Story Generation from a Spreadsheet", "year": "2020", "conferenceName": "InfoVis", "authors": "Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi 0007;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, Intelligent Big Data Visualizat Lab, Shanghai, Peoples R China. Shi, Danqing; Xu, Xinyue; Sun, Fuling; Shi, Yang; Cao, Nan, Tongji Univ, Intelligent Big Data Visualizat Lab, Shanghai, Peoples R China.", "countries": "China", "abstract": "Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.", "keywords": "Information Visualization,Visual Storytelling,Data Story", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030403", "refList": ["10.1007/s11063-017-9759-3", "10.1109/tvcg.2016.2598647", "10.1162/0891201054223977", "10.1109/tvcg.2015.2467732", "10.3390/info9030065", "10.1109/tvcg.2019.2934398", "10.1016/j.visinf.2018.12.001", "10.1145/2002353.2002355", "10.1109/tvcg.2007.70594", "10.1111/cgf.12392", "10.14778/2831360.2831371", "10.1093/biomet/33.3.239", "10.1109/mcg.2019.2924636", "10.1109/tvcg.2017.2659744", "10.1109/pacificvis.2009.4906837", "10.1109/pacificvis.2017.8031599", "10.4103/1755-6783.179101", "10.1145/2362394.2362398", "10.1111/cgf.12925", "10.1109/icde.2018.00019", "10.1109/tvcg.2018.2865240", "10.1109/mcg.2015.99", "10.1109/vds48975.2019.8973383", "10.1038/nature16961", "10.1109/tciaig.2012.2186810", "10.1145/3035918.3035922", "10.1145/3197517.3201362", "10.1109/tvcg.2019.2934281", "10.1109/tvcg.2016.2598620", "10.1155/2019/8480905", "10.1109/iccchina.2013.6671183", "10.1109/tvcg.2018.2865145", "10.1145/3299869.3314037", "10.1017/s1351324907004664", "10.1109/tvcg.2019.2934785", "10.1177/1473871618806555", "10.1613/jair.2989", "10.1109/tvcg.2010.179", "10.1145/3303766"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14004", "year": "2020", "title": "Infomages: Embedding Data into Thematic Images", "conferenceName": "EuroVis", "authors": "Darius Coelho;Klaus Mueller", "citationCount": "0", "affiliation": "Coelho, D (Corresponding Author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.\nCoelho, D.; Mueller, K., SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.", "countries": "USA", "abstract": "Recent studies have indicated that visually embellished charts such as infographics have the ability to engage viewers and positively affect memorability. Fueled by these findings, researchers have proposed a variety of infographic design tools. However, these tools do not cover the entire design space. In this work, we identify a subset of infographics that we call infomages. Infomages are casual visuals of data in which a data chart is embedded into a thematic image such that the content of the image reflects the subject and the designer's interpretation of the data. Creating an effective infomage, however, can require a fair amount of design expertise and is thus out of reach for most people. In order to also afford non-artists with the means to design convincing infomages, we first study the principled design of existing infomages and identify a set of key chart embedding techniques. Informed by these findings we build a design tool that links web-scale image search with a set of interactive image processing tools to empower novice users with the ability to design a wide variety of infomages. As the embedding process might introduce some amount of visual distortion of the data our tool also aids users to gauge the amount of this distortion, if any. We experimentally demonstrate the usability of our tool and conclude with a discussion of infomages and our design tool.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14004", "refList": ["10.1109/mcg.2018.2879066", "10.1109/tvcg.2015.2467732", "10.1002/acp.2350030302", "10.1145/361237.361242", "10.1109/tvcg.2019.2934398", "10.1145/2702123.2702275", "10.1109/tvcg.2019.2934810", "10.1111/cgf.12634", "10.1109/tvcg.2012.221", "10.1145/2702123.2702545", "10.1145/2501988.2502046", "10.1109/tvcg.2012.197", "10.1109/tvcg.2016.2598609", "10.1086/209244", "10.1145/1015706.1015720", "10.1109/tvcg.2011.175", "10.1111/cgf.12888", "10.1109/tvcg.2015.2467321", "10.2307/2288400", "10.1109/tvcg.2006.163", "10.1109/tvcg.2015.2440241", "10.1080/10447319509526110", "10.1109/tvcg.2016.2598620", "10.1109/tvcg.2013.234", "10.1109/tvcg.2019.2934785", "10.1126/science.220.4598.671", "10.1111/j.1469-8986.1993.tb03352.x", "10.1179/000870403235002042", "10.1145/2702123.2702608"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/tvcg.2020.3030396", "title": "What Makes a Data-GIF Understandable?", "year": "2020", "conferenceName": "InfoVis", "authors": "Xinhuan Shu;Aoyu Wu;Junxiu Tang;Benjamin Bach;Yingcai Wu;Huamin Qu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Shu, Xinhuan; Tang, Junxiu; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Shu, Xinhuan; Wu, Aoyu; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Shu, Xinhuan, Zhejiang Univ, Hangzhou, Peoples R China. Bach, Benjamin, Univ Edinburgh, Edinburgh, Midlothian, Scotland.", "countries": "Scotland;China", "abstract": "GIFs are enjoying increasing popularity on social media as a format for data-driven storytelling with visualization; simple visual messages are embedded in short animations that usually last less than 15 seconds and are played in automatic repetition. In this paper, we ask the question, \u201cWhat makes a data-GIF understandable?\u201d While other storytelling formats such as data videos, infographics, or data comics are relatively well studied, we have little knowledge about the design factors and principles for \u201cdata-GIFs\u201d. To close this gap, we provide results from semi-structured interviews and an online study with a total of 118 participants investigating the impact of design decisions on the understandability of data-GIFs. The study and our consequent analysis are informed by a systematic review and structured design space of 108 data-GIFs that we found online. Our results show the impact of design dimensions from our design space such as animation encoding, context preservation, or repetition on viewers understanding of the GIF's core message. The paper concludes with a list of suggestions for creating more effective Data-GIFs.", "keywords": "Data-GIFs,Data-driven Storytelling,Evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030396", "refList": ["10.1109/tvcg.2016.2598647", "10.1016/j.visinf.2020.07.001", "10.1177/1473871615594652", "10.1109/tvcg.2014.2346424", "10.1111/cgf.13195", "10.1109/vlsicircuits18222.2020.9162811", "10.1109/tvcg.2018.2864909", "10.1109/tvcg.2018.2864903", "10.1145/2702123.2702431", "10.1016/j.visinf.2019.12.002", "10.1145/3274349", "10.1111/cgf.13444", "10.1145/3206505.3206552", "10.1145/3290605.3300280", "10.1109/jstqe.2020.3021589", "10.1145/2647868.2656408", "10.1006/ijhc.1017", "10.1145/3290605.3300335", "10.1007/s12650-020-00689-0", "10.1145/2818048.2819936", "10.1111/cgf.13325", "10.1145/2858036.2858387", "10.1145/3027063.3053139", "10.1145/3290605.3300474", "10.1109/tvcg.2016.2598920", "10.1109/tvcg.2018.2864899", "10.1145/3290605.3300483", "10.1145/3173574.3173612", "10.1145/2909132.2909255", "10.1109/tvcg.2016.2598620", "10.1016/j.learninstruc.2007.09.013", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2008.125", "10.1145/3173574.3173909", "10.1109/tvcg.2019.2934397", "10.1111/cgf.13709", "10.1109/tvcg.2013.234", "10.1109/tvcg.2019.2934401", "10.1145/2858036.2858532", "10.1016/j.visinf.2020.08.001", "10.1109/tvcg.2010.179", "10.1109/cicc48029.2020.9075900"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2019.00036", "year": "2019", "title": "Designing Narrative Slideshows for Learning Analytics", "conferenceName": "PacificVis", "authors": "Qing Chen;Zhen Li;Ting{-}Chuen Pong;Huamin Qu", "citationCount": "0", "affiliation": "Chen, Q (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.\nChen, Qing; Li, Zhen; Pong, Ting-Chuen; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "The practical power of data visualization is currently attracting much attention in the e-learning domain. A growing number of studies have been conducted in recent years to help instructors better analyze learner behavior and reflect on their teaching. However, current e-learning dashboards and visualization systems usually require a lot of time and effort into the exploration process. Moreover, the lack of communication power of existing systems constrains users from organizing the narrative of information pieces into a compelling data story. In this paper, we have proposed a narrative visualization approach with an interactive slideshow that helps instructors and education experts explore potential learning patterns and convey data stories. This approach contains three key components: guided-tour concept, drill-down path, and dig-in exploration dimension. The use cases further demonstrate the potential of employing this visual narrative approach in the e-learning context.", "keywords": "Human-centered computing; Visualization; Visualization application domains; Information Visualization", "link": "https://doi.org/10.1109/PacificVis.2019.00036", "refList": ["10.1145/2723576.2723662", "10.1111/cgf.13195", "10.1145/2702123.2702431", "10.1007/s00779-013-0751-2", "10.1109/tvcg.2015.2505305", "10.1109/pacificvis.2015.7156373", "10.1145/2212776.2212860", "10.1145/2858036.2858387", "10.1109/mcg.2015.99", "10.1016/j.chb.2014.07.002", "10.1109/tvcg.2013.119", "10.1111/cogs.12016", "10.1016/j.chb.2014.05.045", "10.1111/cgf.13194", "10.1007/978-3-319-66610-5\\_10", "10.1145/2090116.2090118", "10.1145/2460296.2460350", "10.1109/tvcg.2007.70577", "10.1007/978-3-319-13296-9\\_16", "10.1145/3027385.3027403", "10.1145/2723576.2723627", "10.1145/2724660.2724683", "10.3928/00220124-20121227-69", "10.1145/3027385.3027451", "10.1109/tvcg.2010.179", "10.1109/tvcg.2007.70584", "10.1109/tlt.2016.2599522", "10.1109/tvcg.2011.255"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13444", "year": "2018", "title": "Information Visualization Evaluation Using Crowdsourcing", "conferenceName": "EuroVis", "authors": "Rita Borgo;Luana Micallef;Benjamin Bach;Fintan McGee;Bongshin Lee", "citationCount": "10", "affiliation": "Borgo, R (Corresponding Author), Kingss Coll London, Dept Informat, London, England.\nBorgo, R., Kingss Coll London, Dept Informat, London, England.\nMicallef, L., Aalto Univ, Dept Comp Sci, Espoo, Finland.\nBach, B., Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.\nMcGee, F., LIST, Environm Informat Unit, Luxembourg, Luxembourg.\nLee, B., Microsoft Res, Redmond, WA USA.", "countries": "USA;Finland;Luxembourg;Scotland;England", "abstract": "Visualization researchers have been increasingly leveraging crowdsourcing approaches to overcome a number of limitations of controlled laboratory experiments, including small participant sample sizes and narrow demographic backgrounds of study participants. However, as a community, we have little understanding on when, where, and how researchers use crowdsourcing approaches for visualization research. In this paper, we review the use of crowdsourcing for evaluation in visualization research. We analyzed 190 crowdsourcing experiments, reported in 82 papers that were published in major visualization conferences and journals between 2006 and 2017. We tagged each experiment along 36 dimensions that we identified for crowdsourcing experiments. We grouped our dimensions into six important aspects: study design \\& procedure, task type, participants, measures \\& metrics, quality assurance, and reproducibility. We report on the main findings of our review and discuss challenges and opportunities for improvements in conducting crowdsourcing studies for visualization research.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13444", "refList": ["10.1145/2631775.2631819", "10.1111/cgf.13195", "10.1109/tvcg.2014.2346575", "10.1145/1453152.1453156", "10.1145/2858036.2858063", "10.1111/cgf.12634", "10.1145/2835776.2835835", "10.1109/tvcg.2015.2467759", "10.1145/2858036.2858440", "10.1163/156856897x00357", "10.1145/2047196.2047199", "10.1145/2702123.2702545", "10.1145/3119930", "10.1145/2702123.2702594", "10.1109/tvcg.2013.247", "10.1145/3025453.3025917", "10.1145/2207676.2208556", "10.1007/978-3-319-66435-4\\_5", "10.1109/tvcg.2010.186", "10.1145/1746259.1746260", "10.1177/1071181311551228", "10.1518/155723409x448017", "10.1111/cgf.12233", "10.1109/tvcg.2015.2467671", "10.1109/tvcg.2016.2598862", "10.1145/2063576.2063860", "10.1145/3025453.3025477", "10.1145/2470654.2481281", "10.1145/1869086.1869094", "10.5194/isprsannals-ii-3-w5-325-2015", "10.1145/2858036.2858280", "10.1111/cgf.12127", "10.1109/tvcg.2015.2465151", "10.1145/2591677", "10.1109/iv.2014.47", "10.1109/cce.2014.6916756", "10.1109/vast.2011.6102470", "10.1109/tvcg.2012.234", "10.1145/2702123.2702443", "10.1145/3025453.3025781", "10.1109/tvcg.2013.234", "10.1109/tvcg.2017.2746018", "10.1109/tvcg.2015.2467758", "10.1109/passat/socialcom.2011.203", "10.1145/989863.989880", "10.1111/cgf.12387", "10.1109/tvcg.2015.2500240", "10.1109/vast.2012.6400540", "10.1109/tvcg.2016.2599058", "10.1145/2702123.2702608", "10.1145/2660398.2660403", "10.1109/tvcg.2014.2346979", "10.1145/2702123.2702452", "10.1145/2598153.2598168", "10.1109/pacificvis.2016.7465249", "10.1145/2858036.2858107", "10.1145/3025453.3025820", "10.1109/tvcg.2014.2315995", "10.1111/j.1467-8659.2009.01442.x", "10.1109/tvcg.2015.2467451", "10.1145/2675133.2675246", "10.1145/2598153.2602248", "10.1109/tvcg.2015.2468151", "10.1145/3027063.3053113", "10.1007/978-3-319-20267-9\\_14", "10.1145/2928269", "10.1109/tvcg.2014.2346320", "10.1145/2468356.2468529", "10.1145/2810012", "10.1109/vast.2010.5652890", "10.1364/josa.35.000268", "10.1109/tvcg.2017.2674978", "10.1111/cgf.12657", "10.1145/2441776.2441912", "10.1109/tvcg.2013.164", "10.1109/tvcg.2016.2598591", "10.1371/journal.pbio.1002456", "10.1145/2818048.2820005", "10.1145/1837885.1837889", "10.1145/2598153.2602249", "10.1109/tvcg.2012.230", "10.1145/3025453.3025870", "10.1145/2470654.2481410", "10.1117/12.2076745", "10.1109/tvcg.2015.2467201", "10.1145/2441776.2441847", "10.1145/2470654.2466424", "10.1109/vl.1996.545307", "10.1145/3025453.3025592", "10.1007/s10791-012-9205-0", "10.1145/2858036.2858300", "10.1145/2488388.2488489", "10.1109/tvcg.2010.174", "10.1109/pacificvis.2012.6183570", "10.1111/cgf.13018", "10.1109/tvcg.2011.279", "10.1145/3025453.3025922", "10.1145/3131275", "10.1007/978-3-319-66435-4\\_2", "10.1109/tvcg.2012.220", "10.1145/2598153.2602225", "10.1109/tvcg.2013.183", "10.1145/1314683.1314684", "10.1109/tvcg.2012.236", "10.1145/2810188.2810192", "10.1109/tvcg.2012.196", "10.1109/tvcg.2016.2599106", "10.1109/t-affc.2012.19", "10.1145/2556288.2557379", "10.1109/vast.2014.7042496", "10.1109/tvcg.2012.199", "10.1109/pacificvis.2017.8031598", "10.1145/2669557.2669564", "10.1145/2207676.2207709", "10.1007/978-3-540-70956-5\\_2", "10.1016/j.jebo.2013.03.003", "10.1111/cgf.13009", "10.1109/mcg.2017.23", "10.1109/vast.2011.6102445", "10.1007/978-3-319-66435-4\\_4", "10.1145/3025453.3025969", "10.1145/2858036.2858101", "10.1109/tvcg.2008.155", "10.1111/cgf.12888", "10.1145/2598153.2598182", "10.1109/tvcg.2013.119", "10.1109/tvcg.2014.2346978", "10.1007/978-3-319-66435-4\\_3", "10.1145/2858036.2858237", "10.1177/0272989x07304449", "10.1109/qomex.2012.6263866", "10.1145/2858036.2858558"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934397", "title": "A Comparative Evaluation of Animation and Small Multiples for Trend Visualization on Mobile Phones", "year": "2019", "conferenceName": "InfoVis", "authors": "Matthew Brehmer;Bongshin Lee;Petra Isenberg;Eun Kyoung Choe", "citationCount": "3", "affiliation": "Brehmer, M (Corresponding Author), Microsoft Res, Redmond, WA 98052 USA. Brehmer, Matthew; Lee, Bongshin, Microsoft Res, Redmond, WA 98052 USA. Isenberg, Petra, INRIA, Rocquencourt, France. Choe, Eun Kyoung, Univ Maryland, College Pk, MD 20742 USA.", "countries": "USA;France", "abstract": "We compare the efficacy of animated and small multiples variants of scatterplots on mobile phones for comparing trends in multivariate datasets. Visualization is increasingly prevalent in mobile applications and mobile-first websites, yet there is little prior visualization research dedicated to small displays. In this paper, we build upon previous experimental research carried out on larger displays that assessed animated and non-animated variants of scatterplots. Incorporating similar experimental stimuli and tasks, we conducted an experiment where 96 crowdworker participants performed nine trend comparison tasks using their mobile phones. We found that those using a small multiples design consistently completed tasks in less time, albeit with slightly less confidence than those using an animated design. The accuracy results were more task-dependent, and we further interpret our results according to the characteristics of the individual tasks, with a specific focus on the trajectories of target and distractor data items in each task. We identify cases that appear to favor either animation or small multiples, providing new questions for further experimental research and implications for visualization design on mobile devices. Lastly, we provide a reflection on our evaluation methodology.", "keywords": "Evaluation,graphical perception,mobile phones,trend visualization,animation,small multiples,crowdsourcing", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934397", "refList": ["10.1109/mcg.2015.113", "10.1111/cgf.12106", "10.1109/tvcg.2011.185", "10.1111/cgf.13444", "10.1145/2786567.2786571", "10.1109/tvcg.2013.254", "10.1007/978-3-319-68517-5\\_1", "10.1145/3123266.3123274", "10.1007/978-3-319-66435-4\\_5", "10.1109/tvcg.2018.2865234", "10.1111/j.1467-8659.2012.03093.x", "10.1145/1133265.1133364", "10.1145/2470654.2481318", "10.1145/2993901.2993906", "10.1145/3290605.3300786", "10.1109/tvcg.2016.2598876", "10.2307/2288400", "10.1038/nmeth.2659", "10.1109/tvcg.2018.2865142", "10.1167/7.13.14", "10.1109/tvcg.2008.125", "10.1145/3025453.3025752", "10.1109/tvcg.2015.2502587", "10.1109/tvcg.2010.78", "10.1145/2396636.2396675", "10.1007/978-3-319-26633-6\\_13", "10.1145/2858036.2858558", "10.1145/3290605.3300771"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030423", "title": "MobileVisFixer: Tailoring Web Visualizations for Mobile Phones Leveraging an Explainable Reinforcement Learning Framework", "year": "2020", "conferenceName": "InfoVis", "authors": "Aoyu Wu;Wai Tong;Tim Dwyer;Bongshin Lee;Petra Isenberg;Huamin Qu", "citationCount": "0", "affiliation": "Wu, AY (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wu, Aoyu; Tong, Wai; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Lee, Bongshin, Microsoft Res, Redmond, WA USA. Isenberg, Petra, INRIA, Le Chesnay Rocquencourt, France.", "countries": "USA;China;France;Australia", "abstract": "We contribute MobileVisFixer, a new method to make visualizations more mobile-friendly. Although mobile devices have become the primary means of accessing information on the web, many existing visualizations are not optimized for small screens and can lead to a frustrating user experience. Currently, practitioners and researchers have to engage in a tedious and time-consuming process to ensure that their designs scale to screens of different sizes, and existing toolkits and libraries provide little support in diagnosing and repairing issues. To address this challenge, MobileVisFixer automates a mobile-friendly visualization re-design process with a novel reinforcement learning framework. To inform the design of MobileVisFixer, we first collected and analyzed SVG-based visualizations on the web, and identified five common mobile-friendly issues. MobileVisFixer addresses four of these issues on single-view Cartesian visualizations with linear or discrete scales by a Markov Decision Process model that is both generalizable across various visualizations and fully explainable. MobileVisFixer deconstructs charts into declarative formats, and uses a greedy heuristic based on Policy Gradient methods to find solutions to this difficult, multi-criteria optimization problem in reasonable time. In addition, MobileVisFixer can be easily extended with the incorporation of optimization algorithms for data visualizations. Quantitative evaluation on two real-world datasets demonstrates the effectiveness and generalizability of our method.", "keywords": "Mobile visualization,Responsive visualization,Machine learning for visualizations,Reinforcement learning", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030423", "refList": ["10.1109/mcg.2014.82", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2012.196", "10.1145/3092703.3092726", "10.1109/pimrc.2015.7343276", "10.1109/tvcg.2014.48", "10.1145/3025453.3025957", "10.1145/2047196.2047247", "10.1145/3173574.3174168", "10.1109/icst.2019.00027", "10.1109/tvcg.2007.70594", "10.1145/3197517.3201311", "10.1145/3126594.3126653", "10.1109/mcg.2019.2924636", "10.1145/3180155.3180262", "10.1109/tvcg.2017.2659744", "10.1145/3123266.3123274", "10.1109/tvcg.2019.2934538", "10.1109/cvpr.2018.00592", "10.1109/tvcg.2018.2865138", "10.1109/tsmcc.2012.2218595", "10.1109/tvcg.2018.2865240", "10.1111/cgf.13193", "10.1145/2775441.2775478", "10.1145/3313831.3376777", "10.1109/mc.2006.109", "10.1111/cgf.13686", "10.1109/tvcg.2015.2467091", "10.4230/dagrep.9.7.78", "10.1007/978-3-319-71249-9\\_9", "10.1007/s00778-019-00588-3", "10.1109/ase.2015.31", "10.1007/bf02124742", "10.1109/tvcg.2019.2934397", "10.1109/adprl.2007.368196", "10.1109/mcg.2020.2968244", "10.1145/3092703.3092712", "10.1109/tvcg.2017.2744320", "10.1023/a:1017992615625", "10.1145/2642918", "10.1145/2858036.2858558", "10.1145/3290605.3300358", "10.1109/tvcg.2019.2934431", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030406", "title": "Palettailor: Discriminable Colorization for Categorical Data", "year": "2020", "conferenceName": "InfoVis", "authors": "Kecheng Lu;Mi Feng;Xin Chen;Michael Sedlmair;Oliver Deussen;Dani Lischinski;Zhanglin Cheng;Yunhai Wang", "citationCount": "0", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Jinan, Peoples R China. Cheng, ZL (Corresponding Author), SIAT, Shenzhen VesuCA Key Lab, Shenzhen, Peoples R China. Lu, Kecheng; Chen, Xin; Wang, Yunhai, Shandong Univ, Jinan, Peoples R China. Feng, Mi, Twitter Inc, San Francisco, CA USA. Lu, Kecheng; Deussen, Oliver; Cheng, Zhanglin, SIAT, Shenzhen VesuCA Key Lab, Shenzhen, Peoples R China. Sedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany. Deussen, Oliver, Konstanz Univ, Constance, Germany. Lischinski, Dani, Hebrew Univ Jerusalem, Jerusalem, Israel.", "countries": "USA;Israel;Germany;China", "abstract": "We present an integrated approach for creating and assigning color palettes to different visualizations such as multi-class scatterplots, line, and bar charts. While other methods separate the creation of colors from their assignment, our approach takes data characteristics into account to produce color palettes, which are then assigned in a way that fosters better visual discrimination of classes. To do so, we use a customized optimization based on simulated annealing to maximize the combination of three carefully designed color scoring functions: point distinctness, name difference, and color discrimination. We compare our approach to state-of-the-art palettes with a controlled user study for scatterplots and line charts, furthermore we performed a case study. Our results show that Palettailor, as a fully-automated approach, generates color palettes with a higher discrimination quality than existing approaches. The efficiency of our optimization allows us also to incorporate user modifications into the color selection process.", "keywords": "Color Palette,Discriminability,Multi-Class Scatterplot,Line Chart,Bar Chart", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030406", "refList": ["10.1109/mcg.2014.82", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2012.196", "10.1145/3092703.3092726", "10.1109/tvcg.2014.48", "10.1145/3025453.3025957", "10.1145/2047196.2047247", "10.1145/3173574.3174168", "10.1109/icst.2019.00027", "10.1109/tvcg.2007.70594", "10.1145/3197517.3201311", "10.1145/3126594.3126653", "10.1109/mcg.2019.2924636", "10.1145/3180155.3180262", "10.1109/tvcg.2017.2659744", "10.1145/3123266.3123274", "10.1109/tvcg.2018.2865234", "10.1109/tvcg.2019.2934538", "10.1109/cvpr.2018.00592", "10.1145/2815833.2816956", "10.1109/tvcg.2018.2865138", "10.1109/tsmcc.2012.2218595", "10.1109/tvcg.2018.2865240", "10.1111/cgf.13193", "10.1145/2775441.2775478", "10.1145/3313831.3376777", "10.1109/mc.2006.109", "10.1111/cgf.13686", "10.1109/tvcg.2015.2467091", "10.4230/dagrep.9.7.78", "10.1007/978-3-319-71249-9\\_9", "10.1007/s00778-019-00588-3", "10.1109/ase.2015.31", "10.1007/bf02124742", "10.1145/3025453.3025768", "10.1109/tvcg.2019.2934397", "10.1109/adprl.2007.368196", "10.1109/mcg.2020.2968244", "10.1145/3092703.3092712", "10.1109/tvcg.2017.2744320", "10.1023/a:1017992615625", "10.1145/2642918", "10.1145/2858036.2858558", "10.1145/3290605.3300358", "10.1109/tvcg.2019.2934431", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030385", "title": "Staged Animation Strategies for Online Dynamic Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Tarik Crnovrsanin;Shilpika;Senthil K. Chandrasegaran;Kwan-Liu Ma", "citationCount": "0", "affiliation": "Crnovrsanin, T (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Crnovrsanin, Tarik; Shilpika; Chandrasegaran, Senthil; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "Dynamic networks-networks that change over time-can be categorized into two types: offline dynamic networks, where all states of the network are known, and online dynamic networks, where only the past states of the network are known. Research on staging animated transitions in dynamic networks has focused more on offline data, where rendering strategies can take into account past and future states of the network. Rendering online dynamic networks is a more challenging problem since it requires a balance between timeliness for monitoring tasks-so that the animations do not lag too far behind the events-and clarity for comprehension tasks-to minimize simultaneous changes that may be difficult to follow. To illustrate the challenges placed by these requirements, we explore three strategies to stage animations for online dynamic networks: time-based, event-based, and a new hybrid approach that we introduce by combining the advantages of the first two. We illustrate the advantages and disadvantages of each strategy in representing low- and high-throughput data and conduct a user study involving monitoring and comprehension of dynamic networks. We also conduct a follow-up, think-aloud study combining monitoring and comprehension with experts in dynamic network visualization. Our findings show that animation staging strategies that emphasize comprehension do better for participant response times and accuracy. However, the notion of \u201ccomprehension\u201d is not always clear when it comes to complex changes in highly dynamic networks, requiring some iteration in staging that the hybrid approach affords. Based on our results, we make recommendations for balancing event-based and time-based parameters for our hybrid approach.", "keywords": "Dynamic networks,graph visualization,animation,mental map,user study", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030385", "refList": ["10.1109/infvis.2004.18", "10.1109/tvcg.2008.141", "10.1017/s0033291717001751", "10.1109/tvcg.2014.2346424", "10.1007/978-3-319-73915-1\\_31", "10.1145/2702123.27023123", "10.1111/cgf.12791", "10.1109/tvcg.2011.226", "10.1145/985692.985748", "10.1109/tvcg.2008.11", "10.1007/s10654-016-0149-3", "10.1109/tvcg.2011.185", "10.1145/1143518.1143521", "10.1109/tvcg.2011.213", "10.1109/tvcg.2013.254", "10.1006/ijhc.1017", "10.1109/tvcg.2011.169", "10.1111/j.1467-8659.2012.03093.x", "10.1016/j.ins.2015.04.017", "10.1057/ivs.2010.10", "10.1145/2576099", "10.1145/2858036.2858387", "10.1007/3-540-31190-4", "10.1109/tvcg.2013.238", "10.1093/aje/kwx259", "10.1145/3290607.3310432", "10.1109/visual.2019.8933748", "10.1109/tvcg", "10.1007/1155526124", "10.1145/1165734.1165736", "10.1080/00031305.2016.1154108", "10.1109/tvcg.2016.2592906", "10.1002/spe.4380211102", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2006.166", "10.1109/tvcg.2018.2886901", "10.1109/tvcg.2019.2934397", "10.1109/tvcg.2015.2467751", "10.1109/wsc.2003.1261490", "10.1109/tvcg.2010.78", "10.1109/mc.2018.2890217", "10.1007/3", "10.1038/sdata.2014.56", "10.1111/cgf.12804", "10.1111/j.1467-8659.2009.01687.x", "10.2312/eurovisshort.20141149", "10.1109/tvcg.2013.205"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030396", "title": "What Makes a Data-GIF Understandable?", "year": "2020", "conferenceName": "InfoVis", "authors": "Xinhuan Shu;Aoyu Wu;Junxiu Tang;Benjamin Bach;Yingcai Wu;Huamin Qu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Shu, Xinhuan; Tang, Junxiu; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Shu, Xinhuan; Wu, Aoyu; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Shu, Xinhuan, Zhejiang Univ, Hangzhou, Peoples R China. Bach, Benjamin, Univ Edinburgh, Edinburgh, Midlothian, Scotland.", "countries": "Scotland;China", "abstract": "GIFs are enjoying increasing popularity on social media as a format for data-driven storytelling with visualization; simple visual messages are embedded in short animations that usually last less than 15 seconds and are played in automatic repetition. In this paper, we ask the question, \u201cWhat makes a data-GIF understandable?\u201d While other storytelling formats such as data videos, infographics, or data comics are relatively well studied, we have little knowledge about the design factors and principles for \u201cdata-GIFs\u201d. To close this gap, we provide results from semi-structured interviews and an online study with a total of 118 participants investigating the impact of design decisions on the understandability of data-GIFs. The study and our consequent analysis are informed by a systematic review and structured design space of 108 data-GIFs that we found online. Our results show the impact of design dimensions from our design space such as animation encoding, context preservation, or repetition on viewers understanding of the GIF's core message. The paper concludes with a list of suggestions for creating more effective Data-GIFs.", "keywords": "Data-GIFs,Data-driven Storytelling,Evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030396", "refList": ["10.1109/tvcg.2016.2598647", "10.1016/j.visinf.2020.07.001", "10.1177/1473871615594652", "10.1109/tvcg.2014.2346424", "10.1111/cgf.13195", "10.1109/vlsicircuits18222.2020.9162811", "10.1109/tvcg.2018.2864909", "10.1109/tvcg.2018.2864903", "10.1145/2702123.2702431", "10.1016/j.visinf.2019.12.002", "10.1145/3274349", "10.1111/cgf.13444", "10.1145/3206505.3206552", "10.1145/3290605.3300280", "10.1109/jstqe.2020.3021589", "10.1145/2647868.2656408", "10.1006/ijhc.1017", "10.1145/3290605.3300335", "10.1007/s12650-020-00689-0", "10.1145/2818048.2819936", "10.1111/cgf.13325", "10.1145/2858036.2858387", "10.1145/3027063.3053139", "10.1145/3290605.3300474", "10.1109/tvcg.2016.2598920", "10.1109/tvcg.2018.2864899", "10.1145/3290605.3300483", "10.1145/3173574.3173612", "10.1145/2909132.2909255", "10.1109/tvcg.2016.2598620", "10.1016/j.learninstruc.2007.09.013", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2008.125", "10.1145/3173574.3173909", "10.1109/tvcg.2019.2934397", "10.1111/cgf.13709", "10.1109/tvcg.2013.234", "10.1109/tvcg.2019.2934401", "10.1145/2858036.2858532", "10.1016/j.visinf.2020.08.001", "10.1109/tvcg.2010.179", "10.1109/cicc48029.2020.9075900"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934784", "title": "A Comparison of Radial and Linear Charts for Visualizing Daily Patterns", "year": "2019", "conferenceName": "InfoVis", "authors": "Manuela Waldner;Alexandra Diehl;Denis Gracanin;Rainer Splechtna;Claudio Delrieux;Kresimir Matkovic", "citationCount": "0", "affiliation": "Waldner, M (Corresponding Author), TU Wien, Vienna, Austria. Waldner, Manuela, TU Wien, Vienna, Austria. Diehl, Alexandra, Univ Zurich, Zurich, Switzerland. Gracanin, Denis, Virginia Tech, Blacksburg, VA USA. Splechtna, Rainer; Matkovic, Kresimir, VRVis Res Ctr, Vienna, Austria. Delrieux, Claudio, Univ Nacl Sur, Elect \\& Comp Eng Dept, Bahia Blanca, Buenos Aires, Argentina.", "countries": "Argentina;Switzerland;USA;Austria", "abstract": "Radial charts are generally considered less effective than linear charts. Perhaps the only exception is in visualizing periodical time-dependent data, which is believed to be naturally supported by the radial layout. It has been demonstrated that the drawbacks of radial charts outweigh the benefits of this natural mapping. Visualization of daily patterns, as a special case, has not been systematically evaluated using radial charts. In contrast to yearly or weekly recurrent trends, the analysis of daily patterns on a radial chart may benefit from our trained skill on reading radial clocks that are ubiquitous in our culture. In a crowd-sourced experiment with 92 non-expert users, we evaluated the accuracy, efficiency, and subjective ratings of radial and linear charts for visualizing daily traffic accident patterns. We systematically compared juxtaposed 12-hours variants and single 24-hours variants for both layouts in four low-level tasks and one high-level interpretation task. Our results show that over all tasks, the most elementary 24-hours linear bar chart is most accurate and efficient and is also preferred by the users. This provides strong evidence for the use of linear layouts \u2013 even for visualizing periodical daily patterns.", "keywords": "Radial charts,time series series data,daily patterns,crowd-sourced experiment", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934784", "refList": ["10.1109/infvis.2000.885091", "10.1109/tvcg.2018.2865158", "10.1177/1473871611406623", "10.1111/cgf.13444", "10.1038/scientificamerican0384-128", "10.1109/tvcg.2018.2865234", "10.1109/tvcg.2015.2467771", "10.1109/tvcg.2018.2865077", "10.1109/tvcg.2013.184", "10.1109/tvcg.2014.2346320", "10.1145/2858036.2858300", "10.1109/tvcg.2017.2674958", "10.1109/iv.2013.12", "10.1145/2470654.2466443", "10.1007/s10763-012-9362-z", "10.1207/s15427625tcq1402\\_3", "10.2307/2288400", "10.1136/jnnp.64.5.588", "10.1109/tvcg.2014.2346426", "10.1109/infvis.1998.729557", "10.1007/bf03217308", "10.1145/506443.506505", "10.1109/infvis.2001.963273", "10.1111/j.1467-8659.2011.01947.x", "10.1109/tvcg.2013.234", "10.1109/38.974517", "10.2312/pe/eurovisshort/eurovisshort2012/097-101", "10.1145/1743546.1743567", "10.1109/tvcg.2010.162", "10.2307/2289447", "10.1145/2110192.2110202", "10.1109/tvcg.2009.23", "10.3102/10769986030004353", "10.1109/tvcg.2010.209"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3028891", "title": "A Structured Review of Data Management Technology for Interactive Visualization and Analysis", "year": "2020", "conferenceName": "InfoVis", "authors": "Leilani Battle;Carlos Scheidegger", "citationCount": "0", "affiliation": "Battle, L (Corresponding Author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA. Battle, Leilani, Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA. Scheidegger, Carlos, Univ Arizona, Dept Comp Sci, HDC Lab, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "In the last two decades, interactive visualization and analysis have become a central tool in data-driven decision making. Concurrently to the contributions in data visualization, research in data management has produced technology that directly benefits interactive analysis. Here, we contribute a systematic review of 30 years of work in this adjacent field, and highlight techniques and principles we believe to be underappreciated in visualization work. We structure our review along two axes. First, we use task taxonomies from the visualization literature to structure the space of interactions in usual systems. Second, we created a categorization of data management work that strikes a balance between specificity and generality. Concretely, we contribute a characterization of 131 research papers along these two axes. We find that five notions in data management venues fit interactive visualization systems well: materialized views, approximate query processing, user modeling and query prediction, muiti-query optimization, lineage techniques, and indexing techniques. In addition, we find a preponderance of work in materialized views and approximate query processing, most targeting a limited subset of the interaction tasks in the taxonomy we used. This suggests natural avenues of future research both in visualization and data management. Our categorization both changes how we visualization researchers design and build our systems, and highlights where future work is necessary.", "keywords": "", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028891", "refList": ["10.1109/tvcg.2012.233", "10.1016/s0022-5371(74)80015-0", "10.1109/tvcg.2017.2744359", "10.1037/0096-3445.136.4.623", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2012.196", "10.1037/a0029856", "10.1109/tvcg.2014.2346979", "10.1037/h0030300", "10.1109/tvcg.2016.2598918", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2018.2864909", "10.1111/cgf.13079", "10.3389/fpsyg.2012.00355", "10.1145/2858036.2858465", "10.1080/01621459.1989.10478821", "10.1037/0278-7393.24.3.732", "10.1109/tvcg.2011.127", "10.1145/2858036.2858063", "10.4249/scholarpedia.3325", "10.4324/9781410611949", "10.1111/cgf.13444", "10.1145/2993901.2993909", "10.1037/0033-295x.96.2.267", "10.1006/ijhc.1017", "10.1086/405615", "10.1109/tvcg.2019.2934801", "10.1038/17953", "10.1037/xhp0000314", "10.1109/tvcg.2019.2934400", "10.1145/2470654.2470723", "10.1037/0096-1523.16.2.332", "10.1167/16.5.11", "10.3758/s13423-016-1174-7", "10.3758/bf03207704", "10.1146/annurev.psych.55.090902.141415", "10.2307/2288400", "10.3758/bf03204258", "10.1109/tvcg.2011.279", "10.1109/vissoft.2014.36", "10.3758/s13423-011-0055-3", "10.1145/3025453.3025922", "10.1109/tvcg.2019.2934284", "10.3758/bf03210498", "10.3758/bf03200774", "10.2307/1419876", "10.1038/s41562-017-0058", "10.1109/tvcg.2010.237", "10.1109/pacificvis.2012.6183556", "10.1109/infvis.1997.636792", "10.1093/acprof:oso/9780198523192.003.0005", "10.1073/pnas.1117465109", "10.1109/tvcg.2013.234", "10.1038/nn.3655", "10.1111/cgf.12379", "10.1146/annurev-psych-010416-044232", "10.1111/cgf.13695", "10.1037/0033-295x.107.3.500", "10.1109/tvcg.2013.183", "10.1146/annurev.psych.53.100901.135125", "10.1037//0022-3514.79.6.995", "10.1559/152304003100010929", "10.1109/tvcg.2018.2865147", "10.1037/0096-1523.18.3.849", "10.1111/j.1467-8659.2009.01694.x", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030440", "title": "Context-aware Sampling of Large Networks via Graph Representation Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Zhiguang Zhou;Chen Shi;Xilong Shen;Lihong Cai;Haoxuan Wang;Yuhua Liu;Ying Zhao;Wei Chen", "citationCount": "0", "affiliation": "Zhao, Y (Corresponding Author), Cent South Univ, Changsha, Peoples R China. Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Zhou, Zhiguang; Shi, Chen; Shen, Xilong; Cai, Lihong; Wang, Haoxuan; Liu, Yuhua, Zhejiang Univ Finance \\& Econ, Sch Informat, Hangzhou, Peoples R China. Zhao, Ying, Cent South Univ, Changsha, Peoples R China. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China.", "countries": "China", "abstract": "Numerous sampling strategies have been proposed to simplify large-scale networks for highly readable visualizations. It is of great challenge to preserve contextual structures formed by nodes and edges with tight relationships in a sampled graph, because they are easily overlooked during the process of sampling due to their irregular distribution and immunity to scale. In this paper, a new graph sampling method is proposed oriented to the preservation of contextual structures. We first utilize a graph representation learning (GRL) model to transform nodes into vectors so that the contextual structures in a network can be effectively extracted and organized. Then, we propose a multi-objective blue noise sampling model to select a subset of nodes in the vectorized space to preserve contextual structures with the retention of relative data and cluster densities in addition to those features of significance, such as bridging nodes and graph connections. We also design a set of visual interfaces enabling users to interactively conduct context-aware sampling, visually compare results with various sampling strategies, and deeply explore large networks. Case studies and quantitative comparisons based on real-world datasets have demonstrated the effectiveness of our method in the abstraction and exploration of large networks.", "keywords": "Graph sampling,Graph representation learning,Blue noise sampling,Graph evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030440", "refList": ["10.1145/2491159.2491168", "10.1016/j.physa.2015.04.035", "10.1145/1830252.1830274", "10.1109/icdmw.2007.91", "10.1002/net.21834", "10.1109/tvcg.2018.2864503", "10.1016/j.cag.2018.01.010", "10.1109/icc.2016.7511156", "10.1111/cgf.13444", "10.1145/956750.956831", "10.1145/364099.364331", "10.1007/s00180-016-0663-5", "10.1109/tvcg.2013.223", "10.1007/s12650-018-0530-2", "10.1103/physreve.73.016102", "10.1109/access.2018.2870684", "10.1007/978-3-319-06793-3\\_1", "10.2312/vissym/eurovis05/239-246", "10.1016/j.ins.2015.02.014", "10.1145/2339530.2339723", "10.1109/icde.2015.7113345", "10.1109/tvcg.2011.233", "10.14778/2809974.2809980", "10.1109/glocom.2015.7417471", "10.1145/2578153.2578175", "10.1214/aoms/1177705148", "10.1109/tvcg.2008.130", "10.14232/actacyb.20.1.2011.6", "10.1504/ijitm.2019.099809", "10.1109/tvcg.2018.2865020", "10.1145/956750", "10.1002/cpe.4330060203", "10.1145/1150402.1150479", "10.1103/physreve.72.036118", "10.1109/tvcg.2017.2744098", "10.1145/2020408.2020512", "10.1142/s0129183114400075", "10.1109/jsac.2011.111005", "10.1016/j.camwa.2011.11.057", "10.1145/2470654.2466444", "10.1109/tvcg.2017.2674999", "10.1214/aos/1013203451", "10.1109/icdcsw.2011.34", "10.1016/j.physa.2013.11.015", "10.1145/1081870.1081893", "10.1109/tnet.2008.2001730", "10.1109/access.2016.2633485", "10.1145/1879141.1879192", "10.1371/journal.pone.0098679", "10.1126/science.220.4598.671", "10.1109/pacificvis.2015.7156355", "10.1088/1475-7516/2011/08/011", "10.1007/978-3-319-27261-0\\_41", "10.1111/cgf.13410", "10.1109/tvcg.2018.2865139", "10.1109/tvcg.2016.2598831", "10.1016/j.physa.2014.06.065"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030396", "title": "What Makes a Data-GIF Understandable?", "year": "2020", "conferenceName": "InfoVis", "authors": "Xinhuan Shu;Aoyu Wu;Junxiu Tang;Benjamin Bach;Yingcai Wu;Huamin Qu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Shu, Xinhuan; Tang, Junxiu; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Shu, Xinhuan; Wu, Aoyu; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Shu, Xinhuan, Zhejiang Univ, Hangzhou, Peoples R China. Bach, Benjamin, Univ Edinburgh, Edinburgh, Midlothian, Scotland.", "countries": "Scotland;China", "abstract": "GIFs are enjoying increasing popularity on social media as a format for data-driven storytelling with visualization; simple visual messages are embedded in short animations that usually last less than 15 seconds and are played in automatic repetition. In this paper, we ask the question, \u201cWhat makes a data-GIF understandable?\u201d While other storytelling formats such as data videos, infographics, or data comics are relatively well studied, we have little knowledge about the design factors and principles for \u201cdata-GIFs\u201d. To close this gap, we provide results from semi-structured interviews and an online study with a total of 118 participants investigating the impact of design decisions on the understandability of data-GIFs. The study and our consequent analysis are informed by a systematic review and structured design space of 108 data-GIFs that we found online. Our results show the impact of design dimensions from our design space such as animation encoding, context preservation, or repetition on viewers understanding of the GIF's core message. The paper concludes with a list of suggestions for creating more effective Data-GIFs.", "keywords": "Data-GIFs,Data-driven Storytelling,Evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030396", "refList": ["10.1109/tvcg.2016.2598647", "10.1016/j.visinf.2020.07.001", "10.1177/1473871615594652", "10.1109/tvcg.2014.2346424", "10.1111/cgf.13195", "10.1109/vlsicircuits18222.2020.9162811", "10.1109/tvcg.2018.2864909", "10.1109/tvcg.2018.2864903", "10.1145/2702123.2702431", "10.1016/j.visinf.2019.12.002", "10.1145/3274349", "10.1111/cgf.13444", "10.1145/3206505.3206552", "10.1145/3290605.3300280", "10.1109/jstqe.2020.3021589", "10.1145/2647868.2656408", "10.1006/ijhc.1017", "10.1145/3290605.3300335", "10.1007/s12650-020-00689-0", "10.1145/2818048.2819936", "10.1111/cgf.13325", "10.1145/2858036.2858387", "10.1145/3027063.3053139", "10.1145/3290605.3300474", "10.1109/tvcg.2016.2598920", "10.1109/tvcg.2018.2864899", "10.1145/3290605.3300483", "10.1145/3173574.3173612", "10.1145/2909132.2909255", "10.1109/tvcg.2016.2598620", "10.1016/j.learninstruc.2007.09.013", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2008.125", "10.1145/3173574.3173909", "10.1109/tvcg.2019.2934397", "10.1111/cgf.13709", "10.1109/tvcg.2013.234", "10.1109/tvcg.2019.2934401", "10.1145/2858036.2858532", "10.1016/j.visinf.2020.08.001", "10.1109/tvcg.2010.179", "10.1109/cicc48029.2020.9075900"], "wos": 1, "children": [], "len": 1}], "len": 21}, {"doi": "10.1111/cgf.13720", "year": "2019", "title": "Capture \\& Analysis of Active Reading Behaviors for Interactive Articles on the Web", "conferenceName": "EuroVis", "authors": "Matthew Conlen;Alex Kale;Jeffrey Heer", "citationCount": "1", "affiliation": "Conlen, M (Corresponding Author), Univ Washington, Paul G Allen Sch Comp Sci \\& Engn, Seattle, WA 98195 USA.\nConlen, Matthew; Kale, Alex; Heer, Jeffrey, Univ Washington, Paul G Allen Sch Comp Sci \\& Engn, Seattle, WA 98195 USA.", "countries": "USA", "abstract": "Journalists, educators, and technical writers are increasingly publishing interactive content on the web. However, popular analytics tools provide only coarse information about how readers interact with individual pages, and laboratory studies often fail to capture the variability of a real-world audience. We contribute extensions to the Idyll markup language to automate the detailed instrumentation of interactive articles and corresponding visual analysis tools for inspecting reader behavior at both micro- and macro-levels. We present three case studies of interactive articles that were instrumented, posted online, and promoted via social media to reach broad audiences, and share data from over 50,000 reader sessions. We demonstrate the use of our tools to characterize article-specific interaction patterns, compare behavior across desktop and mobile devices, and reveal reading patterns common across articles. Our contributed findings, tools, and corpus of behavioral data can help advance and inform more comprehensive studies of narrative visualization.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13720", "refList": ["10.1111/cgf.13195", "10.1145/2702123.2702452", "10.1371/journal.pone.0142444", "10.1145/2470654.2466442", "10.1002/wics.101", "10.1111/cgf.12392", "10.1109/tvcg.2017.2745958", "10.1007/978-3-642-25289-1\\_37", "10.1109/tvcg.2016.2598797", "10.1108/10662240410555306", "10.1080/21670811.2018.1488598", "10.1145/3025171.3025184", "10.1109/mcg.2015.99", "10.1109/tvcg.2016.2539960", "10.1109/tvcg.2013.119", "10.1016/s0079-7421(02)80005-6", "10.1109/isemc.2014.6899034", "10.1007/s11144-015-0848-x", "10.1145/3025453.3025866", "10.2307/469105", "10.1109/mc.2013.36", "10.1111/cgf.13208"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030418", "title": "CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization", "year": "2020", "conferenceName": "VAST", "authors": "Zijie J. Wang;Robert Turko;Omar Shaikh;Haekyu Park;Nilaksh Das;Fred Hohman;Minsuk Kahng;Duen Horng Chau", "citationCount": "0", "affiliation": "Wang, ZJ (Corresponding Author), Georgia Tech, Atlanta, GA 30332 USA. Wang, Zijie J.; Turko, Robert; Shaikh, Omar; Park, Haekyu; Das, Nilaksh; Hohman, Fred; Chau, Duen Horng (Polo), Georgia Tech, Atlanta, GA 30332 USA. Kahng, Minsuk, Oregon State Univ, Corvallis, OR 97331 USA.", "countries": "USA", "abstract": "Deep learning's great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN's structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern deep learning techniques.", "keywords": "Deep learning,machine learning,convolutional neural networks,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030418", "refList": ["10.1016/j.cag.2018.09.018", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2017.2744938", "10.1037/0022-0663.83.4.484", "10.1016/j.patcog.2017.10.013", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2011.185", "10.1016/s0360-1315(99)00023-8", "10.1080/07380569.2012.651422", "10.1006/s1045-926x(02)00027-7", "10.1145/1821996.1821997", "10.1109/vast.2018.8802509", "10.1109/vl.2000.874346", "10.1007/978-3-319-27857-5\\_77", "10.1145/1227504.1227384", "10.1038/nature14539", "10.1109/tvcg.2018.2816223", "10.23915/distill.00016", "10.1162/neco.1997.9.8.1735", "10.1109/tvcg.2016.2640960", "10.1006/ijhc.2000.0409", "10.1109/tvcg.2017.2744718", "10.1006/s1045-926x(02)00028-9", "10.1111/cgf.13720", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1145/782941.782998", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.13719", "year": "2019", "title": "Linking and Layout: Exploring the Integration of Text and Visualization in Storytelling", "conferenceName": "EuroVis", "authors": "Qiyu Zhi;Alvitta Ottley;Ronald A. Metoyer", "citationCount": "0", "affiliation": "Zhi, Q (Corresponding Author), Univ Notre Dame, Notre Dame, IN 46556 USA.\nZhi, Qiyu; Metoyer, Ronald, Univ Notre Dame, Notre Dame, IN 46556 USA.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.", "countries": "USA", "abstract": "Modern web technologies are enabling authors to create various forms of text visualization integration for storytelling. This integration may shape the stories' flow and thereby affect the reading experience. In this paper, we seek to understand two text visualization integration forms: (i) different text and visualization spatial arrangements (layout), namely, vertical and slideshow; and (ii) interactive linking of text and visualization (linking). Here, linking refers to a bidirectional interaction mode that explicitly highlights the explanatory visualization element when selecting narrative text and vice versa. Through a crowdsourced study with 180 participants, we measured the effect of layout and linking on the degree to which users engage with the story (user engagement), their understanding of the story content (comprehension), and their ability to recall the story information (recall). We found that participants performed significantly better in comprehension tasks with the slideshow layout. Participant recall was better with the slideshow layout under conditions with linking versus no linking. We also found that linking significantly increased user engagement. Additionally, linking and the slideshow layout were preferred by the participants. We also explored user reading behaviors with different conditions.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13719", "refList": ["10.1111/cgf.13195", "10.1145/2702123.2702452", "10.1109/tvcg.2014.2346575", "10.1145/3025453.3025870", "10.1145/3172944.3173007", "10.1109/tvcg.2011.185", "10.1145/381641.381653", "10.1111/cgf.12392", "10.1006/ijhc.2000.0418", "10.1145/2702123.2702248", "10.1145/2556288.2557241", "10.1109/infvis.2005.1532136", "10.1016/s0079-7421(02)80005-6", "10.1145/2556288.2557141", "10.1145/2470654.2481374", "10.1016/j.compedu.2012.07.011", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2015.2467531", "10.1109/tvcg.2013.191", "10.1109/mc.2013.36", "10.1002/asi.21229", "10.1109/tvcg.2013.234", "10.1109/tvcg.2010.179", "10.1109/tvcg.2011.255", "10.1145/2993901.2993903", "10.1111/cgf.13207", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13964", "year": "2020", "title": "Reading Traces: Scalable Exploration in Elastic Visualizations of Cultural Heritage Data", "conferenceName": "EuroVis", "authors": "Mark{-}Jan Bludau;Viktoria Br{\\\"{u}}ggemann;Anna Busch;Marian D{\\\"{o}}rk", "citationCount": "1", "affiliation": "Bludau, MJ (Corresponding Author), Univ Appl Sci Potsdam, UCLAB, Potsdam, Germany.\nBludau, M. -J.; Brueggemann, V.; Doerk, M., Univ Appl Sci Potsdam, UCLAB, Potsdam, Germany.\nBusch, A., Univ Potsdam, Theodor Fontane Archiv, Potsdam, Germany.", "countries": "Germany", "abstract": "Through a design study, we develop an approach to data exploration that utilizes elastic visualizations designed to support varying degrees of detail and abstraction. Examining the notions of scalability and elasticity in interactive visualizations, we introduce a visualization of personal reading traces such as marginalia or markings inside the reference library of German realist author Theodor Fontane. To explore such a rich and extensive collection, meaningful visual forms of abstraction and detail are as important as the transitions between those states. Following a growing research interest in the role of fluid interactivity and animations between views, we are particularly interested in the potential of carefully designed transitions and consistent representations across scales. The resulting prototype addresses humanistic research questions about the interplay of distant and close reading with visualization research on continuous navigation along several granularity levels, using scrolling as one of the main interaction mechanisms. In addition to presenting the design process and resulting prototype, we present findings from a qualitative evaluation of the tool, which suggest that bridging between distant and close views can enhance exploration, but that transitions between views need to be crafted very carefully to facilitate comprehension.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13964", "refList": ["10.1007/s41244-017-0048-4", "10.1109/tvcg.2009.108", "10.1145/1456650.1456652", "10.1109/tvcg.2014.2346424", "10.1177/1473871611416549", "10.1111/cgf.13195", "10.1145/2207676.2208607", "10.1145/1376616.1376618", "10.1006/ijhc.2002.1017", "10.1109/tvcg.2011.185", "10.1177/1473871611413180", "10.1109/vl.1996.545307", "10.1006/ijhc.1017", "10.1109/tvcg.2019.2934539", "10.1109/tvcg.2018.2830759", "10.1145/1556262.1556300", "10.1109/tvcg.2014.2346677", "10.1145/2909132.2909255", "10.1109/tvcg.2007.70539", "10.1145/2396636.2396675", "10.1145/1978942.1979124", "10.1016/j.ijhcs.2003.08.005", "10.2312/eurovisstar.20151113", "10.1109/infvis.2005.1532127"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13980", "year": "2020", "title": "Structure and Empathy in Visual Data Storytelling: Evaluating their Influence on Attitude", "conferenceName": "EuroVis", "authors": "Johannes Liem;Charles Perin;Jo Wood", "citationCount": "0", "affiliation": "Liem, J (Corresponding Author), City Univ London, London, England.\nLiem, J.; Wood, J., City Univ London, London, England.\nPerin, C., Univ Victoria, Victoria, BC, Canada.", "countries": "Canada;England", "abstract": "In the visualization community, it is often assumed that visual data storytelling increases memorability and engagement, making it more effective at communicating information. However, many assumptions about the efficacy of storytelling in visualization lack empirical evaluation. Contributing to an emerging body of work, we study whether selected techniques commonly used in visual data storytelling influence people's attitudes towards immigration. We compare (a) personal visual narratives designed to generate empathy; (b) structured visual narratives of aggregates of people; and (c) an exploratory visualization without narrative acting as a control condition. We conducted two crowdsourced between-subject studies comparing the three conditions, each with 300 participants. To assess the differences in attitudes between conditions, we adopted established scales from the social sciences used in the European Social Survey (ESS). Although we found some differences between conditions, the effects on people's attitudes are smaller than we expected. Our findings suggest that we need to be more careful when it comes to our expectations about the effects visual data storytelling can have on attitudes. Additional material: https://flowstory.github.io/attitudes/.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13980", "refList": ["10.1109/mcg.2011.23", "10.1109/tvcg.2016.2598647", "10.1109/tvcg.2015.2467732", "10.1111/cgf.13195", "10.1145/2702123.2702452", "10.1145/2702123.2702431", "10.1080/00087041.2017.1304498", "10.4135/9781849209458", "10.4135/9781849209458.n9", "10.1145/3025453.3025870", "10.1145/3206505.3206552", "10.1145/381641.381653", "10.1109/tvcg.2014.2346419", "10.1145/3313831.3376887", "10.1109/iv.2014.79", "10.1002/9781119055259", "10.1080/15230406.2016.1262280", "10.1179/1743277412y.0000000032", "10.1093/migration/mnt015", "10.1109/mcg.2015.99", "10.1109/tvcg.2013.119", "10.1145/3025453.3025512", "10.1080/1369183x.2012.667985", "10.1109/mcg.2017.33", "10.1038/nmeth.2659", "10.1016/j.earscirev.2012.09.005", "10.1109/iv.2014.78", "10.1109/beliv.2018.8634072", "10.1145/1753846.1754024", "10.1016/j.jebo.2009.04.018", "10.1080/00031305.2017.1322143", "10.1002/sim.5713", "10.1007/s11577-014-0274-5", "10.1109/tvcg.2013.234", "10.1109/tvcg.2010.179", "10.1075/idj.23.1.07thu", "10.1093/esr/jcn020", "10.1109/tvcg.2011.255", "10.14714/cp84.1374", "10.1145/3173574.3174012", "10.1109/mcg.2012.24", "10.1109/tvcg.2018.2864836", "10.1016/0362-3319(94)90026-4", "10.1109/tvcg.2016.2614803"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13974", "year": "2020", "title": "Understanding the Design Space and Authoring Paradigms for Animated Data Graphics", "conferenceName": "EuroVis", "authors": "John Thompson;Zhicheng Liu;Wilmot Li;John T. Stasko", "citationCount": "0", "affiliation": "Thompson, J (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA.\nThompson, J.; Stasko, J., Georgia Inst Technol, Atlanta, GA 30332 USA.\nLiu, Z.; Li, W., Adobe Res, Seattle, WA USA.", "countries": "USA", "abstract": "Creating expressive animated data graphics often requires designers to possess highly specialized programming skills. Alternatively, the use of direct manipulation tools is popular among animation designers, but these tools have limited support for generating graphics driven by data. Our goal is to inform the design of next-generation animated data graphic authoring tools. To understand the composition of animated data graphics, we survey real-world examples and contribute a description of the design space. We characterize animated transitions based on object, graphic, data, and timing dimensions. We synthesize the primitives from the object, graphic, and data dimensions as a set of 10 transition types, and describe how timing primitives compose broader pacing techniques. We then conduct an ideation study that uncovers how people approach animation creation with three authoring paradigms: keyframe animation, procedural animation, and presets \\& templates. Our analysis shows that designers have an overall preference for keyframe animation. However, we find evidence that an authoring tool should combine these three paradigms as designers' preferences depend on the characteristics of the animated transition design and the authoring task. Based on these findings, we contribute guidelines and design considerations for developing future animated data graphic authoring tools.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13974", "refList": ["10.1145/168642.168648.1", "10.1109/tvcg.2016.2598647", "10.1007/978-0-387-98141-3\\_1", "10.1109/tvcg.2014.2346424", "10.1111/cgf.13195", "10.1145/2702123.2702431", "10.2307/2529310", "10.1109/tvcg.2011.185", "10.1145/2556288.2556987", "10.1177/001316446002000104", "10.1145/2897586.2897597", "10.1109/tvcg.2015.2467271", "10.1007/s00146-006-0050-9", "10.1145/2909132.2909255", "10.1145/3277893.3277896", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2007.70415", "10.1145/168642.168648", "10.1109/chase.2016.020", "10.1111/cgf.12635", "10.1145/2642918"], "wos": 1, "children": [], "len": 1}], "len": 47}, "index": 1413, "embedding": [4.802605152130127, 3.004760265350342, -1.0222129821777344, 0.2137279510498047, -0.630656361579895, 0.16650904715061188, -0.7150075435638428, 5.3174147605896, 0.7833898067474365, 3.4383654594421387, 2.9006409645080566, 2.7143797874450684, 2.5982956886291504, 4.487164497375488, -0.7281984686851501, 3.9169921875, 2.3301291465759277, 2.7946760654449463, 0.014919859357178211, 8.219093322753906, -0.06630208343267441, 2.615870714187622, 0.12157320231199265, 4.658920764923096, -2.458800792694092, 3.6920790672302246, -0.5019459128379822, 2.1498560905456543, 5.0984272956848145, 0.48967015743255615, 2.190389633178711, 3.6477229595184326], "projection": [0.8523048758506775, 10.024272918701172], "size": 24, "height": 4, "width": 10}