{"data": {"doi": "10.1109/tvcg.2018.2865145", "title": "Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication", "year": "2018", "conferenceName": "InfoVis", "authors": "Arjun Srinivasan;Steven Mark Drucker;Alex Endert;John T. Stasko", "citationCount": "12", "affiliation": "Srinivasan, A (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA. Srinivasan, Arjun; Endert, Alex; Stasko, John, Georgia Inst Technol, Atlanta, GA 30332 USA. Drucker, Steven M., Microsoft Res, Washington, DC USA.", "countries": "USA", "abstract": "Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.", "keywords": "Natural Language Generation,Mixed-initiative Interaction,Visualization Recommendation,Data-driven Communication", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865145", "refList": ["10.2307/1269768", "10.1109/tvcg.2017.2744843", "10.1007/s00371-015-1132-9", "10.1145/3172944.3173007", "10.1109/tvcg.2007.70594", "10.1109/visual.1990.146375", "10.1145/1502650.1502695", "10.1145/108360.108361", "10.14778/2733004.2733035", "10.1109/pacificvis.2017.8031599", "10.1109/tvcg.2013.124", "10.1145/2556288.2557241", "10.1109/visual.1992.235203", "10.1109/infvis.2005.1532136", "10.1109/tvcg.2017.2745219", "10.1109/tvcg.2013.119", "10.1145/2984511.2984588", "10.1109/tvcg.2012.229", "10.1145/3025453.3025866", "10.1145/3035918.3035922", "10.1145/2807442.2807478", "10.1109/tvcg.2010.164", "10.1145/302979.303030", "10.1109/mcg.2006.70", "10.1109/mc.2013.36", "10.1109/tvcg.2015.2467191", "10.1017/s1351324997001502", "10.1111/cgf.13207", "10.1109/mcg.2009.22", "10.1145/2702123.2702608"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934669", "title": "Exploranative Code Quality Documents", "year": "2019", "conferenceName": "VAST", "authors": "Haris Mumtaz;Shahid Latif;Fabian Beck;Daniel Weiskopf", "citationCount": "0", "affiliation": "Mumtaz, H (Corresponding Author), Univ Stuttgart, VISUS, Stuttgart, Germany. Mumtaz, Haris; Weiskopf, Daniel, Univ Stuttgart, VISUS, Stuttgart, Germany. Latif, Shahid; Beck, Fabian, Univ Duisburg Essen, Paluno, Duisburg, Germany.", "countries": "Germany", "abstract": "Good code quality is a prerequisite for efficiently developing maintainable software. In this paper, we present a novel approach to generate exploranative (explanatory and exploratory) data-driven documents that report code quality in an interactive, exploratory environment. We employ a template-based natural language generation method to create textual explanations about the code quality, dependent on data from software metrics. The interactive document is enriched by different kinds of visualization, including parallel coordinates plots and scatterplots for data exploration and graphics embedded into text. We devise an interaction model that allows users to explore code quality with consistent linking between text and visualizations; through integrated explanatory text, users are taught background knowledge about code quality aspects. Our approach to interactive documents was developed in a design study process that included software engineering and visual analytics experts. Although the solution is specific to the software engineering scenario, we discuss how the concept could generalize to multivariate data and report lessons learned in a broader scope.", "keywords": "Code quality,interactive documents,natural language generation,sparklines", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934669", "refList": ["10.1109/tvcg.2014.2346435", "10.1109/tvcg.2018.2865022", "10.1016/j.jvlc.2018.10.001", "10.1002/smr.521", "10.1109/tvcg.2006.69", "10.1016/0004-3702(93)90022-4", "10.1145/3173574.3174106", "10.1007/s10648-010-9136-5", "10.3115/974557.974594", "10.1109/vl.1996.545307", "10.1109/tse.1976.233837", "10.1109/vissoft.2018.00010", "10.2312/vissym/vissym04/261-266", "10.1046/j.1365-2575.2002.00117.x", "10.1109/wcre.2002.1173068", "10.1109/icpc.2013.6613830", "10.1145/1985362.1985365", "10.3115/v1/w14-4401", "10.1007/s00766-007-0054-0", "10.1109/vissoft.2017.11", "10.1007/s10515-011-0098-8", "10.1109/vissof.2011", "10.1109/tvcg.2017.2674958", "10.1002/smr.404", "10.1109/32.979986", "10.1016/b978-0-12-397174-6.00010-6", "10.1145/3242587.3242617", "10.1109/mcg.2018.032421649", "10.1613/jair.5477", "10.1109/tfuzz.2014.2328011", "10.1145/2095654.2095665", "10.1109/icpc.2013.6613834", "10.1109/tvcg.2018.2865145", "10.1145/2597008.2597149", "10.1109/live.2013.6617345", "10.1145/1985793.1985868", "10.1002/int.21835", "10.1145/3139295.3139312", "10.1109/32.295895", "10.1109/scam.2014.14", "10.5281/zenodo.3336019", "10.1016/j.visinf.2019.03.004", "10.1109/aswec.2010.18", "10.2312/eurovisshort.20181084"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030403", "title": "Calliope: Automatic Visual Data Story Generation from a Spreadsheet", "year": "2020", "conferenceName": "InfoVis", "authors": "Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi 0007;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, Intelligent Big Data Visualizat Lab, Shanghai, Peoples R China. Shi, Danqing; Xu, Xinyue; Sun, Fuling; Shi, Yang; Cao, Nan, Tongji Univ, Intelligent Big Data Visualizat Lab, Shanghai, Peoples R China.", "countries": "China", "abstract": "Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.", "keywords": "Information Visualization,Visual Storytelling,Data Story", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030403", "refList": ["10.1007/s11063-017-9759-3", "10.1109/tvcg.2016.2598647", "10.1162/0891201054223977", "10.1109/tvcg.2015.2467732", "10.3390/info9030065", "10.1109/tvcg.2019.2934398", "10.1016/j.visinf.2018.12.001", "10.1145/2002353.2002355", "10.1109/tvcg.2007.70594", "10.1111/cgf.12392", "10.14778/2831360.2831371", "10.1093/biomet/33.3.239", "10.1109/mcg.2019.2924636", "10.1109/tvcg.2017.2659744", "10.1109/pacificvis.2009.4906837", "10.1109/pacificvis.2017.8031599", "10.4103/1755-6783.179101", "10.1145/2362394.2362398", "10.1111/cgf.12925", "10.1109/icde.2018.00019", "10.1109/tvcg.2018.2865240", "10.1109/mcg.2015.99", "10.1109/vds48975.2019.8973383", "10.1038/nature16961", "10.1109/tciaig.2012.2186810", "10.1145/3035918.3035922", "10.1145/3197517.3201362", "10.1109/tvcg.2019.2934281", "10.1109/tvcg.2016.2598620", "10.1155/2019/8480905", "10.1109/iccchina.2013.6671183", "10.1109/tvcg.2018.2865145", "10.1145/3299869.3314037", "10.1017/s1351324907004664", "10.1109/tvcg.2019.2934785", "10.1177/1473871618806555", "10.1613/jair.2989", "10.1109/tvcg.2010.179", "10.1145/3303766"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.1043", "year": "2020", "title": "AutoCaption: An Approach to Generate Natural Language Description from Visualization Automatically", "conferenceName": "PacificVis", "authors": "Can Liu;Liwenhan Xie;Yun Han;Datong Wei;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.\nYuan, XR (Corresponding Author), Peking Univ, Sch EECS, Beijing, Peoples R China.\nYuan, XR (Corresponding Author), Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China.\nLiu, Can; Xie, Liwenhan; Han, Yun; Wei, Datong; Yuan, Xiaoru, Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.\nLiu, Can; Xie, Liwenhan; Han, Yun; Wei, Datong; Yuan, Xiaoru, Peking Univ, Sch EECS, Beijing, Peoples R China.\nYuan, Xiaoru, Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China.", "countries": "China", "abstract": "In this paper, we propose a novel approach to generate captions for visualization charts automatically. In the proposed method, visual marks and visual channels, together with the associated text information in the original charts, are first extracted and identified with a multilayer perceptron classifier. Meanwhile, data information can also be retrieved by parsing visual marks with extracted mapping relationships. Then a 1-D convolutional residual network is employed to analyze the relationship between visual elements, and recognize significant features of the visualization charts, with both data and visual information as input. In the final step, the full description of the visual charts can be generated through a template-based approach. The generated captions can effectively cover the main visual features of the visual charts and support major feature types in commons charts. We further demonstrate the effectiveness of our approach through several cases.", "keywords": "", "link": "https://doi.org/10.1109/PacificVis48177.2020.1043", "refList": ["10.1145/1414471.1414525", "10.1111/cgf.13193", "10.1109/tvcg.2018.2865145", "10.1109/is.2002.1044219", "10.1145/2470654.2481374", "10.1080/13614568.2010.534186", "10.1017/s1351324997001502", "10.1145/3035918.3035922", "10.1109/cvpr.2016.90", "10.1145/2047196.2047247", "10.1007/s11257-006-9002-9", "10.1109/cvpr.2015.7298935", "10.1145/1148170.1148270", "10.1109/72.279181", "10.1109/icsmc.2011.6084067"], "wos": 1, "children": [], "len": 1}], "len": 9}, "index": 827, "embedding": [2.2242534160614014, 0.7694656848907471, 0.48594731092453003, 3.472853660583496, -0.6745967864990234, 0.16650904715061188, -0.700136661529541, 0.37319251894950867, -0.43282294273376465, -0.47443535923957825, 0.796142578125, 0.3199712336063385, -0.13324885070323944, 1.6677789688110352, -0.678647518157959, 2.125734806060791, 0.21378953754901886, 1.312252163887024, -0.650787889957428, 4.443755149841309, -0.06630208343267441, 1.1341187953948975, 1.324524164199829, 0.8151678442955017, 0.20582416653633118, 1.2566741704940796, -0.5500518083572388, -0.16255053877830505, 1.7107207775115967, 0.2904123365879059, 0.3567606806755066, 1.0996607542037964], "projection": [-2.144073486328125, 7.451296329498291], "size": 5, "height": 2, "width": 4}