{"data": {"doi": "10.1109/tvcg.2019.2934656", "title": "EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos", "year": "2019", "conferenceName": "VAST", "authors": "Haipeng Zeng;Xingbo Wang;Aoyu Wu;Yong Wang;Quan Li;Alex Endert;Huamin Qu", "citationCount": "0", "affiliation": "Wang, Y (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Haipeng; Wang, Xingbo; Wu, Aoyu; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Li, Quan, WeBank, AI Grp, Shenzhen, Peoples R China. Endert, Alex, Georgia Inst Technol, Atlanta, GA 30332 USA.", "countries": "USA;China", "abstract": "Emotions play a key role in human communication and public presentations. Human emotions are usually expressed through multiple modalities. Therefore, exploring multimodal emotions and their coherence is of great value for understanding emotional expressions in presentations and improving presentation skills. However, manually watching and studying presentation videos is often tedious and time-consuming. There is a lack of tool support to help conduct an efficient and in-depth multi-level analysis. Thus, in this paper, we introduce EmoCo, an interactive visual analytics system to facilitate efficient analysis of emotion coherence across facial, text, and audio modalities in presentation videos. Our visualization system features a channel coherence view and a sentence clustering view that together enable users to obtain a quick overview of emotion coherence and its temporal evolution. In addition, a detail view and word view enable detailed exploration and comparison from the sentence level and word level, respectively. We thoroughly evaluate the proposed system and visualization techniques through two usage scenarios based on TED Talk videos and interviews with two domain experts. The results demonstrate the effectiveness of our system in gaining insights into emotion coherence in presentations.", "keywords": "Emotion,coherence,video analysis,visual analysis", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934656", "refList": ["10.1145/2642918.2647366", "10.1177/1754073912457228", "10.1080/01973533.2010.495659", "10.1016/j.neuropsychologia.2016.07.038", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2745181", "10.1007/978-3-642-14715-9\\_15", "10.1177/1059712316664017", "10.1371/j0urnal.p0ne.0196391", "10.1111/j.1530-9290.2008.00004.x", "10.1109/vl.1996.545307", "10.1109/event.2001.938870", "10.1177/1754073912457229", "10.1007/s13735-012-0016-2", "10.1109/tmm.2016.2614184", "10.1109/tvcg.2010.183", "10.1511/2001.4.344", "10.1145/3025453.3025821", "10.1109/vast.2009.5333919", "10.1016/j.inffus.2017.02.003", "10.1109/tmm.2016.2614224", "10.1109/t-affc.2011.37", "10.1109/mis.2016.94", "10.1109/tvcg.2013.168", "10.1109/t-affc.2011.8", "10.1145/3219819.3219853", "10.1109/jstsp.2017.2764438", "10.1016/j.neunet.2014.10.005", "10.1007/s11042-010-0645-5", "10.1111/j.1467-8659.2011.01939.x", "10.1145/3218585.3224223", "10.1145/2818346.2820765", "10.1126/science.1224313", "10.1016/j.neuroimage.2010.10.047", "10.1109/tvcg.2015.2467851", "10.1109/tmm.2013.2238521"], "wos": 1, "children": [], "len": 1}, "index": 938, "embedding": [0.06700067222118378, 0.3471166789531708, -1.0366979837417603, -0.4105203449726105, -0.5557070970535278, 0.16650904715061188, 0.19272492825984955, -0.19396813213825226, -0.4727596938610077, 0.32127636671066284, -0.5543903708457947, -0.45955395698547363, -0.15391238033771515, 0.5638307332992554, -0.9044764637947083, 1.1864277124404907, -0.24321970343589783, 1.8627097606658936, -0.7390042543411255, 1.2086089849472046, -0.06630208343267441, 0.4195092022418976, 0.12235184013843536, 0.8474600911140442, -1.1886526346206665, 0.9430093169212341, -0.561912477016449, -0.2069312334060669, 0.8167466521263123, -1.550209641456604, -0.9279162287712097, 0.15728986263275146], "projection": [-0.3133224546909332, 7.500406742095947], "size": 1, "height": 1, "width": 1}