{"data": {"doi": "10.1109/tvcg.2018.2864899", "title": "iStoryline: Effective Convergence to Hand-drawn Storylines", "year": "2018", "conferenceName": "InfoVis", "authors": "Tan Tang;Sadia Rubab;Jiewen Lai;Weiwei Cui;Lingyun Yu;Yingcai Wu", "citationCount": "4", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ \\& Alibaba Zhejiang Univ Joint Inst, State Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Tang, Tan; Rubab, Sadia; Lai, Jiewen; Wu, Yingcai, Zhejiang Univ \\& Alibaba Zhejiang Univ Joint Inst, State Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Cui, Weiwei, Microsoft Res, Redmond, WA USA. Yu, Lingyun, Univ Groningen, Bernoulli Inst, Groningen, Netherlands.", "countries": "USA;China;Netherlands", "abstract": "Storyline visualization techniques have progressed significantly to generate illustrations of complex stories automatically. However, the visual layouts of storylines are not enhanced accordingly despite the improvement in the performance and extension of its application area. Existing methods attempt to achieve several shared optimization goals, such as reducing empty space and minimizing line crossings and wiggles. However, these goals do not always produce optimal results when compared to hand-drawn storylines. We conducted a preliminary study to learn how users translate a narrative into a hand-drawn storyline and check whether the visual elements in hand-drawn illustrations can be mapped back to appropriate narrative contexts. We also compared the hand-drawn storylines with storylines generated by the state-of-the-art methods and found they have significant differences. Our findings led to a design space that summarizes (1) how artists utilize narrative elements and (2) the sequence of actions artists follow to portray expressive and attractive storylines. We developed iStoryline, an authoring tool for integrating high-level user interactions into optimization algorithms and achieving a balance between hand-drawn storylines and automatic layouts. iStoryline allows users to create novel storyline visualizations easily according to their preferences by modifying the automatically generated layouts. The effectiveness and usability of iStoryline are studied with qualitative evaluations.", "keywords": "Hand-drawn illustrations,automatic layout,design space,interactions,optimization", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864899", "refList": ["10.1109/tvcg.2015.2509990", "10.1109/tvcg.2016.2598647", "10.1109/mcg.2016.7", "10.1109/tvcg.2017.2745878", "10.1145/3132272.3132299", "10.1109/tvcg.2015.2392771", "10.1177/1529100614525555", "10.1109/tvcg.2012.212", "10.1109/tvcg.2015.2467451", "10.3102/0013189x09353787", "10.1109/cscwd.2014.6846815", "10.1109/tmm.2016.2614221", "10.1109/tvcg.2009.109", "10.1109/icse.2012.6227086", "10.1145/2598784.2598806", "10.1109/tvcg.2017.2743990", "10.1145/2598510.2598566", "10.1109/tvcg.2013.196", "10.1109/tvcg.2016.2598620", "10.1111/j.1467-8659.2011.01955.x", "10.1109/tvcg.2015.2467531", "10.1109/tvcg.2014.2346291", "10.1109/tvcg.2013.191", "10.1145/1842993.1843035", "10.1145/568522.568523", "10.1109/tvcg.2014.2346913", "10.1109/tvcg.2016.2598831", "10.1109/tvcg.2016.2614803"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030461", "title": "CNNPruner: Pruning Convolutional Neural Networks with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Guan Li;Junpeng Wang;Han-Wei Shen;Kaixin Chen;Guihua Shan;Zhonghua Lu", "citationCount": "0", "affiliation": "Shan, GH (Corresponding Author), Chinese Acad Sci, Comp Network Informat Ctr, Beijing, Peoples R China. Li, Guan; Chen, Kaixin; Shan, Guihua; Lu, Zhonghua, Chinese Acad Sci, Comp Network Informat Ctr, Beijing, Peoples R China. Li, Guan; Chen, Kaixin; Shan, Guihua; Lu, Zhonghua, Univ Chinese Acad Sci, Beijing, Peoples R China. Wang, Junpeng, Visa Res, Beijing, Peoples R China. Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA;China", "abstract": "Convolutional neural networks (CNNs) have demonstrated extraordinarily good performance in many computer vision tasks. The increasing size of CNN models, however, prevents them from being widely deployed to devices with limited computational resources, e.g., mobile/embedded devices. The emerging topic of model pruning strives to address this problem by removing less important neurons and fine-tuning the pruned networks to minimize the accuracy loss. Nevertheless, existing automated pruning solutions often rely on a numerical threshold of the pruning criteria, lacking the flexibility to optimally balance the trade-off between efficiency and accuracy. Moreover, the complicated interplay between the stages of neuron pruning and model fine-tuning makes this process opaque, and therefore becomes difficult to optimize. In this paper, we address these challenges through a visual analytics approach, named CNNPruner. It considers the importance of convolutional filters through both instability and sensitivity, and allows users to interactively create pruning plans according to a desired goal on model size or accuracy. Also, CNNPruner integrates state-of-the-art filter visualization techniques to help users understand the roles that different filters played and refine their pruning plans. Through comprehensive case studies on CNNs with real-world sizes, we validate the effectiveness of CNNPruner.", "keywords": "visualization,model pruning,convolutional neural network,explainable artificial intelligence", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030461", "refList": ["10.1109/tvcg.2015.2509990", "10.1145/2468356.2468434", "10.1007/978-3-642-18469-7\\_22", "10.1007/bf02603120", "10.1145/2702123.2702419", "10.1109/pacificvis.2018.00025", "10.1145/345513.345271", "10.1109/32.221135", "10.1007/s10654-016-0149-3", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.89", "10.1109/infvis.2005.1532152", "10.1007/3-540-49381-6\\_9", "10.1109/gl0bec0m38437.2019.9014197", "10.2337/dc18-s006", "10.11234/gi1990.5.90", "10.1007/bf01187020", "10.1016/j.jbi.2015.06.020", "10.1109/pacificvis.2013.6596125", "10.1109/tvcg.2015.2468151", "10.1109/tvcg.2013.124", "10.1111/cgf.13988", "10.1109/pacificvis.2011.5742371", "10.1109/tvcg.2016.2539960", "10.1109/tvcg.2013.200", "10.1109/tsmc.1977.4309760", "10.1109/tvcg.2012.225", "10.1109/tvcg.2015.2465151", "10.1186/1753-6561-8-s2-s8", "10.1109/tvcg.2018.2864899", "10.1109/tits.2015.2436897", "10.1109/tvcg.2013.196", "10.1214/ss/1177013815", "10.1111/j.2517-6161.1995.tb02031.x", "10.2144/00286ir01", "10.1109/tsmc.1981.4308636", "10.1145/3025453.3026024", "10.2337/dc14-2919", "10.2307/2683905", "10.3238/arztebl.2009.0335", "10.1109/vast.2011.6102453"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030367", "title": "Lyra 2: Designing Interactive Visualizations by Demonstration", "year": "2020", "conferenceName": "InfoVis", "authors": "Jonathan Zong;Dhiraj Barnwal;Rupayan Neogy;Arvind Satyanarayan", "citationCount": "0", "affiliation": "Zong, J (Corresponding Author), MIT, Cambridge, MA 02139 USA. Zong, Jonathan; Neogy, Rupayan; Satyanarayan, Arvind, MIT, Cambridge, MA 02139 USA. Barnwal, Dhiraj, Indian Inst Technol Kharagpur, Kharagpur, W Bengal, India.", "countries": "India;USA", "abstract": "Recent graphical interfaces offer direct manipulation mechanisms for authoring visualizations, but are largely restricted to static output. To author interactive visualizations, users must instead turn to textual specification, but such approaches impose a higher technical burden. To bridge this gap, we introduce Lyra 2, a system that extends a prior visualization design environment with novel methods for authoring interaction techniques by demonstration. Users perform an interaction (e.g., button clicks, drags, or key presses) directly on the visualization they are editing. The system interprets this performance using a set of heuristics and enumerates suggestions of possible interaction designs. These heuristics account for the properties of the interaction (e.g., target and event type) as well as the visualization (e.g., mark and scale types, and multiple views). Interaction design suggestions are displayed as thumbnails; users can preview and test these suggestions, iteratively refine them through additional demonstrations, and finally apply and customize them via property inspectors. We evaluate our approach through a gallery of diverse examples, and evaluate its usability through a first-use study and via an analysis of its cognitive dimensions. We find that, in Lyra 2, interaction design by demonstration enables users to rapidly express a wide range of interactive visualizations.", "keywords": "Direct manipulation,interactive visualization,interaction design by demonstration", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030367", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030467", "title": "PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch;Lingyun Yu;Peiran Ren;Thomas Ertl;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Knittel, Johannes; Koch, Steffen; Ertl, Thomas, Univ Stuttgart, VIS VISUS, Stuttgart, Germany. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Dept Comp Sci \\& Software Engn, Suzhou, Peoples R China. Ren, Peiran, Alibaba Grp, Hangzhou, Peoples R China.", "countries": "Germany;China", "abstract": "Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.", "keywords": "Storyline visualization,reinforcement learning,mixed-initiative design", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030467", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030396", "title": "What Makes a Data-GIF Understandable?", "year": "2020", "conferenceName": "InfoVis", "authors": "Xinhuan Shu;Aoyu Wu;Junxiu Tang;Benjamin Bach;Yingcai Wu;Huamin Qu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Shu, Xinhuan; Tang, Junxiu; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Shu, Xinhuan; Wu, Aoyu; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Shu, Xinhuan, Zhejiang Univ, Hangzhou, Peoples R China. Bach, Benjamin, Univ Edinburgh, Edinburgh, Midlothian, Scotland.", "countries": "Scotland;China", "abstract": "GIFs are enjoying increasing popularity on social media as a format for data-driven storytelling with visualization; simple visual messages are embedded in short animations that usually last less than 15 seconds and are played in automatic repetition. In this paper, we ask the question, \u201cWhat makes a data-GIF understandable?\u201d While other storytelling formats such as data videos, infographics, or data comics are relatively well studied, we have little knowledge about the design factors and principles for \u201cdata-GIFs\u201d. To close this gap, we provide results from semi-structured interviews and an online study with a total of 118 participants investigating the impact of design decisions on the understandability of data-GIFs. The study and our consequent analysis are informed by a systematic review and structured design space of 108 data-GIFs that we found online. Our results show the impact of design dimensions from our design space such as animation encoding, context preservation, or repetition on viewers understanding of the GIF's core message. The paper concludes with a list of suggestions for creating more effective Data-GIFs.", "keywords": "Data-GIFs,Data-driven Storytelling,Evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030396", "refList": ["10.1109/tvcg.2016.2598647", "10.1016/j.visinf.2020.07.001", "10.1177/1473871615594652", "10.1109/tvcg.2014.2346424", "10.1111/cgf.13195", "10.1109/vlsicircuits18222.2020.9162811", "10.1109/tvcg.2018.2864909", "10.1109/tvcg.2018.2864903", "10.1145/2702123.2702431", "10.1016/j.visinf.2019.12.002", "10.1145/3274349", "10.1111/cgf.13444", "10.1145/3206505.3206552", "10.1145/3290605.3300280", "10.1109/jstqe.2020.3021589", "10.1145/2647868.2656408", "10.1006/ijhc.1017", "10.1145/3290605.3300335", "10.1007/s12650-020-00689-0", "10.1145/2818048.2819936", "10.1111/cgf.13325", "10.1145/2858036.2858387", "10.1145/3027063.3053139", "10.1145/3290605.3300474", "10.1109/tvcg.2016.2598920", "10.1109/tvcg.2018.2864899", "10.1145/3290605.3300483", "10.1145/3173574.3173612", "10.1145/2909132.2909255", "10.1109/tvcg.2016.2598620", "10.1016/j.learninstruc.2007.09.013", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2008.125", "10.1145/3173574.3173909", "10.1109/tvcg.2019.2934397", "10.1111/cgf.13709", "10.1109/tvcg.2013.234", "10.1109/tvcg.2019.2934401", "10.1145/2858036.2858532", "10.1016/j.visinf.2020.08.001", "10.1109/tvcg.2010.179", "10.1109/cicc48029.2020.9075900"], "wos": 1, "children": [], "len": 1}], "len": 9}, "index": 785, "embedding": [2.145601749420166, 1.6760441064834595, 2.9725139141082764, 3.136829376220703, -0.6539726853370667, 0.16650904715061188, -0.7323254346847534, 0.6017783284187317, -0.4441397190093994, -0.8419216275215149, 0.8071479797363281, 1.7449946403503418, -0.14601923525333405, 3.467040777206421, 1.814831256866455, 1.5859787464141846, 1.697567105293274, 0.07072390615940094, -0.7047576308250427, 1.5560822486877441, -0.06630208343267441, 0.2414102405309677, 1.480733036994934, -0.07681386172771454, 2.147707462310791, 0.08711085468530655, -0.5452967882156372, 0.8750221729278564, 2.354262351989746, 1.6904025077819824, 2.088900327682495, 2.581568717956543], "projection": [-3.8542463779449463, 7.745846748352051], "size": 5, "height": 2, "width": 4}