{"data": {"doi": "10.1111/cgf.12644", "year": "2015", "title": "Detangler: Visual Analytics for Multiplex Networks", "conferenceName": "EuroVis", "authors": "Benjamin Renoust;Guy Melan{\\c{c}}on;Tamara Munzner", "citationCount": "17", "affiliation": "Renoust, B (Corresponding Author), Natl Inst Informat, Tokyo, Japan.\nRenoust, B., Natl Inst Informat, Tokyo, Japan.\nRenoust, B., JFLI CNRS UMI 3527, Tokyo, Japan.\nRenoust, B.; Melancon, G., Univ Bordeaux, Bordeaux, France.\nRenoust, B.; Melancon, G., LaBRI CNRS UMR 5800, Bordeaux, France.\nRenoust, B.; Melancon, G., INRIA Bordeaux Sud Ouest, Bordeaux, France.\nMunzner, T., Univ British Columbia, Vancouver, BC V5Z 1M9, Canada.", "countries": "Japan;Canada;France", "abstract": "A multiplex network has links of different types, allowing it to express many overlapping types of relationships. A core task in network analysis is to evaluate and understand group cohesion; that is, to explain why groups of elements belong together based on the underlying structure of the network. We present Detangler, a system that supports visual analysis of group cohesion in multiplex networks through dual linked views. These views feature new data abstractions derived from the original multiplex network: the substrate network and the catalyst network. We contribute two novel techniques that allow the user to analyze the complex structure of the multiplex network without the extreme visual clutter that would result from simply showing it directly. The harmonized layout visual encoding technique provides spatial stability between the substrate and catalyst views. The pivot brushing interaction technique supports linked highlighting between the views based on computations in the underlying multiplex network to leapfrog between subsets of catalysts and substrates. We present results from the motivating application domain of annotated news documents with a usage scenario and preliminary expert feedback. A second usage scenario presents group cohesion analysis of the social network of the early American independence movement.", "keywords": "", "link": "https://doi.org/10.1111/cgf.12644", "refList": ["10.1109/tvcg.2012.252", "10.1109/tvcg.2013.223", "10.1109/tvcg.2012.255", "10.1109/infvis.2005.1532142", "10.1103/physrevx.3.041022", "10.1177/1473871613488591", "10.1057/palgrave.ivs.9500143", "10.1177/1473871612462152", "10.1145/2254556.2254652", "10.1109/tvcg.2011.185", "10.1145/1124772.1124891", "10.1109/tvcg.2009.151", "10.1145/2207676.2208293", "10.1109/tvcg.2012.324", "10.1145/2470654.2470724", "10.1109/tvcg.2014.2346279", "10.1109/infvis.2004.1"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2017.2744843", "title": "Graphiti: Interactive Specification of Attribute-Based Edges for Network Modeling and Visualization", "year": "2017", "conferenceName": "VAST", "authors": "Arjun Srinivasan;Hyunwoo Park;Alex Endert;Rahul C. Basole", "citationCount": "5", "affiliation": "Srinivasan, A (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA. Srinivasan, Arjun; Endert, Alex; Basole, Rahul C., Georgia Inst Technol, Atlanta, GA 30332 USA. Park, Hyunwoo, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Network visualizations, often in the form of node-link diagrams, are an effective means to understand relationships between entities, discover entities with interesting characteristics, and to identify clusters. While several existing tools allow users to visualize pre-defined networks, creating these networks from raw data remains a challenging task, often requiring users to program custom scripts or write complex SQL commands. Some existing tools also allow users to both visualize and model networks. Interaction techniques adopted by these tools often assume users know the exact conditions for defining edges in the resulting networks. This assumption may not always hold true, however. In cases where users do not know much about attributes in the dataset or when there are several attributes to choose from, users may not know which attributes they could use to formulate linking conditions. We propose an alternate interaction technique to model networks that allows users to demonstrate to the system a subset of nodes and links they wish to see in the resulting network. The system, in response, recommends conditions that can be used to model networks based on the specified nodes and links. In this paper, we show how such a demonstration-based interaction technique can be used to model networks by employing it in a prototype tool, Graphiti. Through multiple usage scenarios, we show how Graphiti not only allows users to model networks from a tabular dataset but also facilitates updating a pre-defined network with additional edge types.", "keywords": "Network modeling,visual analytics,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2017.2744843", "refList": ["10.1109/tvcg.2009.108", "10.1109/tvcg.2016.2598839", "10.1145/2909132.2909246", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2011.185", "10.1111/cgf.12880", "10.1145/1124772.1124891", "10.1109/2945.981849", "10.1093/bioinformatics/btq675", "10.1145/286498.286511", "10.1136/qshc.2004.010033", "10.1177/1473871613488591", "10.1109/2945.841119", "10.1145/985692.985767", "10.1016/j.socnet.2007.04.006", "10.1145/263407.263525", "10.1007/978-3-642-03658-3\\_47", "10.1057/palgrave.ivs.9500143", "10.1109/tvcg.2009.151", "10.1177/1473871612462152", "10.1109/tvcg.2006.166", "10.1145/1378773.1378792", "10.1147/sj.164.0324", "10.1109/vast.2010.5652520", "10.1162/jmlr.2003.3.4-5.993", "10.1103/physrevx.3.041022", "10.1109/tvcg.2006.106", "10.1017/cb09780511996368", "10.1145/1502650.1502667", "10.1109/iv.2010.28", "10.1111/cgf.12644"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865145", "title": "Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication", "year": "2018", "conferenceName": "InfoVis", "authors": "Arjun Srinivasan;Steven Mark Drucker;Alex Endert;John T. Stasko", "citationCount": "12", "affiliation": "Srinivasan, A (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA. Srinivasan, Arjun; Endert, Alex; Stasko, John, Georgia Inst Technol, Atlanta, GA 30332 USA. Drucker, Steven M., Microsoft Res, Washington, DC USA.", "countries": "USA", "abstract": "Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.", "keywords": "Natural Language Generation,Mixed-initiative Interaction,Visualization Recommendation,Data-driven Communication", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865145", "refList": ["10.2307/1269768", "10.1109/tvcg.2017.2744843", "10.1007/s00371-015-1132-9", "10.1145/3172944.3173007", "10.1109/tvcg.2007.70594", "10.1109/visual.1990.146375", "10.1145/1502650.1502695", "10.1145/108360.108361", "10.14778/2733004.2733035", "10.1109/pacificvis.2017.8031599", "10.1109/tvcg.2013.124", "10.1145/2556288.2557241", "10.1109/visual.1992.235203", "10.1109/infvis.2005.1532136", "10.1109/tvcg.2017.2745219", "10.1109/tvcg.2013.119", "10.1145/2984511.2984588", "10.1109/tvcg.2012.229", "10.1145/3025453.3025866", "10.1145/3035918.3035922", "10.1145/2807442.2807478", "10.1109/tvcg.2010.164", "10.1145/302979.303030", "10.1109/mcg.2006.70", "10.1109/mc.2013.36", "10.1109/tvcg.2015.2467191", "10.1017/s1351324997001502", "10.1111/cgf.13207", "10.1109/mcg.2009.22", "10.1145/2702123.2702608"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934669", "title": "Exploranative Code Quality Documents", "year": "2019", "conferenceName": "VAST", "authors": "Haris Mumtaz;Shahid Latif;Fabian Beck;Daniel Weiskopf", "citationCount": "0", "affiliation": "Mumtaz, H (Corresponding Author), Univ Stuttgart, VISUS, Stuttgart, Germany. Mumtaz, Haris; Weiskopf, Daniel, Univ Stuttgart, VISUS, Stuttgart, Germany. Latif, Shahid; Beck, Fabian, Univ Duisburg Essen, Paluno, Duisburg, Germany.", "countries": "Germany", "abstract": "Good code quality is a prerequisite for efficiently developing maintainable software. In this paper, we present a novel approach to generate exploranative (explanatory and exploratory) data-driven documents that report code quality in an interactive, exploratory environment. We employ a template-based natural language generation method to create textual explanations about the code quality, dependent on data from software metrics. The interactive document is enriched by different kinds of visualization, including parallel coordinates plots and scatterplots for data exploration and graphics embedded into text. We devise an interaction model that allows users to explore code quality with consistent linking between text and visualizations; through integrated explanatory text, users are taught background knowledge about code quality aspects. Our approach to interactive documents was developed in a design study process that included software engineering and visual analytics experts. Although the solution is specific to the software engineering scenario, we discuss how the concept could generalize to multivariate data and report lessons learned in a broader scope.", "keywords": "Code quality,interactive documents,natural language generation,sparklines", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934669", "refList": ["10.1109/tvcg.2014.2346435", "10.1109/tvcg.2018.2865022", "10.1016/j.jvlc.2018.10.001", "10.1002/smr.521", "10.1109/tvcg.2006.69", "10.1016/0004-3702(93)90022-4", "10.1145/3173574.3174106", "10.1007/s10648-010-9136-5", "10.3115/974557.974594", "10.1109/vl.1996.545307", "10.1109/tse.1976.233837", "10.1109/vissoft.2018.00010", "10.2312/vissym/vissym04/261-266", "10.1046/j.1365-2575.2002.00117.x", "10.1109/wcre.2002.1173068", "10.1109/icpc.2013.6613830", "10.1145/1985362.1985365", "10.3115/v1/w14-4401", "10.1007/s00766-007-0054-0", "10.1109/vissoft.2017.11", "10.1007/s10515-011-0098-8", "10.1109/vissof.2011", "10.1109/tvcg.2017.2674958", "10.1002/smr.404", "10.1109/32.979986", "10.1016/b978-0-12-397174-6.00010-6", "10.1145/3242587.3242617", "10.1109/mcg.2018.032421649", "10.1613/jair.5477", "10.1109/tfuzz.2014.2328011", "10.1145/2095654.2095665", "10.1109/icpc.2013.6613834", "10.1109/tvcg.2018.2865145", "10.1145/2597008.2597149", "10.1109/live.2013.6617345", "10.1145/1985793.1985868", "10.1002/int.21835", "10.1145/3139295.3139312", "10.1109/32.295895", "10.1109/scam.2014.14", "10.5281/zenodo.3336019", "10.1016/j.visinf.2019.03.004", "10.1109/aswec.2010.18", "10.2312/eurovisshort.20181084"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030403", "title": "Calliope: Automatic Visual Data Story Generation from a Spreadsheet", "year": "2020", "conferenceName": "InfoVis", "authors": "Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi 0007;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, Intelligent Big Data Visualizat Lab, Shanghai, Peoples R China. Shi, Danqing; Xu, Xinyue; Sun, Fuling; Shi, Yang; Cao, Nan, Tongji Univ, Intelligent Big Data Visualizat Lab, Shanghai, Peoples R China.", "countries": "China", "abstract": "Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.", "keywords": "Information Visualization,Visual Storytelling,Data Story", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030403", "refList": ["10.1007/s11063-017-9759-3", "10.1109/tvcg.2016.2598647", "10.1162/0891201054223977", "10.1109/tvcg.2015.2467732", "10.3390/info9030065", "10.1109/tvcg.2019.2934398", "10.1016/j.visinf.2018.12.001", "10.1145/2002353.2002355", "10.1109/tvcg.2007.70594", "10.1111/cgf.12392", "10.14778/2831360.2831371", "10.1093/biomet/33.3.239", "10.1109/mcg.2019.2924636", "10.1109/tvcg.2017.2659744", "10.1109/pacificvis.2009.4906837", "10.1109/pacificvis.2017.8031599", "10.4103/1755-6783.179101", "10.1145/2362394.2362398", "10.1111/cgf.12925", "10.1109/icde.2018.00019", "10.1109/tvcg.2018.2865240", "10.1109/mcg.2015.99", "10.1109/vds48975.2019.8973383", "10.1038/nature16961", "10.1109/tciaig.2012.2186810", "10.1145/3035918.3035922", "10.1145/3197517.3201362", "10.1109/tvcg.2019.2934281", "10.1109/tvcg.2016.2598620", "10.1155/2019/8480905", "10.1109/iccchina.2013.6671183", "10.1109/tvcg.2018.2865145", "10.1145/3299869.3314037", "10.1017/s1351324907004664", "10.1109/tvcg.2019.2934785", "10.1177/1473871618806555", "10.1613/jair.2989", "10.1109/tvcg.2010.179", "10.1145/3303766"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.1043", "year": "2020", "title": "AutoCaption: An Approach to Generate Natural Language Description from Visualization Automatically", "conferenceName": "PacificVis", "authors": "Can Liu;Liwenhan Xie;Yun Han;Datong Wei;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.\nYuan, XR (Corresponding Author), Peking Univ, Sch EECS, Beijing, Peoples R China.\nYuan, XR (Corresponding Author), Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China.\nLiu, Can; Xie, Liwenhan; Han, Yun; Wei, Datong; Yuan, Xiaoru, Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.\nLiu, Can; Xie, Liwenhan; Han, Yun; Wei, Datong; Yuan, Xiaoru, Peking Univ, Sch EECS, Beijing, Peoples R China.\nYuan, Xiaoru, Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China.", "countries": "China", "abstract": "In this paper, we propose a novel approach to generate captions for visualization charts automatically. In the proposed method, visual marks and visual channels, together with the associated text information in the original charts, are first extracted and identified with a multilayer perceptron classifier. Meanwhile, data information can also be retrieved by parsing visual marks with extracted mapping relationships. Then a 1-D convolutional residual network is employed to analyze the relationship between visual elements, and recognize significant features of the visualization charts, with both data and visual information as input. In the final step, the full description of the visual charts can be generated through a template-based approach. The generated captions can effectively cover the main visual features of the visual charts and support major feature types in commons charts. We further demonstrate the effectiveness of our approach through several cases.", "keywords": "", "link": "https://doi.org/10.1109/PacificVis48177.2020.1043", "refList": ["10.1145/1414471.1414525", "10.1111/cgf.13193", "10.1109/tvcg.2018.2865145", "10.1109/is.2002.1044219", "10.1145/2470654.2481374", "10.1080/13614568.2010.534186", "10.1017/s1351324997001502", "10.1145/3035918.3035922", "10.1109/cvpr.2016.90", "10.1145/2047196.2047247", "10.1007/s11257-006-9002-9", "10.1109/cvpr.2015.7298935", "10.1145/1148170.1148270", "10.1109/72.279181", "10.1109/icsmc.2011.6084067"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/vast47406.2019.8986909", "title": "Origraph: Interactive Network Wrangling", "year": "2019", "conferenceName": "VAST", "authors": "Alex Bigelow;Carolina Nobre;Miriah D. Meyer;Alexander Lex", "citationCount": "2", "affiliation": "Bigelow, A (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bigelow, Alex; Nobre, Carolina; Meyer, Miriah; Lex, Alexander, Univ Utah, Salt Lake City, UT 84112 USA.", "countries": "USA", "abstract": "Networks are a natural way of thinking about many datasets. The data on which a network is based, however, is rarely collected in a form that suits the analysis process, making it necessary to create and reshape networks. Data wrangling is widely acknowledged to be a critical part of the data analysis pipeline, yet interactive network wrangling has received little attention in the visualization research community. In this paper, we discuss a set of operations that are important for wrangling network datasets and introduce a visual data wrangling tool, Origraph, that enables analysts to apply these operations to their datasets. Key operations include creating a network from source data such as tables, reshaping a network by introducing new node or edge classes, filtering nodes or edges, and deriving new node or edge attributes. Our tool, Origraph, enables analysts to execute these operations with little to no programming, and to immediately visualize the results. Origraph provides views to investigate the network model, a sample of the network, and node and edge attributes. In addition, we introduce interfaces designed to aid analysts in specifying arguments for sensible network wrangling operations. We demonstrate the usefulness of Origraph in two Use Cases: first, we investigate gender bias in the film industry, and then the influence of money on the political support for the war in Yemen.", "keywords": "Graph visualization,Data abstraction,Data wrangling,Human-centered computing [Information visualization],[Human-centered computing]: Visualization systems and tools,Information systems [Graph-based database models]", "link": "http://dx.doi.org/10.1109/VAST47406.2019.8986909", "refList": ["10.1101/gr.1239303", "10.1145/1054972.1055032", "10.1016/b978-0-12-382229-1.00002-3", "10.1109/tvcg.2017.2744843", "10.1109/tvcg.2013.154", "10.1007/978-1-4614-7163-9315-1", "10.1073/pnas.1607151113", "10.18637/jss.v040.i01", "10.1145/1124772.1124891", "10.1177/1473871611415994", "10.1109/tvcg.2018.2865149", "10.1145/2598153.2598175", "10.1177/1473871613488591", "10.1109/tvcg.2018.2859973", "10.1186/1471-2105-14-s19-s3", "10.1056/nejmsa066082", "10.1007/978-3-642-36763-2\\_48", "10.1016/j.socscimed.2016.01.049", "10.1111/j.1467-8659.2008.01231.x", "10.1002/cne.24084", "10.2138/am-2017-6104ccbyncnd", "10.1109/tvcg.2014.2346248", "10.1111/j.1467-8659.2009.01710.x", "10.1007/978-3-642-03658-3\\_47", "10.1007/978-3-319-06793-3\\_5", "10.3390/genes9110519", "10.1145/3290605.3300356", "10.1111/cgf.12883", "10.1177/1473871612462152", "10.1016/j.jelectrocard.2010.09.003", "10.1111/cgf.13610", "10.1109/vast.2010.5652520", "10.1111/evo.13318", "10.1109/tvcg.2018.2811488", "10.1109/tvcg.2009.111", "10.1111/cgf.13184", "10.1109/tvcg.2009.116", "10.1109/biovis.2012.6378600"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030462", "title": "Table Scraps: An Actionable Framework for Multi-Table Data Wrangling From An Artifact Study of Computational Journalism", "year": "2020", "conferenceName": "InfoVis", "authors": "Stephen Kasica;Charles Berret;Tamara Munzner", "citationCount": "0", "affiliation": "Kasica, S (Corresponding Author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada. Kasica, Stephen; Munzner, Tamara, Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada. Berret, Charles, Univ British Columbia, Sch Journalism Writing \\& Media, Vancouver, BC, Canada.", "countries": "Canada", "abstract": "For the many journalists who use data and computation to report the news, data wrangling is an integral part of their work. Despite an abundance of literature on data wrangling in the context of enterprise data analysis, little is known about the specific operations, processes, and pain points journalists encounter while performing this tedious, time-consuming task. To better understand the needs of this user group, we conduct a technical observation study of 50 public repositories of data and analysis code authored by 33 professional journalists at 26 news organizations. We develop two detailed and cross-cutting taxonomies of data wrangling in computational journalism, for actions and for processes. We observe the extensive use of multiple tables, a notable gap in previous wrangling analyses. We develop a concise, actionable framework for general multi-table data wrangling that includes wrangling operations documented in our taxonomy that are without clear parallels in other work. This framework, the first to incorporate tables as first-class objects, will support future interactive wrangling tools for both computational journalism and general-purpose use. We assess the generative and descriptive power of our framework through discussion of its relationship to our set of taxonomies.", "keywords": "Computational journalism,Data journalism,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030462", "refList": ["10.1145/1378773.1378792", "10.1109/tvcg.2012.219", "10.1109/vast47406.2019.8986909", "10.1145/1084805.1084812", "10.1007/s00778-008-0098-x", "10.1016/j.websem.2008.09.005", "10.18637/jss.v040.i01", "10.1145/989863.989865", "10.1109/tvcg.2015.2467551", "10.5281/zenodo.3509134", "10.1109/tvcg.2019.2934539", "10.1109/tvcg.2019.2934593", "10.1109/tse.2018.2796554", "10.17349/jmc117309", "10.1109/2945.981851", "10.1109/vast.2011.6102440", "10.1177/1473871611415994", "10.1145/2001269.2001288"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2020.3030355", "title": "Guidelines For Pursuing and Revealing Data Abstractions", "year": "2020", "conferenceName": "InfoVis", "authors": "Alex Bigelow;Katy Williams;Katherine E. Isaacs", "citationCount": "0", "affiliation": "Bigelow, A (Corresponding Author), Univ Arizona, Tucson, AZ 85721 USA. Bigelow, Alex; Williams, Katy; Isaacs, Katherine E., Univ Arizona, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "Many data abstraction types, such as networks or set relationships, remain unfamiliar to data workers beyond the visualization research community. We conduct a survey and series of interviews about how people describe their data, either directly or indirectly. We refer to the latter as latent data abstractions. We conduct a Grounded Theory analysis that (1) interprets the extent to which latent data abstractions exist, (2) reveals the far-reaching effects that the interventionist pursuit of such abstractions can have on data workers, (3) describes why and when data workers may resist such explorations, and (4) suggests how to take advantage of opportunities and mitigate risks through transparency about visualization research perspectives and agendas. We then use the themes and codes discovered in the Grounded Theory analysis to develop guidelines for data abstraction in visualization projects. To continue the discussion, we make our dataset open along with a visual interface for further exploration.", "keywords": "Data abstraction,Grounded theory,Survey design,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030355", "refList": ["10.1080/2159676x.2016.1251701", "10.1109/infvis.2000.885092", "10.1145/2702123.2702298", "10.4135/9781848607941.n14", "10.1007/978-1-4939", "10.1109/tvcg.2014.2346331", "10.1109/tvcg.2017.2744843", "10.1177/1473871613510429", "10.1007/978-1-4939-0378-8\\_2", "10.1145/2598153.2598175", "10.1109/tvcg.2019.2934285", "10.1177/1473871613488591", "10.1145/2501105.2501106", "10.1109/tvcg.2019.2934538", "10.1109/tvcg.2019.2934539", "10.1017/s1049096510990781", "10.1145/3025453.3025837", "10.1145/3290605.3300474", "10.1145/3290605.3300356", "10.1002/nur.1025", "10.1145/2993901.2993916", "10.1145/3392826", "10.1086/269268", "10.1109/tvcg.2018.2865241", "10.1145/2998181.2998331", "10.1145/291224.291229", "10.1057/ivs.2009.13", "10.1145/2047196.2047205", "10.1109/tvcg.2012.213", "10.1145/3274405", "10.1109/tvcg.2013.145", "10.1016/0040-6031(92)85160-w", "10.1109/iv.2013.45", "10.1109/tvcg.2009.111", "10.1109/mcg.2019.2914844", "10.1109/tvcg.2009.116"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 21}, {"doi": "10.1109/pacificvis.2016.7465267", "year": "2016", "title": "Multilayer graph edge bundling", "conferenceName": "PacificVis", "authors": "Romain Bourqui;Dino Ienco;Arnaud Sallaberry;Pascal Poncelet", "citationCount": "9", "affiliation": "Bourqui, R (Corresponding Author), Univ Bordeaux, LaBRI, Bordeaux, France.\nBourqui, Romain, Univ Bordeaux, LaBRI, Bordeaux, France.\nIenco, Dino, IRSTEA, UMR TETIS, Montpellier, France.\nSallaberry, Arnaud, Univ Montpellier 3, LIRMM, Montpellier, France.\nPoncelet, Pascal, Univ Montpellier, LIRMM, Montpellier, France.", "countries": "France", "abstract": "Many real world information can be represented by a graph with a set of nodes interconnected with each other by multiple type of relations called edge layers (e.g., social network, biological data). Edge bundling techniques have been proposed to solve cluttering issue for standard graphs while few efforts were done to deal with the similar issue for multilayer graphs. In multilayer graphs scenario, not only the clutter induced by large amount of edges is a problem but also the fact that different type of edges can overlap each other making useless the final visualization. In this paper we introduce a new multilayer graph edge bundling technique that firstly produces a preliminary edge bundling independently of the different edge layers and then deals with the specificity of multilayer graphs where more than one type of edges can be routed on the same bundle. The proposed visualization is tested on a real world case study and the outcomes point out the ability of our proposal to discover patterns present in the data.", "keywords": "I.3.3 {[}Computer Graphics]: Picture/Image Generation-Line and curve generation", "link": "https://doi.org/10.1109/PACIFICVIS.2016.7465267", "refList": ["10.1109/tvcg.2008.135", "10.1016/j.socnet.2013.12.002", "10.1109/pacificvis.2015.7156359", "10.1111/j.1467-8659.2009.01700.x", "10.1109/tvcg.2011.233", "10.1111/j.1467-8659.2009.01680.x", "10.1109/pacificvis.2015.7156356", "10.1109/tvcg.2006.147", "10.1017/cbo9780511626593", "10.1111/j.1467-8659.2012.03079.x", "10.1109/iv.2015.20", "10.1109/tvcg.2014.2346441", "10.1109/tvcg.2011.104", "10.1109/tvcg.2011.190", "10.1109/tsmc.1981.4308636", "10.1093/comnet/cnu038", "10.1109/iv.2010.53", "10.1111/j.1467-8659.2009.01450.x", "10.1111/cgf.12644"], "wos": 1, "children": [{"doi": "10.1111/cgf.13213", "year": "2017", "title": "State of the Art in Edge and Trail Bundling Techniques", "conferenceName": "EuroVis", "authors": "Antoine Lhuillier;Christophe Hurter;Alexandru Telea", "citationCount": "24", "affiliation": "Lhuillier, A (Corresponding Author), Univ Toulouse, ENAC, Toulouse, France.\nLhuillier, A.; Hurter, C., Univ Toulouse, ENAC, Toulouse, France.\nTelea, A., Univ Groningen, Inst Johann Bernoulli, Groningen, Netherlands.", "countries": "France;Netherlands", "abstract": "Bundling techniques provide a visual simplification of a graph drawing or trail set, by spatially grouping similar graph edges or trails. This way, the structure of the visualization becomes simpler and thereby easier to comprehend in terms of assessing relations that are encoded by such paths, such as finding groups of strongly interrelated nodes in a graph, finding connections between spatial regions on a map linked by a number of vehicle trails, or discerning the motion structure of a set of objects by analyzing their paths. In this state of the art report, we aim to improve the understanding of graph and trail bundling via the following main contributions. First, we propose a data-based taxonomy that organizes bundling methods on the type of data they work on (graphs vs trails, which we refer to as paths). Based on a formal definition of path bundling, we propose a generic framework that describes the typical steps of all bundling algorithms in terms of high-level operations and show how existing method classes implement these steps. Next, we propose a description of tasks that bundling aims to address. Finally, we provide a wide set of example applications of bundling techniques and relate these to the above-mentioned taxonomies. Through these contributions, we aim to help both researchers and users to understand the bundling landscape as well as its technicalities.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13213", "refList": ["10.1109/tvcg.2008.135", "10.1109/tvcg.2014.2346578", "10.1145/2245276.2245338", "10.1111/j.1467-8659.2008.01214.x", "10.1109/pacificvis.2014.61", "10.1080/03098269708725407", "10.1111/j.1467-8659.2009.01700.x", "10.1002/1097-024x(200009)30:11", "10.1109/infvis.2003.1249008", "10.1109/tvcg.2011.233", "10.1109/tvcg.2011.181", "10.1109/tvcg.2006.147", "10.1109/pacificvis.2011.5742390", "10.1145/2470654.2466444", "10.1145/2049662.2049663", "10.1007/s12031-007-0029-0", "10.1109/tsmc.1981.4308636", "10.1109/tvcg.2015.2467112", "10.1109/dsnw.2011.5958797", "10.1109/iv.2010.53", "10.1016/j.scico.2012.05.002", "10.1109/tvcg.2011.202", "10.1016/j.cag.2014.01.006", "10.1109/visual.1999.809865", "10.1145/2833165.2833168", "10.1109/pacificvis.2016.7465267", "10.1109/tvcg.2016.2598838", "10.1109/pacificvis.2011.5742387", "10.1145/2254556.2254652", "10.1145/331499.331504", "10.1109/tse.2009.28", "10.1109/infvis.2004.66", "10.1111/j.1467-8659.2008.01232.x", "10.1109/tvcg.2015.2403323", "10.1109/pacificvis.2014.46", "10.1109/tvcg.2016.2515611", "10.1109/iv.2003.1217950", "10.1145/1322432.1322434", "10.1109/tvcg.2008.34", "10.1109/tvcg.2011.223", "10.1057/palgrave.ivs.95000/3", "10.1109/pacificvis.2011.5742384", "10.1109/tvcg.2013.114", "10.1002/smr.220", "10.1111/j.1467-8659.2008.01239.x", "10.1109/tst.2013.6509098", "10.1016/j.nurt.2007.05.011", "10.1111/j.1467-8659.2008.01241.x", "10.1006/jvlc.1998.0094", "10.7155/jgaa.00099", "10.1016/j.entcs.2008.06.039", "10.1111/j.1467-8659.2008.01205.x", "10.1109/tvcg.2013.246", "10.1109/vl.1996.545307", "10.1109/tvcg.2013.151", "10.1109/tvcg.2013.124", "10.1007/s00450-007-0036-y", "10.1111/j.1467-8659.2009.01680.x", "10.1109/tvcg.2003.1196007", "10.1007/978-3-642-25591-5\\_27", "10.1109/infvis.2005.1532150", "10.1371/journal.pcbi.1000108", "10.1007/978-3-642-41939-3\\_38", "10.1007/978-3-319-03841-4\\_34", "10.1109/tvcg.2009.145", "10.1179/000870410x12658023467367", "10.5194/npg-22-545-2015", "10.1002/smr.270", "10.1109/pacificvis.2011.5742389", "10.1111/j.1467-8659.2009.01683.x", "10.1068/i0382", "10.1109/infvis.1999.801860", "10.1111/j.0033-0124.1981.00419.x", "10.1007/978-3-319-27261-0\\_41", "10.1111/j.1467-8659.2009.01666.x", "10.1109/tvcg.2007.70582", "10.1109/tvcg.2006.120", "10.1111/j.1467-8659.2011.02037.x", "10.1109/tvcg.2009.108", "10.1111/j.1467-8659.2011.01951.x", "10.1111/j.1467-8659.2011.01898.x", "10.1117/12.704612", "10.1109/tvcg.2011.155", "10.1109/iv.2010.78", "10.1109/vast.2008.4677380", "10.1111/cgf.12881", "10.1109/2945.841119", "10.1016/0020-0190(89)90102-6", "10.1093/bioinformatics/bth078", "10.1109/34.1000236", "10.1109/tvcg.2007.70535", "10.1016/j.jss.2008.02.068", "10.1007/978-3-319-06793-3\\_2", "10.1007/978-3-642-18469-7\\_30", "10.1145/1168149.1168168", "10.1145/1773965.1773969", "10.1111/j.1467-8659.2012.03079.x", "10.1016/s0364-0213(87)80026-5", "10.1109/tvcg.2007.70521", "10.1145/2049662.2049670", "10.1109/tvcg.2011.104", "10.1007/bf00336961", "10.1016/j.trc.2014.03.005", "10.1109/pacificvis.2015.7156354", "10.1080/13658816.2010.511718", "10.2307/2685881", "10.1111/j.1467-8659.2003.00723.x", "10.1109/mcg.2009.87", "10.1109/tvcg.2016.2598885", "10.1016/j.cosrev.2007.05.001", "10.1111/j.1467-8659.2009.01450.x", "10.1145/2254556.2254670"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2017.2744338", "title": "Functional Decomposition for Bundled Simplification of Trail Sets", "year": "2017", "conferenceName": "InfoVis", "authors": "Christophe Hurter;St\u00e9phane Puechmorel;Florence Nicol;Alexandru Telea", "citationCount": "4", "affiliation": "Hurter, C (Corresponding Author), ENAC, Toulouse, France. Hurter, Christophe; Puechmorel, Stephane; Nicol, Florence, ENAC, Toulouse, France. Telea, Alexandru, Univ Groningen, Groningen, Netherlands.", "countries": "France;Netherlands", "abstract": "Bundling visually aggregates curves to reduce clutter and help finding important patterns in trail-sets or graph drawings. We propose a new approach to bundling based on functional decomposition of the underling dataset. We recover the functional nature of the curves by representing them as linear combinations of piecewise-polynomial basis functions with associated expansion coefficients. Next, we express all curves in a given cluster in terms of a centroid curve and a complementary term, via a set of so-called principal component functions. Based on the above, we propose a two-fold contribution: First, we use cluster centroids to design a new bundling method for 2D and 3D curve-sets. Secondly, we deform the cluster centroids and generate new curves along them, which enables us to modify the underlying data in a statistically-controlled way via its simplified (bundled) view. We demonstrate our method by applications on real-world 2D and 3D datasets for graph bundling, trajectory analysis, and vector field and tensor field visualization.", "keywords": "path visualization,trajectory visualization,edge bundles,functional decomposition,path generation,streamlines", "link": "http://dx.doi.org/10.1109/TVCG.2017.2744338", "refList": ["10.1109/tvcg.2008.135", "10.1111/cgf.13213", "10.1111/j.1751-5823.2011.001496.x", "10.1080/01441640110074773", "10.1007/s11634-013-0158-y", "10.1145/2254556.2254652", "10.1109/tst.2013.6509098", "10.1007/978-3-7091-6876-9\\_5", "10.1145/331499.331504", "10.1117/12.704612", "10.1145/2501988.2502046", "10.1016/s1361-8415(02)00053-1", "10.1109/tvcg.2011.155", "10.1109/tvcg.2013.246", "10.1109/iv.2010.78", "10.1109/vast.2008.4677380", "10.1111/j.1751-5823.2011.00149\\_6.x", "10.2307/2291115", "10.1111/j.1467-8659.2009.01700.x", "10.1109/tvcg.2011.233", "10.1109/tvcg.2015.2403323", "10.1214/09-aos741", "10.1109/tvcg.2013.124", "10.1109/visual.2004.32", "10.1093/bioinformatics/bth078", "10.2200/s00688ed1v01y201512vis006", "10.1007/978-3-642-36763-2\\_36", "10.1111/j.1467-8659.2009.01680.x", "10.1109/34.1000236", "10.1111/j.1467-8659.2008.01244.x", "10.1109/tvcg.2016.2515611", "10.1109/infvis.2005.1532150", "10.1109/iv.2003.1217950", "10.1214/009053605000000660", "10.1109/tvcg.2011.181", "10.1109/vissof.2011.6069451", "10.1109/tvcg.2006.147", "10.1016/j.jss.2008.02.068", "10.1007/978-3-642-18469-7\\_30", "10.1145/293347.293348", "10.1007/978-3-642-97385-7", "10.1109/tvcg.2011.223", "10.1111/j.1467-8659.2012.03079.x", "10.1093/comjnl/16.1.30", "10.1007/s10816-016-9307-x", "10.1090/s0002-9947-1950-0051437-7", "10.2307/2289936", "10.1109/tvcg.2011.104", "10.1007/s00453-013-9867-z", "10.1002/cem.1129", "10.1007/s12031-007-0029-0", "10.1109/tvcg.2013.114", "10.1093/comjnl/20.4.364", "10.1109/pacificvis.2013.6596126", "10.1109/tvcg.2011.190", "10.1109/cvpr.2007.383096", "10.1007/978-3-540-68721-4", "10.1109/pacificvis.2015.7156354", "10.5555/uri:pii:001650859291831n", "10.1109/tvcg.2009.138", "10.1111/j.1467-8659.2009.01450.x", "10.1137/1022106", "10.7155/jgaa.00028", "10.1109/tvcg.2011.202", "10.1007/b98888"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865191", "title": "FiberClay: Sculpting Three Dimensional Trajectories to Reveal Structural Insights", "year": "2018", "conferenceName": "InfoVis", "authors": "Christophe Hurter;Nathalie Henry Riche;Steven Mark Drucker;Maxime Cordeil;Richard Alligier;Romain Vuillemot", "citationCount": "11", "affiliation": "Hurter, C (Corresponding Author), French Civil Aviat Univ, ENAC, Toulouse, France. Hurter, C (Corresponding Author), Toulouse Univ, Toulouse, France. Hurter, Christophe ll; Alligier, Richard, French Civil Aviat Univ, ENAC, Toulouse, France. Hurter, Christophe ll; Alligier, Richard, Toulouse Univ, Toulouse, France. Riche, Nathalie Henry; Drucker, Steven M., Microsoft Res, Redmond, WA USA. Cordeil, Maxime, Monash Univ, Clayton, Vic, Australia. Vuillemot, Romain, Univ Lyon, Ecole Cent Lyon, CNRS, UMR 5205,LIRIS, F-69134 Lyon, France.", "countries": "USA;France;Australia", "abstract": "Visualizing 3D trajectories to extract insights about their similarities and spatial configuration is a critical task in several domains. Air traffic controllers for example deal with large quantities of aircrafts routes to optimize safety in airspace and neuroscientists attempt to understand neuronal pathways in the human brain by visualizing bundles of fibers from DTI images. Extracting insights from masses of 3D trajectories is challenging as the multiple three dimensional lines have complex geometries, may overlap, cross or even merge with each other, making it impossible to follow individual ones in dense areas. As trajectories are inherently spatial and three dimensional, we propose FiberClay: a system to display and interact with 3D trajectories in immersive environments. FiberClay renders a large quantity of trajectories in real time using GP-GPU techniques. FiberClay also introduces a new set of interactive techniques for composing complex queries in 3D space leveraging immersive environment controllers and user position. These techniques enable an analyst to select and compare sets of trajectories with specific geometries and data properties. We conclude by discussing insights found using FiberClay with domain experts in air traffic control and neurology.", "keywords": "Immersive Analytics,3D Visualization,Dynamic Queries,Bimanual Interaction,Multidimensional Data", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865191", "refList": ["10.1016/b978-0-08-051574-8.50047-9", "10.1016/j.ijhcs.2013.03.003", "10.1057/palgrave.ivs.9500097", "10.1109/tvcg.2008.153", "10.2307/1269768", "10.1109/tvcg.2013.226", "10.1109/tvcg.2005.59", "10.1016/j.compenvurbsys.2014.01.005", "10.1017/cbo9781139128926.013", "10.1109/pacificvis.2014.61", "10.1145/1279640.1279642", "10.1109/pacificvis.2017.8031577", "10.1109/tvcg.2016.2599217", "10.1109/visual.1991.175794", "10.1007/978-1-4615-1177-9\\_27", "10.1007/978-0-387-35504-714", "10.1109/tvcg.2011.233", "10.3390/informatics4030026", "10.1109/tvcg.2011.192", "10.1109/tvcg.2015.2403323", "10.1145/800186.810616", "10.1145/3013971.3014006", "10.1016/j.ijhcs.2005.02.001", "10.1109/3dvis.2014.7160096", "10.1111/j.1467-8659.2012.03115.x", "10.1109/38.946631", "10.1145/3126594.3126613", "10.1109/pacificvis.2011.5742390", "10.4230/dagrep.6.6.1", "10.1111/j.1467-8659.2012.03079.x", "10.1057/palgrave.ivs.9500061", "10.1109/tvcg.2017.2744338", "10.1109/tvcg.2009.145", "10.1109/tvcg.2012.265", "10.1007/s12031-007-0029-0", "10.1145/3025453.3025566", "10.1109/tvcg.2013.153", "10.1109/tvcg.2016.2520921", "10.1016/j.trc.2014.03.005", "10.1109/3dvis.2014.7160093", "10.1109/ipsn.2014.6846743", "10.1109/tvcg.2015.2467112", "10.1109/tvcg.2017.2744079", "10.1109/tvcg.2011.224", "10.1111/cgf.12804", "10.1111/j.1467-8659.2009.01687.x", "10.1016/s1088-467x(99)00013-x", "10.1109/tvcg.2012.217", "10.1145/2992154.2996365"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934415", "title": "Evaluating an Immersive Space-Time Cube Geovisualization for Intuitive Trajectory Data Exploration", "year": "2019", "conferenceName": "InfoVis", "authors": "Jorge A. Wagner Filho;Wolfgang Stuerzlinger;Luciana Porcher Nedel", "citationCount": "4", "affiliation": "Wagner, JA (Corresponding Author), Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil. Wagner, JA (Corresponding Author), Simon Fraser Univ, Burnaby, BC, Canada. Wagner Filho, Jorge A.; Nedel, Luciana, Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil. Wagner Filho, Jorge A.; Stuerzlinger, Wolfgang, Simon Fraser Univ, Burnaby, BC, Canada.", "countries": "Canada;Brazil", "abstract": "A Space-Time Cube enables analysts to clearly observe spatio-temporal features in movement trajectory datasets in geovisualization. However, its general usability is impacted by a lack of depth cues, a reported steep learning curve, and the requirement for efficient 3D navigation. In this work, we investigate a Space-Time Cube in the Immersive Analytics domain. Based on a review of previous work and selecting an appropriate exploration metaphor, we built a prototype environment where the cube is coupled to a virtual representation of the analyst's real desk, and zooming and panning in space and time are intuitively controlled using mid-air gestures. We compared our immersive environment to a desktop-based implementation in a user study with 20 participants across 7 tasks of varying difficulty, which targeted different user interface features. To investigate how performance is affected in the presence of clutter, we explored two scenarios with different numbers of trajectories. While the quantitative performance was similar for the majority of tasks, large differences appear when we analyze the patterns of interaction and consider subjective metrics. The immersive version of the Space-Time Cube received higher usability scores, much higher user preference, and was rated to have a lower mental workload, without causing participants discomfort in 25-minute-long VR sessions.", "keywords": "Space-time cube,Trajectory visualization,Immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934415", "refList": ["10.1109/mcg.2006.74", "10.1080/13658816.2015.1058386", "10.1145/1456650.1456652", "10.1111/cgf.13430", "10.1145/1944745.1944777", "10.1016/b978-155860819-1/50001-7", "10.1109/bdva.2018.8534024", "10.1109/pacificvis.2017.8031578", "10.1111/j.1467-8306.1994.tb01869.x", "10.2307/3001968", "10.1007/978-3-642-15300-6\\_21", "10.1080/13658816.2010.511223", "10.1007/978-3-030-01388-2", "10.1080/00087041.2018.1495898", "10.1145/1773965.1773970", "10.1002/9780470987643.ch15", "10.1109/glocom.2015.7417476", "10.2312/eurovisstar.20141171", "10.1080/17489725.2015.1074736", "10.1109/bdva.2016.7787050", "10.1109/wevr.2017.7957707", "10.1016/s1045-926x(03)00046-6", "10.1109/iv.2018.00026", "10.1179/1743277413y.0000000061", "10.1109/mcg.2019.2898856", "10.1109/iv.2004.1320137", "10.1111/j.1467-8659.2011.01929.x", "10.1109/tvcg.2008.194", "10.1016/j.jtrangeo.2010.11.002", "10.1177/154193120605000909", "10.1109/bigdata.2015.7364040", "10.1111/cgf.12466", "10.1109/icsens.2015.7370446", "10.1080/13658816.2010.508043", "10.1057/ivs.2009.8", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.14236/ewic/hci2016.22", "10.1145/2655691", "10.1007/bf01936872", "10.1109/infvis.2004.27", "10.1109/tvcg.2014.2329308", "10.1111/j.1435-5597.1970.tb01464.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934332", "title": "LassoNet: Deep Lasso-Selection of 3D Point Clouds", "year": "2019", "conferenceName": "SciVis", "authors": "Zhutian Chen;Wei Zeng 0004;Zhiguang Yang;Lingyun Yu;Chi-Wing Fu;Huamin Qu", "citationCount": "4", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Zhutian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei; Yang, Zhiguang, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yu, Lingyun, Univ Groningen, Groningen, Netherlands. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China.", "countries": "China;Netherlands", "abstract": "Selection is a fundamental task in exploratory analysis and visualization of 3D point clouds. Prior researches on selection methods were developed mainly based on heuristics such as local point density, thus limiting their applicability in general data. Specific challenges root in the great variabilities implied by point clouds (e.g., dense vs. sparse), viewpoint (e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this work, we introduce LassoNet, a new deep neural network for lasso selection of 3D point clouds, attempting to learn a latent mapping from viewpoint and lasso to point cloud regions. To achieve this, we couple user-target points with viewpoint and lasso information through 3D coordinate transform and naive selection, and improve the method scalability via an intention filtering and farthest point sampling. A hierarchical network is trained using a dataset with over 30K lasso-selection records on two different point cloud data. We conduct a formal user study to compare LassoNet with two state-of-the-art lasso-selection methods. The evaluations confirm that our approach improves the selection effectiveness and efficiency across different combinations of 3D point clouds, viewpoints, and lasso selections. Project Website: https://LassoNet.github.io", "keywords": "Point Clouds,Lasso Selection,Deep Learning", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934332", "refList": ["10.1111/j.1467-8659.2009.01515.x", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2016.2599049", "10.1109/iccv.2015.114", "10.1109/vppc.2018.8604993", "10.1111/cgf.13405", "10.1109/tvcg.2018.2843369", "10.1109/cvpr.2018.00278", "10.1145/3025453.3025957", "10.1109/mc.2013.178", "10.1145/2980179.2980238", "10.1109/msp.2017.2693418", "10.1145/237091.237105", "10.1145/2835487", "10.1109/cvpr.2010.5539838", "10.1016/j.visinf.2017.01.006", "10.1109/tcst.2018.2819965", "10.1109/tvcg.2018.2865138", "10.1109/83.623193", "10.1145/3072959.3073608", "10.1145/1053427.1053445", "10.1016/s0039-9140(96)02179-0", "10.1109/cvpr.2015.7298801", "10.1109/cvpr.2017.693", "10.1007/s12650-014-0206-5", "10.1145/3272127.3275110", "10.1109/cvpr.2015.7298845", "10.1109/iccvw.2015.112", "10.1109/iros.2015.7353481", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2012.217", "10.1109/tridui.2006.1618279", "10.1109/tvcg.2012.292", "10.1109/cvpr.2016.609", "10.1016/j.cag.2012.12.003"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028947", "title": "A Fluid Flow Data Set for Machine Learning and its Application to Neural Flow Map Interpolation", "year": "2020", "conferenceName": "SciVis", "authors": "Jakob Jakob;Markus H. Gross;Tobias G\u00fcnther", "citationCount": "0", "affiliation": "Jakob, J (Corresponding Author), Swiss Fed Inst Technol, Zurich, Switzerland. Jakob, Jakob; Gross, Markus; Guenther, Tobias, Swiss Fed Inst Technol, Zurich, Switzerland.", "countries": "Switzerland", "abstract": "In recent years, deep learning has opened countless research opportunities across many different disciplines. At present, visualization is mainly applied to explore and explain neural networks. Its counterpart-the application of deep learning to visualization problems-requires us to share data more openly in order to enable more scientists to engage in data-driven research. In this paper, we construct a large fluid flow data set and apply it to a deep learning problem in scientific visualization. Parameterized by the Reynolds number, the data set contains a wide spectrum of laminar and turbulent fluid flow regimes. The full data set was simulated on a high-performance compute cluster and contains 8000 time-dependent 2D vector fields, accumulating to more than 16 TB in size. Using our public fluid data set, we trained deep convolutional neural networks in order to set a benchmark for an improved post-hoc Lagrangian fluid flow analysis. In in-situ settings, flow maps are exported and interpolated in order to assess the transport characteristics of time-dependent fluids. Using deep learning, we improve the accuracy of flow map interpolations, allowing a more precise flow analysis at a reduced memory IO footprint.", "keywords": "Scientific visualization,deep learning,flow maps", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028947", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1145/3355089.3356560", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1007/978-3-030-00533-7\\_36", "10.3390/rs11161921", "10.1007/978-3-030-48457-6\\_1", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/mcg.2018.2881523", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1145/3072959.3073643", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/tvcg.2013.128", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1007/978-3-319-46475-6\\_43", "10.1126/science.1127647", "10.1007/978-3-319-46475-6\\_25", "10.1111/cgf.13689"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030346", "title": "V2V: A Deep Learning Approach to Variable-to-Variable Selection and Translation for Multivariate Time-Varying Data", "year": "2020", "conferenceName": "SciVis", "authors": "Jun Han;Hao Zheng 0006;Yunhao Xing;Danny Ziyi Chen;Chaoli Wang", "citationCount": "0", "affiliation": "Han, J (Corresponding Author), Univ Notre Dame, Notre Dame, IN 46556 USA. Han, Jun; Zheng, Hao; Chen, Danny Z.; Wang, Chaoli, Univ Notre Dame, Notre Dame, IN 46556 USA. Xing, Yunhao, Sichuan Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China;USA", "abstract": "We present V2V, a novel deep learning framework, as a general-purpose solution to the variable-to-variable (V2V) selection and translation problem for multivariate time-varying data (MTVD) analysis and visualization. V2V leverages a representation learning algorithm to identify transferable variables and utilizes Kullback-Leibler divergence to determine the source and target variables. It then uses a generative adversarial network (GAN) to learn the mapping from the source variable to the target variable via the adversarial, volumetric, and feature losses. V2V takes the pairs of time steps of the source and target variable as input for training, Once trained, it can infer unseen time steps of the target variable given the corresponding time steps of the source variable. Several multivariate time-varying data sets of different characteristics are used to demonstrate the effectiveness of V2V, both quantitatively and qualitatively. We compare V2V against histogram matching and two other deep learning solutions (Pix2Pix and CycleGAN).", "keywords": "Multivariate time-varying data,variable selection and translation,generative adversarial network,data extrapolation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030346", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/cvpr.2017.19", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1146/annurev", "10.3390/rs11161921", "10.1109/tvcg.2007.70551", "10.1145/3309993", "10.1007/978-3-030-48457-6\\_1", "10.1109/igarss.2018.8518411", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1146/annurev-fluid-010313-141322", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1126/science.1127647"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030374", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data", "year": "2020", "conferenceName": "SciVis", "authors": "Yifan Wang;Guoli Yan;Haikuan Zhu;Sagar Buch;Ying Wang;E. Mark Haacke;Jing Hua;Zichun Zhong", "citationCount": "0", "affiliation": "Wang, YF (Corresponding Author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Wang, Yifan; Yan, Guoli; Zhu, Haikuan; Hua, Jing; Zhong, Zichun, Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Buch, Sagar; Wang, Ying; Haacke, Ewart Mark, Wayne State Univ, Dept Radiol, Detroit, MI 48201 USA.", "countries": "USA", "abstract": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.", "keywords": "Deep neural network,3D cerebrovascular segmentation and visualization,maximum intensity projection (MIP),joint embedding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030374", "refList": ["10.1109/cluster.2018.00036", "10.1109/iccv.2015.123", "10.1109/tvcg.2013.133", "10.1109/cvpr.2017.19", "10.1109/tvcg.2019.2934312", "10.1007/978-3-319-46487-9\\_40", "10.1109/tvcg.2006.175", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2018.2880207", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/tvcg.2018.2816059", "10.1109/pacificvis.2009.4906852", "10.1109/tvcg.2018.2864808", "10.1109/mcg.2018.2881523", "10.1109/tvcg.2015.2467431", "10.1109/tpami.2015.2439281", "10.1109/pacificvis48177.2020.8737", "10.1109/cvpr.2019.00244", "10.1109/tvcg.2006.165", "10.1109/bigdata.2018.8622520", "10.1007/978-3-319-46466-4\\_29", "10.1109/visual.2019.8933759", "10.1145/3197517.3201304", "10.1109/pacificvis.2011.5742378", "10.1109/cvpr.2006.91", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1007/978-3-319-24574-4\\_28", "10.1109/pacificvis.2019.00041", "10.1109/tvcg.2019.2934332", "10.1109/cvpr.2018.00916", "10.1007/978-3-319-46475-6\\_43"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934803", "title": "There Is No Spoon: Evaluating Performance, Space Use, and Presence with Expert Domain Users in Immersive Analytics", "year": "2019", "conferenceName": "InfoVis", "authors": "Andrea Batch;Andrew Cunningham;Maxime Cordeil;Niklas Elmqvist;Tim Dwyer;Bruce H. Thomas;Kim Marriott", "citationCount": "6", "affiliation": "Batch, A (Corresponding Author), Univ Maryland, College Pk, MD 20742 USA. Batch, Andrea; Elmqvist, Niklas, Univ Maryland, College Pk, MD 20742 USA. Cunningham, Andrew; Thomas, Bruce H., Univ South Australia Adelaide, Adelaide, SA, Australia. Cordeil, Maxime; Dwyer, Tim; Marriott, Kim, Univ Melbourne, Melbourne, Vic, Australia.", "countries": "USA;Australia", "abstract": "Immersive analytics turns the very space surrounding the user into a canvas for data analysis, supporting human cognitive abilities in myriad ways. We present the results of a design study, contextual inquiry, and longitudinal evaluation involving professional economists using a Virtual Reality (VR) system for multidimensional visualization to explore actual economic data. Results from our preregistered evaluation highlight the varied use of space depending on context (exploration vs. presentation), the organization of space to support work, and the impact of immersion on navigation and orientation in the 3D analysis space.", "keywords": "Design study,evaluation,economic analysis,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934803", "refList": ["10.1145/3139131.3139141", "10.1109/2.60882", "10.1007/978-3-030-01388-2\\_1", "10.1109/38.250911", "10.1145/302979.303166", "10.1073/pnas.1306779110", "10.1109/tvcg.2012.219", "10.1162/105474698565686", "10.1016/0364-0213(94)90007-8", "10.1145/642611.642650", "10.1162/pres.1992.1.4.482", "10.1007/978-3-030-01388-2", "10.1109/2.19829", "10.1111/1467-8284.00096", "10.1007/978-3-030-01388-2\\_7", "10.1109/hicss.2011.339", "10.1109/tvcg.2016.2599107", "10.1145/3173574.3173664", "10.1145/1008653.1008669", "10.1109/tvcg.2017.2743990", "10.1145/642611", "10.1145/3126594.3126613", "10.1007/978-94-007-6833-89", "10.1162/pres\\_a\\_00124", "10.1016/0004-3702(94)00017-u", "10.1109/tvcg.2014.20", "10.1145/1168149.1168158", "10.1109/visual.1995.480800", "10.1109/icsens.2015.7370446", "10.1207/s15516709cog1701\\_1", "10.1162/105474601300343603", "10.1109/tvcg.2016.2518135", "10.1109/tvcg.2012.213", "10.1016/s0097-8493(02)00113-9", "10.1162/pres.1994.3.2.130", "10.1109/tvcg.2018.2865191", "10.1109/visual.1990.146402", "10.1162/105474601300343612", "10.1145/2702123.2702406", "10.1162/pres.1992.1.1.120", "10.1145/3009939.3009955", "10.1109/glocom.2015.7417476", "10.1162/pres\\_a\\_00261"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030450", "title": "Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment", "year": "2020", "conferenceName": "InfoVis", "authors": "Benjamin Lee;Xiaoyun Hu;Maxime Cordeil;Arnaud Prouzeau;Bernhard Jenny;Tim Dwyer", "citationCount": "0", "affiliation": "Lee, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Lee, Benjamin; Hu, Xiaoyun; Cordeil, Maxime; Prouzeau, Arnaud; Jenny, Bernhard; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia.", "countries": "Australia", "abstract": "Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA: the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner's personal workspace.", "keywords": "Immersive analytics,collaboration,virtual reality,qualitative study,multivariate data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030450", "refList": ["10.1109/tvcg.2008.153", "10.1007/s10606-004-5062-8", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2019.2914677", "10.1016/0020-7373(91)90039-a", "10.1109/tvcg.2011.287", "10.1117/12.2005484", "10.1109/mmul.2009.35", "10.1109/tvcg.2019.2934803", "10.1145/3359996.3364242", "10.1109/immersive.2016.7932384", "10.1016/j.future.2008.07.015", "10.1057/palgrave.ivs.9500167", "10.1109/mcg.2019.2898941", "10.1145/3343055.3360746", "10.1007/978-3-319-45853-3\\_8", "10.1145/2576099", "10.1145/2858036.2858039", "10.1109/tvcg.2016.2599107", "10.1145/3173574.3173664", "10.1007/978-3-030-01388-2\\_2", "10.1109/vr.2019.8797978", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1007/978-3-030-01388-2\\_8", "10.1023/a:1021271517844", "10.1109/bigdata.2014.7004282", "10.1109/tvcg.2019.2934395", "10.1109/ismar.2010.5643530", "10.1007/978-3-030-01388-22", "10.1145/2133806.2133821", "10.1109/3dvis.2014.7160093", "10.1145/2556288.2557058", "10.1109/vr.2019.8797845"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030392", "title": "ShuttleSpace: Exploring and Analyzing Movement Trajectory in Immersive Visualization", "year": "2020", "conferenceName": "InfoVis", "authors": "Shuainan Ye;Zhutian Chen;Xiangtong Chu;Yifan Wang;Siwei Fu;Lejun Shen;Kun Zhou;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD CG, Hangzhou, Peoples R China. Ye, Shuainan; Chu, Xiangtong; Wang, Yifan; Zhou, Kun; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD CG, Hangzhou, Peoples R China. Chen, Zhutian, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Fu, Siwei, Zhejiang Lab, Hangzhou, Zhejiang, Peoples R China. Shen, Lejun, Chengdu Sports Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China", "abstract": "We present ShuttleSpace, an immersive analytics system to assist experts in analyzing trajectory data in badminton. Trajectories in sports, such as the movement of players and balls, contain rich information on player behavior and thus have been widely analyzed by coaches and analysts to improve the players' performance. However, existing visual analytics systems often present the trajectories in court diagrams that are abstractions of reality, thereby causing difficulty for the experts to imagine the situation on the court and understand why the player acted in a certain way. With recent developments in immersive technologies, such as virtual reality (VR), experts gradually have the opportunity to see, feel, explore, and understand these 3D trajectories from the player's perspective. Yet, few research has studied how to support immersive analysis of sports data from such a perspective. Specific challenges are rooted in data presentation (e.g., how to seamlessly combine 2D and 3D visualizations) and interaction (e.g., how to naturally interact with data without keyboard and mouse) in VR. To address these challenges, we have worked closely with domain experts who have worked for a top national badminton team to design ShuttleSpace. Our system leverages 1) the peripheral vision to combine the 2D and 3D visualizations and 2) the VR controller to support natural interactions via a stroke metaphor. We demonstrate the effectiveness of ShuttleSpace through three case studies conducted by the experts with useful insights. We further conduct interviews with the experts whose feedback confirms that our first-person immersive analytics system is suitable and useful for analyzing badminton data.", "keywords": "Movement trajectory,badminton analytics,virtual reality", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030392", "refList": ["10.1016/j.eswa.2012.01.204", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1142/s1005386719000178", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/978-981-10-3994-2\\_1", "10.1007/978-3-319-07812-0\\_12", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1016/j.eswa.2017.07.027", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 35}, {"doi": "10.1109/tvcg.2019.2934805", "title": "Interactive Structure-aware Blending of Diverse Edge Bundling Visualizations", "year": "2019", "conferenceName": "InfoVis", "authors": "Yunhai Wang;Mingliang Xue;Yanyan Wang;Xinyuan Yan;Baoquan Chen;Chi-Wing Fu;Christophe Hurter", "citationCount": "1", "affiliation": "Fu, CW (Corresponding Author), Chinese Univ Hong Kong, Hong Kong, Peoples R China. Fu, CW (Corresponding Author), SIAT, Guangdong Prov Key Lab CV \\& VR Tech, Guangzhou, Peoples R China. Wang, Yunhai; Xue, Mingliang; Wang, Yanyan; Yan, Xinyuan, Shandong Univ, Jinan, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China. Fu, Chi-Wing, SIAT, Guangdong Prov Key Lab CV \\& VR Tech, Guangzhou, Peoples R China. Hurter, Christophe, ENAC, Toulouse, France.", "countries": "China;France", "abstract": "Many edge bundling techniques (i.e., data simplification as a support for data visualization and decision making) exist but they are not directly applicable to any kind of dataset and their parameters are often too abstract and difficult to set up. As a result, this hinders the user ability to create efficient aggregated visualizations. To address these issues, we investigated a novel way of handling visual aggregation with a task-driven and user-centered approach. Given a graph, our approach produces a decluttered view as follows: first, the user investigates different edge bundling results and specifies areas, where certain edge bundling techniques would provide user-desired results. Second, our system then computes a smooth and structural preserving transition between these specified areas. Lastly, the user can further fine-tune the global visualization with a direct manipulation technique to remove the local ambiguity and to apply different visual deformations. In this paper, we provide details for our design rationale and implementation. Also, we show how our algorithm gives more suitable results compared to current edge bundling techniques, and in the end, we provide concrete instances of usages, where the algorithm combines various edge bundling results to support diverse data exploration and visualizations.", "keywords": "path visualization,trajectory visualization,edge bundles", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934805", "refList": ["10.1111/cgf.13213", "10.1145/2254556.2254652", "10.1111/cgf.12791", "10.1111/j.1467-8659.2011.01898.x", "10.1111/cgf.12871", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1177/1473871617738122", "10.1111/j.1467-8659.2009.01700.x", "10.2312/eurovisstar.20141172", "10.1109/tvcg.2011.233", "10.1016/0020-0190(89)90102-6", "10.2200/s00688ed1v01y201512vis006", "10.3390/informatics4030026", "10.1109/tvcg.2017.2745919", "10.1111/j.1467-8659.2009.01680.x", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2016.2515611", "10.1111/cgf.13176", "10.1109/tvcg.2006.147", "10.1145/2992154.2992168", "10.1109/tvcg.2016.2598958", "10.1109/tvcg.2010.269", "10.1002/spe.4380211102", "10.1109/pacificvis.2017.8031596", "10.1109/tvcg.2011.223", "10.1145/1168149.1168168", "10.1006/s1045-926x(02)00016-2", "10.2312/vissym/vissym04/221-230", "10.1111/j.1467-8659.2012.03079.x", "10.1016/j.jvlc.2017.09.004", "10.3390/e20090625", "10.1109/tvcg.2014.2346441", "10.1109/tvcg.2017.2744338", "10.1109/pacificvis.2017.8031594", "10.1109/tvcg.2011.104", "10.1007/978-3-319-06793-310", "10.1109/pacificvis.2011.5742389", "10.1111/j.1467-8659.2009.01449.x", "10.1371/journal.pone.0098679", "10.1109/pacificvis.2015.7156354", "10.7155/jgaa.00370", "10.1109/pacificvis.2013.6596147", "10.1109/tvcg.2006.156", "10.1111/j.1467-8659.2009.01450.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934657", "title": "OD Morphing: Balancing Simplicity with Faithfulness for OD Bundling", "year": "2019", "conferenceName": "VAST", "authors": "Yan Lyu;Xu Liu;Hanyi Chen;Arpan Mangal;Kai Liu;Chao Chen 0004;Brian Y. Lim", "citationCount": "0", "affiliation": "Lyu, Y (Corresponding Author), Natl Univ Singapore, Singapore, Singapore. Lyu, Yan; Lim, Brian, Natl Univ Singapore, Singapore, Singapore. Liu, Xu, Southeast Univ, Nanjing, Peoples R China. Chen, Hanyi, Zhejiang Univ, Hangzhou, Peoples R China. Mangal, Arpan, Indian Inst Technol, Delhi, India. Liu, Kai; Chen, Chao, Chongqing Univ, Chongqing, Peoples R China.", "countries": "India;China;Singapore", "abstract": "OD bundling is a promising method to identify key origin-destination (OD) patterns, but the bundling can mislead the interpretation of actual trajectories traveled. We present OD Morphing, an interactive OD bundling technique that improves geographical faithfulness to actual trajectories while preserving visual simplicity for OD patterns. OD Morphing iteratively identifies critical waypoints from the actual trajectory network with a min-cut algorithm and transitions OD bundles to pass through the identified waypoints with a smooth morphing method. Furthermore, we extend OD Morphing to support bundling at interaction speeds to enable users to interactively transition between degrees of faithfulness to aid sensemaking. We introduce metrics for faithfulness and simplicity to evaluate their trade-off achieved by OD morphed bundling. We demonstrate OD Morphing on real-world city-scale taxi trajectory and USA domestic planned flight datasets.", "keywords": "OD Visualization,Edge Bundling,Trajectory", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934657", "refList": ["10.1109/tvcg.2008.135", "10.1145/2833165.2833168", "10.1111/cgf.13213", "10.1109/tvcg.2018.2864503", "10.1109/tvcg.2010.44", "10.1145/1653771.1653820", "10.1016/j.comgeo.2015.10.005", "10.1109/tvcg.2014.2337333", "10.1016/j.sbspro.2014.12.218", "10.1109/tvcg.2016.2535234", "10.1109/pacificvis.2011.5742386", "10.1145/321694.321699", "10.1109/tvcg.2013.246", "10.1111/cgf.12881", "10.1111/j.1467-8659.2009.01700.x", "10.1109/tvcg.2015.2467771", "10.1109/tvcg.2011.233", "10.1109/infvis.2003.1249008", "10.1109/itsc.2011.6082850", "10.1111/j.1467-8659.2009.01680.x", "10.1109/tvcg.2014.2346271", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2016.2515611", "10.1016/j.physa.2006.12.003", "10.1147/sj.41.0025", "10.1007/978-3-642-00304-2\\_1", "10.1109/tvcg.2006.147", "10.1109/tvcg.2016.2598958", "10.1109/tpami.2004.60", "10.1109/tvcg.2011.223", "10.1109/tits.2018.2817282", "10.1111/j.1467-8659.2012.03079.x", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2017.2744338", "10.1109/pacificvis.2017.8031594", "10.1179/000870410x12658023467367", "10.1145/2030112.2030126", "10.1109/tvcg.2011.104", "10.1145/2133416.2146416", "10.1109/tvcg.2011.190", "10.1142/s0218195995000064", "10.1002/cnm.1630040603", "10.1109/pacificvis.2011.5742389", "10.1109/pacificvis.2015.7156354", "10.1111/cgf.12778", "10.1109/dsnw.2011.5958797", "10.1109/cvprw.2008.4563095", "10.1109/tvcg.2016.2598885", "10.1109/tvcg.2017.2744322", "10.1111/j.1467-8659.2009.01450.x", "10.1109/tvcg.2016.2598416", "10.1109/cce.2012.6315867"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13712", "year": "2019", "title": "Route-Aware Edge Bundling for Visualizing Origin-Destination Trails in Urban Traffic", "conferenceName": "EuroVis", "authors": "Wei Zeng;Q. Shen;Y. Jiang;A. Telea", "citationCount": "1", "affiliation": "Shen, Q (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.\nZeng, W., Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China.\nShen, Q.; Jiang, Y., Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.\nTelea, A., Univ Groningen, Groningen, Netherlands.", "countries": "China;Netherlands", "abstract": "Origin-destination (OD) trails describe movements across space. Typical visualizations thereof use either straight lines or plot the actual trajectories. To reduce clutter inherent to visualizing large OD datasets, bundling methods can be used. Yet, bundling OD trails in urban traffic data remains challenging. Two specific reasons hereof are the constraints implied by the underlying road network and the difficulty of finding good bundling settings. To cope with these issues, we propose a new approach called Route Aware Edge Bundling (RAEB). To handle road constraints, we first generate a hierarchical model of the road-and-trajectory data. Next, we derive optimal bundling parameters, including kernel size and number of iterations, for a user-selected level of detail of this model, thereby allowing users to explicitly trade off simplification vs accuracy. We demonstrate the added value of RAEB compared to state-of-the-art trail bundling methods on both synthetic and real-world traffic data for tasks that include the preservation of road network topology and the support of multiscale exploration.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13712", "refList": ["10.1109/tvcg.2008.135", "10.1145/2833165.2833168", "10.1111/cgf.13213", "10.1109/mcg.2011.88", "10.1109/tvcg.2013.226", "10.1109/tvcg.2010.44", "10.1145/1653771.1653820", "10.1109/tvcg.2015.2468111", "10.1109/tst.2013.6509098", "10.1111/j.1467-8659.2009.01440.x", "10.1111/cgf.12922", "10.1109/tvcg.2018.2816219", "10.1109/vast.2014.7042486", "10.1198/tas.2009.0033", "10.1111/cgf.12132", "10.1111/j.1467-8659.2009.01700.x", "10.1016/j.trc.2007.05.002", "10.1007/978-3-540-72680-7\\_22", "10.1109/tvcg.2011.233", "10.1109/tvcg.2014.2346271", "10.1109/tvcg.2016.2515611", "10.1109/infvis.2005.1532150", "10.1109/tvcg.2016.2598472", "10.1109/tvcg.2006.147", "10.1109/tvcg.2016.2598958", "10.1109/tits.2017.2683539", "10.1109/tits.2015.2436897", "10.1111/j.1467-8659.2012.03079.x", "10.1109/tvcg.2017.2744338", "10.1109/pacificvis.2017.8031594", "10.1179/000870410x12658023467367", "10.1057/palgrave.ivs.9500182", "10.1109/tvcg.2011.104", "10.1109/tvcg.2011.190", "10.1109/tvcg.2013.114", "10.1111/cgf.12778", "10.1109/tvcg.2015.2467112", "10.1111/cgf.12107", "10.1109/iv.2010.53", "10.1109/42.563664", "10.1109/tvcg.2017.2666146", "10.1145/2530531", "10.1111/j.1467-8659.2009.01450.x", "10.4028/www.scientific.net/kem.342-343.593", "10.1109/tvcg.2011.202", "10.1038/srep00612"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030410", "title": "Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Wei Zeng 0002;Chengqiao Lin;Juncong Lin;Jincheng Jiang;Jiazhi Xia;Cagatay Turkay;Wei Chen", "citationCount": "0", "affiliation": "Lin, JC (Corresponding Author), Xiamen Univ, Xiamen, Peoples R China. Zeng, Wei; Jiang, Jincheng, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Lin, Chengqiao; Lin, Juncong, Xiamen Univ, Xiamen, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Turkay, Cagatay, Univ Warwick, Coventry, W Midlands, England. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China.", "countries": "China;England", "abstract": "Deep learning methods are being increasingly used for urban traffic prediction where spatiotemporal traffic data is aggregated into sequentially organized matrices that are then fed into convolution-based residual neural networks. However, the widely known modifiable areal unit problem within such aggregation processes can lead to perturbations in the network inputs. This issue can significantly destabilize the feature embeddings and the predictions - rendering deep networks much less useful for the experts. This paper approaches this challenge by leveraging unit visualization techniques that enable the investigation of many-to-many relationships between dynamically varied multi-scalar aggregations of urban traffic data and neural network predictions. Through regular exchanges with a domain expert, we design and develop a visual analytics solution that integrates 1) a Bivariate Map equipped with an advanced bivariate colormap to simultaneously depict input traffic and prediction errors across space, 2) a Moran's I Scatterplot that provides local indicators of spatial association analysis, and 3) a Multi-scale Attribution View that arranges non-linear dot plots in a tree layout to promote model analysis and comparison across scales. We evaluate our approach through a series of case studies involving a real-world dataset of Shenzhen taxi trips, and through interviews with domain experts. We observe that geographical scale variations have important impact on prediction performances, and interactive visual exploration of dynamically varying inputs and outputs benefit experts in the development of deep traffic prediction models.", "keywords": "MAUP,traffic prediction,deep learning,model diagnostic,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030410", "refList": ["10.1038/srep26377", "10.1109/mcg.2011.88", "10.1080/13658816.2015.1119279", "10.1109/tvcg.2013.226", "10.1109/pacificvis.2011.5742387", "10.1038/s41467-017-01882-w", "10.1109/tvcg.2019.2934670", "10.1111/j.1467-8659.2009.01440.x", "10.1111/cgf.13712", "10.1016/j.compenvurbsys.2008.09.006", "10.1109/pacificvis.2014.50", "10.1109/tvcg.2018.2816219", "10.1109/tvcg.2016.2535234", "10.1109/tvcg.2014.2346893", "10.3390/ijgi8080344", "10.1109/tvcg.2013.246", "10.1007/s10940-005-9003-6", "10.1016/j.compenvurbsys.2008.05.001", "10.1007/s10661-019-7831-3", "10.1111/j.1538-4632.2007.00699.x", "10.1016/j.aap.2016.08.015", "10.1080/13658816.2018.1541177", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2011.181", "10.1137/090759069", "10.1109/pacificvis.2011.5742390", "10.1214/10-aos799", "10.1109/tits.2017.2683539", "10.1109/tits.2015.2436897", "10.3390/ijerph16071150", "10.1109/tvcg.2009.145", "10.1109/tvcg.2012.265", "10.1080/10106049.2017.1404140", "10.3390/ijgi8020063", "10.3390/info6020134", "10.1080/13658816.2014.955027", "10.1109/tits.2016.2639320", "10.2307/143141", "10.1109/tvcg.2016.2598432"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030469", "title": "Topology Density Map for Urban Data Visualization and Analysis", "year": "2020", "conferenceName": "VAST", "authors": "Zezheng Feng;Haotian Li;Wei Zeng 0004;Shuang-Hua Yang;Huamin Qu", "citationCount": "0", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Feng, Zezheng; Li, Haotian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yang, Shuang-Hua, Southern Univ Sci \\& Technol, Shenzhen, Peoples R China.", "countries": "China", "abstract": "Density map is an effective visualization technique for depicting the scalar field distribution in 2D space. Conventional methods for constructing density maps are mainly based on Euclidean distance, limiting their applicability in urban analysis that shall consider road network and urban traffic. In this work, we propose a new method named Topology Density Map, targeting for accurate and intuitive density maps in the context of urban environment. Based on the various constraints of road connections and traffic conditions, the method first constructs a directed acyclic graph (DAG) that propagates nonlinear scalar fields along 1D road networks. Next, the method extends the scalar fields to a 2D space by identifying key intersecting points in the DAG and calculating the scalar fields for every point, yielding a weighted Voronoi diagram like effect of space division. Two case studies demonstrate that the Topology Density Map supplies accurate information to users and provides an intuitive visualization for decision making. An interview with domain experts demonstrates the feasibility, usability, and effectiveness of our method.", "keywords": "Density map,network topology,urban data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030469", "refList": ["10.1109/vast.2009.5332584", "10.1109/tvcg.2013.193", "10.1080/03081060.2013.844903", "10.1109/tvcg.2018.2864503", "10.1145/2702123.2702419", "10.1109/tvcg.2019.2934670", "10.1109/tits.2015.2496783", "10.1177/1473871615581216", "10.3141/1617-02", "10.1145/2024156.2024169", "10.1111/cgf.13712", "10.1016/j.ejor.2007.02.005", "10.1109/tvcg.2014.2346893", "10.1007/11871842\\_29", "10.1109/vast.2010.5652478", "10.1016/j.visinf.2019.10.002", "10.1109/tvcg.2016.2616404", "10.1109/vl.1996.545307", "10.1145/2629592", "10.1155/2018/2696037", "10.1061/(asce)0733-947x(1998)124:4(368", "10.3141/1899-21", "10.1023/a:1026123329433", "10.1109/mcg.2010.79", "10.1057/palgrave.ivs.9500174", "10.1109/tcyb.2019.2963681", "10.1109/tvcg.2015.2467554", "10.1111/cgf.12114", "10.1145/2814575", "10.1016/j.jcps.2014.08.002", "10.1109/2945.981847", "10.1080/03052150210909", "10.1109/tciaig.2012.2186810", "10.1109/tits.2017.2683539", "10.1109/iv.2004.1320137", "10.1016/0377-2217(80)90126-5", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2015.2467196", "10.1145/3097983.3098056", "10.1007/s11432-018-9801-4", "10.1109/vast.2014.7042490", "10.1061/(asce)0733-947x(2006)132:2(122", "10.1016/j.tra.2008.03.011", "10.1109/tits.2014.2298892", "10.1016/j.trb.2005.12.003", "10.1007/bf01840357", "10.1109/vast.2011.6102454", "10.1109/tvcg.2013.145", "10.1007/bf02289588", "10.1109/pacificvis.2014.56", "10.1109/mcg.2018.053491730", "10.1109/tvcg.2009.111", "10.1057/palgrave.ivs.9500184", "10.1109/tvcg.2013.173", "10.1109/tvcg.2016.2598432", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 49}, {"doi": "10.1109/tvcg.2019.2934805", "title": "Interactive Structure-aware Blending of Diverse Edge Bundling Visualizations", "year": "2019", "conferenceName": "InfoVis", "authors": "Yunhai Wang;Mingliang Xue;Yanyan Wang;Xinyuan Yan;Baoquan Chen;Chi-Wing Fu;Christophe Hurter", "citationCount": "1", "affiliation": "Fu, CW (Corresponding Author), Chinese Univ Hong Kong, Hong Kong, Peoples R China. Fu, CW (Corresponding Author), SIAT, Guangdong Prov Key Lab CV \\& VR Tech, Guangzhou, Peoples R China. Wang, Yunhai; Xue, Mingliang; Wang, Yanyan; Yan, Xinyuan, Shandong Univ, Jinan, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China. Fu, Chi-Wing, SIAT, Guangdong Prov Key Lab CV \\& VR Tech, Guangzhou, Peoples R China. Hurter, Christophe, ENAC, Toulouse, France.", "countries": "China;France", "abstract": "Many edge bundling techniques (i.e., data simplification as a support for data visualization and decision making) exist but they are not directly applicable to any kind of dataset and their parameters are often too abstract and difficult to set up. As a result, this hinders the user ability to create efficient aggregated visualizations. To address these issues, we investigated a novel way of handling visual aggregation with a task-driven and user-centered approach. Given a graph, our approach produces a decluttered view as follows: first, the user investigates different edge bundling results and specifies areas, where certain edge bundling techniques would provide user-desired results. Second, our system then computes a smooth and structural preserving transition between these specified areas. Lastly, the user can further fine-tune the global visualization with a direct manipulation technique to remove the local ambiguity and to apply different visual deformations. In this paper, we provide details for our design rationale and implementation. Also, we show how our algorithm gives more suitable results compared to current edge bundling techniques, and in the end, we provide concrete instances of usages, where the algorithm combines various edge bundling results to support diverse data exploration and visualizations.", "keywords": "path visualization,trajectory visualization,edge bundles", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934805", "refList": ["10.1111/cgf.13213", "10.1145/2254556.2254652", "10.1111/cgf.12791", "10.1111/j.1467-8659.2011.01898.x", "10.1111/cgf.12871", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1177/1473871617738122", "10.1111/j.1467-8659.2009.01700.x", "10.2312/eurovisstar.20141172", "10.1109/tvcg.2011.233", "10.1016/0020-0190(89)90102-6", "10.2200/s00688ed1v01y201512vis006", "10.3390/informatics4030026", "10.1109/tvcg.2017.2745919", "10.1111/j.1467-8659.2009.01680.x", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2016.2515611", "10.1111/cgf.13176", "10.1109/tvcg.2006.147", "10.1145/2992154.2992168", "10.1109/tvcg.2016.2598958", "10.1109/tvcg.2010.269", "10.1002/spe.4380211102", "10.1109/pacificvis.2017.8031596", "10.1109/tvcg.2011.223", "10.1145/1168149.1168168", "10.1006/s1045-926x(02)00016-2", "10.2312/vissym/vissym04/221-230", "10.1111/j.1467-8659.2012.03079.x", "10.1016/j.jvlc.2017.09.004", "10.3390/e20090625", "10.1109/tvcg.2014.2346441", "10.1109/tvcg.2017.2744338", "10.1109/pacificvis.2017.8031594", "10.1109/tvcg.2011.104", "10.1007/978-3-319-06793-310", "10.1109/pacificvis.2011.5742389", "10.1111/j.1467-8659.2009.01449.x", "10.1371/journal.pone.0098679", "10.1109/pacificvis.2015.7156354", "10.7155/jgaa.00370", "10.1109/pacificvis.2013.6596147", "10.1109/tvcg.2006.156", "10.1111/j.1467-8659.2009.01450.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934657", "title": "OD Morphing: Balancing Simplicity with Faithfulness for OD Bundling", "year": "2019", "conferenceName": "VAST", "authors": "Yan Lyu;Xu Liu;Hanyi Chen;Arpan Mangal;Kai Liu;Chao Chen 0004;Brian Y. Lim", "citationCount": "0", "affiliation": "Lyu, Y (Corresponding Author), Natl Univ Singapore, Singapore, Singapore. Lyu, Yan; Lim, Brian, Natl Univ Singapore, Singapore, Singapore. Liu, Xu, Southeast Univ, Nanjing, Peoples R China. Chen, Hanyi, Zhejiang Univ, Hangzhou, Peoples R China. Mangal, Arpan, Indian Inst Technol, Delhi, India. Liu, Kai; Chen, Chao, Chongqing Univ, Chongqing, Peoples R China.", "countries": "India;China;Singapore", "abstract": "OD bundling is a promising method to identify key origin-destination (OD) patterns, but the bundling can mislead the interpretation of actual trajectories traveled. We present OD Morphing, an interactive OD bundling technique that improves geographical faithfulness to actual trajectories while preserving visual simplicity for OD patterns. OD Morphing iteratively identifies critical waypoints from the actual trajectory network with a min-cut algorithm and transitions OD bundles to pass through the identified waypoints with a smooth morphing method. Furthermore, we extend OD Morphing to support bundling at interaction speeds to enable users to interactively transition between degrees of faithfulness to aid sensemaking. We introduce metrics for faithfulness and simplicity to evaluate their trade-off achieved by OD morphed bundling. We demonstrate OD Morphing on real-world city-scale taxi trajectory and USA domestic planned flight datasets.", "keywords": "OD Visualization,Edge Bundling,Trajectory", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934657", "refList": ["10.1109/tvcg.2008.135", "10.1145/2833165.2833168", "10.1111/cgf.13213", "10.1109/tvcg.2018.2864503", "10.1109/tvcg.2010.44", "10.1145/1653771.1653820", "10.1016/j.comgeo.2015.10.005", "10.1109/tvcg.2014.2337333", "10.1016/j.sbspro.2014.12.218", "10.1109/tvcg.2016.2535234", "10.1109/pacificvis.2011.5742386", "10.1145/321694.321699", "10.1109/tvcg.2013.246", "10.1111/cgf.12881", "10.1111/j.1467-8659.2009.01700.x", "10.1109/tvcg.2015.2467771", "10.1109/tvcg.2011.233", "10.1109/infvis.2003.1249008", "10.1109/itsc.2011.6082850", "10.1111/j.1467-8659.2009.01680.x", "10.1109/tvcg.2014.2346271", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2016.2515611", "10.1016/j.physa.2006.12.003", "10.1147/sj.41.0025", "10.1007/978-3-642-00304-2\\_1", "10.1109/tvcg.2006.147", "10.1109/tvcg.2016.2598958", "10.1109/tpami.2004.60", "10.1109/tvcg.2011.223", "10.1109/tits.2018.2817282", "10.1111/j.1467-8659.2012.03079.x", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2017.2744338", "10.1109/pacificvis.2017.8031594", "10.1179/000870410x12658023467367", "10.1145/2030112.2030126", "10.1109/tvcg.2011.104", "10.1145/2133416.2146416", "10.1109/tvcg.2011.190", "10.1142/s0218195995000064", "10.1002/cnm.1630040603", "10.1109/pacificvis.2011.5742389", "10.1109/pacificvis.2015.7156354", "10.1111/cgf.12778", "10.1109/dsnw.2011.5958797", "10.1109/cvprw.2008.4563095", "10.1109/tvcg.2016.2598885", "10.1109/tvcg.2017.2744322", "10.1111/j.1467-8659.2009.01450.x", "10.1109/tvcg.2016.2598416", "10.1109/cce.2012.6315867"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13728", "year": "2019", "title": "The State of the Art in Visualizing Multivariate Networks", "conferenceName": "EuroVis", "authors": "Carolina Nobre;Miriah D. Meyer;Marc Streit;Alexander Lex", "citationCount": "5", "affiliation": "Nobre, C (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA.\nNobre, C.; Meyer, M.; Lex, A., Univ Utah, Salt Lake City, UT 84112 USA.\nStreit, M., Johannes Kepler Univ Linz, Linz, Austria.", "countries": "USA;Austria", "abstract": "Multivariate networks are made up of nodes and their relationships (links), but also data about those nodes and links as attributes. Most real-world networks are associated with several attributes, and many analysis tasks depend on analyzing both, relationships and attributes. Visualization of multivariate networks, however, is challenging, especially when both the topology of the network and the attributes need to be considered concurrently. In this state-of-the-art report, we analyze current practices and classify techniques along four axes: layouts, view operations, layout operations, and data operations. We also provide an analysis of tasks specific to multivariate networks and give recommendations for which technique to use in which scenario. Finally, we survey application areas and evaluation methodologies.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13728", "refList": ["10.2312/eurovisstar.20151110", "10.1111/cgf.12106", "10.1109/iv.2016.19", "10.1111/j.1467-8659.2011.02087.x", "10.1111/j.1467-8659.2008.01214.x", "10.1109/tvcg.2014.2346893", "10.1145/1124772.1124891", "10.1109/vast.2014.7042484", "10.1109/tvcg.2018.2865149", "10.1080/10447318.2010.516722", "10.1109/icsmc.2011.6084125", "10.1109/mcg.2011.103", "10.1117/12.378894", "10.1145/2207676.2208293", "10.1109/tvcg.2009.122", "10.1111/cgf.12935", "10.1016/s0020-0255(02)00191-3", "10.1038/nmeth.1436", "10.1109/tvcg.2011.187", "10.1109/32.177365", "10.1007/978-3-642-03658-3\\_47", "10.1109/tvcg.2006.147", "10.1109/pacificvis.2011.5742390", "10.1145/2470654.2466444", "10.1109/tvcg.2006.166", "10.1145/2470654.2470724", "10.1109/tvcg.2014.2346441", "10.1109/vizsec.2005.1532070", "10.1101/gr.092759.109", "10.1111/cgf.13184", "10.1109/tvcg.2018.2865940", "10.1016/j.scico.2012.05.002", "10.1109/biovis.2012.6378600", "10.1111/cgf.13213", "10.1109/noms.2006.1687547", "10.1109/tvcg.2008.141", "10.1007/978-3-540-78243-8\\_13", "10.1145/1029208.1029217", "10.1145/345513.345271", "10.1109/tvcg.2015.2467811", "10.1109/visual.1991.175815", "10.1186/1471-2105-10-375", "10.1007/978-3-319-06793-3\\_1", "10.1186/1471-2105-13-275", "10.1109/tvcg.2010.79", "10.1109/tvcg.2011.217", "10.1186/1471-2105-14-s19-s3", "10.3389/fmicb.2017.00010", "10.1109/infvis.2003.1249009", "10.1111/cgf.13187", "10.1111/j.1467-8659.2008.01231.x", "10.1109/tvcg.2011.144", "10.1145/2556288.2557010", "10.1111/j.1467-8659.2009.01710.x", "10.1109/infvis.2004.46", "10.1109/iv.2009.97", "10.1109/tvcg.2008.34", "10.1109/infvis.2003.1249011", "10.1109/iv.2016.41", "10.1109/pacificvis.2010.5429609", "10.1109/tvcg.2006.160", "10.1109/tvcg.2010.205", "10.1109/38.486685", "10.1109/tvcg.2006.106", "10.1057/palgrave.ivs.9500092", "10.1145/568522.568523", "10.1111/j.1467-8659.2008.01221.x", "10.1109/iv.2010.15", "10.1145/22339.22342", "10.1109/csmr.2009.17", "10.1109/tvcg.2017.2744898", "10.1109/tvcg.2008.117", "10.1186/1752-0509-3-82", "10.1109/tvcg.2008.61", "10.1109/pacificvis.2013.6596127", "10.1109/tvcg.2007.70529", "10.1186/1471-2105-15-198", "10.1109/tvcg.2009.128", "10.2312/eurovisshort.20151124", "10.1093/bioinformatics/btq675", "10.1109/tvcg.2009.143", "10.1109/tvcg.2014.2346752", "10.1109/tvcg.2013.124", "10.1007/s00450-007-0036-y", "10.1093/bioinformatics/17.suppl\\_1.s22", "10.2312/eurovisstar.20151109", "10.1007/978-3-319-06793-3\\_5", "10.1109/vissof.2007.4290706", "10.1109/tvcg.2009.145", "10.1109/mcas.2003.1228503", "10.1109/infvis.2005.1532128", "10.1145/989863.989941", "10.1109/infvis.1999.801860", "10.1111/j.1467-8659.2009.01687.x", "10.1109/tvcg.2013.120", "10.1109/tvcg.2007.70582", "10.1109/tvcg.2009.167", "10.1109/pacificvis.2010.5429590", "10.1109/infvis.2000.885091", "10.1109/tvcg.2012.189", "10.1109/tvcg.2009.108", "10.1109/tvcg.2013.154", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2010.159", "10.1016/s0306-4573(98)00024-7", "10.1109/tvcg.2013.223", "10.1002/sam.10071", "10.2312/vissym/eurovis07/083-090", "10.1109/infvis.2002.1173156", "10.1186/1471-2105-7-109", "10.1177/1473871612455983", "10.1073/pnas.95.25.14863", "10.1007/978-3-319-06793-3\\_2", "10.1111/cgf.12883", "10.1007/978-3-540-78243-8\\_9", "10.1057/palgrave.ivs.9500180", "10.1109/pacificvis.2012.6183556", "10.1117/12.872578", "10.1145/1168149.1168169", "10.1109/iv.2013.3", "10.1109/tvcg.2011.186", "10.2307/2685881", "10.1109/pacificvis.2015.7156354", "10.1109/tvcg.2016.2598885", "10.1109/tvcg.2018.2811488", "10.1109/infvis.2005.1532129", "10.1111/j.1467-8659.2009.01450.x", "10.1057/palgrave.ivs.9500162", "10.1109/tvcg.2009.116", "10.1109/2945.468391"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3029413", "title": "A Design Space of Vision Science Methods for Visualization Research", "year": "2020", "conferenceName": "InfoVis", "authors": "Madison A. Elliott;Christine Nothelfer;Cindy Xiong;Danielle Albers Szafir", "citationCount": "0", "affiliation": "Elliott, MA (Corresponding Author), Univ British Columbia, Vancouver, BC, Canada. Elliott, Madison A., Univ British Columbia, Vancouver, BC, Canada. Nothelfer, Christine, Northwestern Univ, Evanston, IL 60208 USA. Xiong, Cindy, Univ Massachusetts, Amherst, MA 01003 USA. Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "Canada;USA", "abstract": "A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.", "keywords": "Perception,human vision,empirical research,evaluation,HCI", "link": "http://dx.doi.org/10.1109/TVCG.2020.3029413", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1177/0886109909354981", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1093/bioinformatics/btq110", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1177/1744987107081254", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1177/174498710501000305", "10.1017/s1049096513001789", "10.1109/tvcg.2012.213", "10.1093/nar/gkz239", "10.1093/sysbio/sys062", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030405", "title": "Insights From Experiments With Rigor in an EvoBio Design Study", "year": "2020", "conferenceName": "InfoVis", "authors": "Jennifer Rogers;Austin H. Patton;Luke Harmon;Alexander Lex;Miriah D. Meyer", "citationCount": "0", "affiliation": "Rogers, J (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Rogers, Jen; Lex, Alexander; Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Patton, Austin H., Washington State Univ, Pullman, WA 99164 USA. Harmon, Luke, Univ Idaho, Moscow, ID 83843 USA.", "countries": "USA", "abstract": "Design study is an established approach of conducting problem-driven visualization research. The academic visualization community has produced a large body of work for reporting on design studies, informed by a handful of theoretical frameworks, and applied to a broad range of application areas. The result is an abundance of reported insights into visualization design, with an emphasis on novel visualization techniques and systems as the primary contribution of these studies. In recent work we proposed a new, interpretivist perspective on design study and six companion criteria for rigor that highlight the opportunities for researchers to contribute knowledge that extends beyond visualization idioms and software. In this work we conducted a year-long collaboration with evolutionary biologists to develop an interactive tool for visual exploration of multivariate datasets and phylogenetic trees. During this design study we experimented with methods to support three of the rigor criteria: ABUNDANT, REFLEXIVE, and TRANSPARENT. As a result we contribute two novel visualization techniques for the analysis of multivariate phylogenetic datasets, three methodological recommendations for conducting design studies drawn from reflections over our process of experimentation, and two writing devices for reporting interpretivist design study. We offer this work as an example for implementing the rigor criteria to produce a diverse range of knowledge contributions.", "keywords": "Methodologies,Application Motivated Visualization,Guidelines,Life Sciences Visualization,Health,Medicine,Biology,Bioinformatics,Genomics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030405", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1080/17493460802276893", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1109/tvcg.2012.213", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13963", "year": "2020", "title": "MotionGlyphs: Visual Abstraction of Spatio-Temporal Networks in Collective Animal Behavior", "conferenceName": "EuroVis", "authors": "Eren Cakmak;Hanna Sch{\\\"{a}}fer;Juri Buchm{\\\"{u}}ller;Johannes Fuchs;Tobias Schreck;A. Jordan;Daniel A. Keim", "citationCount": "0", "affiliation": "Cakmak, E (Corresponding Author), Univ Konstanz, Constance, Germany.\nCakmak, E (Corresponding Author), Ctr Adv Study Collect Behav, Constance, Germany.\nCakmak, E.; Schaefer, H.; Buchmueller, J.; Fuchs, J.; Jordan, A.; Keim, D., Univ Konstanz, Constance, Germany.\nCakmak, E.; Jordan, A.; Keim, D., Ctr Adv Study Collect Behav, Constance, Germany.\nSchreck, T., Graz Univ Technol, Graz, Austria.\nJordan, A., Max Planck Inst Anim Behav, Radolfzell am Bodensee, Germany.", "countries": "Germany;Austria", "abstract": "Domain experts for collective animal behavior analyze relationships between single animal movers and groups of animals over time and space to detect emergent group properties. A common way to interpret this type of data is to visualize it as a spatio-temporal network. Collective behavior data sets are often large, and may hence result in dense and highly connected node-link diagrams, resulting in issues of node-overlap and edge clutter. In this design study, in an iterative design process, we developed glyphs as a design for seamlessly encoding relationships and movement characteristics of a single mover or clusters of movers. Based on these glyph designs, we developed a visual exploration prototype, MotionGlyphs, that supports domain experts in interactively filtering, clustering, and animating spatio-temporal networks for collective animal behavior analysis. By means of an expert evaluation, we show how MotionGlyphs supports important tasks and analysis goals of our domain experts, and we give evidence of the usefulness for analyzing spatio-temporal networks of collective animal behavior.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13963", "refList": ["10.1109/tvcg.2007.70582", "10.1111/cgf.13213", "10.1109/pacificvis.2014.13", "10.1145/2093973.2094038", "10.1111/j.1467-8659.2009.01664.x", "10.1109/tvcg.2010.44", "10.1111/cgf.12106", "10.1111/cgf.12791", "10.1080/15230406.2014.890071", "10.1111/tgis.12100", "10.1111/j.1467-8659.2009.01451.x", "10.1179/000870409x12525737905042", "10.1111/cgf.12923", "10.1006/ijhc.2002.1017", "10.1145/1124772.1124891", "10.1109/tvcg.2011.213", "10.1109/vast.2014.7042484", "10.1007/s12650-016-0375-5", "10.1109/vlhcc.2012.6344514", "10.1073/pnas.1420068112", "10.1006/ijhc.1017", "10.1007/s00371-017-1461-y", "10.1109/infvis.2003.1249008", "10.1111/cgf.13728", "10.2312/conf/eg2013/stars/039-063", "10.1109/tvcg.2014.2346271", "10.1109/hicss.2011.339", "10.1068/p3104", "10.1145/2931002.2931012", "10.1109/asonam.2012.39", "10.1179/000870403235002042", "10.1145/2556288.2557010", "10.1145/2470654.2466443", "10.1109/tvcg.2011.209", "10.1016/j.tree.2013.06.002", "10.1111/1365-2656.12418", "10.1111/cgf.12872", "10.1145/2470654.2466444", "10.1109/tvcg.2014.2322594", "10.1109/tvcg.2008.125", "10.1109/tvcg.2014.2346426", "10.1109/tvcg.2006.166", "10.1111/cgf.12615", "10.1057/palgrave.ivs.9500170", "10.1007/3-540-36151-0", "10.1117/12.872578", "10.1016/j.tics.2008.10.002", "10.1006/jtbi.2002.3065", "10.1109/tvcg.2010.78", "10.1109/iv.2013.3", "10.1073/pnas.1001763107", "10.1109/pacificvis.2015.7156354", "10.1371/journal.pbio.1001805", "10.5220/0005303801230130", "10.1111/j.1467-8659.2009.01687.x", "10.1007/s10844-011-0159-2", "10.1007/s12650-018-00543-4", "10.1145/2669557.2669572", "10.1111/cgf.13184", "10.1186/s40462-015-0032-y", "10.1016/j.ins.2016.06.048", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13987", "year": "2020", "title": "Augmenting Node-Link Diagrams with Topographic Attribute Maps", "conferenceName": "EuroVis", "authors": "Reinhold Preiner;Johanna Schmidt;Katharina Kr{\\\"{o}}sl;Tobias Schreck;Gabriel Mistelbauer", "citationCount": "0", "affiliation": "Preiner, R (Corresponding Author), Graz Univ Technol, Inst Comp Graph \\& Knowledge Visualizat, Graz, Austria.\nPreiner, R.; Schreck, T., Graz Univ Technol, Inst Comp Graph \\& Knowledge Visualizat, Graz, Austria.\nSchmidt, J.; Kroesl, K., Virtual Real \\& Visualisierung Forsch GmbH, VRVis Zentrum, Vienna, Austria.\nKroesl, K., TU Wien, Inst Visual Comp \\& Human Ctr Technol, Vienna, Austria.\nMistelbauer, G., Otto von Guericke Univ, Dept Simulat \\& Graph, Magdeburg, Germany.", "countries": "Germany;Austria", "abstract": "We propose a novel visualization technique for graphs that are attributed with scalar data. In many scenarios, these attributes (e.g., birth date in a family network) provide ambient context information for the graph structure, whose consideration is important for different visual graph analysis tasks. Graph attributes are usually conveyed using different visual representations (e.g., color, size, shape) or by reordering the graph structure according to the attribute domain (e.g., timelines). While visual encodings allow graphs to be arranged in a readable layout, assessing contextual information such as the relative similarities of attributes across the graph is often cumbersome. In contrast, attribute-based graph reordering serves the comparison task of attributes, but typically strongly impairs the readability of the structural information given by the graph's topology. In this work, we augment force-directed node-link diagrams with a continuous ambient representation of the attribute context. This way, we provide a consistent overview of the graph's topological structure as well as its attributes, supporting a wide range of graph-related analysis tasks. We resort to an intuitive height field metaphor, illustrated by a topographic map rendering using contour lines and suitable color maps. Contour lines visually connect nodes of similar attribute values, and depict their relative arrangement within the global context. Moreover, our contextual representation supports visualizing attribute value ranges associated with graph nodes (e.g., lifespans in a family network) as trajectories routed through this height field. We discuss how user interaction with both the structural and the contextual information fosters exploratory graph analysis tasks. The effectiveness and versatility of our technique is confirmed in a user study and case studies from various application domains.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13987", "refList": ["10.1109/tvcg.2013.269", "10.1109/pacificvis.2010.5429590", "10.1073/pnas.0307654100", "10.1145/2505515.2505758", "10.1559/152304082783948286", "10.1109/pacificvis.2014.47", "10.1093/bioinformatics/btp432", "10.1111/j.1467-8659.2011.01898.x", "10.1111/cgf.12931", "10.1111/cgf.12880", "10.1109/tvcg.2014.2346422", "10.1111/j.1467-8659.2009.01706.x", "10.1109/tvcg.2016.2598795", "10.1111/cgf.12800", "10.1109/tvcg.2014.2315995", "10.1111/cgf.12656", "10.1111/cgf.13728", "10.1109/tvcg.2009.122", "10.1111/cgf.13211", "10.1109/tvcg.2007.70596", "10.1109/infvis.2002.1173152", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2003.1196007", "10.1109/infvis.2005.1532150", "10.1145/3243250.3243266", "10.1080/02693799008941549", "10.1371/journal.pone.0058779", "10.1109/infvis.1995.528686", "10.1111/cgf.12872", "10.1002/spe.4380211102", "10.1109/38.974518", "10.1145/3097983.3098130", "10.1002/aris.1440370106", "10.1145/1360612.1360691", "10.1109/mc.2016.145", "10.2307/3006914", "10.1111/j.1467-8659.2009.01683.x", "10.1145/1639714.1639784"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1111/cgf.13712", "year": "2019", "title": "Route-Aware Edge Bundling for Visualizing Origin-Destination Trails in Urban Traffic", "conferenceName": "EuroVis", "authors": "Wei Zeng;Q. Shen;Y. Jiang;A. Telea", "citationCount": "1", "affiliation": "Shen, Q (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.\nZeng, W., Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China.\nShen, Q.; Jiang, Y., Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.\nTelea, A., Univ Groningen, Groningen, Netherlands.", "countries": "China;Netherlands", "abstract": "Origin-destination (OD) trails describe movements across space. Typical visualizations thereof use either straight lines or plot the actual trajectories. To reduce clutter inherent to visualizing large OD datasets, bundling methods can be used. Yet, bundling OD trails in urban traffic data remains challenging. Two specific reasons hereof are the constraints implied by the underlying road network and the difficulty of finding good bundling settings. To cope with these issues, we propose a new approach called Route Aware Edge Bundling (RAEB). To handle road constraints, we first generate a hierarchical model of the road-and-trajectory data. Next, we derive optimal bundling parameters, including kernel size and number of iterations, for a user-selected level of detail of this model, thereby allowing users to explicitly trade off simplification vs accuracy. We demonstrate the added value of RAEB compared to state-of-the-art trail bundling methods on both synthetic and real-world traffic data for tasks that include the preservation of road network topology and the support of multiscale exploration.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13712", "refList": ["10.1109/tvcg.2008.135", "10.1145/2833165.2833168", "10.1111/cgf.13213", "10.1109/mcg.2011.88", "10.1109/tvcg.2013.226", "10.1109/tvcg.2010.44", "10.1145/1653771.1653820", "10.1109/tvcg.2015.2468111", "10.1109/tst.2013.6509098", "10.1111/j.1467-8659.2009.01440.x", "10.1111/cgf.12922", "10.1109/tvcg.2018.2816219", "10.1109/vast.2014.7042486", "10.1198/tas.2009.0033", "10.1111/cgf.12132", "10.1111/j.1467-8659.2009.01700.x", "10.1016/j.trc.2007.05.002", "10.1007/978-3-540-72680-7\\_22", "10.1109/tvcg.2011.233", "10.1109/tvcg.2014.2346271", "10.1109/tvcg.2016.2515611", "10.1109/infvis.2005.1532150", "10.1109/tvcg.2016.2598472", "10.1109/tvcg.2006.147", "10.1109/tvcg.2016.2598958", "10.1109/tits.2017.2683539", "10.1109/tits.2015.2436897", "10.1111/j.1467-8659.2012.03079.x", "10.1109/tvcg.2017.2744338", "10.1109/pacificvis.2017.8031594", "10.1179/000870410x12658023467367", "10.1057/palgrave.ivs.9500182", "10.1109/tvcg.2011.104", "10.1109/tvcg.2011.190", "10.1109/tvcg.2013.114", "10.1111/cgf.12778", "10.1109/tvcg.2015.2467112", "10.1111/cgf.12107", "10.1109/iv.2010.53", "10.1109/42.563664", "10.1109/tvcg.2017.2666146", "10.1145/2530531", "10.1111/j.1467-8659.2009.01450.x", "10.4028/www.scientific.net/kem.342-343.593", "10.1109/tvcg.2011.202", "10.1038/srep00612"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030410", "title": "Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Wei Zeng 0002;Chengqiao Lin;Juncong Lin;Jincheng Jiang;Jiazhi Xia;Cagatay Turkay;Wei Chen", "citationCount": "0", "affiliation": "Lin, JC (Corresponding Author), Xiamen Univ, Xiamen, Peoples R China. Zeng, Wei; Jiang, Jincheng, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Lin, Chengqiao; Lin, Juncong, Xiamen Univ, Xiamen, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Turkay, Cagatay, Univ Warwick, Coventry, W Midlands, England. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China.", "countries": "China;England", "abstract": "Deep learning methods are being increasingly used for urban traffic prediction where spatiotemporal traffic data is aggregated into sequentially organized matrices that are then fed into convolution-based residual neural networks. However, the widely known modifiable areal unit problem within such aggregation processes can lead to perturbations in the network inputs. This issue can significantly destabilize the feature embeddings and the predictions - rendering deep networks much less useful for the experts. This paper approaches this challenge by leveraging unit visualization techniques that enable the investigation of many-to-many relationships between dynamically varied multi-scalar aggregations of urban traffic data and neural network predictions. Through regular exchanges with a domain expert, we design and develop a visual analytics solution that integrates 1) a Bivariate Map equipped with an advanced bivariate colormap to simultaneously depict input traffic and prediction errors across space, 2) a Moran's I Scatterplot that provides local indicators of spatial association analysis, and 3) a Multi-scale Attribution View that arranges non-linear dot plots in a tree layout to promote model analysis and comparison across scales. We evaluate our approach through a series of case studies involving a real-world dataset of Shenzhen taxi trips, and through interviews with domain experts. We observe that geographical scale variations have important impact on prediction performances, and interactive visual exploration of dynamically varying inputs and outputs benefit experts in the development of deep traffic prediction models.", "keywords": "MAUP,traffic prediction,deep learning,model diagnostic,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030410", "refList": ["10.1038/srep26377", "10.1109/mcg.2011.88", "10.1080/13658816.2015.1119279", "10.1109/tvcg.2013.226", "10.1109/pacificvis.2011.5742387", "10.1038/s41467-017-01882-w", "10.1109/tvcg.2019.2934670", "10.1111/j.1467-8659.2009.01440.x", "10.1111/cgf.13712", "10.1016/j.compenvurbsys.2008.09.006", "10.1109/pacificvis.2014.50", "10.1109/tvcg.2018.2816219", "10.1109/tvcg.2016.2535234", "10.1109/tvcg.2014.2346893", "10.3390/ijgi8080344", "10.1109/tvcg.2013.246", "10.1007/s10940-005-9003-6", "10.1016/j.compenvurbsys.2008.05.001", "10.1007/s10661-019-7831-3", "10.1111/j.1538-4632.2007.00699.x", "10.1016/j.aap.2016.08.015", "10.1080/13658816.2018.1541177", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2011.181", "10.1137/090759069", "10.1109/pacificvis.2011.5742390", "10.1214/10-aos799", "10.1109/tits.2017.2683539", "10.1109/tits.2015.2436897", "10.3390/ijerph16071150", "10.1109/tvcg.2009.145", "10.1109/tvcg.2012.265", "10.1080/10106049.2017.1404140", "10.3390/ijgi8020063", "10.3390/info6020134", "10.1080/13658816.2014.955027", "10.1109/tits.2016.2639320", "10.2307/143141", "10.1109/tvcg.2016.2598432"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030469", "title": "Topology Density Map for Urban Data Visualization and Analysis", "year": "2020", "conferenceName": "VAST", "authors": "Zezheng Feng;Haotian Li;Wei Zeng 0004;Shuang-Hua Yang;Huamin Qu", "citationCount": "0", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Feng, Zezheng; Li, Haotian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yang, Shuang-Hua, Southern Univ Sci \\& Technol, Shenzhen, Peoples R China.", "countries": "China", "abstract": "Density map is an effective visualization technique for depicting the scalar field distribution in 2D space. Conventional methods for constructing density maps are mainly based on Euclidean distance, limiting their applicability in urban analysis that shall consider road network and urban traffic. In this work, we propose a new method named Topology Density Map, targeting for accurate and intuitive density maps in the context of urban environment. Based on the various constraints of road connections and traffic conditions, the method first constructs a directed acyclic graph (DAG) that propagates nonlinear scalar fields along 1D road networks. Next, the method extends the scalar fields to a 2D space by identifying key intersecting points in the DAG and calculating the scalar fields for every point, yielding a weighted Voronoi diagram like effect of space division. Two case studies demonstrate that the Topology Density Map supplies accurate information to users and provides an intuitive visualization for decision making. An interview with domain experts demonstrates the feasibility, usability, and effectiveness of our method.", "keywords": "Density map,network topology,urban data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030469", "refList": ["10.1109/vast.2009.5332584", "10.1109/tvcg.2013.193", "10.1080/03081060.2013.844903", "10.1109/tvcg.2018.2864503", "10.1145/2702123.2702419", "10.1109/tvcg.2019.2934670", "10.1109/tits.2015.2496783", "10.1177/1473871615581216", "10.3141/1617-02", "10.1145/2024156.2024169", "10.1111/cgf.13712", "10.1016/j.ejor.2007.02.005", "10.1109/tvcg.2014.2346893", "10.1007/11871842\\_29", "10.1109/vast.2010.5652478", "10.1016/j.visinf.2019.10.002", "10.1109/tvcg.2016.2616404", "10.1109/vl.1996.545307", "10.1145/2629592", "10.1155/2018/2696037", "10.1061/(asce)0733-947x(1998)124:4(368", "10.3141/1899-21", "10.1023/a:1026123329433", "10.1109/mcg.2010.79", "10.1057/palgrave.ivs.9500174", "10.1109/tcyb.2019.2963681", "10.1109/tvcg.2015.2467554", "10.1111/cgf.12114", "10.1145/2814575", "10.1016/j.jcps.2014.08.002", "10.1109/2945.981847", "10.1080/03052150210909", "10.1109/tciaig.2012.2186810", "10.1109/tits.2017.2683539", "10.1109/iv.2004.1320137", "10.1016/0377-2217(80)90126-5", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2015.2467196", "10.1145/3097983.3098056", "10.1007/s11432-018-9801-4", "10.1109/vast.2014.7042490", "10.1061/(asce)0733-947x(2006)132:2(122", "10.1016/j.tra.2008.03.011", "10.1109/tits.2014.2298892", "10.1016/j.trb.2005.12.003", "10.1007/bf01840357", "10.1109/vast.2011.6102454", "10.1109/tvcg.2013.145", "10.1007/bf02289588", "10.1109/pacificvis.2014.56", "10.1109/mcg.2018.053491730", "10.1109/tvcg.2009.111", "10.1057/palgrave.ivs.9500184", "10.1109/tvcg.2013.173", "10.1109/tvcg.2016.2598432", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13963", "year": "2020", "title": "MotionGlyphs: Visual Abstraction of Spatio-Temporal Networks in Collective Animal Behavior", "conferenceName": "EuroVis", "authors": "Eren Cakmak;Hanna Sch{\\\"{a}}fer;Juri Buchm{\\\"{u}}ller;Johannes Fuchs;Tobias Schreck;A. Jordan;Daniel A. Keim", "citationCount": "0", "affiliation": "Cakmak, E (Corresponding Author), Univ Konstanz, Constance, Germany.\nCakmak, E (Corresponding Author), Ctr Adv Study Collect Behav, Constance, Germany.\nCakmak, E.; Schaefer, H.; Buchmueller, J.; Fuchs, J.; Jordan, A.; Keim, D., Univ Konstanz, Constance, Germany.\nCakmak, E.; Jordan, A.; Keim, D., Ctr Adv Study Collect Behav, Constance, Germany.\nSchreck, T., Graz Univ Technol, Graz, Austria.\nJordan, A., Max Planck Inst Anim Behav, Radolfzell am Bodensee, Germany.", "countries": "Germany;Austria", "abstract": "Domain experts for collective animal behavior analyze relationships between single animal movers and groups of animals over time and space to detect emergent group properties. A common way to interpret this type of data is to visualize it as a spatio-temporal network. Collective behavior data sets are often large, and may hence result in dense and highly connected node-link diagrams, resulting in issues of node-overlap and edge clutter. In this design study, in an iterative design process, we developed glyphs as a design for seamlessly encoding relationships and movement characteristics of a single mover or clusters of movers. Based on these glyph designs, we developed a visual exploration prototype, MotionGlyphs, that supports domain experts in interactively filtering, clustering, and animating spatio-temporal networks for collective animal behavior analysis. By means of an expert evaluation, we show how MotionGlyphs supports important tasks and analysis goals of our domain experts, and we give evidence of the usefulness for analyzing spatio-temporal networks of collective animal behavior.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13963", "refList": ["10.1109/tvcg.2007.70582", "10.1111/cgf.13213", "10.1109/pacificvis.2014.13", "10.1145/2093973.2094038", "10.1111/j.1467-8659.2009.01664.x", "10.1109/tvcg.2010.44", "10.1111/cgf.12106", "10.1111/cgf.12791", "10.1080/15230406.2014.890071", "10.1111/tgis.12100", "10.1111/j.1467-8659.2009.01451.x", "10.1179/000870409x12525737905042", "10.1111/cgf.12923", "10.1006/ijhc.2002.1017", "10.1145/1124772.1124891", "10.1109/tvcg.2011.213", "10.1109/vast.2014.7042484", "10.1007/s12650-016-0375-5", "10.1109/vlhcc.2012.6344514", "10.1073/pnas.1420068112", "10.1006/ijhc.1017", "10.1007/s00371-017-1461-y", "10.1109/infvis.2003.1249008", "10.1111/cgf.13728", "10.2312/conf/eg2013/stars/039-063", "10.1109/tvcg.2014.2346271", "10.1109/hicss.2011.339", "10.1068/p3104", "10.1145/2931002.2931012", "10.1109/asonam.2012.39", "10.1179/000870403235002042", "10.1145/2556288.2557010", "10.1145/2470654.2466443", "10.1109/tvcg.2011.209", "10.1016/j.tree.2013.06.002", "10.1111/1365-2656.12418", "10.1111/cgf.12872", "10.1145/2470654.2466444", "10.1109/tvcg.2014.2322594", "10.1109/tvcg.2008.125", "10.1109/tvcg.2014.2346426", "10.1109/tvcg.2006.166", "10.1111/cgf.12615", "10.1057/palgrave.ivs.9500170", "10.1007/3-540-36151-0", "10.1117/12.872578", "10.1016/j.tics.2008.10.002", "10.1006/jtbi.2002.3065", "10.1109/tvcg.2010.78", "10.1109/iv.2013.3", "10.1073/pnas.1001763107", "10.1109/pacificvis.2015.7156354", "10.1371/journal.pbio.1001805", "10.5220/0005303801230130", "10.1111/j.1467-8659.2009.01687.x", "10.1007/s10844-011-0159-2", "10.1007/s12650-018-00543-4", "10.1145/2669557.2669572", "10.1111/cgf.13184", "10.1186/s40462-015-0032-y", "10.1016/j.ins.2016.06.048", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [], "len": 1}], "len": 79}], "len": 81}], "len": 105}, "index": 1302, "embedding": [-1.8784593343734741, 0.6817814707756042, -1.059349536895752, -2.3949873447418213, -0.6495316028594971, 0.16650904715061188, 1.3870497941970825, 1.2765285968780518, 2.1358888149261475, 1.4558751583099365, 0.017003212124109268, -0.5769720673561096, 0.8952110409736633, 1.0321780443191528, 1.0298465490341187, 0.7469233274459839, -0.20203587412834167, 0.07991757988929749, -0.14038807153701782, 3.0863943099975586, -0.06630208343267441, 3.020435094833374, 1.2496130466461182, 2.678956985473633, -2.1849491596221924, 4.840447425842285, -0.5399249196052551, -0.17674389481544495, 0.24017085134983063, -2.2096946239471436, 0.3289570212364197, 1.0236440896987915], "projection": [0.4047478437423706, 9.582368850708008], "size": 53, "height": 7, "width": 13}