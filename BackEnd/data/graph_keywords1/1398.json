{"data": {"doi": "10.1111/cgf.13180", "year": "2017", "title": "Integrating Visual Analytics Support for Grounded Theory Practice in Qualitative Text Analysis", "conferenceName": "EuroVis", "authors": "Senthil K. Chandrasegaran;Sriram Karthik Badam;Lorraine G. Kisselburgh;Karthik Ramani;Niklas Elmqvist", "citationCount": "8", "affiliation": "Chandrasegaran, S (Corresponding Author), Univ Maryland, Coll Informat Studies, College Pk, MD 20742 USA.\nChandrasegaran, S (Corresponding Author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.\nChandrasegaran, Senthil; Elmqvist, Niklas, Univ Maryland, Coll Informat Studies, College Pk, MD 20742 USA.\nChandrasegaran, Senthil; Elmqvist, Niklas, Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.\nKisselburgh, Lorraine, Purdue Univ, Ctr Entrepreneurship, W Lafayette, IN 47907 USA.\nRamani, Karthik, Purdue Univ, Sch Mech Engn, W Lafayette, IN 47907 USA.\nRamani, Karthik, Purdue Univ, Sch Elect \\& Comp Engn, W Lafayette, IN 47907 USA.", "countries": "USA", "abstract": "We present an argument for using visual analytics to aid Grounded Theory methodologies in qualitative data analysis. Grounded theory methods involve the inductive analysis of data to generate novel insights and theoretical constructs. Making sense of unstructured text data is uniquely suited for visual analytics. Using natural language processing techniques such as parts-of-speech tagging, retrieving information content, and topic modeling, different parts of the data can be structured and semantically associated, and interactively explored, thereby providing conceptual depth to the guided discovery process. We review grounded theory methods and identify processes that can be enhanced through visual analytic techniques. Next, we develop an interface for qualitative text analysis, and evaluate our design with qualitative research practitioners who analyze texts with and without visual analytics support. The results of our study suggest how visual analytics can be incorporated into qualitative data analysis tools, and the analytic and interpretive benefits that can result.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13180", "refList": ["10.1109/vast.2014.7042493", "10.1057/ivs.2009.10", "10.1007/bf00988593", "10.1109/vast.2009.5333443", "10.1109/tvcg.2015.2467759", "10.3115/1219840.1219885", "10.3115/v1/w14-3110", "10.1109/tvcg.2011.185", "10.1177/1525822x06287602", "10.1007/978-3-540-70956-5", "10.1177/1525822x02239569", "10.1145/219717.219748", "10.1145/1835804.1835827", "10.1111/j.1467-8659.2009.01439.x", "10.1109/tvcg.2013.162", "10.1109/infvis.2002.1173155", "10.1109/32.177365", "10.1109/tvcg.2014.2388208", "10.13140/2.1.2393.1847", "10.1109/tvcg.2007.70589", "10.1109/tvcg.2014.2346677", "10.1177/0894439315596311", "10.1057/palgrave.ivs.9500180", "10.1162/jmlr.2003.3.4-5.993", "10.3115/1073445.1073478", "10.1109/tvcg.2008.172"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865119", "title": "Elastic Documents: Coupling Text and Tables through Contextual Visualizations for Enhanced Document Reading", "year": "2018", "conferenceName": "InfoVis", "authors": "Sriram Karthik Badam;Zhicheng Liu;Niklas Elmqvist", "citationCount": "1", "affiliation": "Badam, SK (Corresponding Author), Univ Maryland, College Pk, MD 20742 USA. Badam, Sriram Karthik; Elmqvist, Niklas, Univ Maryland, College Pk, MD 20742 USA. Liu, Zhicheng, Adobe Res, Seattle, WA USA.", "countries": "USA", "abstract": "Today's data-rich documents are often complex datasets in themselves, consisting of information in different formats such as text, figures, and data tables. These additional media augment the textual narrative in the document. However, the static layout of a traditional for-print document often impedes deep understanding of its content because of the need to navigate to access content scattered throughout the text. In this paper, we seek to facilitate enhanced comprehension of such documents through a contextual visualization technique that couples text content with data tables contained in the document. We parse the text content and data tables, cross-link the components using a keyword-based matching algorithm, and generate on-demand visualizations based on the reader's current focus within a document. We evaluate this technique in a user study comparing our approach to a traditional reading experience. Results from our study show that (1) participants comprehend the content better with tighter coupling of text and data, (2) the contextual visualizations enable participants to develop better summaries that capture the main data-rich insights within the document, and (3) overall, our method enables participants to develop a more detailed understanding of the document content.", "keywords": "Document reading,contextual visualizations,visual aids,comprehension,summarization", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865119", "refList": ["10.1109/tvcg.2014.2346435", "10.1109/infvis.2000.885091", "10.1109/icdar.2009.12", "10.1145/2702613.2732501", "10.3322/caac.21332", "10.1145/2559206.2578881", "10.1145/2556288.2557228", "10.1109/tvcg.2016.2618797", "10.1007/978-3-319-16199-0\\_43", "10.1016/0749-596x(92)90008-l", "10.4300/jgme-d-12-00156.1", "10.3322/caac.21254", "10.1177/0956797613504966", "10.1109/tvcg.2011.185", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2009.139", "10.1109/tvcg.2009.165", "10.1177/0022057409189001-208", "10.14778/2536274.2536276", "10.1145/2556288.2557241", "10.1109/tvcg.2014.2346279", "10.1145/2702613.2732778", "10.1145/2623330.2623617", "10.1145/1993316.1993536", "10.1111/cgf.12104", "10.1109/tvcg.2016.2598594", "10.1145/3025453.3025631", "10.1145/102377.115768", "10.1111/cgf.13180", "10.1109/tvcg.2009.171", "10.1145/2501988.2502036", "10.1016/s0364-0213(87)80026-5", "10.3322/caac.21208", "10.20380/gi2017.06", "10.1109/mcg.2003.1231178", "10.1109/iccitechn.2016.7860200", "10.1145/2858036.2858430", "10.1109/tnn.2003.820440", "10.1109/tvcg.2011.255", "10.1145/1012037.1012063", "10.2312/eurovisstar.20151113", "10.1061/(asce)1527-6988(2008)9:1(29"], "wos": 1, "children": [], "len": 1}], "len": 3}, "index": 1398, "embedding": [-0.2571665942668915, 0.2544153928756714, 0.4363288879394531, 1.3710405826568604, -0.6195418834686279, 0.16650904715061188, 0.23334284126758575, -0.09250186383724213, -0.41535258293151855, -0.7390465140342712, -0.1741667091846466, -0.3079303503036499, -0.1268409788608551, 0.727498471736908, -0.5983787775039673, 0.5466946959495544, -0.20178551971912384, 0.06584581732749939, -0.6218824982643127, 1.3247367143630981, -0.06630208343267441, 0.6096858978271484, 0.1614578813314438, -0.12045589834451675, 0.23481203615665436, 0.10673608630895615, -0.4969606399536133, -0.16244997084140778, 0.7870776653289795, -0.547282874584198, -0.2729009687900543, 0.24579966068267822], "projection": [-3.0415594577789307, 6.204528331756592], "size": 2, "height": 2, "width": 1}