{"data": {"doi": "10.1109/tvcg.2015.2467871", "title": "VA2: A Visual Analytics Approach for Evaluating Visual Analytics Applications", "year": "2015", "conferenceName": "VAST", "authors": "Tanja Blascheck;Markus John;Kuno Kurzhals;Steffen Koch;Thomas Ertl", "citationCount": "33", "affiliation": "Blascheck, T (Corresponding Author), Univ Stuttgart, Inst Visualizat \\& Interact Syst VIS, Stuttgart, Germany. Kurzhals, Kuno, Univ Stuttgart, Visualizat Res Ctr, Stuttgart, Germany. Blascheck, Tanja; John, Markus; Koch, Steffen; Ertl, Thomas, Univ Stuttgart, Inst Visualizat \\& Interact Syst VIS, Stuttgart, Germany.", "countries": "Germany", "abstract": "Evaluation has become a fundamental part of visualization research and researchers have employed many approaches from the field of human-computer interaction like measures of task performance, thinking aloud protocols, and analysis of interaction logs. Recently, eye tracking has also become popular to analyze visual strategies of users in this context. This has added another modality and more data, which requires special visualization techniques to analyze this data. However, only few approaches exist that aim at an integrated analysis of multiple concurrent evaluation procedures. The variety, complexity, and sheer amount of such coupled multi-source data streams require a visual analytics approach. Our approach provides a highly interactive visualization environment to display and analyze thinking aloud, interaction, and eye movement data in close relation. Automatic pattern finding algorithms allow an efficient exploratory search and support the reasoning process to derive common eye-interaction-thinking patterns between participants. In addition, our tool equips researchers with mechanisms for searching and verifying expected usage patterns. We apply our approach to a user study involving a visual analytics application and we discuss insights gained from this joint analysis. We anticipate our approach to be applicable to other combinations of evaluation techniques and a broad class of visualization applications.", "keywords": "visual analytics, qualitative evaluation, thinking aloud, interaction logs, eye tracking, time series data", "link": "http://dx.doi.org/10.1109/TVCG.2015.2467871", "refList": ["10.1145/2669557.2669569", "10.1145/2509315.2509320", "10.1145/355017.355022", "10.1057/ivs.2009.22", "10.1016/0022-2836(70)90057-4", "10.1016/s0022-2836(05)80360-2", "10.1109/tvcg.2012.276", "10.1145/1842993.1843029", "10.1109/34.877520", "10.1007/978-3-540-70956-5\\_2", "10.1109/mcg.2009.53", "10.1006/ijhc.2000.0422", "10.1109/vast.2010.5653598", "10.1109/tvcg.2013.124", "10.1109/mcg.2009.49", "10.1109/mcg.2012.120", "10.1145/634067.634234", "10.1145/1117309.1117360", "10.1109/hicss.2014.231", "10.1179/000870403235002042", "10.1145/2168556/2168645", "10.1109/tvcg.2005.53", "10.1109/tvcg.2012.323", "10.1145/259963.260531", "10.1109/tvcg.2012.273", "10.1145/1377966.1377971", "10.1109/tvcg.2014.2346677", "10.1145/2525314.2525467", "10.1037/0033-295x.87.4.329", "10.1145/1168149.1168158", "10.1109/mcg.2006.70", "10.1109/vast.2009.5333878", "10.3758/brm.42.3.692", "10.1109/vast.2008.4677361", "10.1145/503112.503114", "10.1145/989863.989880", "10.2312/eurovisstar.20141173", "10.1145/2578153.2578173", "10.1109/mcg.2009.22", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2016.2598471", "title": "A Grammar-based Approach for Modeling User Interactions and Generating Suggestions During the Data Exploration Process", "year": "2016", "conferenceName": "VAST", "authors": "Filip Dabek;Jesus J. Caban", "citationCount": "11", "affiliation": "Dabek, F (Corresponding Author), Walter Reed Natl Mil Med Ctr, Natl Intrepid Ctr Excellence, Bethesda, MD 20889 USA. Dabek, Filip; Caban, Jesus J., Walter Reed Natl Mil Med Ctr, Natl Intrepid Ctr Excellence, Bethesda, MD 20889 USA.", "countries": "USA", "abstract": "Despite the recent popularity of visual analytics focusing on big data, little is known about how to support users that use visualization techniques to explore multi-dimensional datasets and accomplish specific tasks. Our lack of models that can assist end-users during the data exploration process has made it challenging to learn from the user's interactive and analytical process. The ability to model how a user interacts with a specific visualization technique and what difficulties they face are paramount in supporting individuals with discovering new patterns within their complex datasets. This paper introduces the notion of visualization systems understanding and modeling user interactions with the intent of guiding a user through a task thereby enhancing visual data exploration. The challenges faced and the necessary future steps to take are discussed; and to provide a working example, a grammar-based model is presented that can learn from user interactions, determine the common patterns among a number of subjects using a K-Reversible algorithm, build a set of rules, and apply those rules in the form of suggestions to new users with the goal of guiding them along their visual analytic process. A formal evaluation study with 300 subjects was performed showing that our grammar-based model is effective at capturing the interactive process followed by users and that further research in this area has the potential to positively impact how users interact with a visualization system.", "keywords": "Machine Learning;Visual Analytics;User Interactions;Analytic Provenance", "link": "http://dx.doi.org/10.1109/TVCG.2016.2598471", "refList": ["10.1109/tvcg.2014.2346575", "10.1016/s0019-9958(67)91165-5", "10.1109/visual.2005.1532788", "10.1109/tvcg.2015.2467613", "10.1080/01449290512331333700", "10.1145/1502650.1502695", "10.1109/tvcg.2012.219", "10.1145/643477.643478", "10.1109/tvcg.2011.108", "10.1109/tvcg.2015.2467871", "10.1111/j.0956-7976.2005.00782.x", "10.1145/1378773.1378788", "10.1109/tvcg.2015.2467611", "10.1109/mcg.2009.49", "10.1214/aos/1176344136", "10.1109/tvcg.2015.2467551", "10.1007/s11665-016-2173-6", "10.1109/tvcg.2012.323", "10.1109/vast.2009.5333020", "10.1109/tvcg.2007.70589", "10.1109/vast.2010.5650854", "10.1016/s0169-023x(03)00066-1", "10.1109/tkde.2004.47", "10.1109/5254.796083", "10.1142/9789812797919\\_0007", "10.1109/vast.2008.4677365", "10.1207/s15327051hci0903", "10.1109/hicss.2005.286", "10.1145/322326.322334"], "wos": 1, "children": [{"doi": "10.1109/vast.2017.8585498", "title": "The Role of Explicit Knowledge: A Conceptual Model of Knowledge-Assisted Visual Analytics", "year": "2017", "conferenceName": "VAST", "authors": "Paolo Federico;Markus Wagner 0008;Alexander Rind;Albert Amor-Amoros;Silvia Miksch;Wolfgang Aigner", "citationCount": "3", "affiliation": "Federico, P (Corresponding Author), TU Wien, Vienna, Austria. Federico, Paolo; Wagner, Markus; Rind, Alexander; Amor-Amoros, Albert; Miksch, Silvia; Aigner, Wolfgang, TU Wien, Vienna, Austria. Wagner, Markus; Rind, Alexander; Aigner, Wolfgang, St Poelten Univ Appl Sci, Sankt Polten, Austria.", "countries": "Austria", "abstract": "Visual Analytics (VA) aims to combine the strengths of humans and computers for effective data analysis. In this endeavor, humans' tacit knowledge from prior experience is an important asset that can be leveraged by both human and computer to improve the analytic process. While VA environments are starting to include features to formalize, store, and utilize such knowledge, the mechanisms and degree in which these environments integrate explicit knowledge varies widely. Additionally, this important class of VA environments has never been elaborated on by existing work on VA theory. This paper proposes a conceptual model of Knowledge-assisted VA conceptually grounded on the visualization model by van Wijk. We apply the model to describe various examples of knowledge-assisted VA from the literature and elaborate on three of them in finer detail. Moreover, we illustrate the utilization of the model to compare different design alternatives and to evaluate existing approaches with respect to their use of knowledge. Finally, the model can inspire designers to generate novel VA environments using explicit knowledge effectively.", "keywords": "Automated analysis,tacit knowledge,explicit knowledge,visual analytics,information visualization,theory and model", "link": "http://dx.doi.org/10.1109/VAST.2017.8585498", "refList": ["10.1016/j.artmed.2006.03.001", "10.1057/ivs.2009.22", "10.1109/infvis.2000.885092", "10.2312/pe/eurovast/eurova11/009-012", "10.1109/tvcg.2016.2598839", "10.1109/tvcg.2014.2346575", "10.1111/cgf.13169", "10.1186/1471-2105-13-s8-s3", "10.1007/s00371-015-1132-9", "10.1109/tvcg.2013.146", "10.1109/tvcg.2016.2598471", "10.1016/j.cag.2009.06.004", "10.1145/2993901.2993915", "10.1111/cgf.12090", "10.1177/1473871611412817", "10.1145/2598153.2598172", "10.1016/j.cag.2009.06.006", "10.1109/tvcg.2016.2598460", "10.1016/j.autcon.2014.03.012", "10.1145/989863.989865", "10.1109/21.44068", "10.1057/palgrave.ivs.9500045", "10.2312/eurova.20151108", "10.1002/9781444303179.ch3", "10.1109/hicss.2016.183", "10.1177/0165551506070706", "10.1145/2494188.2494202", "10.1145/1556262.1556327", "10.1016/j.artmed.2006.04.002", "10.1007/978-3-540-71080-6\\_6", "10.1111/j.1467-8659.2008.01230.x", "10.1109/pacificvis.2011.5742371", "10.1109/vast.2014.7042530", "10.1109/mcg.2010.8", "10.1109/mcg.2015.25", "10.1109/vast.2012.6400555", "10.1109/tvcg.2014.2346481", "10.1145/2836034.2836040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2008.109", "10.1016/j.artmed.2005.10.003", "10.1109/mcg.2014.73", "10.1016/s0004-3702(96)00025-2", "10.1093/intqhc/mzm007", "10.1016/j.dss.2012.06.009", "10.1016/j.cose.2017.02.003", "10.1109/infvis.1998.729560", "10.1109/vast.2010.5654451", "10.1109/tvcg.2016.2598829", "10.1109/vast.2007.4389021", "10.1145/1562849.1562851", "10.1145/302979.303030", "10.1145/985692.985706", "10.1109/infvis.1997.636792", "10.1111/j.1467-8659.2012.03092.x", "10.1109/mcg.2009.6", "10.1057/ivs.2008.28", "10.1109/mcg.2005.91", "10.1016/j.artmed.2010.02.001", "10.1177/0272989x14565822", "10.1109/mcg.2010.15", "10.1007/s10844-014-0304-9", "10.2312/pe.eurovast.eurova13.043-047", "10.1109/mcg.2014.33", "10.1109/tvcg.2014.2346574", "10.1111/j.1467-8659.2009.01708.x", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934654", "title": "Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections", "year": "2019", "conferenceName": "VAST", "authors": "Mennatallah El-Assady;Rebecca Kehlbeck;Christopher Collins;Daniel A. Keim;Oliver Deussen", "citationCount": "3", "affiliation": "El-Assady, M (Corresponding Author), Univ Konstanz, Constance, Germany. El-Assady, M (Corresponding Author), Ontario Tech Univ, Oshawa, ON, Canada. El-Assady, Mennatallah; Kehlbeck, Rebecca; Keim, Daniel; Deussen, Oliver, Univ Konstanz, Constance, Germany. El-Assady, Mennatallah; Collins, Christopher, Ontario Tech Univ, Oshawa, ON, Canada.", "countries": "Canada;Germany", "abstract": "We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decision-making process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.", "keywords": "Topic Model Optimization,Word Embedding,Mixed-Initiative Refinement,Guided Visual Analytics,Semantic Mapping", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934654", "refList": ["10.1109/vast.2014.7042493", "10.1145/2133806.2133826", "10.1016/j.visinf.2018.09.003", "10.1007/978-3-319-67008-9\\_26", "10.1109/tvcg.2013.126", "10.1162/tacl\\_a\\_00140", "10.1007/s13202-018-0509-5", "10.1109/bdva.2018.8534018", "10.1108/eb026526", "10.1007/s10994-013-5413-0", "10.1145/564376.564421", "10.1016/j.visinf.2017.01.006", "10.3115/v1/p14-2050", "10.1007/bf00288933", "10.1109/tvcg.2013.212", "10.1145/2207676.2207741", "10.1109/tvcg.2013.162", "10.1109/tvcg.2016.2515592", "10.1109/tvcg.2017.2745080", "10.1109/mcg.2013.53", "10.1109/tvcg.2017.2744199", "10.1145/3091108", "10.18653/v1/p17-4009", "10.1162/jmlr.2003.3.4-5.951", "10.1109/vast.2017.8585498", "10.1109/tvcg.2017.2723397", "10.1109/tvcg.2018.2864769", "10.1007/s10618-005-0361-3", "10.3115/v1/d14-1167", "10.1007/bf01840357", "10.1162/jmlr.2003.3.4-5.993", "10.1145/2678025.2701370", "10.1016/j.ins.2016.06.040", "10.1109/tvcg.2017.2746018", "10.1109/vast.2011.6102461", "10.1111/cgf.13092", "10.3115/1117729.1117730", "10.1109/mcg.2015.91", "10.1145/2669557.2669572"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13959", "year": "2020", "title": "Knowledge-Assisted Comparative Assessment of Breast Cancer using Dynamic Contrast-Enhanced Magnetic Resonance Imaging", "conferenceName": "EuroVis", "authors": "Kai Nie;Pascal A. Baltzer;Bernhard Preim;Gabriel Mistelbauer", "citationCount": "0", "affiliation": "Nie, K (Corresponding Author), Otto von Guericke Univ, Dept Simulat \\& Graph, Magdeburg, Germany.\nNie, K.; Preim, B.; Mistelbauer, G., Otto von Guericke Univ, Dept Simulat \\& Graph, Magdeburg, Germany.\nBaltzer, P., Med Univ Vienna, Dept Biomed Imaging \\& Image Guided Therapy, Vienna, Austria.", "countries": "Germany;Austria", "abstract": "Breast perfusion data are dynamic medical image data that depict perfusion characteristics of the investigated tissue. These data consist of a series of static datasets that are acquired at different time points and aggregated into time intensity curves (TICs) for each voxel. The characteristics of these TICs provide important information about a lesion's composition, but their analysis is time-consuming due to their large number. Subsequently, these TICs are used to classify a lesion as benign or malignant. This lesion scoring is commonly done manually by physicians and may therefore be subject to bias. We propose an approach that addresses both of these problems by combining an automated lesion classification with a visual confirmatory analysis, especially for uncertain cases. Firstly, we cluster the TICs of a lesion using ordering points to identify the clustering structure (OPTICS) and then visualize these clusters. Together with their relative size, they are added to a library. We then model fuzzy inference rules by using the lesion's TIC clusters as antecedents and its score as consequent. Using a fuzzy scoring system, we can suggest a score for a new lesion. Secondly, to allow physicians to confirm the suggestion in uncertain cases, we display the TIC clusters together with their spatial distribution and allow them to compare two lesions side by side. With our knowledge-assisted comparative visual analysis, physicians can explore and classify breast lesions. The true positive prediction accuracy of our scoring system achieved 71.4\\% in one-fold cross-validation using 14 lesions.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13959", "refList": ["10.1016/j.patcog.2017.08.004", "10.1109/icinfa.2017.8078962", "10.1109/iccv.2013.222", "10.1002/jmri.1123", "10.1016/j.ejrad.2017.01.020", "10.1007/s00330-016-4612-z", "10.1002/widm.30", "10.1002/jmri.26721", "10.1109/tvcg.2008.95", "10.1007/s00330-015-4075-7", "10.1007/s00330-007-0762-3", "10.1148/radiology.213.3.r99dc01881", "10.1007/s10278-010-9298-1", "10.1016/j.datak.2006.01.013", "10.1016/j.cag.2010.05.016", "10.5121/mlaij.2016.3103", "10.1089/10665270360688057", "10.1117/1.jmi.5.1.014502", "10.1002/mp.12408", "10.1198/jcgs.2011.09224", "10.1109/cbms.2013.6627768", "10.1109/vast.2017.8585498", "10.1118/1.4937787", "10.1016/j.acra.2009.03.017", "10.1145/2503210.2503255", "10.1007/978-3-319-68548-9\\_44", "10.1109/tvcg.2007.70569", "10.1016/j.compmedimag.2007.02.007", "10.1109/tmi.2013.2281984", "10.1148/radiol.2442051620", "10.1016/j.jacr.2009.07.023", "10.1002/j.1538-7305.1957.tb01515.x", "10.1148/radiol.14121031", "10.3322/caac.21492", "10.1148/radiology.211.1.r99ap38101", "10.1016/j.compbiomed.2014.10.006", "10.1016/j.eswa.2016.01.004", "10.3238/arztebl.2018.0316", "10.1559/152304003100010929", "10.1016/s0002-9610(01)00726-7", "10.1016/j.ejrad.2020.108819", "10.1109/tmi.2012.2191302"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/tvcg.2018.2865039", "title": "Enhancing Web-based Analytics Applications through Provenance", "year": "2018", "conferenceName": "VAST", "authors": "Akhilesh Camisetty;Chaitanya Chandurkar;Maoyuan Sun;David Koop", "citationCount": "1", "affiliation": "Camisetty, A (Corresponding Author), UMass Dartmouth, Dartmouth, NS 02747, Canada. Camisetty, Akhilesh; Chandurkar, Chaitanya; Sun, Maoyuan; Koop, David, UMass Dartmouth, Dartmouth, NS 02747, Canada.", "countries": "Canada", "abstract": "Visual analytics systems continue to integrate new technologies and leverage modern environments for exploration and collaboration, making tools and techniques available to a wide audience through web browsers. Many of these systems have been developed with rich interactions, offering users the opportunity to examine details and explore hypotheses that have not been directly encoded by a designer. Understanding is enhanced when users can replay and revisit the steps in the sensemaking process, and in collaborative settings, it is especially important to be able to review not only the current state but also what decisions were made along the way. Unfortunately, many web-based systems lack the ability to capture such reasoning, and the path to a result is transient, forgotten when a user moves to a new view. This paper explores the requirements to augment existing client-side web applications with support for capturing, reviewing, sharing, and reusing steps in the reasoning process. Furthermore, it considers situations where decisions are made with streaming data, and the insights gained from revisiting those choices when more data is available. It presents a proof of concept, the Shareable Interactive Manipulation Provenance framework (SIMProv.js), that addresses these requirements in a modern, client-side JavaScript library, and describes how it can be integrated with existing frameworks.", "keywords": "Collaboration,provenance,streaming data,history,web", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865039", "refList": ["10.1145/2884781.2884864", "10.1109/tvcg.2008.137", "10.1007/978-3-319-40593-3\\_9", "10.1145/2598153.2598168", "10.1007/978-3-642-17819-1\\_34", "10.1109/tvcg.2016.2598471", "10.2218/ijdc.v7i1.213", "10.1145/2669485.2669518", "10.20380/gi2015.16", "10.1177/1473871611412817", "10.1111/cgf.12131", "10.1145/67544.66963", "10.1057/palgrave.ivs.9500167", "10.1063/1.3662391", "10.1145/2882903.2899392", "10.1109/visual.1993.398857", "10.1109/mic.2017.2911428", "10.1109/tvcg.2015.2467611", "10.1109/vast.2010.5652932", "10.1057/ivs.2008.31", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2013.197", "10.1109/visual.1999.809871", "10.1145/1084805.1084812", "10.1016/s0950-7051(00)00101-5", "10.1007/s00778-017-0486-1", "10.1109/tvcg.2007.70589", "10.1109/scc.2008.21", "10.1145/2396636.2396673", "10.1109/tvcg.2017.2745279", "10.1109/wvl.1988.18020", "10.1109/mcg.2003.1185579", "10.1109/tvcg.2015.2467191", "10.1145/2501988.2502050", "10.1145/1979742.1979570", "10.1111/j.1467-8659.2009.01687.x", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1111/cgf.12903", "10.1145/2807442.2807468", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030383", "title": "II-20: Intelligent and pragmatic analytic categorization of image collections", "year": "2020", "conferenceName": "VAST", "authors": "Jan Zah\u00e1lka;Marcel Worring;Jarke J. van Wijk", "citationCount": "0", "affiliation": "Zahalka, J (Corresponding Author), Czech Tech Univ, Prague, Czech Republic. Zahalka, Jan, Czech Tech Univ, Prague, Czech Republic. Worring, Marcel, Univ Amsterdam, Amsterdam, Netherlands. van Wijk, Jarke J., Eindhoven Univ Technol, Eindhoven, Netherlands.", "countries": "Republic;Netherlands", "abstract": "In this paper, we introduce 11\u201320 (Image Insight 2020), a multimedia analytics approach for analytic categorization of image collections. Advanced visualizations for image collections exist, but they need tight integration with a machine model to support the task of analytic categorization. Directly employing computer vision and interactive learning techniques gravitates towards search. Analytic categorization, however, is not machine classification (the difference between the two is called the pragmatic gap): a human adds/redefines/deletes categories of relevance on the fly to build insight, whereas the machine classifier is rigid and non-adaptive. Analytic categorization that truly brings the user to insight requires a flexible machine model that allows dynamic sliding on the exploration-search axis, as well as semantic interactions: a human thinks about image data mostly in semantic terms. 11\u201320 brings three major contributions to multimedia analytics on image collections and towards closing the pragmatic gap. Firstly, a new machine model that closely follows the user's interactions and dynamically models her categories of relevance. II-20's machine model, in addition to matching and exceeding the state of the art's ability to produce relevant suggestions, allows the user to dynamically slide on the exploration-search axis without any additional input from her side. Secondly, the dynamic, 1-image-at-a-time Tetris metaphor that synergizes with the model. It allows a well-trained model to analyze the collection by itself with minimal interaction from the user and complements the classic grid metaphor. Thirdly, the fast-forward interaction, allowing the user to harness the model to quickly expand (\u201cfast-forward\u201d) the categories of relevance, expands the multimedia analytics semantic interaction dictionary. Automated experiments show that II-20's machine model outperforms the existing state of the art and also demonstrate the Tetris metaphor's analytic quality. User studies further confirm that II\u201320 is an intuitive, efficient, and effective multimedia analytics tool.", "keywords": "Multimedia analytics,image data,analytic categorization,pragmatic gap", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030383", "refList": ["10.1109/tvcg.2008.137", "10.1145/2882903.2882919", "10.1145/2556647.2556657", "10.1145/1142473.1142574", "10.1109/tvcg.2016.2598471", "10.1109/vast.2017.8585669", "10.1109/tvcg.2014.2346573", "10.1111/j.0956-7976.2005.00782.x", "10.1007/978-3-540-89965-5\\_27", "10.1109/visual.2019.8933611", "10.1109/vast.2010.5653598", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1109/tvcg.2012.271", "10.1109/mcg.2009.49", "10.1057/ivs.2008.31", "10.1111/cgf.12925", "10.1109/hicss.2016.183", "10.1109/tvcg.2016.2598466", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2018.2865117", "10.1109/vast47406.2019.8986948", "10.1016/s0950-7051(00)00101-5", "10.1109/tvcg.2016.2598594", "10.1109/tvcg.2006.101", "10.1111/cgf.12311", "10.1109/vast.2009.5333020", "10.1109/vast.2010.5652885", "10.1111/cgf.13670", "10.1109/wvl.1988.18020", "10.1145/2133806.2133821", "10.1109/vast.2012.6400486"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13678", "year": "2019", "title": "Characterizing Exploratory Visual Analysis: A Literature Review and Evaluation of Analytic Provenance in Tableau", "conferenceName": "EuroVis", "authors": "Leilani Battle;Jeffrey Heer", "citationCount": "2", "affiliation": "Battle, L (Corresponding Author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.\nBattle, Leilani, Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.\nHeer, Jeffrey, Univ Washington, Paul G Allen Sch Comp Sci \\& Engn, Seattle, WA 98195 USA.", "countries": "USA", "abstract": "Supporting exploratory visual analysis (EVA) is a central goal of visualization research, and yet our understanding of the process is arguably vague and piecemeal. We contribute a consistent definition of EVA through review of the relevant literature, and an empirical evaluation of existing assumptions regarding how analysts perform EVA using Tableau, a popular visual analysis tool. We present the results of a study where 27 Tableau users answered various analysis questions across 3 datasets. We measure task performance, identify recurring patterns across participants' analyses, and assess variance from task specificity and dataset. We find striking differences between existing assumptions and the collected data. Participants successfully completed a variety of tasks, with over 80\\% accuracy across focused tasks with measurably correct answers. The observed cadence of analyses is surprisingly slow compared to popular assumptions from the database community. We find significant overlap in analyses across participants, showing that EVA behaviors can be predictable. Furthermore, we find few structural differences between behavior graphs for open-ended and more focused exploration tasks.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13678", "refList": ["10.1109/dsia.2017.8339089", "10.1109/tvcg.2008.137", "10.1109/tvcg.2007.28", "10.1057/ivs.2009.22", "10.1038/526189a", "10.1109/tvcg.2006.85", "10.1145/2882903.2882919", "10.1145/2588555.2610523", "10.1109/tvcg.2014.2346575", "10.1145/2207676.2208294", "10.1145/2723372.2731084", "10.1109/tvcg.2016.2598471", "10.1145/3173574.3174168", "10.1109/visual.2005.1532788", "10.1109/tvcg.2014.2346452", "10.1109/tvcg.2015.2467613", "10.20380/gi2015.16", "10.14778/2831360.2831371", "10.1145/3173574.3174053", "10.1109/icdew.2006.75", "10.1145/2993901.2993912", "10.1145/1502650.1502695", "10.1109/tvcg.2012.219", "10.1007/978-3-642-23768-3\\_22", "10.1109/tvcg.2016.2598797", "10.1109/vl.1996.545307", "10.1111/j.1467-8659.2010.01830.x", "10.1109/tvcg.2015.2467871", "10.1145/3209900.3209901", "10.1145/2939502.2939513", "10.1145/381641.381656", "10.1109/tvcg.2013.124", "10.1109/vast.2008.4677357", "10.1145/1378773.1378788", "10.1109/mcg.2012.120", "10.1057/ivs.2008.31", "10.1109/infvis.2005.1532136", "10.1109/icde.2014.6816674", "10.1109/tvcg.2016.2598466", "10.1109/tvcg.2015.2467551", "10.1145/2939502.2939506", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2865117", "10.1145/1556262.1556276", "10.1016/s0950-7051(00)00101-5", "10.1109/tvcg.2008.109", "10.1145/2588555.2593666", "10.1109/tvcg.2018.2865040", "10.1109/tvcg.2013.179", "10.1111/cgf.13409", "10.1177/1473871616638546", "10.1109/2945.981851", "10.1109/tvcg.2010.164", "10.1109/vast.2008.4677365", "10.1109/tvcg.2015.2467191", "10.1145/2133806.2133821", "10.1145/2207676.2208412", "10.1109/tvcg.2017.2744319", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.224"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030425", "title": "Visual Analysis of Argumentation in Essays", "year": "2020", "conferenceName": "VAST", "authors": "Dora Kiesel;Patrick Riehmann;Henning Wachsmuth;Benno Stein;Bernd Fr\u00f6hlich", "citationCount": "0", "affiliation": "Kiesel, D (Corresponding Author), Bauhaus Univ Weimar, Weimar, Germany. Kiesel, Dora; Riehmann, Patrick; Stein, Benno; Froehlich, Bernd, Bauhaus Univ Weimar, Weimar, Germany. Wachsmuth, Henning, Paderborn Univ, Paderborn, Germany.", "countries": "Germany", "abstract": "This paper presents a visual analytics system for exploring, analyzing and comparing argument structures in essay corpora. We provide an overview of the corpus by a list of ArguLines which represent the argument units of each essay by a sequence of glyphs. Each glyph encodes the stance, the depth and the relative position of an argument unit. The overview can be ordered in various ways to reveal patterns and outliers. Subsets of essays can be selected and analyzed in detail using the Argument Unit Occurrence Tree which aggregates the argument structures using hierarchical histograms. This hierarchical view facilitates the estimation of statistics and trends concerning the progression of the argumentation in the essays. It also provides insights into the commonalities and differences between selected subsets. The text view is the necessary textual basis to verify conclusions from the other views and the annotation process. Linking the views and interaction techniques for visual filtering, studying the evolution of stance within a subset of essays and scrutinizing the order of argumentative units enable a deep analysis of essay corpora. Our expert reviews confirmed the utility of the system and revealed detailed and previously unknown information about the argumentation in our sample corpus.", "keywords": "Information Visualization,Text Analysis,User Interfaces,Visual Analytics,Argumentation Visualization,Glyph-based Techniques,Text and Document Data,Tree-based Visualization,Coordinated and Multiple Views,Close and Distant Reading", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030425", "refList": ["10.1109/tvcg.2014.2346575", "10.14778/2735479.2735485", "10.14778/3192965.3192971", "10.1111/cgf.12129", "10.1145/2206869.2206874", "10.1109/icde.2016.7498300", "10.14778/2831360.2831371", "10.1145/3035918.3056097", "10.1023/a:1009726021843", "10.1111/cgf.13678", "10.1145/3183713.3196905", "10.14778/3115404.3115418", "10.1109/tvcg.2012.180", "10.1109/icde.1999.754950", "10.14778/1453856.1453924", "10.1145/3209900.3209901", "10.1002/spe.2325", "10.1145/42201.42203", "10.1109/tvcg.2013.124", "10.1109/vast.2008.4677357", "10.1109/icde.2016.7498287", "10.1109/icde.2014.6816674", "10.14778/2732951.2732964", "10.1109/tvcg.2015.2467551", "10.1145/1084805.1084812", "10.1109/2.781635", "10.14778/2732279.2732280", "10.1109/icde.2015.7113427", "10.1109/tvcg.2015.2467091", "10.1007/s00778-017-0486-1", "10.1109/tvcg.2003.1196005", "10.1109/icde.2004.1320035", "10.1109/tvcg.2016.2607714", "10.14778/2732951.2732953", "10.1109/tvcg.2008.131", "10.1109/tvcg.2018.2865018", "10.1145/2133806.2133821", "10.1109/icde.2019.00035", "10.1109/tpds.2005.144", "10.14778/3236187.3236212", "10.1109/tvcg.2009.111", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14033", "year": "2020", "title": "Survey on Individual Differences in Visualization", "conferenceName": "EuroVis", "authors": "Zhengliang Liu;R. Jordan Crouser;Alvitta Ottley", "citationCount": "0", "affiliation": "Liu, ZL (Corresponding Author), Washington Univ, St Louis, MO 63110 USA.\nLiu, Zhengliang; Ottley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nCrouser, R. Jordan, Smith Coll, Northampton, MA 01063 USA.", "countries": "USA", "abstract": "Developments in data visualization research have enabled visualization systems to achieve great general usability and application across a variety of domains. These advancements have improved not only people's understanding of data, but also the general understanding of people themselves, and how they interact with visualization systems. In particular, researchers have gradually come to recognize the deficiency of having one-size-fits-all visualization interfaces, as well as the significance of individual differences in the use of data visualization systems. Unfortunately, the absence of comprehensive surveys of the existing literature impedes the development of this research. In this paper, we review the research perspectives, as well as the personality traits and cognitive abilities, visualizations, tasks, and measures investigated in the existing literature. We aim to provide a detailed summary of existing scholarship, produce evidence-based reviews, and spur future inquiry.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14033", "refList": ["10.1145/3025171.3025192", "10.1037/0278-7393.29.2.298", "10.1145/2470654.2470707", "10.1177/001872086300500103", "10.1109/tvcg.2014.2346575", "10.1146/annurev-psych-113011-143750", "10.1016/j.sbspro.2011.11.312", "10.1177/1073191102092010", "10.1109/tvcg.2016.2598471", "10.1145/169891.169925", "10.1145/2856767.2856779", "10.4102/sajip.v29i1.88", "10.1145/3301275.3302307", "10.1109/mcg.2012.120", "10.1016/0749-596x(89)90040-5", "10.1111/cgf.12393", "10.3389/fpsyg.2018.00755", "10.1037/0033-2909.121.2.219", "10.2466/pms.1978.47.2.599", "10.1037/0022-3514.44.2.419", "10.1016/j.cub.2009.12.014", "10.1002/per.704", "10.1016/j.paid.2006.03.011", "10.1038/36846", "10.1145/3025453.3025877", "10.1037/0022-3514.93.5.880", "10.1002/per.469", "10.1002/tea.3660300407", "10.1037/h0076301", "10.1002/jocb.32", "10.1371/journal.pone.0131115", "10.1002/per.588", "10.1145/2449396.2449439", "10.1108/jmd-12-2013-0160", "10.1037/0022-3514.42.1.116", "10.1016/j.intell.2003.10.005", "10.1016/s0022-5371(80)90312-6", "10.3758/bf03214546", "10.1111/j.1467-8659.2009.01442.x", "10.1002/(sici)1099-0984(200003/04)14:2", "10.1145/2470654.2470696", "10.2190/vqjd-t1yd-5wvb-rypj", "10.1145/2702123.2702590", "10.1109/infvis.2005.1532136", "10.1037/0278-7393.29.4.611", "10.1145/3025453.3025577", "10.1016/0360-8352(91)90009-u", "10.1145/2556288.2557141", "10.1016/j.sbspro.2012.01.055", "10.1016/j.sbspro.2013.04.319", "10.1007/s11257-019-09244-5", "10.1016/j.tics.2013.06.006", "10.1037/a0037009", "10.1037/0003-066x.48.1.26", "10.1109/infvis.2004.70", "10.1037/0003-066x.45.4.489", "10.1037/h0092976", "10.1177/1473871612441542", "10.1145/302979.303030", "10.1016/b978-0-12-386915-9.00003-6", "10.1080/08870449608404995", "10.1109/tvcg.2013.156", "10.1145/2110192.2110202", "10.3758/cabn.2.4.341", "10.1037/0096-1523.27.1.92", "10.1109/tvcg.2007.70515", "10.1145/1385569.1385602", "10.24963/ijcai.2017/217", "10.1080/135467897394419", "10.1007/978-3-642-31454-4\\_23", "10.1037/a0021016", "10.1109/tvcg.2014.2346452", "10.1145/3301275.3302313", "10.1145/2633043", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1109/vl.1996.545307", "10.1037/0033-2909.119.2.197", "10.1523/jneurosci.2145-09.2009", "10.1177/1473871613513227", "10.1037/0022-3514.51.4.875", "10.1109/mcg.2009.49", "10.1002/acp.1344", "10.1037/a0016127", "10.1037/1040-3590.18.2.192", "10.1111/j.1467-8659.2011.01928.x", "10.1016/s0140-6736(02)07441-x", "10.1037/0022-0663.96.3.471", "10.1207/s15327752jpa4803\\_13", "10.1007/s10798-018-9446-3", "10.1002/(sici)1097-4571(2000)51:6", "10.1109/vast.2009.5333468", "10.1109/mcg.2009.22", "10.1145/3079628.3079634", "10.1016/j.jrp.2014.05.003", "10.1016/s0079-6123(07)00020-9", "10.1002/ijop.12511", "10.1111/j.2044-8279.1982.tb00821.x", "10.5539/jel.v4n4p91", "10.1177/1473871615594652", "10.1037/0022-3514.66.5.950", "10.1037/h0042761", "10.1109/tvcg.2012.199", "10.1177/1046496403257228", "10.1080/13614569708914684", "10.1111/cgf.13678", "10.1016/j.lindif.2003.08.001", "10.1016/j.jecp.2009.11.003", "10.1109/tvcg.2014.2346984", "10.1109/vast.2011.6102445", "10.1145/2678025.2701376", "10.1057/ivs.2008.31", "10.1533/9781780630366", "10.1111/j.2044-8260.1992.tb00972.x", "10.1016/j.jrp.2011.12.003", "10.1016/0747-5632(91)90002-i", "10.1080/13546781343000222", "10.3758/s13414-013-0610-2", "10.1145/3301275.3302283", "10.1177/106907279300100107", "10.1109/vast.2012.6400535", "10.1145/3377325.3377502", "10.1037/1089-2699.10.4.249", "10.1016/j.paid.2003.08.018", "10.1016/j.paid.2010.09.015"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1111/cgf.13670", "year": "2019", "title": "Follow The Clicks: Learning and Anticipating Mouse Interactions During Exploratory Data Analysis", "conferenceName": "EuroVis", "authors": "Alvitta Ottley;Roman Garnett;Ran Wan", "citationCount": "1", "affiliation": "Ottley, A (Corresponding Author), Washington Univ, Comp Sci \\& Engn, St Louis, MO 63110 USA.\nOttley, Alvitta; Garnett, Roman; Wan, Ran, Washington Univ, Comp Sci \\& Engn, St Louis, MO 63110 USA.", "countries": "USA", "abstract": "The goal of visual analytics is to create a symbiosis between human and computer by leveraging their unique strengths. While this model has demonstrated immense success, we are yet to realize the full potential of such a human-computer partnership. In a perfect collaborative mixed-initiative system, the computer must possess skills for learning and anticipating the users' needs. Addressing this gap, we propose a framework for inferring attention from passive observations of the user's click, thereby allowing accurate predictions of future events. We demonstrate this technique with a crime map and found that users' clicks can appear in our prediction set 92\\% - 97\\% of the time. Further analysis shows that we can achieve high prediction accuracy typically after three clicks. Altogether, we show that passive observations of interaction data can reveal valuable information that will allow the system to learn and anticipate future events.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13670", "refList": ["10.1057/ivs.2009.22", "10.1145/2882903.2882919", "10.1109/tvcg.2014.2346575", "10.1145/846183.846188", "10.1111/cgf.13405", "10.1109/tvcg.2016.2598471", "10.1145/3173574.3174168", "10.1145/948496.948514", "10.1109/mcg.2006.30", "10.1109/tvcg.2012.195", "10.1016/s0042-6989(99)00163-7", "10.1007/978-3-540-70956-5", "10.1145/643477.643478", "10.1038/35058500", "10.1023/a:1008935410038", "10.1002/9781118360491.ch10", "10.1007/978-94-009-3833-5\\_5", "10.1145/2702123.2702590", "10.1057/ivs.2008.31", "10.1145/1054972.1055012", "10.1109/tvcg.2015.2467551", "10.1145/636772.636798", "10.1109/tpami.2012.89", "10.1109/34.730558", "10.1094/pdis-11-11-0999-pdn", "10.1145/1476589.1476628", "10.1145/964442.964461", "10.1145/302979.303030", "10.1109/5254.796083", "10.1145/360402.360406", "10.1109/icarcv.2018.8581221", "10.1049/ip-f-2.1993.0015", "10.1109/compsac.2017.270", "10.1109/vast.2008.4677361", "10.1145/1979742.1979570", "10.1109/hicss.2005.286", "10.1145/2110192.2110202", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2016.2598468"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030383", "title": "II-20: Intelligent and pragmatic analytic categorization of image collections", "year": "2020", "conferenceName": "VAST", "authors": "Jan Zah\u00e1lka;Marcel Worring;Jarke J. van Wijk", "citationCount": "0", "affiliation": "Zahalka, J (Corresponding Author), Czech Tech Univ, Prague, Czech Republic. Zahalka, Jan, Czech Tech Univ, Prague, Czech Republic. Worring, Marcel, Univ Amsterdam, Amsterdam, Netherlands. van Wijk, Jarke J., Eindhoven Univ Technol, Eindhoven, Netherlands.", "countries": "Republic;Netherlands", "abstract": "In this paper, we introduce 11\u201320 (Image Insight 2020), a multimedia analytics approach for analytic categorization of image collections. Advanced visualizations for image collections exist, but they need tight integration with a machine model to support the task of analytic categorization. Directly employing computer vision and interactive learning techniques gravitates towards search. Analytic categorization, however, is not machine classification (the difference between the two is called the pragmatic gap): a human adds/redefines/deletes categories of relevance on the fly to build insight, whereas the machine classifier is rigid and non-adaptive. Analytic categorization that truly brings the user to insight requires a flexible machine model that allows dynamic sliding on the exploration-search axis, as well as semantic interactions: a human thinks about image data mostly in semantic terms. 11\u201320 brings three major contributions to multimedia analytics on image collections and towards closing the pragmatic gap. Firstly, a new machine model that closely follows the user's interactions and dynamically models her categories of relevance. II-20's machine model, in addition to matching and exceeding the state of the art's ability to produce relevant suggestions, allows the user to dynamically slide on the exploration-search axis without any additional input from her side. Secondly, the dynamic, 1-image-at-a-time Tetris metaphor that synergizes with the model. It allows a well-trained model to analyze the collection by itself with minimal interaction from the user and complements the classic grid metaphor. Thirdly, the fast-forward interaction, allowing the user to harness the model to quickly expand (\u201cfast-forward\u201d) the categories of relevance, expands the multimedia analytics semantic interaction dictionary. Automated experiments show that II-20's machine model outperforms the existing state of the art and also demonstrate the Tetris metaphor's analytic quality. User studies further confirm that II\u201320 is an intuitive, efficient, and effective multimedia analytics tool.", "keywords": "Multimedia analytics,image data,analytic categorization,pragmatic gap", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030383", "refList": ["10.1109/tvcg.2008.137", "10.1145/2882903.2882919", "10.1145/2556647.2556657", "10.1145/1142473.1142574", "10.1109/tvcg.2016.2598471", "10.1109/vast.2017.8585669", "10.1109/tvcg.2014.2346573", "10.1111/j.0956-7976.2005.00782.x", "10.1007/978-3-540-89965-5\\_27", "10.1109/visual.2019.8933611", "10.1109/vast.2010.5653598", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1109/tvcg.2012.271", "10.1109/mcg.2009.49", "10.1057/ivs.2008.31", "10.1111/cgf.12925", "10.1109/hicss.2016.183", "10.1109/tvcg.2016.2598466", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2018.2865117", "10.1109/vast47406.2019.8986948", "10.1016/s0950-7051(00)00101-5", "10.1109/tvcg.2016.2598594", "10.1109/tvcg.2006.101", "10.1111/cgf.12311", "10.1109/vast.2009.5333020", "10.1109/vast.2010.5652885", "10.1111/cgf.13670", "10.1109/wvl.1988.18020", "10.1145/2133806.2133821", "10.1109/vast.2012.6400486"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13680", "year": "2019", "title": "An Exploratory User Study of Visual Causality Analysis", "conferenceName": "EuroVis", "authors": "Chi{-}Hsien Yen;Aditya G. Parameswaran;Wai{-}Tat Fu", "citationCount": "0", "affiliation": "Yen, CHE (Corresponding Author), Univ Illinois, Champaign, IL 61820 USA.\nYen, Chi-Hsien Eric; Parameswaran, Aditya; Fu, Wai-Tat, Univ Illinois, Champaign, IL 61820 USA.", "countries": "USA", "abstract": "Interactive visualization tools are being used by an increasing number of members of the general public; however, little is known about how, and how well, people use visualizations to infer causality. Adapted from the mediation causal model, we designed an analytic framework to systematically evaluate human performance, strategies, and pitfalls in a visual causal reasoning task. We recruited 24 participants and asked them to identify the mediators in a fictitious dataset using bar charts and scatter plots within our visualization interface. The results showed that the accuracy of their responses as to whether a variable is a mediator significantly decreased when a confounding variable directly influenced the variable being analyzed. Further analysis demonstrated how individual visualization exploration strategies and interfaces might influence reasoning performance. We also identified common strategies and pitfalls in their causal reasoning processes. Design implications for how future visual analytics tools can be designed to better support causal inference are discussed.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13680", "refList": ["10.1037/1089-2680.2.2.175", "10.1109/tvcg.2017.2744138", "10.1518/001872008x354183", "10.3758/bf03211151", "10.1109/tvcg.2014.2346452", "10.1016/j.tree.2008.10.008", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2015.2467613", "10.1145/2207676.2208704", "10.1109/tvcg.2014.2346297", "10.1016/j.ijmedinf.2004.08.003", "10.1145/2993901.2993907", "10.1037/0022-3514.51.6.1173", "10.1145/2856767.2856779", "10.1109/tvcg.2015.2467871", "10.1037/0033-295x.101.4.608", "10.1109/vast.2010.5653598", "10.1145/3301275.3302307", "10.1016/j.jesp.2011.05.007", "10.1098/rstb.2006.1991", "10.1057/ivs.2008.31", "10.1177/0146167208331253", "10.1109/tvcg.2015.2467931", "10.2307/2288400", "10.3758/bf03198555", "10.1109/tvcg.2010.164", "10.1109/vast.2011.6102435", "10.1109/tvcg.2012.260", "10.1109/tvcg.2015.2467758", "10.1080/14640746808400161", "10.1111/cgf.13168"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028958", "title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Tiankai Xie;Yuxin Ma;Hanghang Tong;My T. Thai;Ross Maciejewski", "citationCount": "0", "affiliation": "Xie, TK (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Xie, Tiankai; Ma, Yuxin; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. Tong, Hanghang, Univ Illinois, Urbana, IL USA. Thai, My T., Univ Florida, Gainesville, FL 32611 USA.", "countries": "USA", "abstract": "Graph mining plays a pivotal role across a number of disciplines, and a variety of algorithms have been developed to answer who/what type questions. For example, what items shall we recommend to a given user on an e-commerce platform? The answers to such questions are typically returned in the form of a ranked list, and graph-based ranking methods are widely used in industrial information retrieval settings. However, these ranking algorithms have a variety of sensitivities, and even small changes in rank can lead to vast reductions in product sales and page hits. As such, there is a need for tools and methods that can help model developers and analysts explore the sensitivities of graph ranking algorithms with respect to perturbations within the graph structure. In this paper, we present a visual analytics framework for explaining and exploring the sensitivity of any graph-based ranking algorithm by performing perturbation-based what-if analysis. We demonstrate our framework through three case studies inspecting the sensitivity of two classic graph-based ranking algorithms (PageRank and HITS) as applied to rankings in political news media and social networks.", "keywords": "Graph-based ranking,sensitivity analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028958", "refList": ["10.1109/wsc.2017.8247800", "10.1023/a:1022649401552", "10.1515/1559-0410.11416", "10.1109/tvcg.2016.2598919", "10.1177/1473871611416549", "10.1109/tvcg.2019.2934630", "10.1140/epjds29", "10.1109/tvcg.2019.2934670", "10.1016/j.eswa.2015.09.004", "10.1145/2702123.2702509", "10.1016/j.visinf.2018.12.001", "10.2307/3002000", "10.1109/tvcg.2019.2934399", "10.1007/s41060-016-0032-z", "10.1111/cgf.13198", "10.14778/2350229.2350254", "10.1145/2939672.2939764", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/vast.2017.8585647", "10.1007/bf01187020", "10.1109/icdm.2015.26", "10.1145/2362383.2362387", "10.1177/0049124104268644", "10.1109/vast.2011.6102442", "10.1109/infvis.2003.1249025", "10.1109/tvcg.2018.2864475", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2015.2467691", "10.1111/cgf.13210", "10.1214/aos/1176344136", "10.1109/tvcg.2015.2424872", "10.1016/j.visinf.2018.09.001", "10.1177/089443939100900106", "10.1109/tvcg.2015.2467931", "10.1162/neco.1997.9.8.1735", "10.1007/s11162-011-9241-4", "10.1111/cgf.13680", "10.1145/3065386", "10.1109/tvcg.2018.2864889", "10.1177/003804070808100402", "10.1109/icdm.2010.62", "10.1038/s41598-020-59669-x", "10.1162/153244303321897717", "10.1109/tvcg.2019.2934619", "10.1007/bf00356088", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.14033", "year": "2020", "title": "Survey on Individual Differences in Visualization", "conferenceName": "EuroVis", "authors": "Zhengliang Liu;R. Jordan Crouser;Alvitta Ottley", "citationCount": "0", "affiliation": "Liu, ZL (Corresponding Author), Washington Univ, St Louis, MO 63110 USA.\nLiu, Zhengliang; Ottley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nCrouser, R. Jordan, Smith Coll, Northampton, MA 01063 USA.", "countries": "USA", "abstract": "Developments in data visualization research have enabled visualization systems to achieve great general usability and application across a variety of domains. These advancements have improved not only people's understanding of data, but also the general understanding of people themselves, and how they interact with visualization systems. In particular, researchers have gradually come to recognize the deficiency of having one-size-fits-all visualization interfaces, as well as the significance of individual differences in the use of data visualization systems. Unfortunately, the absence of comprehensive surveys of the existing literature impedes the development of this research. In this paper, we review the research perspectives, as well as the personality traits and cognitive abilities, visualizations, tasks, and measures investigated in the existing literature. We aim to provide a detailed summary of existing scholarship, produce evidence-based reviews, and spur future inquiry.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14033", "refList": ["10.1145/3025171.3025192", "10.1037/0278-7393.29.2.298", "10.1145/2470654.2470707", "10.1177/001872086300500103", "10.1109/tvcg.2014.2346575", "10.1146/annurev-psych-113011-143750", "10.1016/j.sbspro.2011.11.312", "10.1177/1073191102092010", "10.1109/tvcg.2016.2598471", "10.1145/169891.169925", "10.1145/2856767.2856779", "10.4102/sajip.v29i1.88", "10.1145/3301275.3302307", "10.1109/mcg.2012.120", "10.1016/0749-596x(89)90040-5", "10.1111/cgf.12393", "10.3389/fpsyg.2018.00755", "10.1037/0033-2909.121.2.219", "10.2466/pms.1978.47.2.599", "10.1037/0022-3514.44.2.419", "10.1016/j.cub.2009.12.014", "10.1002/per.704", "10.1016/j.paid.2006.03.011", "10.1038/36846", "10.1145/3025453.3025877", "10.1037/0022-3514.93.5.880", "10.1002/per.469", "10.1002/tea.3660300407", "10.1037/h0076301", "10.1002/jocb.32", "10.1371/journal.pone.0131115", "10.1002/per.588", "10.1145/2449396.2449439", "10.1108/jmd-12-2013-0160", "10.1037/0022-3514.42.1.116", "10.1016/j.intell.2003.10.005", "10.1016/s0022-5371(80)90312-6", "10.3758/bf03214546", "10.1111/j.1467-8659.2009.01442.x", "10.1002/(sici)1099-0984(200003/04)14:2", "10.1145/2470654.2470696", "10.2190/vqjd-t1yd-5wvb-rypj", "10.1145/2702123.2702590", "10.1109/infvis.2005.1532136", "10.1037/0278-7393.29.4.611", "10.1145/3025453.3025577", "10.1016/0360-8352(91)90009-u", "10.1145/2556288.2557141", "10.1016/j.sbspro.2012.01.055", "10.1016/j.sbspro.2013.04.319", "10.1007/s11257-019-09244-5", "10.1016/j.tics.2013.06.006", "10.1037/a0037009", "10.1037/0003-066x.48.1.26", "10.1109/infvis.2004.70", "10.1037/0003-066x.45.4.489", "10.1037/h0092976", "10.1177/1473871612441542", "10.1145/302979.303030", "10.1016/b978-0-12-386915-9.00003-6", "10.1080/08870449608404995", "10.1109/tvcg.2013.156", "10.1145/2110192.2110202", "10.3758/cabn.2.4.341", "10.1037/0096-1523.27.1.92", "10.1109/tvcg.2007.70515", "10.1145/1385569.1385602", "10.24963/ijcai.2017/217", "10.1080/135467897394419", "10.1007/978-3-642-31454-4\\_23", "10.1037/a0021016", "10.1109/tvcg.2014.2346452", "10.1145/3301275.3302313", "10.1145/2633043", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1109/vl.1996.545307", "10.1037/0033-2909.119.2.197", "10.1523/jneurosci.2145-09.2009", "10.1177/1473871613513227", "10.1037/0022-3514.51.4.875", "10.1109/mcg.2009.49", "10.1002/acp.1344", "10.1037/a0016127", "10.1037/1040-3590.18.2.192", "10.1111/j.1467-8659.2011.01928.x", "10.1016/s0140-6736(02)07441-x", "10.1037/0022-0663.96.3.471", "10.1207/s15327752jpa4803\\_13", "10.1007/s10798-018-9446-3", "10.1002/(sici)1097-4571(2000)51:6", "10.1109/vast.2009.5333468", "10.1109/mcg.2009.22", "10.1145/3079628.3079634", "10.1016/j.jrp.2014.05.003", "10.1016/s0079-6123(07)00020-9", "10.1002/ijop.12511", "10.1111/j.2044-8279.1982.tb00821.x", "10.5539/jel.v4n4p91", "10.1177/1473871615594652", "10.1037/0022-3514.66.5.950", "10.1037/h0042761", "10.1109/tvcg.2012.199", "10.1177/1046496403257228", "10.1080/13614569708914684", "10.1111/cgf.13678", "10.1016/j.lindif.2003.08.001", "10.1016/j.jecp.2009.11.003", "10.1109/tvcg.2014.2346984", "10.1109/vast.2011.6102445", "10.1145/2678025.2701376", "10.1057/ivs.2008.31", "10.1533/9781780630366", "10.1111/j.2044-8260.1992.tb00972.x", "10.1016/j.jrp.2011.12.003", "10.1016/0747-5632(91)90002-i", "10.1080/13546781343000222", "10.3758/s13414-013-0610-2", "10.1145/3301275.3302283", "10.1177/106907279300100107", "10.1109/vast.2012.6400535", "10.1145/3377325.3377502", "10.1037/1089-2699.10.4.249", "10.1016/j.paid.2003.08.018", "10.1016/j.paid.2010.09.015"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 33}, {"doi": "10.1109/tvcg.2016.2598543", "title": "Annotation Graphs: A Graph-Based Visualization for Meta-Analysis of Data Based on User-Authored Annotations", "year": "2016", "conferenceName": "VAST", "authors": "Jian Zhao;Michael Glueck;Simon Breslav;Fanny Chevalier;Azam Khan", "citationCount": "12", "affiliation": "Zhao, J (Corresponding Author), Autodesk Res, Toronto, ON, Canada. Zhao, Jian; Glueck, Michael; Breslav, Simon; Khan, Azam, Autodesk Res, Toronto, ON, Canada. Chevalier, Fanny, INRIA, Rocquencourt, France.", "countries": "Canada;France", "abstract": "User-authored annotations of data can support analysts in the activity of hypothesis generation and sensemaking, where it is not only critical to document key observations, but also to communicate insights between analysts. We present annotation graphs, a dynamic graph visualization that enables meta-analysis of data based on user-authored annotations. The annotation graph topology encodes annotation semantics, which describe the content of and relations between data selections, comments, and tags. We present a mixed-initiative approach to graph layout that integrates an analyst's manual manipulations with an automatic method based on similarity inferred from the annotation semantics. Various visual graph layout styles reveal different perspectives on the annotation semantics. Annotation graphs are implemented within C8, a system that supports authoring annotations during exploratory analysis of a dataset. We apply principles of Exploratory Sequential Data Analysis (ESDA) in designing C8, and further link these to an existing task typology in the visualization literature. We develop and evaluate the system through an iterative user-centered design process with three experts, situated in the domain of analyzing HCI experiment data. The results suggest that annotation graphs are effective as a method of visually extending user-authored annotations to data meta-analysis for discovery and organization of ideas.", "keywords": "Externalization user-authored annotation;exploratory sequential data analysis;graph-based visualization", "link": "http://dx.doi.org/10.1109/TVCG.2016.2598543", "refList": ["10.1109/tvcg.2008.137", "10.1145/2207676.2208288", "10.1145/355017.355022", "10.1145/2598153.2598168", "10.1145/1142473.1142574", "10.1145/1124772.1124891", "10.1109/tvcg.2014.2346573", "10.1007/978-3-540-89965-5\\_10", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2013.124", "10.1109/mcg.2009.49", "10.1109/vast.2011.6102447", "10.1109/vast.2010.5652879", "10.1177/1473871612468877", "10.1145/2470654.2481417", "10.1109/vast.2009.5333023", "10.1109/tvcg.2007.70577", "10.1145/1124772.1124890", "10.1109/vast.2008.4677365", "10.1109/vast.2009.5333878", "10.3115/v1/d14-1162", "10.1145/2133806.2133821", "10.1207/s15327051hci0903", "10.1109/mcg.2015.50"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2017.2745279", "title": "Supporting Handoff in Asynchronous Collaborative Sensemaking Using Knowledge-Transfer Graphs", "year": "2017", "conferenceName": "VAST", "authors": "Jian Zhao;Michael Glueck;Petra Isenberg;Fanny Chevalier;Azam Khan", "citationCount": "9", "affiliation": "Zhao, J (Corresponding Author), FX Palo Alto Lab, Palo Alto, CA 94304 USA. Zhao, Jian, FX Palo Alto Lab, Palo Alto, CA 94304 USA. Glueck, Michael; Khan, Azam, Autodesk Res, San Rafael, CA USA. Isenberg, Petra; Chevalier, Fanny, Inria, Le Chesnay, France.", "countries": "USA;France", "abstract": "During asynchronous collaborative analysis, handoff of partial findings is challenging because externalizations produced by analysts may not adequately communicate their investigative process. To address this challenge, we developed techniques to automatically capture and help encode tacit aspects of the investigative process based on an analyst's interactions, and streamline explicit authoring of handoff annotations. We designed our techniques to mediate awareness of analysis coverage, support explicit communication of progress and uncertainty with annotation, and implicit communication through playback of investigation histories. To evaluate our techniques, we developed an interactive visual analysis system, KTGraph, that supports an asynchronous investigative document analysis task. We conducted a two-phase user study to characterize a set of handoff strategies and to compare investigative performance with and without our techniques. The results suggest that our techniques promote the use of more effective handoff strategies, help increase an awareness of prior investigative process and insights, as well as improve final investigative outcomes.", "keywords": "Collaboration,sensemaking,handoff,handover,structured externalizations,interactive visual analysis", "link": "http://dx.doi.org/10.1109/TVCG.2017.2745279", "refList": ["10.4301/s1807-17752012000200003", "10.1109/tvcg.2008.137", "10.1111/j.1467-8659.2009.01444.x", "10.1145/1357054.1357245", "10.1145/1142473.1142574", "10.1109/vast.2011.6102438", "10.1109/tvcg.2011.287", "10.1109/jisic.2014.13", "10.1145/2669485.2669518", "10.20380/gi2015.16", "10.1177/1473871611412817", "10.1109/tvcg.2014.2346573", "10.1057/palgrave.ivs.9500167", "10.1002/meet.2009.1450460219", "10.1109/tvcg.2009.104", "10.1145/2576099", "10.1109/mcg.2009.49", "10.1109/vast.2010.5652932", "10.1145/1866029.1866054", "10.1109/tvcg.2015.2403312", "10.1109/vast.2011.6102447", "10.1109/tvcg.2016.2598466", "10.1145/1718918.1718976", "10.1109/vast.2008.4677358", "10.1109/vast.2007.4389009", "10.1038/nmeth.2659", "10.1109/vast.2009.5333020", "10.1109/tvcg.2007.70589", "10.1109/tvcg.2007.70577", "10.1145/1240624.1240781", "10.1109/tvcg.2006.65", "10.1057/palgrave.ivs.9500180", "10.1002/meet.2008.1450450234", "10.1145/2396636.2396673", "10.1037/0033-295x.106.4.643", "10.1145/1518701.1518957", "10.1145/1124772.1124890", "10.1109/vast.2009.5333878", "10.1109/tvcg.2016.2598543", "10.1109/tvcg.2012.89", "10.1109/mcg.2015.50", "10.1007/978-3-642-23771-3\\_44", "10.1145/2556288.2557170", "10.1109/tvcg.2007.70568", "10.1109/cts.2014.6867610"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865039", "title": "Enhancing Web-based Analytics Applications through Provenance", "year": "2018", "conferenceName": "VAST", "authors": "Akhilesh Camisetty;Chaitanya Chandurkar;Maoyuan Sun;David Koop", "citationCount": "1", "affiliation": "Camisetty, A (Corresponding Author), UMass Dartmouth, Dartmouth, NS 02747, Canada. Camisetty, Akhilesh; Chandurkar, Chaitanya; Sun, Maoyuan; Koop, David, UMass Dartmouth, Dartmouth, NS 02747, Canada.", "countries": "Canada", "abstract": "Visual analytics systems continue to integrate new technologies and leverage modern environments for exploration and collaboration, making tools and techniques available to a wide audience through web browsers. Many of these systems have been developed with rich interactions, offering users the opportunity to examine details and explore hypotheses that have not been directly encoded by a designer. Understanding is enhanced when users can replay and revisit the steps in the sensemaking process, and in collaborative settings, it is especially important to be able to review not only the current state but also what decisions were made along the way. Unfortunately, many web-based systems lack the ability to capture such reasoning, and the path to a result is transient, forgotten when a user moves to a new view. This paper explores the requirements to augment existing client-side web applications with support for capturing, reviewing, sharing, and reusing steps in the reasoning process. Furthermore, it considers situations where decisions are made with streaming data, and the insights gained from revisiting those choices when more data is available. It presents a proof of concept, the Shareable Interactive Manipulation Provenance framework (SIMProv.js), that addresses these requirements in a modern, client-side JavaScript library, and describes how it can be integrated with existing frameworks.", "keywords": "Collaboration,provenance,streaming data,history,web", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865039", "refList": ["10.1145/2884781.2884864", "10.1109/tvcg.2008.137", "10.1007/978-3-319-40593-3\\_9", "10.1145/2598153.2598168", "10.1007/978-3-642-17819-1\\_34", "10.1109/tvcg.2016.2598471", "10.2218/ijdc.v7i1.213", "10.1145/2669485.2669518", "10.20380/gi2015.16", "10.1177/1473871611412817", "10.1111/cgf.12131", "10.1145/67544.66963", "10.1057/palgrave.ivs.9500167", "10.1063/1.3662391", "10.1145/2882903.2899392", "10.1109/visual.1993.398857", "10.1109/mic.2017.2911428", "10.1109/tvcg.2015.2467611", "10.1109/vast.2010.5652932", "10.1057/ivs.2008.31", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2013.197", "10.1109/visual.1999.809871", "10.1145/1084805.1084812", "10.1016/s0950-7051(00)00101-5", "10.1007/s00778-017-0486-1", "10.1109/tvcg.2007.70589", "10.1109/scc.2008.21", "10.1145/2396636.2396673", "10.1109/tvcg.2017.2745279", "10.1109/wvl.1988.18020", "10.1109/mcg.2003.1185579", "10.1109/tvcg.2015.2467191", "10.1145/2501988.2502050", "10.1145/1979742.1979570", "10.1111/j.1467-8659.2009.01687.x", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1111/cgf.12903", "10.1145/2807442.2807468", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934264", "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;David S. Ebert;Arif Ghafoor", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Karimzadeh, Morteza; Ebert, David S.; Ghafoor, Arif, Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.", "keywords": "Summative evaluation,usefulness,evaluation process,taxonomy,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934264", "refList": ["10.1109/tvcg.2017.2744478", "10.1109/tvcg.2018.2865025", "10.1109/tvcg.2006.85", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2017.2745181", "10.1111/cgf.13677", "10.1109/tvcg.2018.2864844", "10.1109/tvcg.2013.126", "10.1109/tvcg.2018.2864811", "10.1109/infvis.2005.1532147", "10.1177/0956797613504966", "10.1145/2669557.2669579", "10.1109/mcg.2005.102", "10.1109/visual.2003.1250426", "10.1136/bmj.39489.470347.ad", "10.1109/tvcg.2017.2744080", "10.1109/mcg.2009.53", "10.1111/j.1467-8527.2005.00307.x", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2864886", "10.1109/tvcg.2018.2864843", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2018.2865051", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2018.2865026", "10.1007/978-3-540-71080-6\\_6", "10.1109/tvcg.2018.2865020", "10.1177/1473871611407399", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2018.2864526", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2864905", "10.1049/sej.1991.0040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2711030", "10.1109/tvcg.2011.279", "10.1109/vast.2017.8585505", "10.1147/jrd.2010.2042914", "10.1016/s0378-7206(98)00044-5", "10.1145/2993901.2993913", "10.1109/tvcg.2018.2865041", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744758", "10.1145/1168149.1168158", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2012.213", "10.1109/tvcg.2017.2744738", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2018.2864826", "10.1145/1377966.1377974", "10.1109/apec.2009.4802646", "10.1145/1168149.1168152", "10.1016/j.jss.2008.03.059", "10.1109/vast.2017.8585484", "10.1109/tvcg.2017.2744818", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2865042", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030388", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "year": "2020", "conferenceName": "SciVis", "authors": "Pepe Eulzer;Sabine Bauer;Francis Kilian;Kai Lawonn", "citationCount": "0", "affiliation": "Eulzer, P (Corresponding Author), Univ Jena, Jena, Germany. Eulzer, Pepe; Lawonn, Kai, Univ Jena, Jena, Germany. Bauer, Sabine, Univ Koblenz Landau, Koblenz, Germany. Kilian, Francis, Cath Clin Koblenz Montabaur, Dept Spine Surg, Koblenz, Germany.", "countries": "Germany", "abstract": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. $\\mathrm{By}$ linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. $\\mathrm{in}$ a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.", "keywords": "Medical visualization,bioinformatics,coordinated views,focus and context,biomechanical simulation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030388", "refList": ["10.1109/tvcg.2018.2864903", "10.1177/1473871613510429", "10.1093/ehjqcco/qcz052", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2018.2865076", "10.1055/s-0039-1687862", "10.1109/visual.1990.146375", "10.1109/tvcg.2017.2744198", "10.1016/j.ijmedinf.2014.10.001", "10.1109/tvcg.2013.124", "10.1016/j.jacc", "10.1111/cgf.13167", "10.17705/1thci.00055", "10.1136/bmjqs.2009.037895", "10.1109/tvcg.2013.238", "10.1109/tvcg.2018.2865240", "10.1186/1471-2261-6-34", "10.1109/tvcg.2019.2934264", "10.1109/tvcg.2013.200", "10.1109/tvcg.2011.209", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467091", "10.1136/bmjopen-2019-033208", "10.1109/beliv.2018.8634027", "10.1109/tvcg.2012.213", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2015.2467325", "10.1145/2133806.2133821", "10.1145/1806799.1806866", "10.1108/02635570610688869", "10.1002/hbm.20701", "10.1561/1100000039", "10.1145/3025453.3025645", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934797", "title": "VisTA: Integrating Machine Intelligence with Visualization to Support the Investigation of Think-Aloud Sessions", "year": "2019", "conferenceName": "InfoVis", "authors": "Mingming Fan;Ke Wu;Jian Zhao;Yue Li;Winter Wei;Khai N. Truong", "citationCount": "0", "affiliation": "Fan, MM (Corresponding Author), Univ Toronto, Toronto, ON, Canada. Fan, MM (Corresponding Author), Rochester Inst Technol, Rochester, NY 14623 USA. Fan, Mingming; Wu, Ke; Li, Yue; Wei, Winter; Khai N Truong, Univ Toronto, Toronto, ON, Canada. Fan, Mingming, Rochester Inst Technol, Rochester, NY 14623 USA. Zhao, Jian, Univ Waterloo, Waterloo, ON, Canada.", "countries": "Canada;USA", "abstract": "Think-aloud protocols are widely used by user experience (UX) practitioners in usability testing to uncover issues in user interface design. It is often arduous to analyze large amounts of recorded think-aloud sessions and few UX practitioners have an opportunity to get a second perspective during their analysis due to time and resource constraints. Inspired by the recent research that shows subtle verbalization and speech patterns tend to occur when users encounter usability problems, we take the first step to design and evaluate an intelligent visual analytics tool that leverages such patterns to identify usability problem encounters and present them to UX practitioners to assist their analysis. We first conducted and recorded think-aloud sessions, and then extracted textual and acoustic features from the recordings and trained machine learning (ML) models to detect problem encounters. Next, we iteratively designed and developed a visual analytics tool, VisTA, which enables dynamic investigation of think-aloud sessions with a timeline visualization of ML predictions and input features. We conducted a between-subjects laboratory study to compare three conditions, i.e., VisTA, VisTASimple (no visualization of the ML's input features), and Baseline (no ML information at all), with 30 UX professionals. The findings show that UX professionals identified more problem encounters when using VisTA than Baseline by leveraging the problem visualization as an overview, anticipations, and anchors as well as the feature visualization as a means to understand what ML considers and omits. Our findings also provide insights into how they treated ML, dealt with (dis)agreement with ML, and reviewed the videos (i.e., play, pause, and rewind).", "keywords": "Think-aloud,visual analytics,machine intelligence,user study,usability problems,session review behavior,UX practices", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934797", "refList": ["10.1109/tvcg.2008.137", "10.1109/ichi.2015.26", "10.1037/1089-2680.2.2.175", "10.1080/10447318.2012.758529", "10.1037/a0021663", "10.1111/cgf.13079", "10.1109/pacificvis.2017.8031598", "10.1145/2851581.2856492", "10.1145/3290605.3300641", "10.1145/3325281", "10.1109/tvcg.2015.2467871", "10.1080/10447318.2014.930311", "10.1080/0144929x.2012.708786", "10.1109/vast.2010.5653598", "10.1109/tvcg.2015.2468151", "10.1145/3025453.3025739", "10.1109/mcg.2009.49", "10.1145/2207676.2208365", "10.1080/01449290701773842", "10.1080/10447318.2015.1065691", "10.3868/s020-008-019-0001-9", "10.1109/tpc.2011.2182569", "10.1207/s15327590ijhc1304\\_05", "10.1109/tvcg.2015.2465151", "10.1007/978-3-642-21753-1\\_5", "10.1109/tvcg.2017.2745279", "10.1109/vast.2008.4677365", "10.1109/tpc.2012.2206190", "10.1109/47.867942", "10.1145/3173574.3173704", "10.1145/3196709.3196730", "10.1109/tpc.2010.2052859"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13400", "year": "2018", "title": "ChangeCatcher: Increasing Inter-author Awareness for Visualization Development", "conferenceName": "EuroVis", "authors": "Mona Hosseinkhani Loorak;Melanie Tory;Sheelagh Carpendale", "citationCount": "1", "affiliation": "Loorak, M (Corresponding Author), Autodesk Res, Calgary, AB, Canada.\nLoorak, M., Autodesk Res, Calgary, AB, Canada.\nCarpendale, S., Univ Calgary, Calgary, AB, Canada.", "countries": "Canada", "abstract": "We introduce an approach for explicitly revealing changes between versions of a visualization workbook to support version comparison tasks. Visualization authors may need to understand version changes for a variety of reasons, analogous to document editing. An author who has been away for a while may need to catch up on the changes made by their co-author, or a person responsible for formatting compliance may need to check formatting changes that occurred since the last time they reviewed the work. We introduce ChangeCatcher, a prototype tool to help people find and understand changes in a visualization workbook, specifically, a Tableau workbook. Our design is based on interviews we conducted with experts to investigate user needs and practices around version comparison. ChangeCatcher provides an overview of changes across six categories, and employs a multi-level details-on-demand approach to progressively reveal details. Our qualitative study showed that ChangeCatcher's methods for explicitly revealing and categorizing version changes were helpful in version comparison tasks.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13400", "refList": ["10.1109/tvcg.2008.137", "10.1177/1473871611416549", "10.1097/00006324-199104000-00013", "10.1109/tvcg.2011.185", "10.20380/gi2015.16", "10.1007/978-3-642-23768-3\\_22", "10.1002/spe.4380150703", "10.1109/icde.2003.1260818", "10.1109/tvcg.2016.2598586", "10.1109/tvcg.2013.124", "10.1145/1166253.1166263", "10.1145/1101908.1101919", "10.1109/tvcg.2015.2467551", "10.1136/amiajnl-2012-001351", "10.1109/32.177365", "10.1145/1101908.1101940", "10.1007/978-3-540-70956-5\\_5", "10.1145/1056018.1056025", "10.1002/meet.2008.1450450234", "10.1109/tvcg.2017.2745279", "10.1109/32.895984", "10.1109/mcg.2015.50"], "wos": 1, "children": [{"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.13717", "year": "2019", "title": "InsideInsights: Integrating Data-Driven Reporting in Collaborative Visual Analytics", "conferenceName": "EuroVis", "authors": "Andreas Mathisen;Tom Horak;Clemens Nylandsted Klokmose;Kaj Gr{\\o}nb{\\ae}k;Niklas Elmqvist", "citationCount": "2", "affiliation": "Mathisen, A (Corresponding Author), Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nMathisen, A.; Grdnbaek, K., Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHorak, T., Tech Univ Dresden, Interact Media Lab, Dresden, Germany.\nKlokmose, C. N., Aarhus Univ, Dept Digital Design \\& Informat Studies, Aarhus, Denmark.\nElmqvist, N., Univ Maryland, Coll Informat Studies, College Pk, MD 20742 USA.", "countries": "Germany;USA;Denmark", "abstract": "Analyzing complex data is a non-linear process that alternates between identifying discrete facts and developing overall assessments and conclusions. In addition, data analysis rarely occurs in solitude; multiple collaborators can be engaged in the same analysis, or intermediate results can be reported to stakeholders. However, current data-driven communication tools are detached from the analysis process and promote linear stories that forego the hierarchical and branching nature of data analysis, which leads to either too much or too little detail in the final report. We propose a conceptual design for integrated data-driven reporting that allows for iterative structuring of insights into hierarchies linked to analytic provenance and chosen analysis views. The hierarchies become dynamic and interactive reports where collaborators can review and modify the analysis at a desired level of detail. Our web-based InsideInsights system provides interaction techniques to annotate states of analytic components, structure annotations, and link them to appropriate presentation views. We demonstrate the generality and usefulness of our system with two use cases and a qualitative expert review.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13717", "refList": ["10.1016/j.intcom.2009.07.001", "10.1145/2642918.2647360", "10.1186/s13059-017-1205-3", "10.1145/3173574.3173606", "10.1145/2854946.2854958", "10.1109/visual.2005.1532788", "10.1145/2858036.2858435", "10.1007/978-3-540-70956-5\\_2", "10.1109/mcg.2005.102", "10.20380/gi2015.16", "10.1145/381641.381653", "10.1111/cgf.12392", "10.3233/978-1-61499-649-1-87", "10.1109/tvcg.2012.219", "10.1109/tvcg.2014.2346573", "10.1057/palgrave.ivs.9500167", "10.1145/2858036.2858529", "10.1145/2807442.2807446", "10.1109/visual.1993.398857", "10.1111/cgf.12925", "10.1057/ivs.2008.31", "10.1109/tvcg.2017.2723393", "10.1109/tvcg.2015.2467551", "10.1109/mcg.2015.99", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1093/comjnl/27.2.97", "10.1109/tvcg.2017.2743990", "10.1145/3173574.3173973", "10.1145/3242587.3242600", "10.1145/3025453.3025626", "10.1145/3290605.3300272", "10.1109/tvcg.2007.70577", "10.1145/3170427.3188563", "10.1145/3126594.3126642", "10.1007/s00146-010-0272-8", "10.1038/515151a", "10.1002/meet.2008.1450450234", "10.1109/tvcg.2017.2745279", "10.1109/mcg.2006.70", "10.1145/3173574.3173748", "10.1109/tvcg.2015.2467191", "10.1145/1435417.1435439", "10.1109/tvcg.2010.179", "10.1109/vds.2017.8573439", "10.1109/tvcg.2011.255", "10.1145/3274419", "10.1145/989863.989880", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2016.2599030", "10.1145/1376616.1376772", "10.1109/tvcg.2018.2865144"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030438", "title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening", "year": "2020", "conferenceName": "SciVis", "authors": "Mar\u00eda Virginia Sabando;Pavol Ulbrich;Mat\u00edas N. Selzer;Jan Byska;Jan Mican;Ignacio Ponzoni;Axel J. Soto;Maria Luj\u00e1n Ganuza;Barbora Kozl\u00edkov\u00e1", "citationCount": "0", "affiliation": "Sabando, MV (Corresponding Author), Univ Nacl Sur, Inst Comp Sci \\& Engn UNS CONICET, Bahia Blanca, Buenos Aires, Argentina. Sabando, MV (Corresponding Author), Univ Nacl Sur, Dept Comp Sci \\& Engn, Bahia Blanca, Buenos Aires, Argentina. Sabando, Maria Virginia; Selzer, Matias; Ponzoni, Ignacio; Soto, Axel J.; Ganuza, Maria Lujan, Univ Nacl Sur, Inst Comp Sci \\& Engn UNS CONICET, Bahia Blanca, Buenos Aires, Argentina. Sabando, Maria Virginia; Ponzoni, Ignacio; Soto, Axel J., Univ Nacl Sur, Dept Comp Sci \\& Engn, Bahia Blanca, Buenos Aires, Argentina. Ulbrich, Pavol; Byska, Jan; Kozlikova, Barbora, Masaryk Univ, Fac Informat, Visitlab, Brno, Czech Republic. Selzer, Matias; Ganuza, Maria Lujan, Univ Nacl Sur, VyGLab Res Lab UNS CICPBA, Dept Comp Sci \\& Engn, Bahia Blanca, Buenos Aires, Argentina. Mican, Jan, Masaryk Univ, Dept Expt Biol, Loschmidt Labs, Brno, Czech Republic. Mican, Jan, Masaryk Univ, RECETOX, Brno, Czech Republic. Mican, Jan, Masaryk Univ, Fac Med, Brno, Czech Republic.", "countries": "Argentina;Republic", "abstract": "In the modern drug discovery process, medicinal chemists deal with the complexity of analysis of large ensembles of candidate molecules. Computational tools, such as dimensionality reduction (DR) and classification, are commonly used to efficiently process the multidimensional space of features. These underlying calculations often hinder interpretability of results and prevent experts from assessing the impact of individual molecular features on the resulting representations. To provide a solution for scrutinizing such complex data, we introduce ChemVA, an interactive application for the visual exploration of large molecular ensembles and their features. Our tool consists of multiple coordinated views: Hexagonal view, Detail view, 3D view, Table view, and a newly proposed Difference view designed for the comparison of DR projections. These views display DR projections combined with biological activity, selected molecular features, and confidence scores for each of these projections. This conjunction of views allows the user to drill down through the dataset and to efficiently select candidate compounds. Our approach was evaluated on two case studies of finding structurally similar ligands with similar binding affinity to a target protein, as well as on an external qualitative evaluation. The results suggest that our system allows effective visual inspection and comparison of different high-dimensional molecular representations. Furthermore, ChemVA assists in the identification of candidate compounds while providing information on the certainty behind different molecular representations.", "keywords": "Virtual screening,visual analysis,dimensionality reduction,coordinated views,cheminformatics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030438", "refList": ["10.1109/tvcg.2008.137", "10.1057/ivs.2009.10", "10.2312/eurovisstar.20151110", "10.1109/eisic.2015.35", "10.1109/pacificvis.2014.44", "10.1145/1142473.1142574", "10.1109/tvcg.2013.223", "10.1109/tvcg.2014.2346573", "10.1109/tvcg.2019.2934539", "10.1111/cgf.13717", "10.1109/vizsec.2009.5375536", "10.1111/cgf.12925", "10.1109/tvcg.2015.2467551", "10.1109/mcg.2015.99", "10.1007/978-3-319", "10.1109/tvcg.2012.255", "10.1145/1064830.1064834", "10.1177/1473871611433713", "10.1207/s1532690xci0804\\_2", "10.1145/1168149.1168168", "10.1016/j.chb.2006.10.002", "10.1109/tvcg.2014.2346441", "10.1109/eisic.2017.15", "10.1111/1467-8721.00160", "10.1109/tvcg.2018.2865024", "10.1109/infvis.2004.2"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13997", "year": "2020", "title": "Resolving Conflicting Insights in Asynchronous Collaborative Visual Analysis", "conferenceName": "EuroVis", "authors": "Jianping Kelvin Li;Shenyu Xu;Yecong (Chris) Ye;Kwan{-}Liu Ma", "citationCount": "0", "affiliation": "Li, JK (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA.\nLi, Jianping Kelvin; Xu, Shenyu; Ye, Yecong (Chris); Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "Analyzing large and complex datasets for critical decision making can benefit from a collective effort involving a team of analysts. However, insights and findings from different analysts are often incomplete, disconnected, or even conflicting. Most existing analysis tools lack proper support for examining and resolving the conflicts among the findings in order to consolidate the results of collaborative data analysis. In this paper, we present CoVA, a visual analytics system incorporating conflict detection and resolution for supporting asynchronous collaborative data analysis. By using a declarative visualization language and graph representation for managing insights and insight provenance, CoVA effectively leverages distributed revision control workflow from software engineering to automatically detect and properly resolve conflicts in collaborative analysis results. In addition, CoVA provides an effective visual interface for resolving conflicts as well as combining the analysis results. We conduct a user study to evaluate CoVA for collaborative data analysis. The results show that CoVA allows better understanding and use of the findings from different analysts.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13997", "refList": ["10.1109/vast.2010.5652880", "10.1145/2642918.2647360", "10.1109/cluster.2017.26", "10.1145/1142473.1142574", "10.1109/vast.2011.6102438", "10.1145/2872332", "10.1145/2669485.2669518", "10.1177/1473871611412817", "10.1109/ms.2012.61", "10.3233/978-1-61499-649-1-87", "10.1109/tvcg.2014.2346573", "10.1057/palgrave.ivs.9500167", "10.1109/pacificvis.2009.4906837", "10.1109/visual.1993.398857", "10.1145/2207676.2208293", "10.1145/381641.381656", "10.1111/cgf.13717", "10.1111/cgf.13402", "10.1057/palgrave.ivs.9500131", "10.1109/vast.2011.6102447", "10.1109/tvcg.2015.2467551", "10.1109/vast.2008.4677358", "10.1177/104973239300300403", "10.1007/978-1-4419-5874-7\\_12", "10.1109/tst.2013.6509100", "10.1109/vast.2014.7042526", "10.1016/s0950-7051(00)00101-5", "10.1109/tvcg.2015.2467091", "10.1109/vast.2010.5652885", "10.1145/3126594.3126642", "10.1109/tvcg.2006.65", "10.1057/palgrave.ivs.9500180", "10.1109/vast.2008.4677365", "10.1109/vast.2009.5333878", "10.1109/tvcg.2016.2598543", "10.1145/1979742.1979570", "10.1109/tvcg.2003.1207445", "10.1109/mcg.2015.50", "10.1080/09546550701246817", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 23}, {"doi": "10.1111/cgf.13997", "year": "2020", "title": "Resolving Conflicting Insights in Asynchronous Collaborative Visual Analysis", "conferenceName": "EuroVis", "authors": "Jianping Kelvin Li;Shenyu Xu;Yecong (Chris) Ye;Kwan{-}Liu Ma", "citationCount": "0", "affiliation": "Li, JK (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA.\nLi, Jianping Kelvin; Xu, Shenyu; Ye, Yecong (Chris); Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "Analyzing large and complex datasets for critical decision making can benefit from a collective effort involving a team of analysts. However, insights and findings from different analysts are often incomplete, disconnected, or even conflicting. Most existing analysis tools lack proper support for examining and resolving the conflicts among the findings in order to consolidate the results of collaborative data analysis. In this paper, we present CoVA, a visual analytics system incorporating conflict detection and resolution for supporting asynchronous collaborative data analysis. By using a declarative visualization language and graph representation for managing insights and insight provenance, CoVA effectively leverages distributed revision control workflow from software engineering to automatically detect and properly resolve conflicts in collaborative analysis results. In addition, CoVA provides an effective visual interface for resolving conflicts as well as combining the analysis results. We conduct a user study to evaluate CoVA for collaborative data analysis. The results show that CoVA allows better understanding and use of the findings from different analysts.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13997", "refList": ["10.1109/vast.2010.5652880", "10.1145/2642918.2647360", "10.1109/cluster.2017.26", "10.1145/1142473.1142574", "10.1109/vast.2011.6102438", "10.1145/2872332", "10.1145/2669485.2669518", "10.1177/1473871611412817", "10.1109/ms.2012.61", "10.3233/978-1-61499-649-1-87", "10.1109/tvcg.2014.2346573", "10.1057/palgrave.ivs.9500167", "10.1109/pacificvis.2009.4906837", "10.1109/visual.1993.398857", "10.1145/2207676.2208293", "10.1145/381641.381656", "10.1111/cgf.13717", "10.1111/cgf.13402", "10.1057/palgrave.ivs.9500131", "10.1109/vast.2011.6102447", "10.1109/tvcg.2015.2467551", "10.1109/vast.2008.4677358", "10.1177/104973239300300403", "10.1007/978-1-4419-5874-7\\_12", "10.1109/tst.2013.6509100", "10.1109/vast.2014.7042526", "10.1016/s0950-7051(00)00101-5", "10.1109/tvcg.2015.2467091", "10.1109/vast.2010.5652885", "10.1145/3126594.3126642", "10.1109/tvcg.2006.65", "10.1057/palgrave.ivs.9500180", "10.1109/vast.2008.4677365", "10.1109/vast.2009.5333878", "10.1109/tvcg.2016.2598543", "10.1145/1979742.1979570", "10.1109/tvcg.2003.1207445", "10.1109/mcg.2015.50", "10.1080/09546550701246817", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 27}, {"doi": "10.1109/tvcg.2018.2865117", "title": "Patterns and Pace: Quantifying Diverse Exploration Behavior with Visualizations on the Web", "year": "2018", "conferenceName": "InfoVis", "authors": "Mi Feng;Evan M. Peck;Lane Harrison", "citationCount": "3", "affiliation": "Feng, M (Corresponding Author), Worcester Polytech Inst, Worcester, MA 01609 USA. Feng, Mi; Harrison, Lane, Worcester Polytech Inst, Worcester, MA 01609 USA. Peck, Evan, Bucknell Univ, Lewisburg, PA 17837 USA.", "countries": "USA", "abstract": "The diverse and vibrant ecosystem of interactive visualizations on the web presents an opportunity for researchers and practitioners to observe and analyze how everyday people interact with data visualizations. However, existing metrics of visualization interaction behavior used in research do not fully reveal the breadth of peoples' open-ended explorations with visualizations. One possible way to address this challenge is to determine high-level goals for visualization interaction metrics, and infer corresponding features from user interaction data that characterize different aspects of peoples' explorations of visualizations. In this paper, we identify needs for visualization behavior measurement, and develop corresponding candidate features that can be inferred from users' interaction data. We then propose metrics that capture novel aspects of peoples' open-ended explorations, including exploration uniqueness and exploration pacing. We evaluate these metrics along with four other metrics recently proposed in visualization literature by applying them to interaction data from prior visualization studies. The results of these evaluations suggest that these new metrics 1) reveal new characteristics of peoples' use of visualizations, 2) can be used to evaluate statistical differences between visualization designs, and 3) are statistically independent of prior metrics used in visualization research. We discuss implications of these results for future studies, including the potential for applying these metrics in visualization interaction analysis, as well as emerging challenges in developing and selecting metrics depicting visualization explorations.", "keywords": "Interaction,Visualization,Quantitative Evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865117", "refList": ["10.1109/tvcg.2008.137", "10.1109/ldav.2012.6378977", "10.1145/2702123.2702452", "10.1145/2702123.2702419", "10.1109/tvcg.2014.2346575", "10.1145/2702123.2702275", "10.1109/iccv.2013.147", "10.1109/tvcg.2014.2346452", "10.1016/0306-4573(88)90021-0", "10.1109/tvcg.2015.2467613", "10.1109/tvcg.2011.229", "10.1109/tvcg.2015.2467201", "10.1109/vast.2007.4389008", "10.1007/978-1-4419-0492-8\\_2", "10.1109/tvcg.2017.2745958", "10.1145/2020408.2020581", "10.1109/tvcg.2016.2598797", "10.1109/tvcg.2015.2467871", "10.1145/2702123.2702590", "10.1109/tvcg.2016.2598466", "10.1109/tvcg.2013.200", "10.1109/tvcg.2005.53", "10.1109/vast.2007.4389009", "10.1145/2678025.2701407", "10.1111/cgf.13208", "10.1145/989863.989880", "10.1109/tvcg.2007.70515", "10.1145/365024.365325", "10.1109/tvcg.2016.2599058"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030383", "title": "II-20: Intelligent and pragmatic analytic categorization of image collections", "year": "2020", "conferenceName": "VAST", "authors": "Jan Zah\u00e1lka;Marcel Worring;Jarke J. van Wijk", "citationCount": "0", "affiliation": "Zahalka, J (Corresponding Author), Czech Tech Univ, Prague, Czech Republic. Zahalka, Jan, Czech Tech Univ, Prague, Czech Republic. Worring, Marcel, Univ Amsterdam, Amsterdam, Netherlands. van Wijk, Jarke J., Eindhoven Univ Technol, Eindhoven, Netherlands.", "countries": "Republic;Netherlands", "abstract": "In this paper, we introduce 11\u201320 (Image Insight 2020), a multimedia analytics approach for analytic categorization of image collections. Advanced visualizations for image collections exist, but they need tight integration with a machine model to support the task of analytic categorization. Directly employing computer vision and interactive learning techniques gravitates towards search. Analytic categorization, however, is not machine classification (the difference between the two is called the pragmatic gap): a human adds/redefines/deletes categories of relevance on the fly to build insight, whereas the machine classifier is rigid and non-adaptive. Analytic categorization that truly brings the user to insight requires a flexible machine model that allows dynamic sliding on the exploration-search axis, as well as semantic interactions: a human thinks about image data mostly in semantic terms. 11\u201320 brings three major contributions to multimedia analytics on image collections and towards closing the pragmatic gap. Firstly, a new machine model that closely follows the user's interactions and dynamically models her categories of relevance. II-20's machine model, in addition to matching and exceeding the state of the art's ability to produce relevant suggestions, allows the user to dynamically slide on the exploration-search axis without any additional input from her side. Secondly, the dynamic, 1-image-at-a-time Tetris metaphor that synergizes with the model. It allows a well-trained model to analyze the collection by itself with minimal interaction from the user and complements the classic grid metaphor. Thirdly, the fast-forward interaction, allowing the user to harness the model to quickly expand (\u201cfast-forward\u201d) the categories of relevance, expands the multimedia analytics semantic interaction dictionary. Automated experiments show that II-20's machine model outperforms the existing state of the art and also demonstrate the Tetris metaphor's analytic quality. User studies further confirm that II\u201320 is an intuitive, efficient, and effective multimedia analytics tool.", "keywords": "Multimedia analytics,image data,analytic categorization,pragmatic gap", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030383", "refList": ["10.1109/tvcg.2008.137", "10.1145/2882903.2882919", "10.1145/2556647.2556657", "10.1145/1142473.1142574", "10.1109/tvcg.2016.2598471", "10.1109/vast.2017.8585669", "10.1109/tvcg.2014.2346573", "10.1111/j.0956-7976.2005.00782.x", "10.1007/978-3-540-89965-5\\_27", "10.1109/visual.2019.8933611", "10.1109/vast.2010.5653598", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1109/tvcg.2012.271", "10.1109/mcg.2009.49", "10.1057/ivs.2008.31", "10.1111/cgf.12925", "10.1109/hicss.2016.183", "10.1109/tvcg.2016.2598466", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2018.2865117", "10.1109/vast47406.2019.8986948", "10.1016/s0950-7051(00)00101-5", "10.1109/tvcg.2016.2598594", "10.1109/tvcg.2006.101", "10.1111/cgf.12311", "10.1109/vast.2009.5333020", "10.1109/vast.2010.5652885", "10.1111/cgf.13670", "10.1109/wvl.1988.18020", "10.1145/2133806.2133821", "10.1109/vast.2012.6400486"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13678", "year": "2019", "title": "Characterizing Exploratory Visual Analysis: A Literature Review and Evaluation of Analytic Provenance in Tableau", "conferenceName": "EuroVis", "authors": "Leilani Battle;Jeffrey Heer", "citationCount": "2", "affiliation": "Battle, L (Corresponding Author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.\nBattle, Leilani, Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.\nHeer, Jeffrey, Univ Washington, Paul G Allen Sch Comp Sci \\& Engn, Seattle, WA 98195 USA.", "countries": "USA", "abstract": "Supporting exploratory visual analysis (EVA) is a central goal of visualization research, and yet our understanding of the process is arguably vague and piecemeal. We contribute a consistent definition of EVA through review of the relevant literature, and an empirical evaluation of existing assumptions regarding how analysts perform EVA using Tableau, a popular visual analysis tool. We present the results of a study where 27 Tableau users answered various analysis questions across 3 datasets. We measure task performance, identify recurring patterns across participants' analyses, and assess variance from task specificity and dataset. We find striking differences between existing assumptions and the collected data. Participants successfully completed a variety of tasks, with over 80\\% accuracy across focused tasks with measurably correct answers. The observed cadence of analyses is surprisingly slow compared to popular assumptions from the database community. We find significant overlap in analyses across participants, showing that EVA behaviors can be predictable. Furthermore, we find few structural differences between behavior graphs for open-ended and more focused exploration tasks.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13678", "refList": ["10.1109/dsia.2017.8339089", "10.1109/tvcg.2008.137", "10.1109/tvcg.2007.28", "10.1057/ivs.2009.22", "10.1038/526189a", "10.1109/tvcg.2006.85", "10.1145/2882903.2882919", "10.1145/2588555.2610523", "10.1109/tvcg.2014.2346575", "10.1145/2207676.2208294", "10.1145/2723372.2731084", "10.1109/tvcg.2016.2598471", "10.1145/3173574.3174168", "10.1109/visual.2005.1532788", "10.1109/tvcg.2014.2346452", "10.1109/tvcg.2015.2467613", "10.20380/gi2015.16", "10.14778/2831360.2831371", "10.1145/3173574.3174053", "10.1109/icdew.2006.75", "10.1145/2993901.2993912", "10.1145/1502650.1502695", "10.1109/tvcg.2012.219", "10.1007/978-3-642-23768-3\\_22", "10.1109/tvcg.2016.2598797", "10.1109/vl.1996.545307", "10.1111/j.1467-8659.2010.01830.x", "10.1109/tvcg.2015.2467871", "10.1145/3209900.3209901", "10.1145/2939502.2939513", "10.1145/381641.381656", "10.1109/tvcg.2013.124", "10.1109/vast.2008.4677357", "10.1145/1378773.1378788", "10.1109/mcg.2012.120", "10.1057/ivs.2008.31", "10.1109/infvis.2005.1532136", "10.1109/icde.2014.6816674", "10.1109/tvcg.2016.2598466", "10.1109/tvcg.2015.2467551", "10.1145/2939502.2939506", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2865117", "10.1145/1556262.1556276", "10.1016/s0950-7051(00)00101-5", "10.1109/tvcg.2008.109", "10.1145/2588555.2593666", "10.1109/tvcg.2018.2865040", "10.1109/tvcg.2013.179", "10.1111/cgf.13409", "10.1177/1473871616638546", "10.1109/2945.981851", "10.1109/tvcg.2010.164", "10.1109/vast.2008.4677365", "10.1109/tvcg.2015.2467191", "10.1145/2133806.2133821", "10.1145/2207676.2208412", "10.1109/tvcg.2017.2744319", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.224"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030425", "title": "Visual Analysis of Argumentation in Essays", "year": "2020", "conferenceName": "VAST", "authors": "Dora Kiesel;Patrick Riehmann;Henning Wachsmuth;Benno Stein;Bernd Fr\u00f6hlich", "citationCount": "0", "affiliation": "Kiesel, D (Corresponding Author), Bauhaus Univ Weimar, Weimar, Germany. Kiesel, Dora; Riehmann, Patrick; Stein, Benno; Froehlich, Bernd, Bauhaus Univ Weimar, Weimar, Germany. Wachsmuth, Henning, Paderborn Univ, Paderborn, Germany.", "countries": "Germany", "abstract": "This paper presents a visual analytics system for exploring, analyzing and comparing argument structures in essay corpora. We provide an overview of the corpus by a list of ArguLines which represent the argument units of each essay by a sequence of glyphs. Each glyph encodes the stance, the depth and the relative position of an argument unit. The overview can be ordered in various ways to reveal patterns and outliers. Subsets of essays can be selected and analyzed in detail using the Argument Unit Occurrence Tree which aggregates the argument structures using hierarchical histograms. This hierarchical view facilitates the estimation of statistics and trends concerning the progression of the argumentation in the essays. It also provides insights into the commonalities and differences between selected subsets. The text view is the necessary textual basis to verify conclusions from the other views and the annotation process. Linking the views and interaction techniques for visual filtering, studying the evolution of stance within a subset of essays and scrutinizing the order of argumentative units enable a deep analysis of essay corpora. Our expert reviews confirmed the utility of the system and revealed detailed and previously unknown information about the argumentation in our sample corpus.", "keywords": "Information Visualization,Text Analysis,User Interfaces,Visual Analytics,Argumentation Visualization,Glyph-based Techniques,Text and Document Data,Tree-based Visualization,Coordinated and Multiple Views,Close and Distant Reading", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030425", "refList": ["10.1109/tvcg.2014.2346575", "10.14778/2735479.2735485", "10.14778/3192965.3192971", "10.1111/cgf.12129", "10.1145/2206869.2206874", "10.1109/icde.2016.7498300", "10.14778/2831360.2831371", "10.1145/3035918.3056097", "10.1023/a:1009726021843", "10.1111/cgf.13678", "10.1145/3183713.3196905", "10.14778/3115404.3115418", "10.1109/tvcg.2012.180", "10.1109/icde.1999.754950", "10.14778/1453856.1453924", "10.1145/3209900.3209901", "10.1002/spe.2325", "10.1145/42201.42203", "10.1109/tvcg.2013.124", "10.1109/vast.2008.4677357", "10.1109/icde.2016.7498287", "10.1109/icde.2014.6816674", "10.14778/2732951.2732964", "10.1109/tvcg.2015.2467551", "10.1145/1084805.1084812", "10.1109/2.781635", "10.14778/2732279.2732280", "10.1109/icde.2015.7113427", "10.1109/tvcg.2015.2467091", "10.1007/s00778-017-0486-1", "10.1109/tvcg.2003.1196005", "10.1109/icde.2004.1320035", "10.1109/tvcg.2016.2607714", "10.14778/2732951.2732953", "10.1109/tvcg.2008.131", "10.1109/tvcg.2018.2865018", "10.1145/2133806.2133821", "10.1109/icde.2019.00035", "10.1109/tpds.2005.144", "10.14778/3236187.3236212", "10.1109/tvcg.2009.111", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14033", "year": "2020", "title": "Survey on Individual Differences in Visualization", "conferenceName": "EuroVis", "authors": "Zhengliang Liu;R. Jordan Crouser;Alvitta Ottley", "citationCount": "0", "affiliation": "Liu, ZL (Corresponding Author), Washington Univ, St Louis, MO 63110 USA.\nLiu, Zhengliang; Ottley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nCrouser, R. Jordan, Smith Coll, Northampton, MA 01063 USA.", "countries": "USA", "abstract": "Developments in data visualization research have enabled visualization systems to achieve great general usability and application across a variety of domains. These advancements have improved not only people's understanding of data, but also the general understanding of people themselves, and how they interact with visualization systems. In particular, researchers have gradually come to recognize the deficiency of having one-size-fits-all visualization interfaces, as well as the significance of individual differences in the use of data visualization systems. Unfortunately, the absence of comprehensive surveys of the existing literature impedes the development of this research. In this paper, we review the research perspectives, as well as the personality traits and cognitive abilities, visualizations, tasks, and measures investigated in the existing literature. We aim to provide a detailed summary of existing scholarship, produce evidence-based reviews, and spur future inquiry.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14033", "refList": ["10.1145/3025171.3025192", "10.1037/0278-7393.29.2.298", "10.1145/2470654.2470707", "10.1177/001872086300500103", "10.1109/tvcg.2014.2346575", "10.1146/annurev-psych-113011-143750", "10.1016/j.sbspro.2011.11.312", "10.1177/1073191102092010", "10.1109/tvcg.2016.2598471", "10.1145/169891.169925", "10.1145/2856767.2856779", "10.4102/sajip.v29i1.88", "10.1145/3301275.3302307", "10.1109/mcg.2012.120", "10.1016/0749-596x(89)90040-5", "10.1111/cgf.12393", "10.3389/fpsyg.2018.00755", "10.1037/0033-2909.121.2.219", "10.2466/pms.1978.47.2.599", "10.1037/0022-3514.44.2.419", "10.1016/j.cub.2009.12.014", "10.1002/per.704", "10.1016/j.paid.2006.03.011", "10.1038/36846", "10.1145/3025453.3025877", "10.1037/0022-3514.93.5.880", "10.1002/per.469", "10.1002/tea.3660300407", "10.1037/h0076301", "10.1002/jocb.32", "10.1371/journal.pone.0131115", "10.1002/per.588", "10.1145/2449396.2449439", "10.1108/jmd-12-2013-0160", "10.1037/0022-3514.42.1.116", "10.1016/j.intell.2003.10.005", "10.1016/s0022-5371(80)90312-6", "10.3758/bf03214546", "10.1111/j.1467-8659.2009.01442.x", "10.1002/(sici)1099-0984(200003/04)14:2", "10.1145/2470654.2470696", "10.2190/vqjd-t1yd-5wvb-rypj", "10.1145/2702123.2702590", "10.1109/infvis.2005.1532136", "10.1037/0278-7393.29.4.611", "10.1145/3025453.3025577", "10.1016/0360-8352(91)90009-u", "10.1145/2556288.2557141", "10.1016/j.sbspro.2012.01.055", "10.1016/j.sbspro.2013.04.319", "10.1007/s11257-019-09244-5", "10.1016/j.tics.2013.06.006", "10.1037/a0037009", "10.1037/0003-066x.48.1.26", "10.1109/infvis.2004.70", "10.1037/0003-066x.45.4.489", "10.1037/h0092976", "10.1177/1473871612441542", "10.1145/302979.303030", "10.1016/b978-0-12-386915-9.00003-6", "10.1080/08870449608404995", "10.1109/tvcg.2013.156", "10.1145/2110192.2110202", "10.3758/cabn.2.4.341", "10.1037/0096-1523.27.1.92", "10.1109/tvcg.2007.70515", "10.1145/1385569.1385602", "10.24963/ijcai.2017/217", "10.1080/135467897394419", "10.1007/978-3-642-31454-4\\_23", "10.1037/a0021016", "10.1109/tvcg.2014.2346452", "10.1145/3301275.3302313", "10.1145/2633043", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1109/vl.1996.545307", "10.1037/0033-2909.119.2.197", "10.1523/jneurosci.2145-09.2009", "10.1177/1473871613513227", "10.1037/0022-3514.51.4.875", "10.1109/mcg.2009.49", "10.1002/acp.1344", "10.1037/a0016127", "10.1037/1040-3590.18.2.192", "10.1111/j.1467-8659.2011.01928.x", "10.1016/s0140-6736(02)07441-x", "10.1037/0022-0663.96.3.471", "10.1207/s15327752jpa4803\\_13", "10.1007/s10798-018-9446-3", "10.1002/(sici)1097-4571(2000)51:6", "10.1109/vast.2009.5333468", "10.1109/mcg.2009.22", "10.1145/3079628.3079634", "10.1016/j.jrp.2014.05.003", "10.1016/s0079-6123(07)00020-9", "10.1002/ijop.12511", "10.1111/j.2044-8279.1982.tb00821.x", "10.5539/jel.v4n4p91", "10.1177/1473871615594652", "10.1037/0022-3514.66.5.950", "10.1037/h0042761", "10.1109/tvcg.2012.199", "10.1177/1046496403257228", "10.1080/13614569708914684", "10.1111/cgf.13678", "10.1016/j.lindif.2003.08.001", "10.1016/j.jecp.2009.11.003", "10.1109/tvcg.2014.2346984", "10.1109/vast.2011.6102445", "10.1145/2678025.2701376", "10.1057/ivs.2008.31", "10.1533/9781780630366", "10.1111/j.2044-8260.1992.tb00972.x", "10.1016/j.jrp.2011.12.003", "10.1016/0747-5632(91)90002-i", "10.1080/13546781343000222", "10.3758/s13414-013-0610-2", "10.1145/3301275.3302283", "10.1177/106907279300100107", "10.1109/vast.2012.6400535", "10.1145/3377325.3377502", "10.1037/1089-2699.10.4.249", "10.1016/j.paid.2003.08.018", "10.1016/j.paid.2010.09.015"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 13}, {"doi": "10.1109/tvcg.2019.2934797", "title": "VisTA: Integrating Machine Intelligence with Visualization to Support the Investigation of Think-Aloud Sessions", "year": "2019", "conferenceName": "InfoVis", "authors": "Mingming Fan;Ke Wu;Jian Zhao;Yue Li;Winter Wei;Khai N. Truong", "citationCount": "0", "affiliation": "Fan, MM (Corresponding Author), Univ Toronto, Toronto, ON, Canada. Fan, MM (Corresponding Author), Rochester Inst Technol, Rochester, NY 14623 USA. Fan, Mingming; Wu, Ke; Li, Yue; Wei, Winter; Khai N Truong, Univ Toronto, Toronto, ON, Canada. Fan, Mingming, Rochester Inst Technol, Rochester, NY 14623 USA. Zhao, Jian, Univ Waterloo, Waterloo, ON, Canada.", "countries": "Canada;USA", "abstract": "Think-aloud protocols are widely used by user experience (UX) practitioners in usability testing to uncover issues in user interface design. It is often arduous to analyze large amounts of recorded think-aloud sessions and few UX practitioners have an opportunity to get a second perspective during their analysis due to time and resource constraints. Inspired by the recent research that shows subtle verbalization and speech patterns tend to occur when users encounter usability problems, we take the first step to design and evaluate an intelligent visual analytics tool that leverages such patterns to identify usability problem encounters and present them to UX practitioners to assist their analysis. We first conducted and recorded think-aloud sessions, and then extracted textual and acoustic features from the recordings and trained machine learning (ML) models to detect problem encounters. Next, we iteratively designed and developed a visual analytics tool, VisTA, which enables dynamic investigation of think-aloud sessions with a timeline visualization of ML predictions and input features. We conducted a between-subjects laboratory study to compare three conditions, i.e., VisTA, VisTASimple (no visualization of the ML's input features), and Baseline (no ML information at all), with 30 UX professionals. The findings show that UX professionals identified more problem encounters when using VisTA than Baseline by leveraging the problem visualization as an overview, anticipations, and anchors as well as the feature visualization as a means to understand what ML considers and omits. Our findings also provide insights into how they treated ML, dealt with (dis)agreement with ML, and reviewed the videos (i.e., play, pause, and rewind).", "keywords": "Think-aloud,visual analytics,machine intelligence,user study,usability problems,session review behavior,UX practices", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934797", "refList": ["10.1109/tvcg.2008.137", "10.1109/ichi.2015.26", "10.1037/1089-2680.2.2.175", "10.1080/10447318.2012.758529", "10.1037/a0021663", "10.1111/cgf.13079", "10.1109/pacificvis.2017.8031598", "10.1145/2851581.2856492", "10.1145/3290605.3300641", "10.1145/3325281", "10.1109/tvcg.2015.2467871", "10.1080/10447318.2014.930311", "10.1080/0144929x.2012.708786", "10.1109/vast.2010.5653598", "10.1109/tvcg.2015.2468151", "10.1145/3025453.3025739", "10.1109/mcg.2009.49", "10.1145/2207676.2208365", "10.1080/01449290701773842", "10.1080/10447318.2015.1065691", "10.3868/s020-008-019-0001-9", "10.1109/tpc.2011.2182569", "10.1207/s15327590ijhc1304\\_05", "10.1109/tvcg.2015.2465151", "10.1007/978-3-642-21753-1\\_5", "10.1109/tvcg.2017.2745279", "10.1109/vast.2008.4677365", "10.1109/tpc.2012.2206190", "10.1109/47.867942", "10.1145/3173574.3173704", "10.1145/3196709.3196730", "10.1109/tpc.2010.2052859"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13678", "year": "2019", "title": "Characterizing Exploratory Visual Analysis: A Literature Review and Evaluation of Analytic Provenance in Tableau", "conferenceName": "EuroVis", "authors": "Leilani Battle;Jeffrey Heer", "citationCount": "2", "affiliation": "Battle, L (Corresponding Author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.\nBattle, Leilani, Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.\nHeer, Jeffrey, Univ Washington, Paul G Allen Sch Comp Sci \\& Engn, Seattle, WA 98195 USA.", "countries": "USA", "abstract": "Supporting exploratory visual analysis (EVA) is a central goal of visualization research, and yet our understanding of the process is arguably vague and piecemeal. We contribute a consistent definition of EVA through review of the relevant literature, and an empirical evaluation of existing assumptions regarding how analysts perform EVA using Tableau, a popular visual analysis tool. We present the results of a study where 27 Tableau users answered various analysis questions across 3 datasets. We measure task performance, identify recurring patterns across participants' analyses, and assess variance from task specificity and dataset. We find striking differences between existing assumptions and the collected data. Participants successfully completed a variety of tasks, with over 80\\% accuracy across focused tasks with measurably correct answers. The observed cadence of analyses is surprisingly slow compared to popular assumptions from the database community. We find significant overlap in analyses across participants, showing that EVA behaviors can be predictable. Furthermore, we find few structural differences between behavior graphs for open-ended and more focused exploration tasks.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13678", "refList": ["10.1109/dsia.2017.8339089", "10.1109/tvcg.2008.137", "10.1109/tvcg.2007.28", "10.1057/ivs.2009.22", "10.1038/526189a", "10.1109/tvcg.2006.85", "10.1145/2882903.2882919", "10.1145/2588555.2610523", "10.1109/tvcg.2014.2346575", "10.1145/2207676.2208294", "10.1145/2723372.2731084", "10.1109/tvcg.2016.2598471", "10.1145/3173574.3174168", "10.1109/visual.2005.1532788", "10.1109/tvcg.2014.2346452", "10.1109/tvcg.2015.2467613", "10.20380/gi2015.16", "10.14778/2831360.2831371", "10.1145/3173574.3174053", "10.1109/icdew.2006.75", "10.1145/2993901.2993912", "10.1145/1502650.1502695", "10.1109/tvcg.2012.219", "10.1007/978-3-642-23768-3\\_22", "10.1109/tvcg.2016.2598797", "10.1109/vl.1996.545307", "10.1111/j.1467-8659.2010.01830.x", "10.1109/tvcg.2015.2467871", "10.1145/3209900.3209901", "10.1145/2939502.2939513", "10.1145/381641.381656", "10.1109/tvcg.2013.124", "10.1109/vast.2008.4677357", "10.1145/1378773.1378788", "10.1109/mcg.2012.120", "10.1057/ivs.2008.31", "10.1109/infvis.2005.1532136", "10.1109/icde.2014.6816674", "10.1109/tvcg.2016.2598466", "10.1109/tvcg.2015.2467551", "10.1145/2939502.2939506", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2865117", "10.1145/1556262.1556276", "10.1016/s0950-7051(00)00101-5", "10.1109/tvcg.2008.109", "10.1145/2588555.2593666", "10.1109/tvcg.2018.2865040", "10.1109/tvcg.2013.179", "10.1111/cgf.13409", "10.1177/1473871616638546", "10.1109/2945.981851", "10.1109/tvcg.2010.164", "10.1109/vast.2008.4677365", "10.1109/tvcg.2015.2467191", "10.1145/2133806.2133821", "10.1145/2207676.2208412", "10.1109/tvcg.2017.2744319", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.224"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030425", "title": "Visual Analysis of Argumentation in Essays", "year": "2020", "conferenceName": "VAST", "authors": "Dora Kiesel;Patrick Riehmann;Henning Wachsmuth;Benno Stein;Bernd Fr\u00f6hlich", "citationCount": "0", "affiliation": "Kiesel, D (Corresponding Author), Bauhaus Univ Weimar, Weimar, Germany. Kiesel, Dora; Riehmann, Patrick; Stein, Benno; Froehlich, Bernd, Bauhaus Univ Weimar, Weimar, Germany. Wachsmuth, Henning, Paderborn Univ, Paderborn, Germany.", "countries": "Germany", "abstract": "This paper presents a visual analytics system for exploring, analyzing and comparing argument structures in essay corpora. We provide an overview of the corpus by a list of ArguLines which represent the argument units of each essay by a sequence of glyphs. Each glyph encodes the stance, the depth and the relative position of an argument unit. The overview can be ordered in various ways to reveal patterns and outliers. Subsets of essays can be selected and analyzed in detail using the Argument Unit Occurrence Tree which aggregates the argument structures using hierarchical histograms. This hierarchical view facilitates the estimation of statistics and trends concerning the progression of the argumentation in the essays. It also provides insights into the commonalities and differences between selected subsets. The text view is the necessary textual basis to verify conclusions from the other views and the annotation process. Linking the views and interaction techniques for visual filtering, studying the evolution of stance within a subset of essays and scrutinizing the order of argumentative units enable a deep analysis of essay corpora. Our expert reviews confirmed the utility of the system and revealed detailed and previously unknown information about the argumentation in our sample corpus.", "keywords": "Information Visualization,Text Analysis,User Interfaces,Visual Analytics,Argumentation Visualization,Glyph-based Techniques,Text and Document Data,Tree-based Visualization,Coordinated and Multiple Views,Close and Distant Reading", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030425", "refList": ["10.1109/tvcg.2014.2346575", "10.14778/2735479.2735485", "10.14778/3192965.3192971", "10.1111/cgf.12129", "10.1145/2206869.2206874", "10.1109/icde.2016.7498300", "10.14778/2831360.2831371", "10.1145/3035918.3056097", "10.1023/a:1009726021843", "10.1111/cgf.13678", "10.1145/3183713.3196905", "10.14778/3115404.3115418", "10.1109/tvcg.2012.180", "10.1109/icde.1999.754950", "10.14778/1453856.1453924", "10.1145/3209900.3209901", "10.1002/spe.2325", "10.1145/42201.42203", "10.1109/tvcg.2013.124", "10.1109/vast.2008.4677357", "10.1109/icde.2016.7498287", "10.1109/icde.2014.6816674", "10.14778/2732951.2732964", "10.1109/tvcg.2015.2467551", "10.1145/1084805.1084812", "10.1109/2.781635", "10.14778/2732279.2732280", "10.1109/icde.2015.7113427", "10.1109/tvcg.2015.2467091", "10.1007/s00778-017-0486-1", "10.1109/tvcg.2003.1196005", "10.1109/icde.2004.1320035", "10.1109/tvcg.2016.2607714", "10.14778/2732951.2732953", "10.1109/tvcg.2008.131", "10.1109/tvcg.2018.2865018", "10.1145/2133806.2133821", "10.1109/icde.2019.00035", "10.1109/tpds.2005.144", "10.14778/3236187.3236212", "10.1109/tvcg.2009.111", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14033", "year": "2020", "title": "Survey on Individual Differences in Visualization", "conferenceName": "EuroVis", "authors": "Zhengliang Liu;R. Jordan Crouser;Alvitta Ottley", "citationCount": "0", "affiliation": "Liu, ZL (Corresponding Author), Washington Univ, St Louis, MO 63110 USA.\nLiu, Zhengliang; Ottley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nCrouser, R. Jordan, Smith Coll, Northampton, MA 01063 USA.", "countries": "USA", "abstract": "Developments in data visualization research have enabled visualization systems to achieve great general usability and application across a variety of domains. These advancements have improved not only people's understanding of data, but also the general understanding of people themselves, and how they interact with visualization systems. In particular, researchers have gradually come to recognize the deficiency of having one-size-fits-all visualization interfaces, as well as the significance of individual differences in the use of data visualization systems. Unfortunately, the absence of comprehensive surveys of the existing literature impedes the development of this research. In this paper, we review the research perspectives, as well as the personality traits and cognitive abilities, visualizations, tasks, and measures investigated in the existing literature. We aim to provide a detailed summary of existing scholarship, produce evidence-based reviews, and spur future inquiry.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14033", "refList": ["10.1145/3025171.3025192", "10.1037/0278-7393.29.2.298", "10.1145/2470654.2470707", "10.1177/001872086300500103", "10.1109/tvcg.2014.2346575", "10.1146/annurev-psych-113011-143750", "10.1016/j.sbspro.2011.11.312", "10.1177/1073191102092010", "10.1109/tvcg.2016.2598471", "10.1145/169891.169925", "10.1145/2856767.2856779", "10.4102/sajip.v29i1.88", "10.1145/3301275.3302307", "10.1109/mcg.2012.120", "10.1016/0749-596x(89)90040-5", "10.1111/cgf.12393", "10.3389/fpsyg.2018.00755", "10.1037/0033-2909.121.2.219", "10.2466/pms.1978.47.2.599", "10.1037/0022-3514.44.2.419", "10.1016/j.cub.2009.12.014", "10.1002/per.704", "10.1016/j.paid.2006.03.011", "10.1038/36846", "10.1145/3025453.3025877", "10.1037/0022-3514.93.5.880", "10.1002/per.469", "10.1002/tea.3660300407", "10.1037/h0076301", "10.1002/jocb.32", "10.1371/journal.pone.0131115", "10.1002/per.588", "10.1145/2449396.2449439", "10.1108/jmd-12-2013-0160", "10.1037/0022-3514.42.1.116", "10.1016/j.intell.2003.10.005", "10.1016/s0022-5371(80)90312-6", "10.3758/bf03214546", "10.1111/j.1467-8659.2009.01442.x", "10.1002/(sici)1099-0984(200003/04)14:2", "10.1145/2470654.2470696", "10.2190/vqjd-t1yd-5wvb-rypj", "10.1145/2702123.2702590", "10.1109/infvis.2005.1532136", "10.1037/0278-7393.29.4.611", "10.1145/3025453.3025577", "10.1016/0360-8352(91)90009-u", "10.1145/2556288.2557141", "10.1016/j.sbspro.2012.01.055", "10.1016/j.sbspro.2013.04.319", "10.1007/s11257-019-09244-5", "10.1016/j.tics.2013.06.006", "10.1037/a0037009", "10.1037/0003-066x.48.1.26", "10.1109/infvis.2004.70", "10.1037/0003-066x.45.4.489", "10.1037/h0092976", "10.1177/1473871612441542", "10.1145/302979.303030", "10.1016/b978-0-12-386915-9.00003-6", "10.1080/08870449608404995", "10.1109/tvcg.2013.156", "10.1145/2110192.2110202", "10.3758/cabn.2.4.341", "10.1037/0096-1523.27.1.92", "10.1109/tvcg.2007.70515", "10.1145/1385569.1385602", "10.24963/ijcai.2017/217", "10.1080/135467897394419", "10.1007/978-3-642-31454-4\\_23", "10.1037/a0021016", "10.1109/tvcg.2014.2346452", "10.1145/3301275.3302313", "10.1145/2633043", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1109/vl.1996.545307", "10.1037/0033-2909.119.2.197", "10.1523/jneurosci.2145-09.2009", "10.1177/1473871613513227", "10.1037/0022-3514.51.4.875", "10.1109/mcg.2009.49", "10.1002/acp.1344", "10.1037/a0016127", "10.1037/1040-3590.18.2.192", "10.1111/j.1467-8659.2011.01928.x", "10.1016/s0140-6736(02)07441-x", "10.1037/0022-0663.96.3.471", "10.1207/s15327752jpa4803\\_13", "10.1007/s10798-018-9446-3", "10.1002/(sici)1097-4571(2000)51:6", "10.1109/vast.2009.5333468", "10.1109/mcg.2009.22", "10.1145/3079628.3079634", "10.1016/j.jrp.2014.05.003", "10.1016/s0079-6123(07)00020-9", "10.1002/ijop.12511", "10.1111/j.2044-8279.1982.tb00821.x", "10.5539/jel.v4n4p91", "10.1177/1473871615594652", "10.1037/0022-3514.66.5.950", "10.1037/h0042761", "10.1109/tvcg.2012.199", "10.1177/1046496403257228", "10.1080/13614569708914684", "10.1111/cgf.13678", "10.1016/j.lindif.2003.08.001", "10.1016/j.jecp.2009.11.003", "10.1109/tvcg.2014.2346984", "10.1109/vast.2011.6102445", "10.1145/2678025.2701376", "10.1057/ivs.2008.31", "10.1533/9781780630366", "10.1111/j.2044-8260.1992.tb00972.x", "10.1016/j.jrp.2011.12.003", "10.1016/0747-5632(91)90002-i", "10.1080/13546781343000222", "10.3758/s13414-013-0610-2", "10.1145/3301275.3302283", "10.1177/106907279300100107", "10.1109/vast.2012.6400535", "10.1145/3377325.3377502", "10.1037/1089-2699.10.4.249", "10.1016/j.paid.2003.08.018", "10.1016/j.paid.2010.09.015"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1111/cgf.13680", "year": "2019", "title": "An Exploratory User Study of Visual Causality Analysis", "conferenceName": "EuroVis", "authors": "Chi{-}Hsien Yen;Aditya G. Parameswaran;Wai{-}Tat Fu", "citationCount": "0", "affiliation": "Yen, CHE (Corresponding Author), Univ Illinois, Champaign, IL 61820 USA.\nYen, Chi-Hsien Eric; Parameswaran, Aditya; Fu, Wai-Tat, Univ Illinois, Champaign, IL 61820 USA.", "countries": "USA", "abstract": "Interactive visualization tools are being used by an increasing number of members of the general public; however, little is known about how, and how well, people use visualizations to infer causality. Adapted from the mediation causal model, we designed an analytic framework to systematically evaluate human performance, strategies, and pitfalls in a visual causal reasoning task. We recruited 24 participants and asked them to identify the mediators in a fictitious dataset using bar charts and scatter plots within our visualization interface. The results showed that the accuracy of their responses as to whether a variable is a mediator significantly decreased when a confounding variable directly influenced the variable being analyzed. Further analysis demonstrated how individual visualization exploration strategies and interfaces might influence reasoning performance. We also identified common strategies and pitfalls in their causal reasoning processes. Design implications for how future visual analytics tools can be designed to better support causal inference are discussed.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13680", "refList": ["10.1037/1089-2680.2.2.175", "10.1109/tvcg.2017.2744138", "10.1518/001872008x354183", "10.3758/bf03211151", "10.1109/tvcg.2014.2346452", "10.1016/j.tree.2008.10.008", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2015.2467613", "10.1145/2207676.2208704", "10.1109/tvcg.2014.2346297", "10.1016/j.ijmedinf.2004.08.003", "10.1145/2993901.2993907", "10.1037/0022-3514.51.6.1173", "10.1145/2856767.2856779", "10.1109/tvcg.2015.2467871", "10.1037/0033-295x.101.4.608", "10.1109/vast.2010.5653598", "10.1145/3301275.3302307", "10.1016/j.jesp.2011.05.007", "10.1098/rstb.2006.1991", "10.1057/ivs.2008.31", "10.1177/0146167208331253", "10.1109/tvcg.2015.2467931", "10.2307/2288400", "10.3758/bf03198555", "10.1109/tvcg.2010.164", "10.1109/vast.2011.6102435", "10.1109/tvcg.2012.260", "10.1109/tvcg.2015.2467758", "10.1080/14640746808400161", "10.1111/cgf.13168"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028958", "title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Tiankai Xie;Yuxin Ma;Hanghang Tong;My T. Thai;Ross Maciejewski", "citationCount": "0", "affiliation": "Xie, TK (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Xie, Tiankai; Ma, Yuxin; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. Tong, Hanghang, Univ Illinois, Urbana, IL USA. Thai, My T., Univ Florida, Gainesville, FL 32611 USA.", "countries": "USA", "abstract": "Graph mining plays a pivotal role across a number of disciplines, and a variety of algorithms have been developed to answer who/what type questions. For example, what items shall we recommend to a given user on an e-commerce platform? The answers to such questions are typically returned in the form of a ranked list, and graph-based ranking methods are widely used in industrial information retrieval settings. However, these ranking algorithms have a variety of sensitivities, and even small changes in rank can lead to vast reductions in product sales and page hits. As such, there is a need for tools and methods that can help model developers and analysts explore the sensitivities of graph ranking algorithms with respect to perturbations within the graph structure. In this paper, we present a visual analytics framework for explaining and exploring the sensitivity of any graph-based ranking algorithm by performing perturbation-based what-if analysis. We demonstrate our framework through three case studies inspecting the sensitivity of two classic graph-based ranking algorithms (PageRank and HITS) as applied to rankings in political news media and social networks.", "keywords": "Graph-based ranking,sensitivity analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028958", "refList": ["10.1109/wsc.2017.8247800", "10.1023/a:1022649401552", "10.1515/1559-0410.11416", "10.1109/tvcg.2016.2598919", "10.1177/1473871611416549", "10.1109/tvcg.2019.2934630", "10.1140/epjds29", "10.1109/tvcg.2019.2934670", "10.1016/j.eswa.2015.09.004", "10.1145/2702123.2702509", "10.1016/j.visinf.2018.12.001", "10.2307/3002000", "10.1109/tvcg.2019.2934399", "10.1007/s41060-016-0032-z", "10.1111/cgf.13198", "10.14778/2350229.2350254", "10.1145/2939672.2939764", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/vast.2017.8585647", "10.1007/bf01187020", "10.1109/icdm.2015.26", "10.1145/2362383.2362387", "10.1177/0049124104268644", "10.1109/vast.2011.6102442", "10.1109/infvis.2003.1249025", "10.1109/tvcg.2018.2864475", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2015.2467691", "10.1111/cgf.13210", "10.1214/aos/1176344136", "10.1109/tvcg.2015.2424872", "10.1016/j.visinf.2018.09.001", "10.1177/089443939100900106", "10.1109/tvcg.2015.2467931", "10.1162/neco.1997.9.8.1735", "10.1007/s11162-011-9241-4", "10.1111/cgf.13680", "10.1145/3065386", "10.1109/tvcg.2018.2864889", "10.1177/003804070808100402", "10.1109/icdm.2010.62", "10.1038/s41598-020-59669-x", "10.1162/153244303321897717", "10.1109/tvcg.2019.2934619", "10.1007/bf00356088", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13170", "year": "2017", "title": "Visual Comparison of Eye Movement Patterns", "conferenceName": "EuroVis", "authors": "Tanja Blascheck;Markus Schweizer;Fabian Beck;Thomas Ertl", "citationCount": "3", "affiliation": "Blascheck, T (Corresponding Author), Univ Stuttgart, Inst Visualizat \\& Interact Syst, Stuttgart, Germany.\nBlascheck, Tanja; Schweizer, Markus; Ertl, Thomas, Univ Stuttgart, Inst Visualizat \\& Interact Syst, Stuttgart, Germany.\nBeck, Fabian, Univ Duisburg Essen, Inst Comp Sci \\& Business Informat Syst, Essen, Germany.", "countries": "Germany", "abstract": "In eye tracking research, finding eye movement patterns and similar strategies between participants' eye movements is important to understand task solving strategies and obstacles. In this application paper, we present a graph comparison method using radial graphs that show Areas of Interest (AOIs) and their transitions. An analyst investigates a single graph based on dwell times, directed transitions, and temporal AOI sequences. Two graphs can be compared directly and temporal changes may be analyzed. A list and matrix approach facilitate the analyst to contrast more than two graphs guided by visually encoded graph similarities. We evaluated our approach in case studies with three eye tracking and visualization experts. They identified temporal transition patterns of eye movements across participants, groups of participants, and outliers.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13170", "refList": ["10.1145/2857491.2857524", "10.1109/tvcg.2008.117", "10.1177/1473871611416549", "10.1109/tvcg.2015.2468111", "10.1111/cgf.12791", "10.1109/iv.2011.49", "10.1109/tvcg.2012.276", "10.1109/infvis.2001.963281", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2013.263", "10.1109/tvcg.2009.181", "10.1080/13875868.2016.1226839", "10.1109/tvcg.2015.2467871", "10.1109/vissoft.2013.6650549", "10.1145/2578153.2578175", "10.1109/iv.2009.108", "10.1177/1473871612455983", "10.1145/1743666.1743721", "10.1109/vast.2016.7883520", "10.1007/978-3-319-47024-5\\_7", "10.1016/b978-044451020-4/50035-9", "10.1109/iv.2016.28", "10.1109/tvcg.2014.2346677", "10.1145/2470654.2470724", "10.1016/s0169-8141(98)00068-7", "10.1109/tvcg.2007.70521", "10.1111/cgf.12115", "10.1145/2509315.2509326", "10.1109/pacificvis.2016.7465266", "10.1145/2669557.2669558", "10.1109/tvcg.2010.149", "10.1109/tvcg.2009.150", "10.1179/000870403235002042"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00037", "year": "2019", "title": "A Visual Approach for the Comparative Analysis of Character Networks in Narrative Texts", "conferenceName": "PacificVis", "authors": "Markus John;Martin Baumann", "citationCount": "1", "affiliation": "John, M (Corresponding Author), Univ Stuttgart, Inst Visualizat \\& Interact Syst VIS, Stuttgart, Germany.\nJohn, Markus; Baumann, Martin; Schuetz, David; Koch, Steffen; Ertl, Thomas, Univ Stuttgart, Inst Visualizat \\& Interact Syst VIS, Stuttgart, Germany.", "countries": "Germany", "abstract": "The analysis of a novel's plot and characters are challenging and time-consuming tasks in literary criticism. Typically, humanities scholars want to describe and compare characters' personality traits, their roles, their relationships, and the evolution of these aspects over the course of a novel. Nowadays, due to the digitization of literature, humanities scholars can be supported in these endeavors with computational methods. In this paper, we present an approach that offers several means to analyze the plot and characters of a novel visually. Analysts can easily switch between an adjacency matrix and a node-link representation, which provide an overview of the characters and the relationships between them. Both views enable analysts to select different text ranges of the novel for studying the commonalities and differences of the character constellations within these ranges. We offer interactive visual representations to help investigate the relationships between the characters in more detail. Additionally, we link the visual representations with the novels' texts to support the inspection and verification of previously generated ideas and hypotheses. To demonstrate the benefits and limitations of our approach, we present two usage scenarios. The first one is based on a fictitious analysis and the second one discusses applications that were carried out during joint workshops with humanities scholars. Finally, we present and discuss the insights gained by an expert study and the design decisions of our approach.", "keywords": "Visual text analysis; document analysis; close reading; distant reading; digital humanities; graph comparison", "link": "https://doi.org/10.1109/PacificVis.2019.00037", "refList": ["10.1177/1473871611416549", "10.1145/2702123.2702476", "10.1111/cgf.13181", "10.1111/cgf.12791", "10.1111/cgf.13170", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2011.185", "10.1109/vast.2009.5333248", "10.1109/tvcg.2015.2467971", "10.1080/10447318.2010.516722", "10.1136/qshc.2004.010033", "10.1109/tvcg.2011.169", "10.3115/v1/p14-5010", "10.1109/tmm.2016.2614184", "10.1109/tvcg.2009.106", "10.1111/cgf.12124", "10.1109/iv.2016.28", "10.1145/2470654.2470724", "10.1057/palgrave.ivs.9500180", "10.1109/pacificvis.2011.5742388", "10.2190/ec.44.1.a", "10.13140/2.1.1341.1520", "10.1109/tvcg.2011.232", "10.1109/vast.2007.4389004", "10.1057/palgrave.ivs.9500092", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 97}, "index": 504, "embedding": [-1.87364661693573, 1.9873930215835571, -1.0404996871948242, -2.2971394062042236, 2.908430814743042, 0.16650904715061188, -0.664513111114502, 7.6263508796691895, 18.756122589111328, 3.051795482635498, 9.562143325805664, 3.9445719718933105, 11.039054870605469, 9.122703552246094, 0.3853433430194855, -0.5263585448265076, 3.7755815982818604, 0.06877747178077698, 3.4537107944488525, 8.20449161529541, -0.06630208343267441, 1.3007805347442627, 2.11156964302063, 6.059995174407959, -2.3829734325408936, 15.707818031311035, -0.5834015607833862, -0.21145495772361755, 2.572855234146118, -2.775730609893799, 6.026547908782959, 1.9589189291000366], "projection": [2.7958624362945557, 11.62341594696045], "size": 49, "height": 5, "width": 18}