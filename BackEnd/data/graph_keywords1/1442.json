{"data": {"doi": "10.1111/cgf.13406", "year": "2018", "title": "Towards User-Centered Active Learning Algorithms", "conferenceName": "EuroVis", "authors": "J{\\\"{u}}rgen Bernard;Matthias Zeppelzauer;Markus Lehmann;Martin M{\\\"{u}}ller;Michael Sedlmair", "citationCount": "5", "affiliation": "Bernard, J (Corresponding Author), Tech Univ Darmstadt, Darmstadt, Germany.\nBernard, J (Corresponding Author), Fraunhofer IGD, Darmstadt, Germany.\nBernard, Juergen; Lehmann, Markus; Mueller, Martin, Tech Univ Darmstadt, Darmstadt, Germany.\nBernard, Juergen, Fraunhofer IGD, Darmstadt, Germany.\nZeppelzauer, Matthias, St Polten Univ Appl Sci, St Polten, Austria.\nSedlmair, Michael, Jacobs Univ Bremen, Bremen, Germany.", "countries": "Germany;Austria", "abstract": "The labeling of data sets is a time-consuming task, which is, however, an important prerequisite for machine learning and visual analytics. Visual-interactive labeling (VIAL) provides users an active role in the process of labeling, with the goal to combine the potentials of humans and machines to make labeling more efficient. Recent experiments showed that users apply different strategies when selecting instances for labeling with visual-interactive interfaces. In this paper, we contribute a systematic quantitative analysis of such user strategies. We identify computational building blocks of user strategies, formalize them, and investigate their potentials for different machine learning tasks in systematic experiments. The core insights of our experiments are as follows. First, we identified that particular user strategies can be used to considerably mitigate the bootstrap (cold start) problem in early labeling phases. Second, we observed that they have the potential to outperform existing active learning strategies in later phases. Third, we analyzed the identified core building blocks, which can serve as the basis for novel selection strategies. Overall, we observed that data-based user strategies (clusters, dense areas) work considerably well in early phases, while model-based user strategies (e.g., class separation) perform better during later phases. The insights gained from this work can be applied to develop novel active learning approaches as well as to better guide users in visual interactive labeling.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13406", "refList": ["10.1109/tvcg.2014.2346578", "10.3115/1613715.1613855", "10.1111/cgf.12632", "10.1109/cvpr.2012.6248050", "10.1109/jstsp.2011.2139193", "10.1109/icdmw.2010.181", "10.1109/vast.2014.7042480", "10.1145/130385.130417", "10.1016/j.patrec.2009.09.011", "10.1023/a:1022627411411", "10.1109/5.726791", "10.1145/1899412.1899414", "10.1109/cvprw.2009.5206627", "10.5220/0006116400750087", "10.1016/0893-6080(89)90020-8", "10.1109/vlhcc.2016.7739668", "10.1016/0377-0427(87)90125-7", "10.1109/tvcg.2012.277", "10.1145/2836034.2836035", "10.1145/1135777.1135870", "10.1145/1015330.1015385", "10.1109/vast.2012.6400492", "10.1109/tpami.1979.4766909", "10.1145/1964897.1964906", "10.1145/3038462.3038469", "10.2200/s00429ed1v01y201207aim018", "10.1111/cgf.12116", "10.1109/tpami.2008.218", "10.1145/361219.361220", "10.1109/icme.2006.262442", "10.1109/ssci.2015.33", "10.2352/issn.2470-1173.2017.1.vda-387", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.1109/vast.2012.6400486"], "wos": 1, "children": [{"doi": "10.1109/vast47406.2019.8986943", "title": "Interactive Correction of Mislabeled Training Data", "year": "2019", "conferenceName": "VAST", "authors": "Shouxing Xiang;Xi Ye;Jiazhi Xia;Jing Wu;Yang Chen;Shixia Liu", "citationCount": "4", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, Sch Software, BNRist, Beijing, Peoples R China. Xiang, Shouxing; Ye, Xi; Chen, Yang; Liu, Shixia, Tsinghua Univ, Sch Software, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Sch Comp Sci \\& Engn, Changsha, Peoples R China. Wu, Jing, Cardiff Univ, Sch Comp Sci \\& Informat, Cardiff, Wales.", "countries": "Wales;China", "abstract": "In this paper, we develop a visual analysis method for interactively improving the quality of labeled data, which is essential to the success of supervised and semi-supervised learning. The quality improvement is achieved through the use of user-selected trusted items. We employ a bi-level optimization model to accurately match the labels of the trusted items and to minimize the training loss. Based on this model, a scalable data correction algorithm is developed to handle tens of thousands of labeled data efficiently. The selection of the trusted items is facilitated by an incremental tSNE with improved computational efficiency and layout stability to ensure a smooth transition between different levels. We evaluated our method on real-world datasets through quantitative evaluation and case studies, and the results were generally favorable.", "keywords": "Labeled data debugging,trusted item,tSNE", "link": "http://dx.doi.org/10.1109/VAST47406.2019.8986943", "refList": ["10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2570755", "10.1109/tvcg.2017.2744938", "10.1145/2254556.2254659", "10.1109/tvcg.2017.2744419", "10.1007/s12008-018-0488-2", "10.1023/b:aire.0000045502.10941.a9", "10.1111/cgf.12878", "10.1137/0108011", "10.1109/vl.1996.545307", "10.1109/tvcg.2018.2864843", "10.1007/978-3-642-21602-2\\_67", "10.1145/3190578", "10.1145/2637748.2638423", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865026", "10.1145/7529.8927", "10.1016/j.infsof.2008.09.005", "10.1145/2669557.2669578", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2009.84", "10.1109/tvcg.2017.2744685", "10.1142/s0218001415510088", "10.1109/vast.2012.6400492", "10.1007/s00371-018-1500-3", "10.1111/cgf.13406", "10.1007/s10462-010-9156-z", "10.1109/cvpr.2014.483", "10.1109/tvcg.2016.2598592", "10.1109/tvcg.2018.2846735", "10.1109/tvcg.2014.2346574", "10.1109/tvcg.2017.2744818", "10.1016/j.cag.2013.10.006", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13973", "year": "2020", "title": "Classifier-Guided Visual Correction of Noisy Labels for Image Classification Tasks", "conferenceName": "EuroVis", "authors": "Alex B{\\\"{a}}uerle;Heiko Neumann;Timo Ropinski", "citationCount": "0", "affiliation": "Bauerle, A (Corresponding Author), Ulm Univ, Ulm, Germany.\nBaeuerle, A.; Neumann, H.; Ropinski, T., Ulm Univ, Ulm, Germany.", "countries": "Germany", "abstract": "Training data plays an essential role in modern applications of machine learning. However, gathering labeled training data is time-consuming. Therefore, labeling is often outsourced to less experienced users, or completely automated. This can introduce errors, which compromise valuable training data, and lead to suboptimal training results. We thus propose a novel approach that uses the power of pretrained classifiers to visually guide users to noisy labels, and let them interactively check error candidates, to iteratively improve the training data set. To systematically investigate training data, we propose a categorization of labeling errors into three different types, based on an analysis of potential pitfalls in label acquisition processes. For each of these types, we present approaches to detect, reason about, and resolve error candidates, as we propose measures and visual guidance techniques to support machine learning users. Our approach has been used to spot errors in well-known machine learning benchmark data sets, and we tested its usability during a user evaluation. While initially developed for images, the techniques presented in this paper are independent of the classification algorithm, and can also be extended to many other types of training data.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13973", "refList": ["10.1109/tvcg.2017.2744683", "10.1145/2818048.2820016", "10.1145/2441776.2441848", "10.1109/tvcg.2016.2598828", "10.1145/2254556.2254659", "10.1002/acp.3140", "10.1109/cvpr.2018.00582", "10.1145/130385.130417", "10.1109/vast.2016.7883508", "10.1109/tvcg.2018.2864843", "10.1145/1401890.1401965", "10.1007/s10618-013-0306-1", "10.1145/2702123.2702553", "10.1109/cvpr.2018.00571", "10.1109/tvcg.2012.277", "10.1109/tip.2003.819861", "10.1111/cgf.13406", "10.1007/978-3-319-63859-1\\_1", "10.1109/tvcg.2013.164", "10.1007/s10462-010-9156-z", "10.1109/tvcg.2014.2346660", "10.1109/cbms.2006.65", "10.1109/itsc.2019.8917021", "10.1145/3025453.3026044"], "wos": 1, "children": [], "len": 1}], "len": 9}, "index": 1442, "embedding": [1.6790475845336914, 0.6649795770645142, -1.0404996871948242, -0.4923953413963318, -0.2559298574924469, 0.16650904715061188, -0.664513111114502, 0.25006523728370667, 0.126210555434227, 0.5101094245910645, 0.137990340590477, 0.503001868724823, -0.1686236411333084, 0.6119160652160645, -0.99098140001297, 1.5507445335388184, 0.7518647909164429, 0.7966058254241943, -0.7571966052055359, 1.5896092653274536, -0.06630208343267441, -0.05041871219873428, 0.24973751604557037, 1.2584428787231445, -2.1253089904785156, 1.4669086933135986, -0.5834015607833862, 0.10206867754459381, 1.2089859247207642, -1.6971185207366943, -0.31752440333366394, 0.24589133262634277], "projection": [-0.4937064051628113, 7.578421115875244], "size": 5, "height": 3, "width": 2}