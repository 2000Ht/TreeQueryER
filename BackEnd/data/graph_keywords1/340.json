{"data": {"doi": "10.1109/tvcg.2014.2346351", "title": "Boundary Aware Reconstruction of Scalar Fields", "year": "2014", "conferenceName": "SciVis", "authors": "Stefan Lindholm;Daniel J\u00f6nsson;Charles D. Hansen;Anders Ynnerman", "citationCount": "7", "affiliation": "Lindholm, S (Corresponding Author), Linkoping Univ, Dept Sci \\& Technol, S-58183 Linkoping, Sweden. Lindholm, Stefan; Jonsson, Daniel; Ynnerman, Anders, Linkoping Univ, Dept Sci \\& Technol, S-58183 Linkoping, Sweden. Hansen, Charles, Univ Utah, Sci Comp \\& Imaging Inst, Salt Lake City, UT 84112 USA.", "countries": "Sweden;USA", "abstract": "In visualization, the combined role of data reconstruction and its classification plays a crucial role. In this paper we propose a novel approach that improves classification of different materials and their boundaries by combining information from the classifiers at the reconstruction stage. Our approach estimates the targeted materials' local support before performing multiple material-specific reconstructions that prevent much of the misclassification traditionally associated with transitional regions and transfer function (TF) design. With respect to previously published methods our approach offers a number of improvements and advantages. For one, it does not rely on TFs acting on derivative expressions, therefore it is less sensitive to noisy data and the classification of a single material does not depend on specialized TF widgets or specifying regions in a multidimensional TF. Additionally, improved classification is attained without increasing TF dimensionality, which promotes scalability to multivariate data. These aspects are also key in maintaining low interaction complexity. The results are simple-to-achieve visualizations that better comply with the user's understanding of discrete features within the studied object.", "keywords": "Reconstruction, signal processing, kernel regression, volume rendering", "link": "http://dx.doi.org/10.1109/TVCG.2014.2346351", "refList": ["10.1016/s0097-8493(02)00055-9", "10.1109/42.668696", "10.1120/jacmp.v14i4.3905", "10.1109/tip.2006.877406", "10.1109/tvcg.2002.1021579", "10.1109/tvcg.2013.16", "10.1109/iccv.1998.710815", "10.1109/visual.2003.1250386", "10.1109/38.511", "10.1109/svv.1998.729588", "10.1109/tvcg.2007.70518", "10.1111/j.1467-8659.2009.01691.x", "10.1109/2945.856997", "10.1109/tip.2006.888330", "10.1109/42.192689", "10.1109/visual.2003.1250387", "10.1109/visual.1998.745311"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2017.2743980", "title": "TopoAngler: Interactive Topology-Based Extraction of Fishes", "year": "2017", "conferenceName": "SciVis", "authors": "Alexander Bock;Harish Doraiswamy;Adam Summers;Cl\u00e1udio T. Silva", "citationCount": "8", "affiliation": "Bock, A (Corresponding Author), NYU, New York, NY 10003 USA. Bock, Alexander; Doraiswamy, Harish; Silva, Claudio, NYU, New York, NY 10003 USA. Summers, Adam, Univ Washington, Seattle, WA 98195 USA.", "countries": "USA", "abstract": "We present TopoAngler, a visualization framework that enables an interactive user-guided segmentation of fishes contained in a micro-CT scan. The inherent noise in the CT scan coupled with the often disconnected (and sometimes broken) skeletal structure of fishes makes an automatic segmentation of the volume impractical. To overcome this, our framework combines techniques from computational topology with an interactive visual interface, enabling the human-in-the-Ioop to effectively extract fishes from the volume. In the first step, the join tree of the input is used to create a hierarchical segmentation of the volume. Through the use of linked views, the visual interface then allows users to interactively explore this hierarchy, and gather parts of individual fishes into a coherent sub-volume, thus reconstructing entire fishes. Our framework was primarily developed for its application to CT scans of fishes, generated as part of the ScanAllFish project, through close collaboration with their lead scientist. However, we expect it to also be applicable in other biological applications where a single dataset contains multiple specimen; a common routine that is now widely followed in laboratories to increase throughput of expensive CT scanners.", "keywords": "Computational topology,join trees,branch decomposition,hierarchical segmentation,interaction,visualization system", "link": "http://dx.doi.org/10.1109/TVCG.2017.2743980", "refList": ["10.1007/s00454-003-2926-5", "10.1007/s00371-013-0892-3", "10.1109/tvcg.2015.2452919", "10.1007/978-3-642-15948-0\\_9", "10.1109/tip.2007.891796", "10.1145/777792.777846", "10.1109/visual.2004.96", "10.1109/tvcg.2010.208", "10.1109/tvcg.2004.1260759", "10.1016/j.gmod.2003.08.002", "10.1109/tvcg.2014.2346449", "10.1016/j.neuroimage.2006.01.015", "10.1109/tvcg.2009.178", "10.1109/tvcg.2006.186", "10.1016/s0925-7721(02)00093-7", "10.1007/s11390-013-1383-8", "10.1109/tvcg.2014.2346351", "10.1109/tvcg.2010.253", "10.1109/tvcg.2007.70565", "10.1109/mcg.2013.101", "10.1109/isbi.2014.6867925", "10.1016/s0169-2607(97)01803-8", "10.1007/b106657\\_2", "10.1109/tvcg.2009.69", "10.1007/s10851-007-0035-4", "10.1007/s00453-003-1052-3", "10.1109/tvcg.2007.47", "10.1109/tvcg.2011.37", "10.1109/tip.2012.2191566", "10.1109/tip.2011.2121080", "10.1029/2009wr008087", "10.1109/tvcg.2013.131", "10.1016/j.ecoinf.2014.08.004", "10.1145/235815.235821", "10.1111/1467-8659.00697"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030353", "title": "Localized Topological Simplification of Scalar Data", "year": "2020", "conferenceName": "SciVis", "authors": "Jonas Lukasczyk;Christoph Garth;Ross Maciejewski;Julien Tierny", "citationCount": "0", "affiliation": "Lukasczyk, J (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Lukasczyk, Jonas; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. Garth, Christoph, Tech Univ Kaiserslautern, Kaiserslautern, Germany. Tierny, Julien, Sorbonne Univ, Paris, France. Tierny, Julien, CNRS, Paris, France.", "countries": "Germany;USA;France", "abstract": "This paper describes a localized algorithm for the topological simplification of scalar data, an essential pre-processing step of topological data analysis (TDA). Given a scalar field $f$ and a selection of extrema to preserve, the proposed localized topological simplification (LTS) derives a function g that is close to $f$ and only exhibits the selected set of extrema. Specifically, sub- and superlevel set components associated with undesired extrema are first locally flattened and then correctly embedded into the global scalar field, such that these regions are guaranteed-from a combinatorial perspective-to no longer contain any undesired extrema. In contrast to previous global approaches, LTS only and independently processes regions of the domain that actually need to be simplified, which already results in a noticeable speedup. Moreover, due to the localized nature of the algorithm, LTS can utilize shared-memory parallelism to simplify regions simultaneously with a high parallel efficiency (70%). Hence, LTS significantly improves interactivity for the exploration of simplification parameters and their effect on subsequent topological analysis. For such exploration tasks, LTS brings the overall execution time of a plethora of TDA pipelines from minutes down to seconds, with an average observed speedup over state-of-the-art techniques of up to $\\times 36$. Furthermore, in the special case where preserved extrema are selected based on topological persistence, an adapted version of LTS partially computes the persistence diagram and simultaneously simplifies features below a predefined persistence threshold. The effectiveness of LTS, its parallel efficiency, and its resulting benefits for TDA are demonstrated on several simulated and acquired datasets from different application domains, including physics, chemistry, and biomedical imaging.", "keywords": "Topological data analysis,scalar data,simplification,feature extraction,parallel computing", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030353", "refList": ["10.1016/j.cag.2014.01.006", "10.1002/jcc.25181", "10.1109/tvcg.2007.70601", "10.2307/2412323", "10.1016/j.jsc.2016.03.009", "10.1007/s00454-006-1276-5", "10.1111/cgf.12596", "10.1007/s00454-002-2885-2", "10.1109/tvcg.2015.2452919", "10.1016/j.neucom.2006.11.018", "10.21105/joss.00726", "10.1109/tvcg.2007.70603", "10.1109/tvcg.2014.2346449", "10.1109/tvcg.2010.213", "10.1109/tvcg.2006.186", "10.1080/10618600.2012.657132", "10.1111/j.1467-8659.2009.01706.x", "10.1109/tvcg.2012.120", "10.1109/tvcg.2011.249", "10.1109/ldav.2016.7874305", "10.1109/tvcg.2017.2743980", "10.1109/pacificvis.2010.5429601", "10.1109/tvcg.2017.2743938", "10.1109/tvcg.2010.253", "10.1002/qua.26133", "10.1109/tvcg.2011.220", "10.1007/s10107-006-0077-1", "10.1145/1377676.1377719", "10.1111/cgf.12933", "10.1109/tvcg.2016.2598495", "10.1126/science.290.5500.2319", "10.1111/cgf.12361", "10.1016/j.neucom.2004.11.042", "10.1109/vast.2010.5652940", "10.1109/ldav48142.2019.8944365", "10.1016/j.patrec.2017.09.032", "10.1145/77635.77639", "10.1109/tvcg.2015.2489660", "10.1111/cgf.12640", "10.1126/science.290.5500.2323", "10.1111/j.1365-2966.2011.18394.x", "10.1109/tvcg.2014.2346403", "10.1090/s0273-0979-09-01249-x", "10.1111/cgf.12655", "10.1109/tvcg.2018.2846735", "10.1145/2535927", "10.1137/060654797", "10.1145/235815.235821"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2018.2864506", "title": "Visualization of Bubble Formation in Porous Media", "year": "2018", "conferenceName": "SciVis", "authors": "Hui Zhang 0027;Steffen Frey;Holger Steeb;David Uribe;Thomas Ertl;Wenping Wang", "citationCount": "0", "affiliation": "Zhang, H (Corresponding Author), Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China. Zhang, Hui; Wang, Wenping, Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China. Frey, Steffen; Ertl, Thomas, Univ Stuttgart, Visualizat Res Ctr, Stuttgart, Germany. Steeb, Holger; Uribe, David, Univ Stuttgart, Inst Appl Mech, Stuttgart, Germany.", "countries": "Germany;China", "abstract": "We present a visualization approach for the analysis of CO<sub>2</sub>bubble-induced attenuation in porous rock formations. As a basis for this, we introduce customized techniques to extract CO<sub>2</sub>bubbles and their surrounding porous structure from X-ray computed tomography data (XCT) measurements. To understand how the structure of porous media influences the occurrence and the shape of formed bubbles, we automatically classify and relate them in terms of morphology and geometric features, and further directly support searching for promising porous structures. To allow for the meaningful direct visual comparison of bubbles and their structures, we propose a customized registration technique considering the bubble shape as well as its points of contact with the porous media surface. With our quantitative extraction of geometric bubble features, we further support the analysis as well as the creation of a physical model. We demonstrate that our approach was successfully used to answer several research questions in the domain, and discuss its high practical relevance to identify critical seismic characteristics of fluid-saturated rock that govern its capability to store CO<sub>2</sub>.", "keywords": "3D volume rendering,bubble visualization,porous media", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864506", "refList": ["10.1126/science.1079033", "10.1063/pt.3.2865", "10.1063/1.4871486", "10.1109/visual.2004.48", "10.1071/eg04025", "10.1109/visual.2005.1532788", "10.1016/j.cag.2010.05.001", "10.1109/tvcg.2007.70603", "10.2312/localchapterevents/italchap/italianchapconf2008/129-136", "10.1109/im.2001.924423", "10.1016/0262-8856(92)90076-f", "10.1016/j.ijggc.2017.10.002", "10.1109/tvcg.2014.2346351", "10.1111/cgf.12605", "10.1109/tvcg.2015.2443804", "10.1016/0196-8904(93)90069-m", "10.1002/2015gl063538", "10.1109/tvcg.2013.177", "10.1109/34.982886", "10.1017/jfm.2016.401", "10.1111/cgf.13214", "10.1007/s00371-007-0197-5", "10.1145/37402.37422", "10.1109/pacificvis.2014.52", "10.1017/jfm.2017.221", "10.1109/tvcg.2012.200", "10.1109/42.906424", "10.1109/tvcg.2013.16", "10.1109/im.2003.1240258", "10.1145/571647.571648", "10.1063/1.4871489", "10.1111/j.1467-8659.2009.01671.x", "10.1145/383259.383295", "10.1016/j.advwatres.2014.02.014", "10.1109/tvcg.2006.124", "10.1109/tip.2010.2069690"], "wos": 1, "children": [{"doi": "10.1111/cgf.13688", "year": "2019", "title": "A Visual Tool for the Analysis of Algorithms for Tomographic Fiber Reconstruction in Materials Science", "conferenceName": "EuroVis", "authors": "Bernhard Fr{\\\"{o}}hler;Tim Elberfeld;Torsten M{\\\"{o}}ller;Hans{-}Christian Hege;Johannes Weissenb{\\\"{o}}ck;Jan De Beenhouwer;Jan Sijbers;Johann Kastner;Christoph Heinzl", "citationCount": "0", "affiliation": "Frohler, B (Corresponding Author), Univ Appl Sci Upper Austria, Res Grp Computed Tomog, Wels, Austria.\nFrohler, B (Corresponding Author), Uni Vienna, Data Sci, Vienna, Austria.\nFrohler, B (Corresponding Author), Univ Vienna, Fac Comp Sci, Vienna, Austria.\nFroehler, B.; Weissenboeck, J.; Kastner, J.; Heinzl, C., Univ Appl Sci Upper Austria, Res Grp Computed Tomog, Wels, Austria.\nElberfeld, T.; De Beenhouwer, J.; Sijbers, J., Univ Antwerp, Imec Vis Lab, Antwerp, Belgium.\nFroehler, B.; Moeller, T., Uni Vienna, Data Sci, Vienna, Austria.\nFroehler, B.; Moeller, T., Univ Vienna, Fac Comp Sci, Vienna, Austria.\nHege, H. C., Zuse Inst Berlin, Visual Data Anal Dept, Berlin, Germany.", "countries": "Belgium;Germany;Austria", "abstract": "We present visual analysis methods for the evaluation of tomographic fiber reconstruction algorithms by means of analysis, visual debugging and comparison of reconstructed fibers in materials science. The methods are integrated in a tool (FIAKER) that supports the entire workflow. It enables the analysis of various fiber reconstruction algorithms, of differently parameterized fiber reconstruction algorithms and of individual steps in iterative fiber reconstruction algorithms. Insight into the performance of fiber reconstruction algorithms is obtained by a list-based ranking interface. A 3D view offers interactive visualization techniques to gain deeper insight, e.g., into the aggregated quality of the examined fiber reconstruction algorithms and parameterizations. The tool was designed in close collaboration with researchers who work with fiber-reinforced polymers on a daily basis and develop algorithms for tomographic reconstruction and characterization of such materials. We evaluate the tool using synthetic datasets as well as tomograms of real materials. Five case studies certify the usefulness of the tool, showing that it significantly accelerates the analysis and provides valuable insights that make it possible to improve the fiber reconstruction algorithms. The main contribution of the paper is the well-considered combination of methods and their seamless integration into a visual tool that supports the entire workflow. Further findings result from the analysis of (dis-)similarity measures for fibers as well as from the discussion of design decisions. It is also shown that the generality of the analytical methods allows a wider range of applications, such as the application in pore space analysis.", "keywords": "Simulation; parameter space analysis; ensemble visualization; industrial CT", "link": "https://doi.org/10.1111/cgf.13688", "refList": ["10.1109/tvcg.2018.2864506", "10.1007/s10921-018-0514-0", "10.1109/tmi.2007.906785", "10.1109/tvcg.2016.2582158", "10.4028/www.scientific.net/msf.825-826.907", "10.1007/s12021-016-9316-7", "10.1109/dasip.2018.8597098", "10.1109/kse.2015.48", "10.1364/oe.24.025129", "10.3139/217.2441", "10.1109/vast.2016.7883516", "10.1109/tvcg.2009.115", "10.1111/cgf.12895", "10.1088/1757-899x/406/1/012014", "10.1109/tvcg.2014.2346321", "10.1111/cgf.13415", "10.1109/pacificvis.2014.52", "10.1109/tvcg.2012.213", "10.1109/vast.2014.7042491", "10.1109/tvcg.2016.2598589", "10.1002/nbm.3902", "10.21105/joss.01185", "10.1109/tvcg.2013.173", "10.1145/3173574.3173943"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.12934", "year": "2016", "title": "State of the Art in Transfer Functions for Direct Volume Rendering", "conferenceName": "EuroVis", "authors": "Patric Ljung;Jens H. Kr{\\\"{u}}ger;M. Eduard Gr{\\\"{o}}ller;Markus Hadwiger;Charles D. Hansen;Anders Ynnerman", "citationCount": "33", "affiliation": "Ljung, P (Corresponding Author), Linkoping Univ, S-58183 Linkoping, Sweden.\nLjung, Patric; Ynnerman, Anders, Linkoping Univ, S-58183 Linkoping, Sweden.\nKrueger, Jens, Univ Duisburg Essen, CoViDAG, Essen, Germany.\nKrueger, Jens; Hansen, Charles D., Univ Utah, Sci Comp \\& Imaging Inst, Salt Lake City, UT 84112 USA.\nGroeller, Eduard, TU Wien, Vienna, Austria.\nGroeller, Eduard, Univ Bergen, N-5020 Bergen, Norway.\nHadwiger, Markus, King Abdullah Univ Sci \\& Technol, Thuwal, Saudi Arabia.", "countries": "USA;Arabia;Germany;Austria;Sweden;Norway", "abstract": "A central topic in scientific visualization is the transfer function (TF) for volume rendering. The TF serves a fundamental role in translating scalar and multivariate data into color and opacity to express and reveal the relevant features present in the data studied. Beyond this core functionality, TFs also serve as a tool for encoding and utilizing domain knowledge and as an expression for visual design of material appearances. TFs also enable interactive volumetric exploration of complex data. The purpose of this state-of-the-art report (STAR) is to provide an overview of research into the various aspects of TFs, which lead to interpretation of the underlying data through the use of meaningful visual representations. The STAR classifies TF research into the following aspects: dimensionality, derived attributes, aggregated attributes, rendering aspects, automation, and user interfaces. The STAR concludes with some interesting research challenges that form the basis of an agenda for the development of next generation TF tools and methodologies.", "keywords": "", "link": "https://doi.org/10.1111/cgf.12934", "refList": ["10.1109/tvcg.2008.198", "10.2312/vissym/vissym04/017-024.8", "10.2312/vissym/vissym02/115-124", "10.1109/tvcg.2011.97", "10.1109/visual.2003.1250413", "10.1109/tvcg.2015.2467031", "10.1109/tvcg.2009.120", "10.1109/pccga.2004.1348348", "10.1111/j.1467-8659.2011.01944.x", "10.1109/tvcg.2008.170", "10.1109/svv.1998.729588", "10.1111/cgf.12365", "10.1109/tvcg.2009.115", "10.1371/journal.pone.0038586", "10.1109/tvcg.2010.195", "10.1109/tvcg.2006.148", "10.1111/cgf.12623", "10.1109/visual.2003.1250386", "10.1109/sccg.2001.945360", "10.1109/ldav.2014.7013202", "10.1109/pacificvis.2014.24", "10.1109/visual.1999.809932", "10.1109/38.865879", "10.1109/tvcg.2011.261", "10.1109/pacificvis.2009.4906854", "10.1109/tvcg.2008.162", "10.2312/vg/vg-pbg08/041-048", "10.2312/vg/vg06/001-008", "10.1109/tvcg.2014.2346411", "10.1109/2945.998670", "10.2312/vissym/eurovis05/263-270", "10.2312/conf/eg2012/stars/075-094", "10.1016/j.cag.2012.02.007", "10.1109/tvcg.2015.2467294", "10.1109/tvcg.2014.2346351", "10.1109/tvcg.2008.25", "10.2312/vissym/eurovis07/115-122", "10.1109/visual.1999.809886", "10.1111/cgf.12624", "10.1109/tvcg.2008.169", "10.1109/tvcg.2002.1021579", "10.1109/tvcg.2007.1051", "10.1109/tvcg.2010.239", "10.1111/cgf.12371", "10.1109/tvcg.2007.70518", "10.1109/tvcg.2014.2346324", "10.1109/visual.1998.745319", "10.1109/jbhi.2013.2263227", "10.1109/tvcg.2012.80", "10.1109/tvcg.2006.100", "10.1007/s10915-011-9501-7", "10.1109/pacificvis.2013.6596129", "10.1109/pacificvis.2010.5429615", "10.1109/tvcg.2006.124", "10.1145/1375714.1375729", "10.1109/visual.2004.48", "10.1109/ldav.2011.6092313", "10.1109/38.920623", "10.2312/vissym/eurovis05/069-076", "10.1109/2945.646238", "10.1016/j.gmod.2003.08.002", "10.1109/tvcg.2010.35", "10.1109/svv.1998.729580", "10.1109/visual.1995.480803", "10.2312/vissym/eurovis06/251-258", "10.2312/vissym/eurovis06/227-234", "10.1109/tvcg.2015.2467431", "10.2312/vissym/vissym04/017-024", "10.1109/tvcg.2006.39", "10.1016/j.cmpb.2007.03.008", "10.1109/pacificvis.2011.5742368", "10.1109/tvcg.2007.47", "10.1111/j.1467-8659.2007.01095.x", "10.1109/tvcg.2010.170", "10.2312/vcbm/vcbm08/101-108", "10.2312/vissym/eurovis06/243-250", "10.1109/visual.2000.885678", "10.2312/vg/vg07/001-008", "10.1109/tvcg.2009.185", "10.1016/j.cag.2008.08.006", "10.1109/icma.2007.4303986", "10.1109/tvcg.2005.38", "10.1109/tvcg.2006.96", "10.1111/j.1467-8659.2009.01474.x", "10.1109/tvcg.2009.189", "10.2312/vissym/eurovis07/131-138", "10.1109/pccga.2002.1167880", "10.1111/j.1467-8659.2012.03123.x", "10.2312/vissym/eurovis05/271-278", "10.1109/tvcg.2009.25", "10.1111/j.1467-8659.2008.01216.x", "10.1109/tvcg.2012.231", "10.1109/tvcg.2011.258", "10.1111/j.1467-8659.2005.00855.x", "10.1109/visual.1996.568113", "10.1109/tvcg.2005.62", "10.1109/tvcg.2011.23", "10.1109/tvcg.2014.2359462", "10.1109/icics.2009.5397587", "10.1109/2945.942694", "10.1109/pacificvis.2009.4906857", "10.1109/tvcg.2007.70591", "10.1109/pacificvis.2010.5429624", "10.1109/visual.2003.1250414", "10.2312/vg/vg05/137-145", "10.1109/tvcg.2012.105", "10.1111/j.1467-8659.2012.03122.x", "10.1109/38.511", "10.1109/2945.856994", "10.1109/tvcg.2006.72", "10.1109/2945.468400", "10.2312/vg/vg10/077-083", "10.1057/ivs.2010.6"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2017.2744078", "title": "An Intelligent System Approach for Probabilistic Volume Rendering Using Hierarchical 3D Convolutional Sparse Coding", "year": "2017", "conferenceName": "SciVis", "authors": "Tran Minh Quan;Junyoung Choi;Haejin Jeong;Won-Ki Jeong", "citationCount": "4", "affiliation": "Ulsan Nat'l Inst. of Science and Technology (UNIST);Ulsan Nat'l Inst. of Science and Technology (UNIST);Ulsan Nat'l Inst. of Science and Technology (UNIST);Ulsan Nat'l Inst. of Science and Technology (UNIST)", "countries": "Ulsan Nat'l Inst. of Science and Technology (UNIST)", "abstract": "In this paper, we propose a novel machine learning-based voxel classification method for highly-accurate volume rendering. Unlike conventional voxel classification methods that incorporate intensity-based features, the proposed method employs dictionary based features learned directly from the input data using hierarchical multi-scale 3D convolutional sparse coding, a novel extension of the state-of-the-art learning-based sparse feature representation method. The proposed approach automatically generates high-dimensional feature vectors in up to 75 dimensions, which are then fed into an intelligent system built on a random forest classifier for accurately classifying voxels from only a handful of selection scribbles made directly on the input data by the user. We apply the probabilistic transfer function to further customize and refine the rendered result. The proposed method is more intuitive to use and more robust to noise in comparison with conventional intensity-based classification methods. We evaluate the proposed method using several synthetic and real-world volume datasets, and demonstrate the methods usability through a user study.", "keywords": "Volume Rendering,Machine Learning,Hierarchically Convolutional Sparse Coding", "link": "http://dx.doi.org/10.1109/TVCG.2017.2744078", "refList": ["10.1207/s15327051hci0701\\_3", "10.1109/tvcg.2008.162", "10.1109/cvpr.2013.57", "10.1109/cvpr.2015.7299149", "10.1109/isbi.2015.7164109", "10.1109/38.920623", "10.1109/tvcg.2009.25", "10.1109/tvcg.2012.231", "10.1109/cvpr.2010.5539957", "10.1007/978-3-319-46726-9\\_56", "10.1109/tsp.2006.881199", "10.1109/cvpr.2006.142", "10.1109/svv.1998.729588", "10.1109/cvpr.2014.394", "10.1111/cgf.12623", "10.1109/icassp.2014.6854992", "10.1111/cgf.12624", "10.1109/tvcg.2002.1021579", "10.1145/54852.378484", "10.1111/cgf.12934", "10.4135/9781412961288.n154", "10.1109/tip.2015.2495260", "10.1109/tvcg.2012.105", "10.1145/3065386", "10.1109/38.511", "10.1109/tvcg.2005.38", "10.1007/978-3-319-12643-2\\_31", "10.1145/360825.360839", "10.2307/1932409", "10.1109/pacificvis.2013.6596129", "10.1109/tvcg.2011.261", "10.1023/a:1010933404324"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00041", "year": "2019", "title": "DNN-VolVis: Interactive Volume Visualization Supported by Deep Neural Network", "conferenceName": "PacificVis", "authors": "Fan Hong;Can Liu;Xiaoru Yuan", "citationCount": "2", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.\nYuan, XR (Corresponding Author), Peking Univ, Sch EECS, Beijing, Peoples R China.\nYuan, XR (Corresponding Author), Peking Univ, Beijing Engn Technol Res Ctr Virtual Simulat \\& Vi, Beijing, Peoples R China.\nHong, Fan; Liu, Can; Yuan, Xiaoru, Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.\nHong, Fan; Liu, Can; Yuan, Xiaoru, Peking Univ, Sch EECS, Beijing, Peoples R China.\nYuan, Xiaoru, Peking Univ, Beijing Engn Technol Res Ctr Virtual Simulat \\& Vi, Beijing, Peoples R China.", "countries": "China", "abstract": "In this work, we propose a novel approach of volume visualization without explicit traditional rendering pipeline. In our proposed method, volumetric images can be interactively `reversed' given the volumetric data and a static volume rendered image under the desired rendering effect. Our pipeline enables 3D-navigation on it for exploring the given volumetric data without explicit transfer function. In our approach, deep neural networks, combined usage of Generative Adversarial Networks (GANs) and Convolutional Neural Networks (CNN) are employed to synthesize high-resolution and perceptually authentic images directly, inheriting the desired transfer function and viewing parameter implicitly given by the input images respectively.", "keywords": "Deep learning; volume rendering; transfer function; generative adversarial network; machine learning", "link": "https://doi.org/10.1109/PacificVis.2019.00041", "refList": ["10.1109/iccv.2017.629", "10.1109/tvcg.2008.162", "10.2312/vissym/eurovis05/271-278", "10.1109/38.920623", "10.1109/tvcg.2009.25", "10.1109/tvcg.2017.2744078", "10.1109/visual.2003.1250413", "10.1109/cvpr.2017.19", "10.1109/tvcg.2010.35", "10.1109/cvpr.2015.7298594", "10.1109/visual.1996.568113", "10.1109/5.726791", "10.1109/svv.1998.729588", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/83.535842", "10.1016/0893-6080(91)90009-t", "10.1109/tvcg.2006.148", "10.2312/vissym/eurovis07/115-122", "10.1038/323533a0", "10.1109/tvcg.2002.1021579", "10.1109/tvcg.2007.1051", "10.1145/1830483.1830503", "10.1109/cvpr.2016.90", "10.1109/tvcg.2012.80", "10.1007/978-3-319-24574-4\\_28", "10.1109/tvcg.2005.38", "10.1109/tvcg.2009.189", "10.1109/pacificvis.2013.6596129", "10.1109/visual.1999.809932", "10.1007/978-3-319-46493-0\\_47", "10.1109/tvcg.2011.261", "10.1109/pccga.2002.1167880"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030374", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data", "year": "2020", "conferenceName": "SciVis", "authors": "Yifan Wang;Guoli Yan;Haikuan Zhu;Sagar Buch;Ying Wang;E. Mark Haacke;Jing Hua;Zichun Zhong", "citationCount": "0", "affiliation": "Wang, YF (Corresponding Author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Wang, Yifan; Yan, Guoli; Zhu, Haikuan; Hua, Jing; Zhong, Zichun, Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Buch, Sagar; Wang, Ying; Haacke, Ewart Mark, Wayne State Univ, Dept Radiol, Detroit, MI 48201 USA.", "countries": "USA", "abstract": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.", "keywords": "Deep neural network,3D cerebrovascular segmentation and visualization,maximum intensity projection (MIP),joint embedding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030374", "refList": ["10.1109/cluster.2018.00036", "10.1109/iccv.2015.123", "10.1109/tvcg.2013.133", "10.1109/cvpr.2017.19", "10.1109/tvcg.2019.2934312", "10.1007/978-3-319-46487-9\\_40", "10.1109/tvcg.2006.175", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2018.2880207", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/tvcg.2018.2816059", "10.1109/pacificvis.2009.4906852", "10.1109/tvcg.2018.2864808", "10.1109/mcg.2018.2881523", "10.1109/tvcg.2015.2467431", "10.1109/tpami.2015.2439281", "10.1109/pacificvis48177.2020.8737", "10.1109/cvpr.2019.00244", "10.1109/tvcg.2006.165", "10.1109/bigdata.2018.8622520", "10.1007/978-3-319-46466-4\\_29", "10.1109/visual.2019.8933759", "10.1145/3197517.3201304", "10.1109/pacificvis.2011.5742378", "10.1109/cvpr.2006.91", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1007/978-3-319-24574-4\\_28", "10.1109/pacificvis.2019.00041", "10.1109/tvcg.2019.2934332", "10.1109/cvpr.2018.00916", "10.1007/978-3-319-46475-6\\_43"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.8737", "year": "2020", "title": "SSR-VFD: Spatial Super-Resolution for Vector Field Data Analysis and Visualization", "conferenceName": "PacificVis", "authors": "Li Guo;Shaojie Ye;Jun Han;Hao Zheng;Han Gao;Danny Z. Chen;Jian{-}Xun Wang;Chaoli Wang", "citationCount": "1", "affiliation": "Guo, L (Corresponding Author), Nankai Univ, Tianjin, Peoples R China.\nGuo, Li, Nankai Univ, Tianjin, Peoples R China.\nYe, Shaojie, Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.\nHan, Jun; Zheng, Hao; Gao, Han; Chen, Danny Z.; Wang, Jian-Xun; Wang, Chaoli, Univ Notre Dame, Notre Dame, IN 46556 USA.", "countries": "USA;China", "abstract": "We present SSR-VFD, a novel deep learning framework that produces coherent spatial super-resolution (SSR) of three-dimensional vector field data (VFD). SSR-VFD is the first work that advocates a machine learning approach to generate high-resolution vector fields from low-resolution ones. The core of SSR-VFD lies in the use of three separate neural nets that take the three components of a low-resolution vector field as input and jointly output a synthesized high-resolution vector field. To capture spatial coherence, we take into account magnitude and angle losses in network optimization. Our method can work in the in situ scenario where VFD are down-sampled at simulation time for storage saving and these reduced VFD are upsampled back to their original resolution during postprocessing. To demonstrate the effectiveness of SSR-VFD, we show quantitative and qualitative results with several vector field data sets of different characteristics and compare our method against volume upscaling using bicubic interpolation, and two solutions based on CNN and GAN, respectively.", "keywords": "Spatial super-resolution; vector field data; convolutional neural network; deep learning", "link": "https://doi.org/10.1109/PacificVis48177.2020.8737", "refList": ["10.1016/j.ijvsm.2017.05.001", "10.1016/j.jvs.2005.01.020", "10.1109/iccv.2015.123", "10.1111/cgf.13620", "10.1109/cvpr.2019.00831", "10.1109/cvpr.2019.00817", "10.1109/cvpr.2019.00399", "10.1109/tvcg.2019.2934312", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2018.2816059", "10.1109/mcg.2018.2881523", "10.1109/tpami.2015.2439281", "10.1109/bigdata.2018.8622520", "10.1145/3197517.3201304", "10.1109/tvcg.2018.2796085", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1109/pacificvis.2019.00041", "10.1111/cgf.13689"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030374", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data", "year": "2020", "conferenceName": "SciVis", "authors": "Yifan Wang;Guoli Yan;Haikuan Zhu;Sagar Buch;Ying Wang;E. Mark Haacke;Jing Hua;Zichun Zhong", "citationCount": "0", "affiliation": "Wang, YF (Corresponding Author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Wang, Yifan; Yan, Guoli; Zhu, Haikuan; Hua, Jing; Zhong, Zichun, Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Buch, Sagar; Wang, Ying; Haacke, Ewart Mark, Wayne State Univ, Dept Radiol, Detroit, MI 48201 USA.", "countries": "USA", "abstract": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.", "keywords": "Deep neural network,3D cerebrovascular segmentation and visualization,maximum intensity projection (MIP),joint embedding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030374", "refList": ["10.1109/cluster.2018.00036", "10.1109/iccv.2015.123", "10.1109/tvcg.2013.133", "10.1109/cvpr.2017.19", "10.1109/tvcg.2019.2934312", "10.1007/978-3-319-46487-9\\_40", "10.1109/tvcg.2006.175", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2018.2880207", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/tvcg.2018.2816059", "10.1109/pacificvis.2009.4906852", "10.1109/tvcg.2018.2864808", "10.1109/mcg.2018.2881523", "10.1109/tvcg.2015.2467431", "10.1109/tpami.2015.2439281", "10.1109/pacificvis48177.2020.8737", "10.1109/cvpr.2019.00244", "10.1109/tvcg.2006.165", "10.1109/bigdata.2018.8622520", "10.1007/978-3-319-46466-4\\_29", "10.1109/visual.2019.8933759", "10.1145/3197517.3201304", "10.1109/pacificvis.2011.5742378", "10.1109/cvpr.2006.91", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1007/978-3-319-24574-4\\_28", "10.1109/pacificvis.2019.00041", "10.1109/tvcg.2019.2934332", "10.1109/cvpr.2018.00916", "10.1007/978-3-319-46475-6\\_43"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.14037", "year": "2020", "title": "State of the Art in Time-Dependent Flow Topology: Interpreting Physical Meaningfulness Through Mathematical Properties", "conferenceName": "EuroVis", "authors": "Roxana Bujack;Lin Yan;Ingrid Hotz;Christoph Garth;Bei Wang", "citationCount": "0", "affiliation": "Bujack, R (Corresponding Author), Los Alamos Natl Lab, Los Alamos, NM 87545 USA.\nBujack, Roxana, Los Alamos Natl Lab, Los Alamos, NM 87545 USA.\nYan, Lin; Wang, Bei, Univ Utah, Sci Comp \\& Imaging Inst, Salt Lake City, UT 84112 USA.\nHotz, Ingrid, Linkopings Univ, Sci Visualizat Grp, Linkoping, Sweden.\nGarth, Christoph, Univ Kaiserslautern, Kaiserslautern, Germany.", "countries": "Sweden;Germany;USA", "abstract": "We present a state-of-the-art report on time-dependent flow topology. We survey representative papers in visualization and provide a taxonomy of existing approaches that generalize flow topology from time-independent to time-dependent settings. The approaches are classified based upon four categories: tracking of steady topology, reference frame adaption, pathline classification or clustering, and generalization of critical points. Our unique contributions include introducing a set of desirable mathematical properties to interpret physical meaningfulness for time-dependent flow visualization, inferring mathematical properties associated with selective research papers, and utilizing such properties for classification. The five most important properties identified in the existing literature include coincidence with the steady case, induction of a partition within the domain, Lagrangian invariance, objectivity, and Galilean invariance.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14037", "refList": ["10.2514/6.1995-1715", "10.1111/cgf.12100", "10.1007/978-3-540-70823-0\\_1", "10.1111/cgf.12885", "10.1063/1.857730", "10.1109/tvcg.2019.2934312", "10.1109/tvcg.2013.92", "10.1063/1.166399", "10.1109/tvcg.2005.68.3", "10.1063/1.4971788", "10.1111/cgf.12933", "10.1145/3072959.3073684", "10.1063/1.4982720", "10.1109/tvcg.2018.2864432", "10.1017/s0022112004002526", "10.1109/tvcg.2010.93", "10.1109/pacificvis.2016.7465253", "10.1109/tvcg.2018.2864505", "10.1063/1.858828", "10.1109/tvcg.2019.2934255", "10.5194/npg-9-237-2002", "10.1063/1.4800210", "10.1007/978-3-540-88606-8\\_12", "10.1111/j.1467-8659.2011.01942.x", "10.1109/2945.928168", "10.1007/978-3-662-10388-31", "10.1016/j.physd.2013.01.013", "10.1111/cgf.12358", "10.1017/s002211209900720x", "10.1017/s0022112097008057", "10.1111/j.1467-8659.2009.01546.x", "10.1109/2.35197", "10.1111/j.1467-8659.2012.03089.x", "10.1111/cgf.13319", "10.1109/tvcg.2019.2934375.3", "10.1016/j.crme.2015.08.002.4", "10.1016/s0167-2789(00)00142-1", "10.1111/cgf.12359", "10.1103/physreve.93.063107", "10.1016/j.physd.2009.05.005", "10.1063/1.1477449", "10.1111/j.1467-8659.2009.01686.x", "10.1109/pacificvis.2011.5742374", "10.1017/jfm.2013.391", "10.1111/cgf.12121", "10.1186/1743-422x-3-15", "10.1063/1.3502450", "10.1063/1.868323", "10.1016/j.cnsns.2013.05.002", "10.1109/visual.2004.99", "10.1109/visual.2004.107", "10.1111/j.1467-8659.2009.01604.x", "10.1146/annurev-fluid-010313-141322", "10.1063/1.3690153", "10.1109/tvcg.2019.2934375", "10.1109/tvcg.2013.143", "10.1137/130940633", "10.1016/j.cag.2014.01.007", "10.1016/s0097-8493(02)00056-0", "10.1109/vl.1996.545307", "10.1109/tvcg.2011.284", "10.1109/tvcg.2011.265", "10.1017/jfm.2016.792", "10.1109/tvcg.2013.208", "10.1007/s12650-016-0348-8", "10.1023/b:elas.0000005548.36767.e7", "10.1007/978-1-4939-0419-8\\_\\_9", "10.1007/bf00849110", "10.1615/int.j.uncertaintyquantification.2012003956", "10.1017/s0022112096001802", "10.1017/s0962492902000065", "10.1109/pacificvis.2019.00041", "10.1111/j.1467-8659.2011.01901.x", "10.1109/tvcg.2008.33", "10.1016/j.physd.2005.10.007", "10.1016/s0894-1777(96)00090-8", "10.1145/2517327.2442526", "10.1109/tvcg.2017.2743938", "10.1109/tvcg.2018.2816059", "10.1109/tvcg.2019.2934242", "10.2514/6.1995-1715.4", "10.1111/cgf.12109", "10.1109/tvcg.2011.269", "10.1109/visual.1990.146359", "10.1111/j.1467-8659.2003.00723.x", "10.5194/npg-18-977-2011", "10.1109/visual.1998.745296", "10.1109/tvcg.2007.70557", "10.1109/tvcg.2014.2312012"], "wos": 1, "children": [], "len": 1}], "len": 9}], "len": 11}, {"doi": "10.1109/tvcg.2018.2864816", "title": "Interactive Visualization of 3D Histopathology in Native Resolution", "year": "2018", "conferenceName": "SciVis", "authors": "Martin Falk;Anders Ynnerman;Darren Treanor;Claes Lundstr\u00f6m", "citationCount": "1", "affiliation": "Falk, M (Corresponding Author), Linkoping Univ, Dept Sci \\& Technol, Linkoping, Sweden. Falk, Martin; Ynnerman, Anders, Linkoping Univ, Dept Sci \\& Technol, Linkoping, Sweden. Treanor, Darren, Leeds Teaching Hosp NHS Trust, Leeds, W Yorkshire, England. Treanor, Darren; Lundstrom, Claes, Linkoping Univ, Ctr Med Image Sci \\& Visualizat CMIV, Linkoping, Sweden. Lundstrom, Claes, Sectra AB, Linkoping, Sweden.", "countries": "Sweden;England", "abstract": "We present a visualization application that enables effective interactive visual analysis of large-scale 3D histopathology, that is, high-resolution 3D microscopy data of human tissue. Clinical work flows and research based on pathology have, until now, largely been dominated by 2D imaging. As we will show in the paper, studying volumetric histology data will open up novel and useful opportunities for both research and clinical practice. Our starting point is the current lack of appropriate visualization tools in histopathology, which has been a limiting factor in the uptake of digital pathology. Visualization of 3D histology data does pose difficult challenges in several aspects. The full-color datasets are dense and large in scale, on the order of 100,000 \u00d7 100,000 \u00d7 100 voxels. This entails serious demands on both rendering performance and user experience design. Despite this, our developed application supports interactive study of 3D histology datasets at native resolution. Our application is based on tailoring and tuning of existing methods, system integration work, as well as a careful study of domain specific demands emanating from a close participatory design process with domain experts as team members. Results from a user evaluation employing the tool demonstrate a strong agreement among the 14 participating pathologists that 3D histopathology will be a valuable and enabling tool for their work.", "keywords": "Histology,Pathology,Volume Rendering,Expert Evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864816", "refList": ["10.4103/2153-3539.129452", "10.2312/vissym/vissym02/115-124", "10.1371/journal.pone.0126817", "10.1093/ajcp/138.suppl1.288", "10.1109/tvcg.2009.150", "10.4103/2153-3539.151890", "10.1109/mcg.2010.26", "10.1109/tbme.2014.2303294", "10.1109/tvcg.2012.240", "10.1111/cgf.12605", "10.1007/s00371-008-0261-9", "10.1109/tvcg.2002.1021579", "10.1016/j.mri.2012.05.001", "10.1109/pacificvis.2017.8031591", "10.1111/his.12629", "10.2312/vissym/vissym00/137-146", "10.4103/2153-3539.151894", "10.1186/s12859-017-1934-z", "10.1117/12.813756", "10.1145/2834117", "10.1111/his.13452", "10.1111/cgf.12934", "10.1001/jama.2017.14585", "10.1016/j.ajpath.2012.01.033", "10.1109/visual.2002.1183757", "10.4103/2153-3539.114206", "10.17629/www.diagnosticpathology.eu-2016-2:232", "10.1109/mcg.2013.55", "10.1073/pnas.1710742114", "10.1117/12.529137", "10.2312/vg/vg-pbg08/163-170", "10.4103/2153-3539.119005"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13183", "year": "2017", "title": "Visual Analysis of Confocal Raman Spectroscopy Data using Cascaded Transfer Function Design", "conferenceName": "EuroVis", "authors": "Christoph M. Schikora;Markus Plack;Rainer Bornemann;Peter Haring Bol{\\'{\\i}}var;Andreas Kolb", "citationCount": "0", "affiliation": "Schikora, CM (Corresponding Author), Univ Siegen, Comp Graph \\& Multimedia Syst, Siegen, Germany.\nSchikora, Christoph M.; Plack, Markus; Kolb, Andreas, Univ Siegen, Comp Graph \\& Multimedia Syst, Siegen, Germany.\nBornemann, Rainer; Bolivar, Peter Haring, Univ Siegen, High Frequency \\& Quantum Elect, Siegen, Germany.", "countries": "Germany", "abstract": "2D Confocal Raman Microscopy (CRM) data consist of high dimensional per-pixel spectral data of 1000 bands and allows for complex spectral and spatial-spectral analysis tasks, i.e., in material discrimination, material thickness, and spatial material distributions. Currently, simple integral methods are commonly applied as visual analysis solutions to CRM data which exhibit restricted discrimination power in various regards. In this paper we present a novel approach for the visual analysis of 2D multispectral CRM data using multi-variate visualization techniques. Due to the large amount of data and the demand of an explorative approach without a-priori restriction, our system allows for arbitrary interactive (de)selection of varaibles w/o limitation and an unrestricted online definition/construction of new, combined properties. Our approach integrates CRM specific quantitative measures and handles material-related features for mixed materials in a quantitative manner. Technically, we realize the online definition/construction of new, combined properties as semi-automatic, cascaded, 1D and 2D multidimensional transfer functions (MD-TFs). By interactively incorporating new (raw or derived) properties, the dimensionality of the MD-TF space grows during the exploration procedure and is virtually unlimited. The final visualization is achieved by an enhanced color mixing step which improves saturation and contrast.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13183", "refList": ["10.1016/j.sab.2006.12.002", "10.1021/nl061702a", "10.1007/978-3-642-12522-5\\_4", "10.1016/j.carbon.2016.01.001", "10.1137/040616024", "10.1016/j.cag.2012.02.007", "10.1111/cgf.12365", "10.2312/vissym/eur0vis05/117-123", "10.1007/s11664-009-0803-6", "10.1109/mcse.2012.27", "10.1109/tvcg.2002.1021579", "10.1007/978-3-540-85567-5\\_50", "10.1109/pacificvis.2010.5429612", "10.1109/tgrs.2010.2051553", "10.1109/tvcg.2007.70591", "10.1021/ac034173t", "10.1366/000370210792434350", "10.1111/cgf.12934", "10.1109/tvcg.2012.110", "10.1109/tvcg.2012.105", "10.1109/tvcg.2006.164", "10.1109/tvcg.2009.199", "10.1109/visual.2003.1250412", "10.1109/igarss.2011.6049397", "10.1007/978-3-642-12522-5", "10.1109/pacificvis.2013.6596129", "10.1039/c4an01061b", "10.1109/tvcg.2011.261"], "wos": 1, "children": [], "len": 1}], "len": 17}], "len": 27}, "index": 340, "embedding": [-0.7157644629478455, 0.8549587726593018, -0.11267352849245071, -2.350348949432373, -0.6415178775787354, 0.16650904715061188, -0.7643030881881714, -0.9931802153587341, -0.3518473505973816, -0.9009531736373901, 1.5595521926879883, -1.4362200498580933, 0.435477077960968, -0.7744754552841187, 0.5949411988258362, 0.2753571569919586, 0.22621992230415344, 0.06589167565107346, 1.0194439888000488, 1.6047289371490479, -0.06630208343267441, -0.6124352812767029, 0.8529813885688782, -0.15392345190048218, -2.343374252319336, 0.05785536766052246, -0.5378978252410889, 1.112848162651062, -0.052440278232097626, -0.6941285133361816, 0.7142332196235657, 0.45103275775909424], "projection": [2.120631694793701, 3.7565243244171143], "size": 14, "height": 6, "width": 5}