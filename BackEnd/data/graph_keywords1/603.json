{"data": {"doi": "10.1109/tvcg.2016.2598867", "title": "Evaluation of Graph Sampling: A Visualization Perspective", "year": "2016", "conferenceName": "InfoVis", "authors": "Yanhong Wu;Nan Cao;Daniel Archambault;Qiaomu Shen;Huamin Qu;Weiwei Cui", "citationCount": "27", "affiliation": "Cui, WW (Corresponding Author), Microsoft Res Asia, Beijing, Peoples R China. Wu, Yanhong; Shen, Qiaomu; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Hong Kong, Peoples R China. Cao, Nan, New York Univ, Shanghai, Peoples R China. Archambault, Daniel, Swansea Univ, Swansea, W Glam, Wales. Cui, Weiwei, Microsoft Res Asia, Beijing, Peoples R China.", "countries": "Wales;China", "abstract": "Graph sampling is frequently used to address scalability issues when analyzing large graphs. Many algorithms have been proposed to sample graphs, and the performance of these algorithms has been quantified through metrics based on graph structural properties preserved by the sampling: degree distribution, clustering coefficient, and others. However, a perspective that is missing is the impact of these sampling strategies on the resultant visualizations. In this paper, we present the results of three user studies that investigate how sampling strategies influence node-link visualizations of graphs. In particular, five sampling strategies widely used in the graph mining literature are tested to determine how well they preserve visual features in node-link diagrams. Our results show that depending on the sampling strategy used different visual features are preserved. These results provide a complimentary view to metric evaluations conducted in the graph mining literature and provide an impetus to conduct future visualization studies.", "keywords": "Graph visualization;graph sampling;empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2016.2598867", "refList": ["10.1109/tvcg.2008.135", "10.1109/tvcg.2006.120", "10.1016/s0378-8733(01)00042-9", "10.1145/2702123.2702476", "10.1007/s00371-013-0892-3", "10.1073/pnas.0501179102", "10.1111/j.1467-8659.2011.01898.x", "10.1109/iv.2008.37", "10.1145/2020408.2020431", "10.1103/physreve.73.016102", "10.1109/tvcg.2013.232", "10.1109/tvcg.2012.238", "10.1109/2945.841119", "10.1109/icde.2015.7113345", "10.1145/1367497.1367620", "10.1109/tvcg.2015.2468151", "10.1109/tvcg.2010.60", "10.1109/tvcg.2007.70535", "10.1038/30918", "10.1109/tvcg.2015.2465151", "10.1109/tvcg.2008.151", "10.1126/science.286.5439.509", "10.1145/1150402.1150479", "10.1109/tvcg.2006.147", "10.1186/1471-2105-15-220", "10.1016/j.physa.2015.03.048", "10.1109/tvcg.2008.34", "10.1145/1168149.1168168", "10.1111/j.1467-8659.2012.03079.x", "10.1145/1134271.1134277", "10.1561/2200000005", "10.1109/infvis.2004.1", "10.1145/1081870.1081893", "10.1038/nbt1116", "10.1109/pacificvis.2011.5742389", "10.1111/j.1467-8659.2009.01449.x", "10.1088/1742-5468/2008/10/p10008", "10.1214/13-aos1125", "10.1145/2601438", "10.1111/j.1467-8659.2009.01683.x", "10.1016/s0378-8733(97)00016-6", "10.1111/j.1467-8659.2009.01450.x", "10.5555/2999134.2999195"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2017.2743858", "title": "What Would a Graph Look Like in this Layout? A Machine Learning Approach to Large Graph Visualization", "year": "2017", "conferenceName": "InfoVis", "authors": "Oh-Hyun Kwon;Tarik Crnovrsanin;Kwan-Liu Ma", "citationCount": "13", "affiliation": "Kwon, OH (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Kwon, Oh-Hyun; Crnovrsanin, Tarik; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "Using different methods for laying out a graph can lead to very different visual appearances, with which the viewer perceives different information. Selecting a \u201cgood\u201d layout method is thus important for visualizing a graph. The selection can be highly subjective and dependent on the given task. A common approach to selecting a good layout is to use aesthetic criteria and visual inspection. However, fully calculating various layouts and their associated aesthetic metrics is computationally expensive. In this paper, we present a machine learning approach to large graph visualization based on computing the topological similarity of graphs using graph kernels. For a given graph, our approach can show what the graph would look like in different layouts and estimate their corresponding aesthetic metrics. An important contribution of our work is the development of a new framework to design graph kernels. Our experimental study shows that our estimation calculation is considerably faster than computing the actual layouts and their aesthetic metrics. Also, our graph kernels outperform the state-of-the-art ones in both time and accuracy. In addition, we conducted a user study to demonstrate that the topological similarity computed with our graph kernel matches perceptual similarity assessed by human users.", "keywords": "Graph visualization,graph layout,aesthetics,machine learning,graph kernel,graphlet", "link": "http://dx.doi.org/10.1109/TVCG.2017.2743858", "refList": ["10.1080/00018730601170527", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/2783258.2783417", "10.2307/2412323", "10.1109/tvcg.2007.70580", "10.1103/physreve.70.066111", "10.7155/jgaa.00405", "10.1111/j.1467-8659.2011.01898.x", "10.1109/infvis.2002.1173159", "10.1177/1473871612455749", "10.1007/3-540-58950-3", "10.1007/3-540-44541-2\\_17", "10.1109/tvcg.2008.158", "10.1145/2629564", "10.1145/264645.264657", "10.1023/a:1022627411411", "10.1109/tvcg.2015.2467451", "10.1109/2945.841119", "10.1109/72.485670", "10.1007/978-3-540-45167-9\\_11", "10.1016/0020-0190(89)90102-6", "10.1007/978-3-642-36763-2", "10.1145/1014052.1014072", "10.3402/qhw.v6i2.5918", "10.1007/978-3-642-36763-2\\_48", "10.1109/tvcg.2008.155", "10.1016/j.ins.2016.01.074", "10.1007/978-3-662-44043-8\\_3", "10.1093/bioinformatics/bth436", "10.13140/2.1.2393.1847", "10.1016/j.camwa.2004.08.015", "10.1109/tvcg.2010.269", "10.1109/tvcg.2017.2674999", "10.1006/s1045-926x(02)00016-2", "10.1016/0925-7721(94)00014-x", "10.1007/s10115-013-0673-3", "10.1109/tvcg.2007.46", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1016/s0378-8733(96)00299-7", "10.1109/tvcg.2016.2598467", "10.1023/b:stco.0000035301.49549.88", "10.1016/0167-8655(83)90033-8", "10.1093/bioinformatics/btl301", "10.1109/34.868688", "10.1098/rsif.2009.0192", "10.1145/1961189.1961199", "10.7155/jgaa.00051"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865139", "title": "Structure-Based Suggestive Exploration: A New Approach for Effective Exploration of Large Networks", "year": "2018", "conferenceName": "InfoVis", "authors": "Wei Chen;Fangzhou Guo;Dongming Han;Jacheng Pan;Xiaotao Nie;Jiazhi Xia;Xiaolong Zhang", "citationCount": "5", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Labo CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Chen, Wei; Guo, Fangzhou; Han, Dongming; Pan, Jacheng; Nie, Xiaotao, Zhejiang Univ, State Key Labo CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Xia, Jiazhi, Cent S Univ, Changsha, Hunan, Peoples R China. Zhang, Xiaolong, Penn State Univ, University Pk, PA 16802 USA.", "countries": "USA;China", "abstract": "When analyzing a visualized network, users need to explore different sections of the network to gain insight. However, effective exploration of large networks is often a challenge. While various tools are available for users to explore the global and local features of a network, these tools usually require significant interaction activities, such as repetitive navigation actions to follow network nodes and edges. In this paper, we propose a structure-based suggestive exploration approach to support effective exploration of large networks by suggesting appropriate structures upon user request. Encoding nodes with vectorized representations by transforming information of surrounding structures of nodes into a high dimensional space, our approach can identify similar structures within a large network, enable user interaction with multiple similar structures simultaneously, and guide the exploration of unexplored structures. We develop a web-based visual exploration system to incorporate this suggestive exploration approach and compare performances of our approach under different vectorizing methods and networks. We also present the usability and effectiveness of our approach through a controlled user study with two datasets.", "keywords": "Large Network Exploration,Structure-Based Exploration,Suggestive Exploration", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865139", "refList": ["10.1109/tvcg.2007.70582", "10.1109/tvcg.2006.120", "10.1109/mc.2013.242", "10.1109/tvcg.2009.108", "10.1145/2939672.2939754", "10.1016/j.comnet.2011.08.019", "10.1145/2702123.2702476", "10.1109/tvcg.2017.2744938", "10.1007/s00371-013-0892-3", "10.1137/1.9781611974973.67", "10.1109/vast.2009.5333893", "10.1109/tst.2013.6509098", "10.1109/35021bigcomp.2015.7072812", "10.1145/2909132.2909246", "10.1111/j.1467-8659.2011.01957.x", "10.1111/j.1467-8659.2011.01898.x", "10.1177/1473871612455749", "10.1109/tvcg.2013.167", "10.1145/3097983.3098061", "10.1007/978-1-4613-0303-9\\_28", "10.1109/vast.2014.7042485", "10.1109/tmm.2016.2614220", "10.1145/1376616.1376675", "10.1145/2623330.2623732", "10.14778/1920841.1920887", "10.1109/tvcg.2017.2745219", "10.1109/tvcg.2017.2743858", "10.1109/tvcg.2008.151", "10.1145/1150402.1150479", "10.1093/bioinformatics/bth436", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2016.2598958", "10.1111/cgf.12883", "10.1145/1556262.1556300", "10.1109/icdm.2012.159", "10.1145/2470654.2466444", "10.1109/tvcg.2013.109", "10.1109/infvis.2004.1", "10.1109/icdmw.2008.99", "10.1002/aris.1440370106", "10.1007/978-3-319-05813-9\\_11", "10.1371/journal.pone.0098679", "10.1109/tvcg.2006.106", "10.1111/j.1467-8659.2011.01935.x", "10.1111/cgf.12397", "10.1111/cgf.13184", "10.1111/cgf.12642", "10.1109/tvcg.2016.2598831", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934396", "title": "A Deep Generative Model for Graph Layout", "year": "2019", "conferenceName": "InfoVis", "authors": "Oh-Hyun Kwon;Kwan-Liu Ma", "citationCount": "4", "affiliation": "Kwon, OH (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Kwon, Oh-Hyun; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "Different layouts can characterize different aspects of the same graph. Finding a \u201cgood\u201d layout of a graph is thus an important task for graph visualization. In practice, users often visualize a graph in multiple layouts by using different methods and varying parameter settings until they find a layout that best suits the purpose of the visualization. However, this trial-and-error process is often haphazard and time-consuming. To provide users with an intuitive way to navigate the layout design space, we present a technique to systematically visualize a graph in diverse layouts using deep generative models. We design an encoder-decoder architecture to learn a model from a collection of example layouts, where the encoder represents training examples in a latent space and the decoder produces layouts from the latent space. In particular, we train the model to construct a two-dimensional latent space for users to easily explore and generate various layouts. We demonstrate our approach through quantitative and qualitative evaluations of the generated layouts. The results of our evaluations show that our model is capable of learning and generalizing abstract concepts of graph layouts, not just memorizing the training examples. In summary, this paper presents a fundamentally new approach to graph visualization where a machine learning model learns to visualize a graph from examples without manually-defined heuristics.", "keywords": "Graph,network,visualization,layout,machine learning,deep learning,neural network,generative model,autoencoder", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934396", "refList": ["10.1103/physreve.74.036104", "10.1145/234535.234538", "10.2307/2412323", "10.1109/tvcg.2007.70580", "10.7155/jgaa.00405", "10.1177/1473871612455749", "10.1109/tvcg.2011.185", "10.1073/pnas.122653799", "10.1007/3-540-58950-3", "10.1145/2897824.2925974", "10.1007/3-540-44541-2\\_17", "10.1007/bf00410640", "10.1021/acscentsci.7b00572", "10.1007/s10208-011-9093-5", "10.1109/tvcg.2015.2467451", "10.1016/0020-0190(89)90102-6", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13187", "10.1214/aoms/1177729586", "10.1109/tvcg.2014.2346277", "10.1109/tvcg.2017.2743858", "10.1007/978-3-662-44043-8\\_3", "10.1038/30918", "10.1109/mcg.2018.2881501", "10.1016/j.camwa.2004.08.015", "10.1109/tvcg.2010.269", "10.1006/s1045-926x(02)00016-2", "10.1016/0925-7721(94)00014-x", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1103/physrevx.4.011047", "10.1371/journal.pone.0098679", "10.1145/2487788.2488173", "10.1007/978-3-030-01418-6\\_41", "10.1007/978-3-030-04414-5\\_12", "10.1109/tvcg.2018.2865139", "10.1142/s0219525903001067", "10.7155/jgaa.00051"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030367", "title": "Lyra 2: Designing Interactive Visualizations by Demonstration", "year": "2020", "conferenceName": "InfoVis", "authors": "Jonathan Zong;Dhiraj Barnwal;Rupayan Neogy;Arvind Satyanarayan", "citationCount": "0", "affiliation": "Zong, J (Corresponding Author), MIT, Cambridge, MA 02139 USA. Zong, Jonathan; Neogy, Rupayan; Satyanarayan, Arvind, MIT, Cambridge, MA 02139 USA. Barnwal, Dhiraj, Indian Inst Technol Kharagpur, Kharagpur, W Bengal, India.", "countries": "India;USA", "abstract": "Recent graphical interfaces offer direct manipulation mechanisms for authoring visualizations, but are largely restricted to static output. To author interactive visualizations, users must instead turn to textual specification, but such approaches impose a higher technical burden. To bridge this gap, we introduce Lyra 2, a system that extends a prior visualization design environment with novel methods for authoring interaction techniques by demonstration. Users perform an interaction (e.g., button clicks, drags, or key presses) directly on the visualization they are editing. The system interprets this performance using a set of heuristics and enumerates suggestions of possible interaction designs. These heuristics account for the properties of the interaction (e.g., target and event type) as well as the visualization (e.g., mark and scale types, and multiple views). Interaction design suggestions are displayed as thumbnails; users can preview and test these suggestions, iteratively refine them through additional demonstrations, and finally apply and customize them via property inspectors. We evaluate our approach through a gallery of diverse examples, and evaluate its usability through a first-use study and via an analysis of its cognitive dimensions. We find that, in Lyra 2, interaction design by demonstration enables users to rapidly express a wide range of interactive visualizations.", "keywords": "Direct manipulation,interactive visualization,interaction design by demonstration", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030367", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030467", "title": "PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch;Lingyun Yu;Peiran Ren;Thomas Ertl;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Knittel, Johannes; Koch, Steffen; Ertl, Thomas, Univ Stuttgart, VIS VISUS, Stuttgart, Germany. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Dept Comp Sci \\& Software Engn, Suzhou, Peoples R China. Ren, Peiran, Alibaba Grp, Hangzhou, Peoples R China.", "countries": "Germany;China", "abstract": "Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.", "keywords": "Storyline visualization,reinforcement learning,mixed-initiative design", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030467", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030459", "title": "Scalability of Network Visualisation from a Cognitive Load Perspective", "year": "2020", "conferenceName": "InfoVis", "authors": "Vahan Yoghourdjian;Yalong Yang;Tim Dwyer;Lawrence Lee;Michael Wybrow;Kim Marriott", "citationCount": "0", "affiliation": "Yoghourdjian, V (Corresponding Author), Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Yoghourdjian, Vahan; Yang, Yalong; Dwyer, Tim; Wybrow, Michael; Marriott, Kim, Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Lawrence, Lee, Monash Univ, Fac Business \\& Econ, Melbourne, Vic, Australia. Yang, Yalong, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA.", "countries": "USA;Australia", "abstract": "Node-link diagrams are widely used to visualise networks. However, even the best network layout algorithms ultimately result in \u2018hairball\u2019 visualisations when the graph reaches a certain degree of complexity, requiring simplification through aggregation or interaction (such as filtering) to remain usable. Until now, there has been little data to indicate at what level of complexity node-link diagrams become ineffective or how visual complexity affects cognitive load. To this end, we conducted a controlled study to understand workload limits for a task that requires a detailed understanding of the network topology-finding the shortest path between two nodes. We tested performance on graphs with 25 to 175 nodes with varying density. We collected performance measures (accuracy and response time), subjective feedback, and physiological measures (EEG, pupil dilation, and heart rate variability). To the best of our knowledge this is the first network visualisation study to include physiological measures. Our results show that people have significant difficulty finding the shortest path in high density node-link diagrams with more than 50 nodes and even low density graphs with more than 100 nodes. From our collected EEG data we observe functional differences in brain activity between hard and easy tasks. We found that cognitive load increased up to certain level of difficulty after which it decreased, likely because participants had given up. We also explored the effects of global network layout features such as size or number of crossings, and features of the shortest path such as length or straightness on task difficulty. We found that global features generally had a greater impact than those of the shortest path.", "keywords": "Data Visualisation,Network Visualisation,Cognitive Load,EEG", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030459", "refList": ["10.1109/tvcg.2019.2934396", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2016.2570755", "10.1007/s00371-013-0892-3", "10.1007/b98835", "10.1109/isda.2014.7066252", "10.1109/tvcg.2015.2467251", "10.1177/1473871612455749", "10.1109/tvcg.2012.299", "10.1007/3-540-58950-3", "10.1111/cgf.12878", "10.1109/mcse.2007.55", "10.1109/tvcg.2012.238", "10.1145/264645.264657", "10.1109/tvcg.2015.2467451", "10.1109/tvcg.2013.151", "10.1016/0020-0190(89)90102-6", "10.1109/tvcg.2019.2934307", "10.1109/tvcg.2015.2468151", "10.1109/tvcg.2017.2745919", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13440", "10.1109/tvcg.2011.220", "10.1111/cgf.13187", "10.1109/t-c.1969.222678", "10.1109/tvcg.2017.2743858", "10.1126/science.290.5500.2319", "10.1109/cahpc.2018.8645912", "10.1109/tvcg.2015.2465151", "10.1109/tvcg.2016.2598958", "10.1109/tvcg.2017.2751473", "10.1002/spe.4380211102", "10.1109/tpds.2018.2869805", "10.1016/j.jpdc.2019.04.008", "10.1109/pacificvis.2017.8031574", "10.1006/s1045-926x(02)00016-2", "10.1109/tvcg.2017.2674999", "10.1145/2872427.2883041", "10.1145/3292500.3330989", "10.1109/tvcg.2017.2744878", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1109/sbac-pad.2018.00060", "10.1007/978-3-662-45803-7\\_27", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1111/j.1469-1809.1936.tb02137.x", "10.1007/bf02289565", "10.1109/tvcg.2017.2689016", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.4722", "year": "2020", "title": "A Study of Mental Maps in Immersive Network Visualization", "conferenceName": "PacificVis", "authors": "Joseph Kotlarek;Oh{-}Hyun Kwon;Kwan{-}Liu Ma;Peter Eades;Andreas Kerren;Karsten Klein;Falk Schreiber", "citationCount": "0", "affiliation": "Kotlarek, J (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA.\nKotlarek, Joseph; Kwon, Oh-Hyun; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.\nEades, Peter, Univ Sydney, Sydney, NSW, Australia.\nKerren, Andreas, Linnaeus Univ, Vaxjo, Sweden.\nKlein, Karsten; Schreiber, Falk, Univ Konstanz, Constance, Germany.", "countries": "Sweden;Germany;USA;Australia", "abstract": "The visualization of a network influences the quality of the mental map that the viewer develops to understand the network. In this study, we investigate the effects of a 3D immersive visualization environment compared to a traditional 2D desktop environment on the comprehension of a network's structure. We compare the two visualization environments using three tasks-interpreting network structure, memorizing a set of nodes, and identifying the structural changes-commonly used for evaluating the quality of a mental map in network visualization. The results show that participants were able to interpret network structure more accurately when viewing the network in an immersive environment, particularly for larger networks. However, we found that 2D visualizations performed better than immersive visualization for tasks that required spatial memory.", "keywords": "Human-centered computing; Visualization; Visualization techniques; Graph drawings; Human-centered computing; Visualization; Empirical studies in visualization", "link": "https://doi.org/10.1109/PacificVis48177.2020.4722", "refList": ["10.1103/physreve.74.036104", "10.1109/tvcg.2019.2934396", "10.1007/978-3-540-87730-1\\_9", "10.1117/12.2005484", "10.1177/1473871612455749", "10.1109/pacificvis.2017.8031577", "10.1007/978-3-319-73207-7", "10.1109/38.888006", "10.1109/2945.841119", "10.1109/mc.2005.297", "10.1007/s10055-018-0346-3", "10.1109/tvcg.2016.2599107", "10.1109/icsmc.1992.271688", "10.1038/30918", "10.1006/jvlc.1995.1010", "10.1089/109493101300117938", "10.1109/vrais.1998.658488", "10.1109/pacificvis.2015.7156357", "10.1109/tvcg.2010.78", "10.1109/tvcg.2016.2520921", "10.1007/978-3-030-01388-22", "10.1145/229459.229467", "10.1145/1056808.1056875", "10.1109/tvcg.2017.2744079", "10.1109/bdva.2015.7314293", "10.1086/jar.33.4.3629752", "10.1016/j.ijhcs.2013.08.004"], "wos": 1, "children": [], "len": 1}], "len": 11}, {"doi": "10.1109/tvcg.2020.3030440", "title": "Context-aware Sampling of Large Networks via Graph Representation Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Zhiguang Zhou;Chen Shi;Xilong Shen;Lihong Cai;Haoxuan Wang;Yuhua Liu;Ying Zhao;Wei Chen", "citationCount": "0", "affiliation": "Zhao, Y (Corresponding Author), Cent South Univ, Changsha, Peoples R China. Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Zhou, Zhiguang; Shi, Chen; Shen, Xilong; Cai, Lihong; Wang, Haoxuan; Liu, Yuhua, Zhejiang Univ Finance \\& Econ, Sch Informat, Hangzhou, Peoples R China. Zhao, Ying, Cent South Univ, Changsha, Peoples R China. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China.", "countries": "China", "abstract": "Numerous sampling strategies have been proposed to simplify large-scale networks for highly readable visualizations. It is of great challenge to preserve contextual structures formed by nodes and edges with tight relationships in a sampled graph, because they are easily overlooked during the process of sampling due to their irregular distribution and immunity to scale. In this paper, a new graph sampling method is proposed oriented to the preservation of contextual structures. We first utilize a graph representation learning (GRL) model to transform nodes into vectors so that the contextual structures in a network can be effectively extracted and organized. Then, we propose a multi-objective blue noise sampling model to select a subset of nodes in the vectorized space to preserve contextual structures with the retention of relative data and cluster densities in addition to those features of significance, such as bridging nodes and graph connections. We also design a set of visual interfaces enabling users to interactively conduct context-aware sampling, visually compare results with various sampling strategies, and deeply explore large networks. Case studies and quantitative comparisons based on real-world datasets have demonstrated the effectiveness of our method in the abstraction and exploration of large networks.", "keywords": "Graph sampling,Graph representation learning,Blue noise sampling,Graph evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030440", "refList": ["10.1145/2491159.2491168", "10.1016/j.physa.2015.04.035", "10.1145/1830252.1830274", "10.1109/icdmw.2007.91", "10.1002/net.21834", "10.1109/tvcg.2018.2864503", "10.1016/j.cag.2018.01.010", "10.1109/icc.2016.7511156", "10.1111/cgf.13444", "10.1145/956750.956831", "10.1145/364099.364331", "10.1007/s00180-016-0663-5", "10.1109/tvcg.2013.223", "10.1007/s12650-018-0530-2", "10.1103/physreve.73.016102", "10.1109/access.2018.2870684", "10.1007/978-3-319-06793-3\\_1", "10.2312/vissym/eurovis05/239-246", "10.1016/j.ins.2015.02.014", "10.1145/2339530.2339723", "10.1109/icde.2015.7113345", "10.1109/tvcg.2011.233", "10.14778/2809974.2809980", "10.1109/glocom.2015.7417471", "10.1145/2578153.2578175", "10.1214/aoms/1177705148", "10.1109/tvcg.2008.130", "10.14232/actacyb.20.1.2011.6", "10.1504/ijitm.2019.099809", "10.1109/tvcg.2018.2865020", "10.1145/956750", "10.1002/cpe.4330060203", "10.1145/1150402.1150479", "10.1103/physreve.72.036118", "10.1109/tvcg.2017.2744098", "10.1145/2020408.2020512", "10.1142/s0129183114400075", "10.1109/jsac.2011.111005", "10.1016/j.camwa.2011.11.057", "10.1145/2470654.2466444", "10.1109/tvcg.2017.2674999", "10.1214/aos/1013203451", "10.1109/icdcsw.2011.34", "10.1016/j.physa.2013.11.015", "10.1145/1081870.1081893", "10.1109/tnet.2008.2001730", "10.1109/access.2016.2633485", "10.1145/1879141.1879192", "10.1371/journal.pone.0098679", "10.1126/science.220.4598.671", "10.1109/pacificvis.2015.7156355", "10.1088/1475-7516/2011/08/011", "10.1007/978-3-319-27261-0\\_41", "10.1111/cgf.13410", "10.1109/tvcg.2018.2865139", "10.1109/tvcg.2016.2598831", "10.1016/j.physa.2014.06.065"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030447", "title": "DRGraph: An Efficient Graph Layout Algorithm for Large-scale Graphs by Dimensionality Reduction", "year": "2020", "conferenceName": "InfoVis", "authors": "Minfeng Zhu;Wei Chen;Yuanzhe Hu;Yuxuan Hou;Liangjun Liu;Kaiyuan Zhang", "citationCount": "0", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Zhu, Minfeng; Chen, Wei; Hu, Yuanzhe; Hou, Yuxuan; Liu, Liangjun; Zhang, Kaiyuan, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China.", "countries": "China", "abstract": "Efficient layout of large-scale graphs remains a challenging problem: the force-directed and dimensionality reduction-based methods suffer from high overhead for graph distance and gradient computation. In this paper, we present a new graph layout algorithm, called DRGraph, that enhances the nonlinear dimensionality reduction process with three schemes: approximating graph distances by means of a sparse distance matrix, estimating the gradient by using the negative sampling technique, and accelerating the optimization process through a multi-level layout scheme. DRGraph achieves a linear complexity for the computation and memory consumption, and scales up to large-scale graphs with millions of nodes. Experimental results and comparisons with state-of-the-art graph layout methods demonstrate that DRGraph can generate visually comparable layouts with a faster running time and a lower memory requirement.", "keywords": "graph visualization,graph layout,dimensionality reduction,force-directed layout", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030447", "refList": ["10.1109/tvcg.2007.70582", "10.1371/journal.pone.0136497", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/3018661.3018731", "10.1109/34.491619", "10.1145/2939672.2939754", "10.1145/3269206.3271788", "10.1016/j.comnet.2011.08.019", "10.1007/s11263-011-0442-2", "10.1016/s0020-0190(98)00108-2", "10.1109/tvcg.2018.2865151", "10.1007/978-3-319-61188-4\\_2", "10.1177/1473871612455749", "10.1177/1473871616666394", "10.1109/tvcg.2015.2467035", "10.1186/1471-2105-10-375", "10.1145/3097983.3098061", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1109/tvcg.2015.2467451", "10.1109/2945.841119", "10.1016/j.swevo.2015.10.002", "10.1016/0020-0190(89)90102-6", "10.1109/infvis.2003.1249009", "10.1109/tvcg.2017.2745919", "10.1111/cgf.13187", "10.1111/cgf.13440", "10.1109/tvcg.2012.245", "10.1109/tvcg.2017.2743858", "10.1177/1473871618821740", "10.1186/s12859-015-0585-1", "10.1002/nav.3800020109", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1006/s1045-926x(02)00016-2", "10.1109/cvpr.2012.6247667", "10.1023/b:jogo.0000042115.44455.f3", "10.1109/pacificvis.2017.8031607", "10.1002/nav.3800030404", "10.1109/cvpr.2008.4587500", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1090/s0002-9904-1920-03322-7", "10.1109/iv.2013.3", "10.1145/568522.568523", "10.1109/tvcg.2006.156", "10.1109/tvcg.2012.236", "10.1109/tvcg.2018.2865139", "10.1145/3219819.3220025", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030393", "title": "Exemplar-based Layout Fine-tuning for Node-link Diagrams", "year": "2020", "conferenceName": "InfoVis", "authors": "Jiacheng Pan;Wei Chen;Xiaodong Zhao;Shuyue Zhou;Wei Zeng;Minfeng Zhu;Jian Chen;Siwei Fu;Yingcai Wu", "citationCount": "1", "affiliation": "Chen, W; Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Lab, Hangzhou, Peoples R China. Pan, Jiacheng; Chen, Wei; Zhao, Xiaodong; Zhou, Shuyue; Zhu, Minfeng; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Fu, Siwei; Wu, Yingcai, Zhejiang Lab, Hangzhou, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Jian, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA;China", "abstract": "We design and evaluate a novel layout fine-tuning technique for node-link diagrams that facilitates exemplar-based adjustment of a group of substructures in batching mode. The key idea is to transfer user modifications on a local substructure to other substructures in the entire graph that are topologically similar to the exemplar. We first precompute a canonical representation for each substructure with node embedding techniques and then use it for on-the-fly substructure retrieval. We design and develop a light-weight interactive system to enable intuitive adjustment, modification transfer, and visual graph exploration. We also report some results of quantitative comparisons, three case studies, and a within-participant user study.", "keywords": "Node-link diagram,graph layout,graph visualization,user interactions", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030393", "refList": ["10.1109/tvcg.2007.70582", "10.1371/journal.pone.0136497", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/3018661.3018731", "10.1109/34.491619", "10.1145/2939672.2939754", "10.1145/3269206.3271788", "10.1016/j.comnet.2011.08.019", "10.1007/s11263-011-0442-2", "10.1016/s0020-0190(98)00108-2", "10.1109/tvcg.2018.2865151", "10.1177/1473871612455749", "10.1177/1473871616666394", "10.1109/tvcg.2015.2467035", "10.1186/1471-2105-10-375", "10.1145/3097983.3098061", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1109/tvcg.2015.2467451", "10.1016/j.swevo.2015.10.002", "10.1016/0020-0190(89)90102-6", "10.1109/infvis.2003.1249009", "10.1109/tvcg.2017.2745919", "10.1111/cgf.13187", "10.1111/cgf.13440", "10.1109/tvcg.2012.245", "10.1109/tvcg.2017.2743858", "10.1177/1473871618821740", "10.1186/s12859-015-0585-1", "10.1002/nav.3800020109", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1006/s1045-926x(02)00016-2", "10.1109/cvpr.2012.6247667", "10.1023/b:jogo.0000042115.44455.f3", "10.1109/pacificvis.2017.8031607", "10.1002/nav.3800030404", "10.1109/infvis.2004.1", "10.1109/cvpr.2008.4587500", "10.1109/pacificvis.2011.5742389", "10.1140/epjb/e2011-10979-2", "10.1371/journal.pone.0098679", "10.1090/s0002-9904-1920-03322-7", "10.1109/iv.2013.3", "10.1145/568522.568523", "10.1109/tvcg.2006.156", "10.1109/tvcg.2012.236", "10.1109/tvcg.2018.2865139", "10.1145/3219819.3220025", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}], "len": 19}, {"doi": "10.1109/tvcg.2019.2934396", "title": "A Deep Generative Model for Graph Layout", "year": "2019", "conferenceName": "InfoVis", "authors": "Oh-Hyun Kwon;Kwan-Liu Ma", "citationCount": "4", "affiliation": "Kwon, OH (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Kwon, Oh-Hyun; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "Different layouts can characterize different aspects of the same graph. Finding a \u201cgood\u201d layout of a graph is thus an important task for graph visualization. In practice, users often visualize a graph in multiple layouts by using different methods and varying parameter settings until they find a layout that best suits the purpose of the visualization. However, this trial-and-error process is often haphazard and time-consuming. To provide users with an intuitive way to navigate the layout design space, we present a technique to systematically visualize a graph in diverse layouts using deep generative models. We design an encoder-decoder architecture to learn a model from a collection of example layouts, where the encoder represents training examples in a latent space and the decoder produces layouts from the latent space. In particular, we train the model to construct a two-dimensional latent space for users to easily explore and generate various layouts. We demonstrate our approach through quantitative and qualitative evaluations of the generated layouts. The results of our evaluations show that our model is capable of learning and generalizing abstract concepts of graph layouts, not just memorizing the training examples. In summary, this paper presents a fundamentally new approach to graph visualization where a machine learning model learns to visualize a graph from examples without manually-defined heuristics.", "keywords": "Graph,network,visualization,layout,machine learning,deep learning,neural network,generative model,autoencoder", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934396", "refList": ["10.1103/physreve.74.036104", "10.1145/234535.234538", "10.2307/2412323", "10.1109/tvcg.2007.70580", "10.7155/jgaa.00405", "10.1177/1473871612455749", "10.1109/tvcg.2011.185", "10.1073/pnas.122653799", "10.1007/3-540-58950-3", "10.1145/2897824.2925974", "10.1007/3-540-44541-2\\_17", "10.1007/bf00410640", "10.1021/acscentsci.7b00572", "10.1007/s10208-011-9093-5", "10.1109/tvcg.2015.2467451", "10.1016/0020-0190(89)90102-6", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13187", "10.1214/aoms/1177729586", "10.1109/tvcg.2014.2346277", "10.1109/tvcg.2017.2743858", "10.1007/978-3-662-44043-8\\_3", "10.1038/30918", "10.1109/mcg.2018.2881501", "10.1016/j.camwa.2004.08.015", "10.1109/tvcg.2010.269", "10.1006/s1045-926x(02)00016-2", "10.1016/0925-7721(94)00014-x", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1103/physrevx.4.011047", "10.1371/journal.pone.0098679", "10.1145/2487788.2488173", "10.1007/978-3-030-01418-6\\_41", "10.1007/978-3-030-04414-5\\_12", "10.1109/tvcg.2018.2865139", "10.1142/s0219525903001067", "10.7155/jgaa.00051"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030367", "title": "Lyra 2: Designing Interactive Visualizations by Demonstration", "year": "2020", "conferenceName": "InfoVis", "authors": "Jonathan Zong;Dhiraj Barnwal;Rupayan Neogy;Arvind Satyanarayan", "citationCount": "0", "affiliation": "Zong, J (Corresponding Author), MIT, Cambridge, MA 02139 USA. Zong, Jonathan; Neogy, Rupayan; Satyanarayan, Arvind, MIT, Cambridge, MA 02139 USA. Barnwal, Dhiraj, Indian Inst Technol Kharagpur, Kharagpur, W Bengal, India.", "countries": "India;USA", "abstract": "Recent graphical interfaces offer direct manipulation mechanisms for authoring visualizations, but are largely restricted to static output. To author interactive visualizations, users must instead turn to textual specification, but such approaches impose a higher technical burden. To bridge this gap, we introduce Lyra 2, a system that extends a prior visualization design environment with novel methods for authoring interaction techniques by demonstration. Users perform an interaction (e.g., button clicks, drags, or key presses) directly on the visualization they are editing. The system interprets this performance using a set of heuristics and enumerates suggestions of possible interaction designs. These heuristics account for the properties of the interaction (e.g., target and event type) as well as the visualization (e.g., mark and scale types, and multiple views). Interaction design suggestions are displayed as thumbnails; users can preview and test these suggestions, iteratively refine them through additional demonstrations, and finally apply and customize them via property inspectors. We evaluate our approach through a gallery of diverse examples, and evaluate its usability through a first-use study and via an analysis of its cognitive dimensions. We find that, in Lyra 2, interaction design by demonstration enables users to rapidly express a wide range of interactive visualizations.", "keywords": "Direct manipulation,interactive visualization,interaction design by demonstration", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030367", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030467", "title": "PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch;Lingyun Yu;Peiran Ren;Thomas Ertl;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Knittel, Johannes; Koch, Steffen; Ertl, Thomas, Univ Stuttgart, VIS VISUS, Stuttgart, Germany. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Dept Comp Sci \\& Software Engn, Suzhou, Peoples R China. Ren, Peiran, Alibaba Grp, Hangzhou, Peoples R China.", "countries": "Germany;China", "abstract": "Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.", "keywords": "Storyline visualization,reinforcement learning,mixed-initiative design", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030467", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030459", "title": "Scalability of Network Visualisation from a Cognitive Load Perspective", "year": "2020", "conferenceName": "InfoVis", "authors": "Vahan Yoghourdjian;Yalong Yang;Tim Dwyer;Lawrence Lee;Michael Wybrow;Kim Marriott", "citationCount": "0", "affiliation": "Yoghourdjian, V (Corresponding Author), Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Yoghourdjian, Vahan; Yang, Yalong; Dwyer, Tim; Wybrow, Michael; Marriott, Kim, Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Lawrence, Lee, Monash Univ, Fac Business \\& Econ, Melbourne, Vic, Australia. Yang, Yalong, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA.", "countries": "USA;Australia", "abstract": "Node-link diagrams are widely used to visualise networks. However, even the best network layout algorithms ultimately result in \u2018hairball\u2019 visualisations when the graph reaches a certain degree of complexity, requiring simplification through aggregation or interaction (such as filtering) to remain usable. Until now, there has been little data to indicate at what level of complexity node-link diagrams become ineffective or how visual complexity affects cognitive load. To this end, we conducted a controlled study to understand workload limits for a task that requires a detailed understanding of the network topology-finding the shortest path between two nodes. We tested performance on graphs with 25 to 175 nodes with varying density. We collected performance measures (accuracy and response time), subjective feedback, and physiological measures (EEG, pupil dilation, and heart rate variability). To the best of our knowledge this is the first network visualisation study to include physiological measures. Our results show that people have significant difficulty finding the shortest path in high density node-link diagrams with more than 50 nodes and even low density graphs with more than 100 nodes. From our collected EEG data we observe functional differences in brain activity between hard and easy tasks. We found that cognitive load increased up to certain level of difficulty after which it decreased, likely because participants had given up. We also explored the effects of global network layout features such as size or number of crossings, and features of the shortest path such as length or straightness on task difficulty. We found that global features generally had a greater impact than those of the shortest path.", "keywords": "Data Visualisation,Network Visualisation,Cognitive Load,EEG", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030459", "refList": ["10.1109/tvcg.2019.2934396", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2016.2570755", "10.1007/s00371-013-0892-3", "10.1007/b98835", "10.1109/isda.2014.7066252", "10.1109/tvcg.2015.2467251", "10.1177/1473871612455749", "10.1109/tvcg.2012.299", "10.1007/3-540-58950-3", "10.1111/cgf.12878", "10.1109/mcse.2007.55", "10.1109/tvcg.2012.238", "10.1145/264645.264657", "10.1109/tvcg.2015.2467451", "10.1109/tvcg.2013.151", "10.1016/0020-0190(89)90102-6", "10.1109/tvcg.2019.2934307", "10.1109/tvcg.2015.2468151", "10.1109/tvcg.2017.2745919", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13440", "10.1109/tvcg.2011.220", "10.1111/cgf.13187", "10.1109/t-c.1969.222678", "10.1109/tvcg.2017.2743858", "10.1126/science.290.5500.2319", "10.1109/cahpc.2018.8645912", "10.1109/tvcg.2015.2465151", "10.1109/tvcg.2016.2598958", "10.1109/tvcg.2017.2751473", "10.1002/spe.4380211102", "10.1109/tpds.2018.2869805", "10.1016/j.jpdc.2019.04.008", "10.1109/pacificvis.2017.8031574", "10.1006/s1045-926x(02)00016-2", "10.1109/tvcg.2017.2674999", "10.1145/2872427.2883041", "10.1145/3292500.3330989", "10.1109/tvcg.2017.2744878", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1109/sbac-pad.2018.00060", "10.1007/978-3-662-45803-7\\_27", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1111/j.1469-1809.1936.tb02137.x", "10.1007/bf02289565", "10.1109/tvcg.2017.2689016", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.4722", "year": "2020", "title": "A Study of Mental Maps in Immersive Network Visualization", "conferenceName": "PacificVis", "authors": "Joseph Kotlarek;Oh{-}Hyun Kwon;Kwan{-}Liu Ma;Peter Eades;Andreas Kerren;Karsten Klein;Falk Schreiber", "citationCount": "0", "affiliation": "Kotlarek, J (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA.\nKotlarek, Joseph; Kwon, Oh-Hyun; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.\nEades, Peter, Univ Sydney, Sydney, NSW, Australia.\nKerren, Andreas, Linnaeus Univ, Vaxjo, Sweden.\nKlein, Karsten; Schreiber, Falk, Univ Konstanz, Constance, Germany.", "countries": "Sweden;Germany;USA;Australia", "abstract": "The visualization of a network influences the quality of the mental map that the viewer develops to understand the network. In this study, we investigate the effects of a 3D immersive visualization environment compared to a traditional 2D desktop environment on the comprehension of a network's structure. We compare the two visualization environments using three tasks-interpreting network structure, memorizing a set of nodes, and identifying the structural changes-commonly used for evaluating the quality of a mental map in network visualization. The results show that participants were able to interpret network structure more accurately when viewing the network in an immersive environment, particularly for larger networks. However, we found that 2D visualizations performed better than immersive visualization for tasks that required spatial memory.", "keywords": "Human-centered computing; Visualization; Visualization techniques; Graph drawings; Human-centered computing; Visualization; Empirical studies in visualization", "link": "https://doi.org/10.1109/PacificVis48177.2020.4722", "refList": ["10.1103/physreve.74.036104", "10.1109/tvcg.2019.2934396", "10.1007/978-3-540-87730-1\\_9", "10.1117/12.2005484", "10.1177/1473871612455749", "10.1109/pacificvis.2017.8031577", "10.1007/978-3-319-73207-7", "10.1109/38.888006", "10.1109/2945.841119", "10.1109/mc.2005.297", "10.1007/s10055-018-0346-3", "10.1109/tvcg.2016.2599107", "10.1109/icsmc.1992.271688", "10.1038/30918", "10.1006/jvlc.1995.1010", "10.1089/109493101300117938", "10.1109/vrais.1998.658488", "10.1109/pacificvis.2015.7156357", "10.1109/tvcg.2010.78", "10.1109/tvcg.2016.2520921", "10.1007/978-3-030-01388-22", "10.1145/229459.229467", "10.1145/1056808.1056875", "10.1109/tvcg.2017.2744079", "10.1109/bdva.2015.7314293", "10.1086/jar.33.4.3629752", "10.1016/j.ijhcs.2013.08.004"], "wos": 1, "children": [], "len": 1}], "len": 11}, {"doi": "10.1109/tvcg.2019.2934798", "title": "DeepDrawing: A Deep Learning Approach to Graph Drawing", "year": "2019", "conferenceName": "InfoVis", "authors": "Yong Wang;Zhihua Jin;Qianwen Wang;Weiwei Cui;Tengfei Ma;Huamin Qu", "citationCount": "4", "affiliation": "Wang, Y (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Yong; Jin, Zhihua; Wang, Qianwen; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Jin, Zhihua, Zhejiang Univ, Hangzhou, Peoples R China. Cui, Weiwei, Microsoft Res Asia, Beijing, Peoples R China. Ma, Tengfei, IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.", "countries": "USA;China", "abstract": "Node-link diagrams are widely used to facilitate network explorations. However, when using a graph drawing technique to visualize networks, users often need to tune different algorithm-specific parameters iteratively by comparing the corresponding drawing results in order to achieve a desired visual effect. This trial and error process is often tedious and time-consuming, especially for non-expert users. Inspired by the powerful data modelling and prediction capabilities of deep learning techniques, we explore the possibility of applying deep learning techniques to graph drawing. Specifically, we propose using a graph-LSTM-based approach to directly map network structures to graph drawings. Given a set of layout examples as the training dataset, we train the proposed graph-LSTM-based model to capture their layout characteristics. Then, the trained model is used to generate graph drawings in a similar style for new networks. We evaluated the proposed approach on two special types of layouts (i.e., grid layouts and star layouts) and two general types of layouts (i.e., ForceAtlas2 and PivotMDS) in both qualitative and quantitative ways. The results provide support for the effectiveness of our approach. We also conducted a time cost assessment on the drawings of small graphs with 20 to 50 nodes. We further report the lessons we learned and discuss the limitations and future work.", "keywords": "Graph Drawing,Deep Learning,LSTM,Procrustes Analysis", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934798", "refList": ["10.1057/ivs.2009.10", "10.1145/2939672.2939754", "10.1016/s0020-0190(98)00108-2", "10.5555/3157382.3157527", "10.1145/2939672.2939753", "10.1111/j.1467-8659.2011.01898.x", "10.1016/j.visinf.2018.12.006", "10.1177/1473871612455749", "10.1111/j.2517-6161.1991.tb01825.x", "10.1109/cvpr.2017.576", "10.1631/fitee.1700808", "10.1109/2945.841119", "10.1109/72.485670", "10.1007/3-540-58950-3\\_393", "10.1007/978-3-642-36763-2", "10.1162/089976600300015015", "10.1016/0020-0190(89)90102-6", "10.1145/2623330.2623732", "10.3402/qhw.v6i2.5918", "10.1007/978-3-642-36763-2\\_48", "10.1111/cgf.13187", "10.1038/nature14539", "10.1016/s0020-0255(02)00191-3", "10.1109/tvcg.2015.2467691", "10.1109/72.279181", "10.1109/tvcg.2017.2743858", "10.1007/978-3-662-44043-8\\_3", "10.1002/spe.4380211102", "10.1007/3-540-44541-2\\_18", "10.1162/neco.1997.9.8.1735", "10.1103/physreve.78.046110", "10.1006/s1045-926x(02)00016-2", "10.1109/pacificvis.2012.6183571", "10.1016/0925-7721(94)00014-x", "10.1109/infvis.2004.1", "10.1145/1830483.1830503", "10.1112/plms/s3-13.1.743", "10.1371/journal.pone.0098679", "10.1007/978-3-030-04414-5\\_12", "10.1147/jrd.2015.2411412"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030367", "title": "Lyra 2: Designing Interactive Visualizations by Demonstration", "year": "2020", "conferenceName": "InfoVis", "authors": "Jonathan Zong;Dhiraj Barnwal;Rupayan Neogy;Arvind Satyanarayan", "citationCount": "0", "affiliation": "Zong, J (Corresponding Author), MIT, Cambridge, MA 02139 USA. Zong, Jonathan; Neogy, Rupayan; Satyanarayan, Arvind, MIT, Cambridge, MA 02139 USA. Barnwal, Dhiraj, Indian Inst Technol Kharagpur, Kharagpur, W Bengal, India.", "countries": "India;USA", "abstract": "Recent graphical interfaces offer direct manipulation mechanisms for authoring visualizations, but are largely restricted to static output. To author interactive visualizations, users must instead turn to textual specification, but such approaches impose a higher technical burden. To bridge this gap, we introduce Lyra 2, a system that extends a prior visualization design environment with novel methods for authoring interaction techniques by demonstration. Users perform an interaction (e.g., button clicks, drags, or key presses) directly on the visualization they are editing. The system interprets this performance using a set of heuristics and enumerates suggestions of possible interaction designs. These heuristics account for the properties of the interaction (e.g., target and event type) as well as the visualization (e.g., mark and scale types, and multiple views). Interaction design suggestions are displayed as thumbnails; users can preview and test these suggestions, iteratively refine them through additional demonstrations, and finally apply and customize them via property inspectors. We evaluate our approach through a gallery of diverse examples, and evaluate its usability through a first-use study and via an analysis of its cognitive dimensions. We find that, in Lyra 2, interaction design by demonstration enables users to rapidly express a wide range of interactive visualizations.", "keywords": "Direct manipulation,interactive visualization,interaction design by demonstration", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030367", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030467", "title": "PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch;Lingyun Yu;Peiran Ren;Thomas Ertl;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Knittel, Johannes; Koch, Steffen; Ertl, Thomas, Univ Stuttgart, VIS VISUS, Stuttgart, Germany. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Dept Comp Sci \\& Software Engn, Suzhou, Peoples R China. Ren, Peiran, Alibaba Grp, Hangzhou, Peoples R China.", "countries": "Germany;China", "abstract": "Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.", "keywords": "Storyline visualization,reinforcement learning,mixed-initiative design", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030467", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2020.3030447", "title": "DRGraph: An Efficient Graph Layout Algorithm for Large-scale Graphs by Dimensionality Reduction", "year": "2020", "conferenceName": "InfoVis", "authors": "Minfeng Zhu;Wei Chen;Yuanzhe Hu;Yuxuan Hou;Liangjun Liu;Kaiyuan Zhang", "citationCount": "0", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Zhu, Minfeng; Chen, Wei; Hu, Yuanzhe; Hou, Yuxuan; Liu, Liangjun; Zhang, Kaiyuan, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China.", "countries": "China", "abstract": "Efficient layout of large-scale graphs remains a challenging problem: the force-directed and dimensionality reduction-based methods suffer from high overhead for graph distance and gradient computation. In this paper, we present a new graph layout algorithm, called DRGraph, that enhances the nonlinear dimensionality reduction process with three schemes: approximating graph distances by means of a sparse distance matrix, estimating the gradient by using the negative sampling technique, and accelerating the optimization process through a multi-level layout scheme. DRGraph achieves a linear complexity for the computation and memory consumption, and scales up to large-scale graphs with millions of nodes. Experimental results and comparisons with state-of-the-art graph layout methods demonstrate that DRGraph can generate visually comparable layouts with a faster running time and a lower memory requirement.", "keywords": "graph visualization,graph layout,dimensionality reduction,force-directed layout", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030447", "refList": ["10.1109/tvcg.2007.70582", "10.1371/journal.pone.0136497", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/3018661.3018731", "10.1109/34.491619", "10.1145/2939672.2939754", "10.1145/3269206.3271788", "10.1016/j.comnet.2011.08.019", "10.1007/s11263-011-0442-2", "10.1016/s0020-0190(98)00108-2", "10.1109/tvcg.2018.2865151", "10.1007/978-3-319-61188-4\\_2", "10.1177/1473871612455749", "10.1177/1473871616666394", "10.1109/tvcg.2015.2467035", "10.1186/1471-2105-10-375", "10.1145/3097983.3098061", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1109/tvcg.2015.2467451", "10.1109/2945.841119", "10.1016/j.swevo.2015.10.002", "10.1016/0020-0190(89)90102-6", "10.1109/infvis.2003.1249009", "10.1109/tvcg.2017.2745919", "10.1111/cgf.13187", "10.1111/cgf.13440", "10.1109/tvcg.2012.245", "10.1109/tvcg.2017.2743858", "10.1177/1473871618821740", "10.1186/s12859-015-0585-1", "10.1002/nav.3800020109", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1006/s1045-926x(02)00016-2", "10.1109/cvpr.2012.6247667", "10.1023/b:jogo.0000042115.44455.f3", "10.1109/pacificvis.2017.8031607", "10.1002/nav.3800030404", "10.1109/cvpr.2008.4587500", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1090/s0002-9904-1920-03322-7", "10.1109/iv.2013.3", "10.1145/568522.568523", "10.1109/tvcg.2006.156", "10.1109/tvcg.2012.236", "10.1109/tvcg.2018.2865139", "10.1145/3219819.3220025", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030393", "title": "Exemplar-based Layout Fine-tuning for Node-link Diagrams", "year": "2020", "conferenceName": "InfoVis", "authors": "Jiacheng Pan;Wei Chen;Xiaodong Zhao;Shuyue Zhou;Wei Zeng;Minfeng Zhu;Jian Chen;Siwei Fu;Yingcai Wu", "citationCount": "1", "affiliation": "Chen, W; Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Lab, Hangzhou, Peoples R China. Pan, Jiacheng; Chen, Wei; Zhao, Xiaodong; Zhou, Shuyue; Zhu, Minfeng; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Fu, Siwei; Wu, Yingcai, Zhejiang Lab, Hangzhou, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Jian, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA;China", "abstract": "We design and evaluate a novel layout fine-tuning technique for node-link diagrams that facilitates exemplar-based adjustment of a group of substructures in batching mode. The key idea is to transfer user modifications on a local substructure to other substructures in the entire graph that are topologically similar to the exemplar. We first precompute a canonical representation for each substructure with node embedding techniques and then use it for on-the-fly substructure retrieval. We design and develop a light-weight interactive system to enable intuitive adjustment, modification transfer, and visual graph exploration. We also report some results of quantitative comparisons, three case studies, and a within-participant user study.", "keywords": "Node-link diagram,graph layout,graph visualization,user interactions", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030393", "refList": ["10.1109/tvcg.2007.70582", "10.1371/journal.pone.0136497", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/3018661.3018731", "10.1109/34.491619", "10.1145/2939672.2939754", "10.1145/3269206.3271788", "10.1016/j.comnet.2011.08.019", "10.1007/s11263-011-0442-2", "10.1016/s0020-0190(98)00108-2", "10.1109/tvcg.2018.2865151", "10.1177/1473871612455749", "10.1177/1473871616666394", "10.1109/tvcg.2015.2467035", "10.1186/1471-2105-10-375", "10.1145/3097983.3098061", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1109/tvcg.2015.2467451", "10.1016/j.swevo.2015.10.002", "10.1016/0020-0190(89)90102-6", "10.1109/infvis.2003.1249009", "10.1109/tvcg.2017.2745919", "10.1111/cgf.13187", "10.1111/cgf.13440", "10.1109/tvcg.2012.245", "10.1109/tvcg.2017.2743858", "10.1177/1473871618821740", "10.1186/s12859-015-0585-1", "10.1002/nav.3800020109", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1006/s1045-926x(02)00016-2", "10.1109/cvpr.2012.6247667", "10.1023/b:jogo.0000042115.44455.f3", "10.1109/pacificvis.2017.8031607", "10.1002/nav.3800030404", "10.1109/infvis.2004.1", "10.1109/cvpr.2008.4587500", "10.1109/pacificvis.2011.5742389", "10.1140/epjb/e2011-10979-2", "10.1371/journal.pone.0098679", "10.1090/s0002-9904-1920-03322-7", "10.1109/iv.2013.3", "10.1145/568522.568523", "10.1109/tvcg.2006.156", "10.1109/tvcg.2012.236", "10.1109/tvcg.2018.2865139", "10.1145/3219819.3220025", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030459", "title": "Scalability of Network Visualisation from a Cognitive Load Perspective", "year": "2020", "conferenceName": "InfoVis", "authors": "Vahan Yoghourdjian;Yalong Yang;Tim Dwyer;Lawrence Lee;Michael Wybrow;Kim Marriott", "citationCount": "0", "affiliation": "Yoghourdjian, V (Corresponding Author), Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Yoghourdjian, Vahan; Yang, Yalong; Dwyer, Tim; Wybrow, Michael; Marriott, Kim, Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Lawrence, Lee, Monash Univ, Fac Business \\& Econ, Melbourne, Vic, Australia. Yang, Yalong, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA.", "countries": "USA;Australia", "abstract": "Node-link diagrams are widely used to visualise networks. However, even the best network layout algorithms ultimately result in \u2018hairball\u2019 visualisations when the graph reaches a certain degree of complexity, requiring simplification through aggregation or interaction (such as filtering) to remain usable. Until now, there has been little data to indicate at what level of complexity node-link diagrams become ineffective or how visual complexity affects cognitive load. To this end, we conducted a controlled study to understand workload limits for a task that requires a detailed understanding of the network topology-finding the shortest path between two nodes. We tested performance on graphs with 25 to 175 nodes with varying density. We collected performance measures (accuracy and response time), subjective feedback, and physiological measures (EEG, pupil dilation, and heart rate variability). To the best of our knowledge this is the first network visualisation study to include physiological measures. Our results show that people have significant difficulty finding the shortest path in high density node-link diagrams with more than 50 nodes and even low density graphs with more than 100 nodes. From our collected EEG data we observe functional differences in brain activity between hard and easy tasks. We found that cognitive load increased up to certain level of difficulty after which it decreased, likely because participants had given up. We also explored the effects of global network layout features such as size or number of crossings, and features of the shortest path such as length or straightness on task difficulty. We found that global features generally had a greater impact than those of the shortest path.", "keywords": "Data Visualisation,Network Visualisation,Cognitive Load,EEG", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030459", "refList": ["10.1109/tvcg.2019.2934396", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2016.2570755", "10.1007/s00371-013-0892-3", "10.1007/b98835", "10.1109/isda.2014.7066252", "10.1109/tvcg.2015.2467251", "10.1177/1473871612455749", "10.1109/tvcg.2012.299", "10.1007/3-540-58950-3", "10.1111/cgf.12878", "10.1109/mcse.2007.55", "10.1109/tvcg.2012.238", "10.1145/264645.264657", "10.1109/tvcg.2015.2467451", "10.1109/tvcg.2013.151", "10.1016/0020-0190(89)90102-6", "10.1109/tvcg.2019.2934307", "10.1109/tvcg.2015.2468151", "10.1109/tvcg.2017.2745919", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13440", "10.1109/tvcg.2011.220", "10.1111/cgf.13187", "10.1109/t-c.1969.222678", "10.1109/tvcg.2017.2743858", "10.1126/science.290.5500.2319", "10.1109/cahpc.2018.8645912", "10.1109/tvcg.2015.2465151", "10.1109/tvcg.2016.2598958", "10.1109/tvcg.2017.2751473", "10.1002/spe.4380211102", "10.1109/tpds.2018.2869805", "10.1016/j.jpdc.2019.04.008", "10.1109/pacificvis.2017.8031574", "10.1006/s1045-926x(02)00016-2", "10.1109/tvcg.2017.2674999", "10.1145/2872427.2883041", "10.1145/3292500.3330989", "10.1109/tvcg.2017.2744878", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1109/sbac-pad.2018.00060", "10.1007/978-3-662-45803-7\\_27", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1111/j.1469-1809.1936.tb02137.x", "10.1007/bf02289565", "10.1109/tvcg.2017.2689016", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.3756", "year": "2020", "title": "BatchLayout: A Batch-Parallel Force-Directed Graph Layout Algorithm in Shared Memory", "conferenceName": "PacificVis", "authors": "Md. Khaledur Rahman;Majedul Haque Sujon;Ariful Azad", "citationCount": "0", "affiliation": "Azad, A (Corresponding Author), Indiana Univ, Dept Intelligent Syst Engn, Bloomington, IN 47405 USA.\nRahman, Md Khaledur, Indiana Univ, Dept Comp Sci, Bloomington, IN 47405 USA.\nSujon, Majedul Haque; Azad, Ariful, Indiana Univ, Dept Intelligent Syst Engn, Bloomington, IN 47405 USA.", "countries": "USA", "abstract": "Force-directed algorithms are widely used to generate aesthetically-pleasing layouts of graphs or networks arisen in many scientific disciplines. To visualize large-scale graphs, several parallel algorithms have been discussed in the literature. However, existing parallel algorithms do not utilize memory hierarchy efficiently and often offer limited parallelism. This paper addresses these limitations with BatchLayout, an algorithm that groups vertices into minibatches and processes them in parallel. BatchLayout also employs cache blocking techniques to utilize memory hierarchy efficiently. More parallelism and improved memory accesses coupled with force approximating techniques, better initialization, and optimized learning rate make BatchLayout significantly faster than other state-of-the-art algorithms such as ForceAtlas2 and OpenOrd. The visualization quality of layouts from BatchLayout is comparable or better than similar visualization tools.", "keywords": "Human-centered computing; Visualization; Visualization techniques; Graph drawings; Human-centered computing; Visualization; Visualization systems and tools; Visualization toolkits", "link": "https://doi.org/10.1109/PacificVis48177.2020.3756", "refList": ["10.1101/gr.1239303", "10.1109/tvcg.2012.299", "10.1109/tpds.2014.2331243", "10.1145/169627.169640", "10.1038/324446a0", "10.1016/j.ins.2016.11.012", "10.1016/0020-0190(89)90102-6", "10.1109/tvcg.2018.2859997", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13187", "10.1109/icpp.2017.47", "10.1109/tvcg.2017.2743858", "10.1103/physreve.79.026102", "10.1002/spe.4380211102", "10.1109/tpds.2018.2869805", "10.1103/physreve.78.046110", "10.1006/s1045-926x(02)00016-2", "10.1117/12.871402", "10.1137/s154034590241370x", "10.1371/journal.pone.0098679", "10.1007/978-3-642-00219-9\\_21"], "wos": 1, "children": [], "len": 1}], "len": 51}, {"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030447", "title": "DRGraph: An Efficient Graph Layout Algorithm for Large-scale Graphs by Dimensionality Reduction", "year": "2020", "conferenceName": "InfoVis", "authors": "Minfeng Zhu;Wei Chen;Yuanzhe Hu;Yuxuan Hou;Liangjun Liu;Kaiyuan Zhang", "citationCount": "0", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Zhu, Minfeng; Chen, Wei; Hu, Yuanzhe; Hou, Yuxuan; Liu, Liangjun; Zhang, Kaiyuan, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China.", "countries": "China", "abstract": "Efficient layout of large-scale graphs remains a challenging problem: the force-directed and dimensionality reduction-based methods suffer from high overhead for graph distance and gradient computation. In this paper, we present a new graph layout algorithm, called DRGraph, that enhances the nonlinear dimensionality reduction process with three schemes: approximating graph distances by means of a sparse distance matrix, estimating the gradient by using the negative sampling technique, and accelerating the optimization process through a multi-level layout scheme. DRGraph achieves a linear complexity for the computation and memory consumption, and scales up to large-scale graphs with millions of nodes. Experimental results and comparisons with state-of-the-art graph layout methods demonstrate that DRGraph can generate visually comparable layouts with a faster running time and a lower memory requirement.", "keywords": "graph visualization,graph layout,dimensionality reduction,force-directed layout", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030447", "refList": ["10.1109/tvcg.2007.70582", "10.1371/journal.pone.0136497", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/3018661.3018731", "10.1109/34.491619", "10.1145/2939672.2939754", "10.1145/3269206.3271788", "10.1016/j.comnet.2011.08.019", "10.1007/s11263-011-0442-2", "10.1016/s0020-0190(98)00108-2", "10.1109/tvcg.2018.2865151", "10.1007/978-3-319-61188-4\\_2", "10.1177/1473871612455749", "10.1177/1473871616666394", "10.1109/tvcg.2015.2467035", "10.1186/1471-2105-10-375", "10.1145/3097983.3098061", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1109/tvcg.2015.2467451", "10.1109/2945.841119", "10.1016/j.swevo.2015.10.002", "10.1016/0020-0190(89)90102-6", "10.1109/infvis.2003.1249009", "10.1109/tvcg.2017.2745919", "10.1111/cgf.13187", "10.1111/cgf.13440", "10.1109/tvcg.2012.245", "10.1109/tvcg.2017.2743858", "10.1177/1473871618821740", "10.1186/s12859-015-0585-1", "10.1002/nav.3800020109", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1006/s1045-926x(02)00016-2", "10.1109/cvpr.2012.6247667", "10.1023/b:jogo.0000042115.44455.f3", "10.1109/pacificvis.2017.8031607", "10.1002/nav.3800030404", "10.1109/cvpr.2008.4587500", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1090/s0002-9904-1920-03322-7", "10.1109/iv.2013.3", "10.1145/568522.568523", "10.1109/tvcg.2006.156", "10.1109/tvcg.2012.236", "10.1109/tvcg.2018.2865139", "10.1145/3219819.3220025", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030393", "title": "Exemplar-based Layout Fine-tuning for Node-link Diagrams", "year": "2020", "conferenceName": "InfoVis", "authors": "Jiacheng Pan;Wei Chen;Xiaodong Zhao;Shuyue Zhou;Wei Zeng;Minfeng Zhu;Jian Chen;Siwei Fu;Yingcai Wu", "citationCount": "1", "affiliation": "Chen, W; Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Lab, Hangzhou, Peoples R China. Pan, Jiacheng; Chen, Wei; Zhao, Xiaodong; Zhou, Shuyue; Zhu, Minfeng; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Fu, Siwei; Wu, Yingcai, Zhejiang Lab, Hangzhou, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Jian, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA;China", "abstract": "We design and evaluate a novel layout fine-tuning technique for node-link diagrams that facilitates exemplar-based adjustment of a group of substructures in batching mode. The key idea is to transfer user modifications on a local substructure to other substructures in the entire graph that are topologically similar to the exemplar. We first precompute a canonical representation for each substructure with node embedding techniques and then use it for on-the-fly substructure retrieval. We design and develop a light-weight interactive system to enable intuitive adjustment, modification transfer, and visual graph exploration. We also report some results of quantitative comparisons, three case studies, and a within-participant user study.", "keywords": "Node-link diagram,graph layout,graph visualization,user interactions", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030393", "refList": ["10.1109/tvcg.2007.70582", "10.1371/journal.pone.0136497", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/3018661.3018731", "10.1109/34.491619", "10.1145/2939672.2939754", "10.1145/3269206.3271788", "10.1016/j.comnet.2011.08.019", "10.1007/s11263-011-0442-2", "10.1016/s0020-0190(98)00108-2", "10.1109/tvcg.2018.2865151", "10.1177/1473871612455749", "10.1177/1473871616666394", "10.1109/tvcg.2015.2467035", "10.1186/1471-2105-10-375", "10.1145/3097983.3098061", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1109/tvcg.2015.2467451", "10.1016/j.swevo.2015.10.002", "10.1016/0020-0190(89)90102-6", "10.1109/infvis.2003.1249009", "10.1109/tvcg.2017.2745919", "10.1111/cgf.13187", "10.1111/cgf.13440", "10.1109/tvcg.2012.245", "10.1109/tvcg.2017.2743858", "10.1177/1473871618821740", "10.1186/s12859-015-0585-1", "10.1002/nav.3800020109", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1006/s1045-926x(02)00016-2", "10.1109/cvpr.2012.6247667", "10.1023/b:jogo.0000042115.44455.f3", "10.1109/pacificvis.2017.8031607", "10.1002/nav.3800030404", "10.1109/infvis.2004.1", "10.1109/cvpr.2008.4587500", "10.1109/pacificvis.2011.5742389", "10.1140/epjb/e2011-10979-2", "10.1371/journal.pone.0098679", "10.1090/s0002-9904-1920-03322-7", "10.1109/iv.2013.3", "10.1145/568522.568523", "10.1109/tvcg.2006.156", "10.1109/tvcg.2012.236", "10.1109/tvcg.2018.2865139", "10.1145/3219819.3220025", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030459", "title": "Scalability of Network Visualisation from a Cognitive Load Perspective", "year": "2020", "conferenceName": "InfoVis", "authors": "Vahan Yoghourdjian;Yalong Yang;Tim Dwyer;Lawrence Lee;Michael Wybrow;Kim Marriott", "citationCount": "0", "affiliation": "Yoghourdjian, V (Corresponding Author), Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Yoghourdjian, Vahan; Yang, Yalong; Dwyer, Tim; Wybrow, Michael; Marriott, Kim, Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Lawrence, Lee, Monash Univ, Fac Business \\& Econ, Melbourne, Vic, Australia. Yang, Yalong, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA.", "countries": "USA;Australia", "abstract": "Node-link diagrams are widely used to visualise networks. However, even the best network layout algorithms ultimately result in \u2018hairball\u2019 visualisations when the graph reaches a certain degree of complexity, requiring simplification through aggregation or interaction (such as filtering) to remain usable. Until now, there has been little data to indicate at what level of complexity node-link diagrams become ineffective or how visual complexity affects cognitive load. To this end, we conducted a controlled study to understand workload limits for a task that requires a detailed understanding of the network topology-finding the shortest path between two nodes. We tested performance on graphs with 25 to 175 nodes with varying density. We collected performance measures (accuracy and response time), subjective feedback, and physiological measures (EEG, pupil dilation, and heart rate variability). To the best of our knowledge this is the first network visualisation study to include physiological measures. Our results show that people have significant difficulty finding the shortest path in high density node-link diagrams with more than 50 nodes and even low density graphs with more than 100 nodes. From our collected EEG data we observe functional differences in brain activity between hard and easy tasks. We found that cognitive load increased up to certain level of difficulty after which it decreased, likely because participants had given up. We also explored the effects of global network layout features such as size or number of crossings, and features of the shortest path such as length or straightness on task difficulty. We found that global features generally had a greater impact than those of the shortest path.", "keywords": "Data Visualisation,Network Visualisation,Cognitive Load,EEG", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030459", "refList": ["10.1109/tvcg.2019.2934396", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2016.2570755", "10.1007/s00371-013-0892-3", "10.1007/b98835", "10.1109/isda.2014.7066252", "10.1109/tvcg.2015.2467251", "10.1177/1473871612455749", "10.1109/tvcg.2012.299", "10.1007/3-540-58950-3", "10.1111/cgf.12878", "10.1109/mcse.2007.55", "10.1109/tvcg.2012.238", "10.1145/264645.264657", "10.1109/tvcg.2015.2467451", "10.1109/tvcg.2013.151", "10.1016/0020-0190(89)90102-6", "10.1109/tvcg.2019.2934307", "10.1109/tvcg.2015.2468151", "10.1109/tvcg.2017.2745919", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13440", "10.1109/tvcg.2011.220", "10.1111/cgf.13187", "10.1109/t-c.1969.222678", "10.1109/tvcg.2017.2743858", "10.1126/science.290.5500.2319", "10.1109/cahpc.2018.8645912", "10.1109/tvcg.2015.2465151", "10.1109/tvcg.2016.2598958", "10.1109/tvcg.2017.2751473", "10.1002/spe.4380211102", "10.1109/tpds.2018.2869805", "10.1016/j.jpdc.2019.04.008", "10.1109/pacificvis.2017.8031574", "10.1006/s1045-926x(02)00016-2", "10.1109/tvcg.2017.2674999", "10.1145/2872427.2883041", "10.1145/3292500.3330989", "10.1109/tvcg.2017.2744878", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1109/sbac-pad.2018.00060", "10.1007/978-3-662-45803-7\\_27", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1111/j.1469-1809.1936.tb02137.x", "10.1007/bf02289565", "10.1109/tvcg.2017.2689016", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2018.00011", "year": "2018", "title": "BC Tree-Based Proxy Graphs for Visualization of Big Graphs", "conferenceName": "PacificVis", "authors": "Seok{-}Hee Hong;Quan Hoang Nguyen;Amyra Meidiana;Jiaxi Li;Peter Eades", "citationCount": "3", "affiliation": "Hong, SH (Corresponding Author), Univ Sydney, Sch IT, Sydney, NSW, Australia.\nHong, Seok-Hee; Quan Nguyen; Meidiana, Amyra; Li, Jiaxi; Eades, Peter, Univ Sydney, Sch IT, Sydney, NSW, Australia.", "countries": "Australia", "abstract": "Recent work for visualizing big graphs uses a proxy graph approach: the original graph is replaced by a proxy graph, which is much smaller than the original graph. The challenge for the proxy graph approach is to ensure that the proxy graph is a good representation of the original graph. However, previous work to compute proxy graphs using graph sampling techniques often fails to preserve connectivity and important global skeletal structure in the original graph. This paper introduces two new families of proxy graph methods BCP-W and BCP-E, tightly integrating graph sampling methods with the BC (Block Cut-vertex) tree, which represents the decomposition of a graph into biconnected components. Experimental results using graph sampling quality metrics show that our new BC tree-based proxy graph methods produce significantly better results than existing sampling-based proxy graph methods: 25\\% improvement by BCP-W and 15\\% by BCP-E on average. We also present DBCP, a BC tree-based proxy graph method for distributed environment. Experiments on the Amazon Cloud EC2 demonstrate that DBCP is scalable for big graph data sets; runtime speed-up of 77\\% for distributed 5-server on average. Visual comparison using a graph layout method and the proxy quality metrics confirm that our new BC tree-based proxy graph methods are significantly better than existing sampling-based proxy graph method. Our main results lead to guidelines for computing sampling-based proxy graphs for visualization of big graphs.", "keywords": "Human-centered computing; Visualization; Visualization techniques", "link": "https://doi.org/10.1109/PacificVis.2018.00011", "refList": ["10.1109/tvcg.2016.2598867", "10.1016/0378-8733(78)90021-7", "10.1103/physreve.73.016102", "10.1103/physreve.67.026126", "10.1145/1150402.1150479", "10.1103/physreve.75.027105", "10.1109/tvcg.2017.2674999", "10.1109/pacificvis.2013.6596147", "10.1073/pnas.0400087101", "10.7155/jgaa.00405", "10.1145/362248.362272", "10.1371/journal.pone.0085777"], "wos": 1, "children": [{"doi": "10.1111/cgf.14003", "year": "2020", "title": "Sublinear Time Force Computation for Big Complex Network Visualization", "conferenceName": "EuroVis", "authors": "Amyra Meidiana;Seok{-}Hee Hong;Marnijati Torkel;Shijun Cai;Peter Eades", "citationCount": "0", "affiliation": "Meidiana, A (Corresponding Author), Univ Sydney, Sch Comp Sci, Sydney, NSW, Australia.\nMeidiana, Amyra; Hong, Seok-Hee; Torkel, Marnijati; Cai, Shijun; Eades, Peter, Univ Sydney, Sch Comp Sci, Sydney, NSW, Australia.", "countries": "Australia", "abstract": "In this paper, we present a new framework for sublinear time force computation for visualization of big complex graphs. Our algorithm is based on the sampling of vertices for computing repulsion forces and edge sparsification for attraction force computation. More specifically, for vertex sampling, we present three types of sampling algorithms, including random sampling, geometric sampling, and combinatorial sampling, to reduce the repulsion force computation to sublinear in the number of vertices. We utilize a spectral sparsification approach to reduce the number of attraction force computations to sublinear in the number of edges for dense graphs. We also present a smart initialization method based on radial tree drawing of the BFS spanning tree rooted at the center. Experiments show that our new sublinear time force computation algorithms run quite fast, while producing good visualization of large and complex networks, with significant improvements in quality metrics such as shape-based and edge crossing metrics.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14003", "refList": ["10.1038/324446a0", "10.1142/9789812773296\\_0012", "10.1002/spe.4380211102", "10.1109/pacificvis.2018.00011", "10.1137/08074489x", "10.7155/jgaa.00405", "10.1007/978-3-642-18469-7\\_8", "10.1007/978-981-10-1503-8\\_1", "10.1007/978-1-4614-7163-9\\textbackslash{}\\_315-1", "10.1109/tvcg.2011.185", "10.1109/dsaa.2017.15"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.13724", "year": "2019", "title": "A Random Sampling O(n) Force-calculation Algorithm for Graph Layouts", "conferenceName": "EuroVis", "authors": "Robert Gove", "citationCount": "1", "affiliation": "Gove, R (Corresponding Author), Two Six Labs, Arlington, VA 22203 USA.\nGove, R., Two Six Labs, Arlington, VA 22203 USA.", "countries": "USA", "abstract": "This paper proposes a linear-time repulsive-force-calculation algorithm with sub-linear auxiliary space requirements, achieving an asymptotic improvement over the Barnes-Hut and Fast Multipole Method force-calculation algorithms. The algorithm, named random vertex sampling (RVS), achieves its speed by updating a random sample of vertices at each iteration, each with a random sample of repulsive forces. This paper also proposes a combination algorithm that uses RVS to derive an initial layout and then applies Barnes-Hut to refine the layout. An evaluation of RVS and the combination algorithm compares their speed and quality on 109 graphs against a Barnes-Hut layout algorithm. The RVS algorithm performs up to 6.1 times faster on the tested graphs while maintaining comparable layout quality. The combination algorithm also performs faster than Barnes-Hut, but produces layouts that are more symmetric than using RVS alone. Data and code: https://osf.io/nb7m8/", "keywords": "", "link": "https://doi.org/10.1111/cgf.13724", "refList": ["10.1145/200836.200853", "10.1103/physreve.74.036104", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.3390/a9030053", "10.1016/j.jmmm.2009.09.033", "10.7155/jgaa.00405", "10.1111/j.1467-8659.2011.01898.x", "10.1109/infvis.2002.1173159", "10.1073/pnas.021544898", "10.1109/tvcg.2011.185", "10.1007/3-540-58950-3", "10.1109/tvcg.2008.158", "10.1145/2384416.2384418", "10.1109/visual.1996.567787", "10.1136/qshc.2004.010033", "10.1038/324446a0", "10.1007/978-3-319-27261-0\\_4", "10.1002/1097-024x(200009)30:11", "10.1109/tvcg.2015.2467451", "10.1007/978-3-662-45803-7\\_20", "10.1145/1232722.1232727", "10.1006/jvlc.1995.1014", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13187", "10.1109/tvcg.2009.109", "10.1142/s0219525914500222", "10.1109/tvcg.2008.155", "10.1007/978-3-319-50106-2\\_2", "10.1007/978-3-319-27261-0\\_5", "10.1038/30918", "10.1126/science.286.5439.509", "10.1007/3-540-62495-3\\_50", "10.1109/iv.2015.56", "10.1002/spe.4380211102", "10.1006/jcph.2000.6451", "10.1109/icdm.2012.159", "10.1109/tvcg.2017.2674999", "10.1111/j.1467-8659.2012.03090.x", "10.1016/j.physa.2011.03.036", "10.1111/ajps.12102", "10.1023/a:1008047806690", "10.1109/tvcg.2007.46", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1109/tvcg.2009.174", "10.1016/j.jelectrocard.2010.09.003", "10.13140/2.1.1341.1520", "10.1057/palgrave.ivs.9500040", "10.1112/plms/s3-13.1.743", "10.1371/journal.pone.0098679", "10.1517/14728222.10.1.1", "10.1007/978-3-319-26633-6\\_13", "10.1145/1054972.1055031", "10.1147/jrd.2015.2411412", "10.1109/pacificvis.2017.8031607"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13169", "year": "2017", "title": "Empirically Measuring Soft Knowledge in Visualization", "conferenceName": "EuroVis", "authors": "Natchaya Kijmongkolchai;Alfie Abdul{-}Rahman;Min Chen", "citationCount": "3", "affiliation": "Kijmongkolchai, N (Corresponding Author), Univ Oxford, Oxford, England.\nKijmongkolchai, Natchaya; Abdul-Rahman, Alfie; Chen, Min, Univ Oxford, Oxford, England.", "countries": "England", "abstract": "In this paper, we present an empirical study designed to evaluate the hypothesis that humans' soft knowledge can enhance the cost-benefit ratio of a visualization process by reducing the potential distortion. In particular, we focused on the impact of three classes of soft knowledge: (i) knowledge about application contexts, (ii) knowledge about the patterns to be observed (i.e., in relation to visualization task), and (iii) knowledge about statistical measures. We mapped these classes into three control variables, and used real-world time series data to construct stimuli. The results of the study confirmed the positive contribution of each class of knowledge towards the reduction of the potential distortion, while the knowledge about the patterns prevents distortion more effectively than the other two classes.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13169", "refList": ["10.1111/cgf.12887", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2006.194", "10.1109/tvcg.2013.166", "10.1109/tvcg.2014.2346420", "10.1111/cgf.12634", "10.1111/cgf.12889", "10.1109/tvcg.2015.2467759", "10.1109/tvcg.2013.187", "10.1109/tvcg.2012.197", "10.1109/tvcg.2012.180", "10.1109/tvcg.2015.2467322", "10.1109/tvcg.2016.2598862", "10.1111/cgf.12127", "10.1111/cgf.12104", "10.1109/tvcg.2015.2513410", "10.1161/01.cir.101.23.e215", "10.1109/tvcg.2012.163", "10.1109/tvcg.2013.234", "10.1109/tvcg.2015.2467758", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346428", "10.1109/tvcg.2012.233", "10.1109/tvcg.2014.2346979", "10.1109/tvcg.2016.2598898", "10.1111/cgf.12638", "10.1111/cgf.12880", "10.1111/j.1467-8659.2012.03129.x", "10.1111/cgf.12092", "10.1109/tvcg.2014.2346320", "10.1109/tvcg.2010.150", "10.1109/tvcg.2016.2598466", "10.1111/cgf.12633", "10.1109/tvcg.2016.2598594", "10.1109/tvcg.2015.2467752", "10.1109/tvcg.2015.2502587", "10.1111/j.1467-8659.2012.03092.x", "10.1109/tvcg.2016.2520921", "10.1145/2858036.2858272", "10.1109/tvcg.2012.215", "10.1109/tvcg.2012.230", "10.1109/tvcg.2014.2371858", "10.1109/visual.2001.964505", "10.1109/tvcg.2015.2467201", "10.1109/tvcg.2014.2346422", "10.1111/cgf.12656", "10.1057/ivs.2008.13", "10.1109/tvcg.2015.2424872", "10.1109/tvcg.2012.279", "10.1109/tvcg.2014.2346998", "10.1109/tvcg.2012.220", "10.1109/tvcg.2015.2467951", "10.1109/tvcg.2013.183", "10.1109/tvcg.2012.223", "10.1109/tvcg.2012.189", "10.1109/tvcg.2012.222", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2012.196", "10.1109/tvcg.2016.2599106", "10.1109/tvcg.2014.2346424", "10.1109/tvcg.2012.199", "10.1109/tvcg.2016.2518158", "10.1109/tvcg.2012.221", "10.1111/cgf.13009", "10.1109/tvcg.2014.2346983", "10.1111/j.1467-8659.2012.03093.x", "10.1109/tvcg.2013.170", "10.1111/cgf.12888", "10.1109/tvcg.2012.245", "10.1109/tvcg.2014.2346426", "10.1109/tvcg.2016.2598544", "10.1109/tvcg.2015.2467751", "10.1109/tvcg.2014.2346298", "10.1109/tvcg.2012.144", "10.1109/tvcg.2012.251", "10.1109/tvcg.2016.2598885"], "wos": 1, "children": [{"doi": "10.1109/vast.2017.8585498", "title": "The Role of Explicit Knowledge: A Conceptual Model of Knowledge-Assisted Visual Analytics", "year": "2017", "conferenceName": "VAST", "authors": "Paolo Federico;Markus Wagner 0008;Alexander Rind;Albert Amor-Amoros;Silvia Miksch;Wolfgang Aigner", "citationCount": "3", "affiliation": "Federico, P (Corresponding Author), TU Wien, Vienna, Austria. Federico, Paolo; Wagner, Markus; Rind, Alexander; Amor-Amoros, Albert; Miksch, Silvia; Aigner, Wolfgang, TU Wien, Vienna, Austria. Wagner, Markus; Rind, Alexander; Aigner, Wolfgang, St Poelten Univ Appl Sci, Sankt Polten, Austria.", "countries": "Austria", "abstract": "Visual Analytics (VA) aims to combine the strengths of humans and computers for effective data analysis. In this endeavor, humans' tacit knowledge from prior experience is an important asset that can be leveraged by both human and computer to improve the analytic process. While VA environments are starting to include features to formalize, store, and utilize such knowledge, the mechanisms and degree in which these environments integrate explicit knowledge varies widely. Additionally, this important class of VA environments has never been elaborated on by existing work on VA theory. This paper proposes a conceptual model of Knowledge-assisted VA conceptually grounded on the visualization model by van Wijk. We apply the model to describe various examples of knowledge-assisted VA from the literature and elaborate on three of them in finer detail. Moreover, we illustrate the utilization of the model to compare different design alternatives and to evaluate existing approaches with respect to their use of knowledge. Finally, the model can inspire designers to generate novel VA environments using explicit knowledge effectively.", "keywords": "Automated analysis,tacit knowledge,explicit knowledge,visual analytics,information visualization,theory and model", "link": "http://dx.doi.org/10.1109/VAST.2017.8585498", "refList": ["10.1016/j.artmed.2006.03.001", "10.1057/ivs.2009.22", "10.1109/infvis.2000.885092", "10.2312/pe/eurovast/eurova11/009-012", "10.1109/tvcg.2016.2598839", "10.1109/tvcg.2014.2346575", "10.1111/cgf.13169", "10.1186/1471-2105-13-s8-s3", "10.1007/s00371-015-1132-9", "10.1109/tvcg.2013.146", "10.1109/tvcg.2016.2598471", "10.1016/j.cag.2009.06.004", "10.1145/2993901.2993915", "10.1111/cgf.12090", "10.1177/1473871611412817", "10.1145/2598153.2598172", "10.1016/j.cag.2009.06.006", "10.1109/tvcg.2016.2598460", "10.1016/j.autcon.2014.03.012", "10.1145/989863.989865", "10.1109/21.44068", "10.1057/palgrave.ivs.9500045", "10.2312/eurova.20151108", "10.1002/9781444303179.ch3", "10.1109/hicss.2016.183", "10.1177/0165551506070706", "10.1145/2494188.2494202", "10.1145/1556262.1556327", "10.1016/j.artmed.2006.04.002", "10.1007/978-3-540-71080-6\\_6", "10.1111/j.1467-8659.2008.01230.x", "10.1109/pacificvis.2011.5742371", "10.1109/vast.2014.7042530", "10.1109/mcg.2010.8", "10.1109/mcg.2015.25", "10.1109/vast.2012.6400555", "10.1109/tvcg.2014.2346481", "10.1145/2836034.2836040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2008.109", "10.1016/j.artmed.2005.10.003", "10.1109/mcg.2014.73", "10.1016/s0004-3702(96)00025-2", "10.1093/intqhc/mzm007", "10.1016/j.dss.2012.06.009", "10.1016/j.cose.2017.02.003", "10.1109/infvis.1998.729560", "10.1109/vast.2010.5654451", "10.1109/tvcg.2016.2598829", "10.1109/vast.2007.4389021", "10.1145/1562849.1562851", "10.1145/302979.303030", "10.1145/985692.985706", "10.1109/infvis.1997.636792", "10.1111/j.1467-8659.2012.03092.x", "10.1109/mcg.2009.6", "10.1057/ivs.2008.28", "10.1109/mcg.2005.91", "10.1016/j.artmed.2010.02.001", "10.1177/0272989x14565822", "10.1109/mcg.2010.15", "10.1007/s10844-014-0304-9", "10.2312/pe.eurovast.eurova13.043-047", "10.1109/mcg.2014.33", "10.1109/tvcg.2014.2346574", "10.1111/j.1467-8659.2009.01708.x", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934654", "title": "Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections", "year": "2019", "conferenceName": "VAST", "authors": "Mennatallah El-Assady;Rebecca Kehlbeck;Christopher Collins;Daniel A. Keim;Oliver Deussen", "citationCount": "3", "affiliation": "El-Assady, M (Corresponding Author), Univ Konstanz, Constance, Germany. El-Assady, M (Corresponding Author), Ontario Tech Univ, Oshawa, ON, Canada. El-Assady, Mennatallah; Kehlbeck, Rebecca; Keim, Daniel; Deussen, Oliver, Univ Konstanz, Constance, Germany. El-Assady, Mennatallah; Collins, Christopher, Ontario Tech Univ, Oshawa, ON, Canada.", "countries": "Canada;Germany", "abstract": "We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decision-making process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.", "keywords": "Topic Model Optimization,Word Embedding,Mixed-Initiative Refinement,Guided Visual Analytics,Semantic Mapping", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934654", "refList": ["10.1109/vast.2014.7042493", "10.1145/2133806.2133826", "10.1016/j.visinf.2018.09.003", "10.1007/978-3-319-67008-9\\_26", "10.1109/tvcg.2013.126", "10.1162/tacl\\_a\\_00140", "10.1007/s13202-018-0509-5", "10.1109/bdva.2018.8534018", "10.1108/eb026526", "10.1007/s10994-013-5413-0", "10.1145/564376.564421", "10.1016/j.visinf.2017.01.006", "10.3115/v1/p14-2050", "10.1007/bf00288933", "10.1109/tvcg.2013.212", "10.1145/2207676.2207741", "10.1109/tvcg.2013.162", "10.1109/tvcg.2016.2515592", "10.1109/tvcg.2017.2745080", "10.1109/mcg.2013.53", "10.1109/tvcg.2017.2744199", "10.1145/3091108", "10.18653/v1/p17-4009", "10.1162/jmlr.2003.3.4-5.951", "10.1109/vast.2017.8585498", "10.1109/tvcg.2017.2723397", "10.1109/tvcg.2018.2864769", "10.1007/s10618-005-0361-3", "10.3115/v1/d14-1167", "10.1007/bf01840357", "10.1162/jmlr.2003.3.4-5.993", "10.1145/2678025.2701370", "10.1016/j.ins.2016.06.040", "10.1109/tvcg.2017.2746018", "10.1109/vast.2011.6102461", "10.1111/cgf.13092", "10.3115/1117729.1117730", "10.1109/mcg.2015.91", "10.1145/2669557.2669572"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13959", "year": "2020", "title": "Knowledge-Assisted Comparative Assessment of Breast Cancer using Dynamic Contrast-Enhanced Magnetic Resonance Imaging", "conferenceName": "EuroVis", "authors": "Kai Nie;Pascal A. Baltzer;Bernhard Preim;Gabriel Mistelbauer", "citationCount": "0", "affiliation": "Nie, K (Corresponding Author), Otto von Guericke Univ, Dept Simulat \\& Graph, Magdeburg, Germany.\nNie, K.; Preim, B.; Mistelbauer, G., Otto von Guericke Univ, Dept Simulat \\& Graph, Magdeburg, Germany.\nBaltzer, P., Med Univ Vienna, Dept Biomed Imaging \\& Image Guided Therapy, Vienna, Austria.", "countries": "Germany;Austria", "abstract": "Breast perfusion data are dynamic medical image data that depict perfusion characteristics of the investigated tissue. These data consist of a series of static datasets that are acquired at different time points and aggregated into time intensity curves (TICs) for each voxel. The characteristics of these TICs provide important information about a lesion's composition, but their analysis is time-consuming due to their large number. Subsequently, these TICs are used to classify a lesion as benign or malignant. This lesion scoring is commonly done manually by physicians and may therefore be subject to bias. We propose an approach that addresses both of these problems by combining an automated lesion classification with a visual confirmatory analysis, especially for uncertain cases. Firstly, we cluster the TICs of a lesion using ordering points to identify the clustering structure (OPTICS) and then visualize these clusters. Together with their relative size, they are added to a library. We then model fuzzy inference rules by using the lesion's TIC clusters as antecedents and its score as consequent. Using a fuzzy scoring system, we can suggest a score for a new lesion. Secondly, to allow physicians to confirm the suggestion in uncertain cases, we display the TIC clusters together with their spatial distribution and allow them to compare two lesions side by side. With our knowledge-assisted comparative visual analysis, physicians can explore and classify breast lesions. The true positive prediction accuracy of our scoring system achieved 71.4\\% in one-fold cross-validation using 14 lesions.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13959", "refList": ["10.1016/j.patcog.2017.08.004", "10.1109/icinfa.2017.8078962", "10.1109/iccv.2013.222", "10.1002/jmri.1123", "10.1016/j.ejrad.2017.01.020", "10.1007/s00330-016-4612-z", "10.1002/widm.30", "10.1002/jmri.26721", "10.1109/tvcg.2008.95", "10.1007/s00330-015-4075-7", "10.1007/s00330-007-0762-3", "10.1148/radiology.213.3.r99dc01881", "10.1007/s10278-010-9298-1", "10.1016/j.datak.2006.01.013", "10.1016/j.cag.2010.05.016", "10.5121/mlaij.2016.3103", "10.1089/10665270360688057", "10.1117/1.jmi.5.1.014502", "10.1002/mp.12408", "10.1198/jcgs.2011.09224", "10.1109/cbms.2013.6627768", "10.1109/vast.2017.8585498", "10.1118/1.4937787", "10.1016/j.acra.2009.03.017", "10.1145/2503210.2503255", "10.1007/978-3-319-68548-9\\_44", "10.1109/tvcg.2007.70569", "10.1016/j.compmedimag.2007.02.007", "10.1109/tmi.2013.2281984", "10.1148/radiol.2442051620", "10.1016/j.jacr.2009.07.023", "10.1002/j.1538-7305.1957.tb01515.x", "10.1148/radiol.14121031", "10.3322/caac.21492", "10.1148/radiology.211.1.r99ap38101", "10.1016/j.compbiomed.2014.10.006", "10.1016/j.eswa.2016.01.004", "10.3238/arztebl.2018.0316", "10.1559/152304003100010929", "10.1016/s0002-9610(01)00726-7", "10.1016/j.ejrad.2020.108819", "10.1109/tmi.2012.2191302"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/tvcg.2018.2864913", "title": "A Framework for Externalizing Implicit Error Using Visualization", "year": "2018", "conferenceName": "InfoVis", "authors": "Nina McCurdy;Julie Gerdes;Miriah D. Meyer", "citationCount": "8", "affiliation": "McCurdy, N (Corresponding Author), Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA. McCurdy, Nina; Meyer, Miriah, Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA. Gerdes, Julie, Texas Tech Univ, Coll Arts \\& Sci, Lubbock, TX 79409 USA.", "countries": "USA", "abstract": "This paper presents a framework for externalizing and analyzing expert knowledge about discrepancies in data through the use of visualization. Grounded in an 18-month design study with global health experts, the framework formalizes the notion of data discrepancies as implicit error, both in global health data and more broadly. We use the term implicit error to describe measurement error that is inherent to and pervasive throughout a dataset, but that isn't explicitly accounted for or defined. Instead, implicit error exists in the minds of experts, is mainly qualitative, and is accounted for subjectively during expert interpretation of the data. Externalizing knowledge surrounding implicit error can assist in synchronizing, validating, and enhancing interpretation, and can inform error analysis and mitigation. The framework consists of a description of implicit error components that are important for downstream analysis, along with a process model for externalizing and analyzing implicit error using visualization. As a second contribution, we provide a rich, reflective, and verifiable description of our research process as an exemplar summary toward the ongoing inquiry into ways of increasing the validity and transferability of design study research.", "keywords": "implicit error,knowledge externalization,design study", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864913", "refList": ["10.1197/jamia.m2342", "10.1109/tvcg.2013.132", "10.1111/cgf.13169", "10.1179/1743277414y.0000000099", "10.1016/j.cag.2009.06.004", "10.1111/j.1467-8659.2009.01678.x", "10.1111/j.0361-3666.2003.00237.x", "10.1111/cgf.12392", "10.1016/j.jvlc.2011.04.002", "10.1186/1471-2334-11-37", "10.1371/journal.pmed.1000376", "10.1109/infvis.2005.1532134", "10.1109/pacificvis.2017.8031599", "10.1109/tvcg.2017.2743898", "10.1197/jamia.m2544", "10.1145/3025453.3025592", "10.1109/tvcg.2015.2468151", "10.1007/978-1-4471-6497-5\\_1", "10.3233/978-1-60750-533-4-23", "10.1177/0165551506070706", "10.1145/642611.642616", "10.6064/2012/875253", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2015.2465151", "10.1111/j.1467-9604.2007.00468.x", "10.1109/mcg.2012.31", "10.1109/tvcg.2007.70589", "10.1145/2993901.2993916", "10.1109/vast.2010.5652885", "10.1145/3025453.3025738", "10.1109/38.689662", "10.1016/s0925-7535(97)00052-0", "10.9745/ghsp-d-15-00207", "10.1518/001872095779049543", "10.1080/15323269.2011.587100", "10.1117/12.587254", "10.1016/j.cie.2014.11.025", "10.1109/tvcg.2017.2745240", "10.1109/tvcg.2012.213", "10.1109/vast.2011.6102457", "10.1109/mcg.2015.50", "10.1145/1385569.1385582", "10.1016/j.jbi.2014.04.006", "10.1136/amiajnl-2011-000486"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934539", "title": "Criteria for Rigor in Visualization Design Study", "year": "2019", "conferenceName": "InfoVis", "authors": "Miriah D. Meyer;Jason Dykes", "citationCount": "16", "affiliation": "Meyer, M (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Dykes, Jason, City Univ London, London, England.", "countries": "USA;England", "abstract": "We develop a new perspective on research conducted through visualization design study that emphasizes design as a method of inquiry and the broad range of knowledge-contributions achieved through it as multiple, subjective, and socially constructed. From this interpretivist position we explore the nature of visualization design study and develop six criteria for rigor. We propose that rigor is established and judged according to the extent to which visualization design study research and its reporting are INFORMED, REFLEXIVE, ABUNDANT, PLAUSIBLE, RESONANT, and TRANSPARENT. This perspective and the criteria were constructed through a four-year engagement with the discourse around rigor and the nature of knowledge in social science, information systems, and design. We suggest methods from cognate disciplines that can support visualization researchers in meeting these criteria during the planning, execution, and reporting of design study. Through a series of deliberately provocative questions, we explore implications of this new perspective for design study research in visualization, concluding that as a discipline, visualization is not yet well positioned to embrace, nurture, and fully benefit from a rigorous, interpretivist approach to design study. The perspective and criteria we present are intended to stimulate dialogue and debate around the nature of visualization design study and the broader underpinnings of the discipline.", "keywords": "design study,relativism,interpretivism,knowledge construction,qualitative research,research through design", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934539", "refList": ["10.1007/978-1-4939-0378-8\\_8", "10.1177/1049732315588501", "10.1177/146879410200200205", "10.2307/1177100", "10.1016/0142-694x(82)90040-0", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1145/2362364.2362371", "10.1080/2159676x.2017.1393221", "10.1177/1473871613510429", "10.1145/2212877.2212889", "10.1080/09650790802011973", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2015.2467195", "10.1109/mcse.2007.106", "10.1080/1750984x.2017.1317357", "10.1109/tvcg.2017.2745958", "10.1177/1525822x0101300203", "10.1080/23265507.2017.1300068", "10.1111/j.1540-4560.1946.tb02295.x", "10.1177/1468794108098034", "10.1007/978-3-7643-8472-2\\_6", "10.1145/2317956.2317968", "10.2307/1511837", "10.1177/104973202129120052", "10.3233/efi-2004-22201", "10.1109/beliv.2018.8634427", "10.1109/beliv.2018.8634261", "10.1016/s0142-694x(01)00009-6", "10.1007/978-3-7643-8472-2\\_3", "10.1145/642611.642616", "10.1177/1468794107085301", "10.1177/1077800410383121", "10.1109/tvcg.2010.137", "10.1145/2405716.2405725", "10.1145/2702123.2702172", "10.1109/tvcg.2014.2346248", "10.1109/tvcg.2011.209", "10.1111/j.1467-8659.2009.01710.x", "10.1145/3173574.3173775", "10.1145/1993060.1993065", "10.1007/978-1-4419-5653-8\\_2", "10.1177/107780049900500402", "10.1109/tvcg.2018.2864905", "10.2307/2288400", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1177/160940690400300403", "10.1145/2993901.2993916", "10.1145/1879831.1879836", "10.2307/3178066", "10.1109/tvcg.2018.2864913", "10.3102/0013189x022004016", "10.1109/tvcg.2015.2511718", "10.1109/tvcg.2018.2865241", "10.1016/j.ijnurstu.2010.06.004", "10.1109/tvcg.2012.213", "10.1111/0735-2751.00040", "10.1109/tvcg.2013.145", "10.1002/ev.1427", "10.1109/tvcg.2018.2811488", "10.1075/idj.23.1.07thu", "10.1109/tvcg.2009.111", "10.1109/mcg.2018.2874523", "10.1111/cgf.13184", "10.1111/cgf.13595"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3029413", "title": "A Design Space of Vision Science Methods for Visualization Research", "year": "2020", "conferenceName": "InfoVis", "authors": "Madison A. Elliott;Christine Nothelfer;Cindy Xiong;Danielle Albers Szafir", "citationCount": "0", "affiliation": "Elliott, MA (Corresponding Author), Univ British Columbia, Vancouver, BC, Canada. Elliott, Madison A., Univ British Columbia, Vancouver, BC, Canada. Nothelfer, Christine, Northwestern Univ, Evanston, IL 60208 USA. Xiong, Cindy, Univ Massachusetts, Amherst, MA 01003 USA. Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "Canada;USA", "abstract": "A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.", "keywords": "Perception,human vision,empirical research,evaluation,HCI", "link": "http://dx.doi.org/10.1109/TVCG.2020.3029413", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1177/0886109909354981", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1093/bioinformatics/btq110", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1177/1744987107081254", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1177/174498710501000305", "10.1017/s1049096513001789", "10.1109/tvcg.2012.213", "10.1093/nar/gkz239", "10.1093/sysbio/sys062", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030438", "title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening", "year": "2020", "conferenceName": "SciVis", "authors": "Mar\u00eda Virginia Sabando;Pavol Ulbrich;Mat\u00edas N. Selzer;Jan Byska;Jan Mican;Ignacio Ponzoni;Axel J. Soto;Maria Luj\u00e1n Ganuza;Barbora Kozl\u00edkov\u00e1", "citationCount": "0", "affiliation": "Sabando, MV (Corresponding Author), Univ Nacl Sur, Inst Comp Sci \\& Engn UNS CONICET, Bahia Blanca, Buenos Aires, Argentina. Sabando, MV (Corresponding Author), Univ Nacl Sur, Dept Comp Sci \\& Engn, Bahia Blanca, Buenos Aires, Argentina. Sabando, Maria Virginia; Selzer, Matias; Ponzoni, Ignacio; Soto, Axel J.; Ganuza, Maria Lujan, Univ Nacl Sur, Inst Comp Sci \\& Engn UNS CONICET, Bahia Blanca, Buenos Aires, Argentina. Sabando, Maria Virginia; Ponzoni, Ignacio; Soto, Axel J., Univ Nacl Sur, Dept Comp Sci \\& Engn, Bahia Blanca, Buenos Aires, Argentina. Ulbrich, Pavol; Byska, Jan; Kozlikova, Barbora, Masaryk Univ, Fac Informat, Visitlab, Brno, Czech Republic. Selzer, Matias; Ganuza, Maria Lujan, Univ Nacl Sur, VyGLab Res Lab UNS CICPBA, Dept Comp Sci \\& Engn, Bahia Blanca, Buenos Aires, Argentina. Mican, Jan, Masaryk Univ, Dept Expt Biol, Loschmidt Labs, Brno, Czech Republic. Mican, Jan, Masaryk Univ, RECETOX, Brno, Czech Republic. Mican, Jan, Masaryk Univ, Fac Med, Brno, Czech Republic.", "countries": "Argentina;Republic", "abstract": "In the modern drug discovery process, medicinal chemists deal with the complexity of analysis of large ensembles of candidate molecules. Computational tools, such as dimensionality reduction (DR) and classification, are commonly used to efficiently process the multidimensional space of features. These underlying calculations often hinder interpretability of results and prevent experts from assessing the impact of individual molecular features on the resulting representations. To provide a solution for scrutinizing such complex data, we introduce ChemVA, an interactive application for the visual exploration of large molecular ensembles and their features. Our tool consists of multiple coordinated views: Hexagonal view, Detail view, 3D view, Table view, and a newly proposed Difference view designed for the comparison of DR projections. These views display DR projections combined with biological activity, selected molecular features, and confidence scores for each of these projections. This conjunction of views allows the user to drill down through the dataset and to efficiently select candidate compounds. Our approach was evaluated on two case studies of finding structurally similar ligands with similar binding affinity to a target protein, as well as on an external qualitative evaluation. The results suggest that our system allows effective visual inspection and comparison of different high-dimensional molecular representations. Furthermore, ChemVA assists in the identification of candidate compounds while providing information on the certainty behind different molecular representations.", "keywords": "Virtual screening,visual analysis,dimensionality reduction,coordinated views,cheminformatics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030438", "refList": ["10.1109/tvcg.2008.137", "10.1057/ivs.2009.10", "10.2312/eurovisstar.20151110", "10.1109/eisic.2015.35", "10.1109/pacificvis.2014.44", "10.1145/1142473.1142574", "10.1109/tvcg.2013.223", "10.1109/tvcg.2014.2346573", "10.1109/tvcg.2019.2934539", "10.1111/cgf.13717", "10.1109/vizsec.2009.5375536", "10.1111/cgf.12925", "10.1109/tvcg.2015.2467551", "10.1109/mcg.2015.99", "10.1007/978-3-319", "10.1109/tvcg.2012.255", "10.1145/1064830.1064834", "10.1177/1473871611433713", "10.1207/s1532690xci0804\\_2", "10.1145/1168149.1168168", "10.1016/j.chb.2006.10.002", "10.1109/tvcg.2014.2346441", "10.1109/eisic.2017.15", "10.1111/1467-8721.00160", "10.1109/tvcg.2018.2865024", "10.1109/infvis.2004.2"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030435", "title": "Data Visceralization: Enabling Deeper Understanding of Data Using Virtual Reality", "year": "2020", "conferenceName": "InfoVis", "authors": "Benjamin Lee;Dave Brown;Bongshin Lee;Christophe Hurter;Steven Mark Drucker;Tim Dwyer", "citationCount": "1", "affiliation": "Lee, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Lee, Benjamin; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Brown, Dave; Lee, Bongshin; Drucker, Steven, Microsoft Res, Redmond, WA USA. Hurter, Christophe, French Civil Aviat Univ, ENAC, Toulouse, France.", "countries": "USA;France;Australia", "abstract": "A fundamental part of data visualization is transforming data to map abstract information onto visual attributes. While this abstraction is a powerful basis for data visualization, the connection between the representation and the original underlying data (i.e., what the quantities and measurements actually correspond with in reality) can be lost. On the other hand, virtual reality (VR) is being increasingly used to represent real and abstract models as natural experiences to users. In this work, we explore the potential of using VR to help restore the basic understanding of units and measures that are often abstracted away in data visualization in an approach we call data visceralization. By building VR prototypes as design probes, we identify key themes and factors for data visceralization. We do this first through a critical reflection by the authors, then by involving external participants. We find that data visceralization is an engaging way of understanding the qualitative aspects of physical measures and their real-life form, which complements analytical and quantitative understanding commonly gained from data visualization. However, data visceralization is most effective when there is a one-to-one mapping between data and representation, with transformations such as scaling affecting this understanding. We conclude with a discussion of future directions for data visceralization.", "keywords": "Data visceralization,virtual reality,exploratory study", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030435", "refList": ["10.1080/01973762.2013.761106", "10.1109/tvcg.2015.2467811", "10.1109/tvcg.2012.221", "10.1145/3284179.3284326", "10.1109/tvcg.2019.2934287", "10.1109/2945.841119", "10.1109/tvcg.2019.2934539", "10.1109/mcg.2013.101", "10.1109/tvcg.2011.175", "10.1109/tvcg.2018.2830759", "10.1109/3dvis.2014.7160096", "10.1109/mcg.2018.2878900", "10.1109/iv.2011.32", "10.1109/tvcg.2013.196", "10.1109/tvcg.2018.2865241", "10.1515/abitech-2017-0002", "10.1145/2468356.2468739", "10.16995/olh.280", "10.1080/15230406.2018.1513343", "10.1109/iv.2004.1320189", "10.1109/tvcg.2012.213", "10.1109/mcg.2006.120", "10.1109/icdar.2017.286", "10.1371/journal.pone.0146368", "10.1080/1472586x.2011.548488", "10.1109/tvcg.2014.2346574", "10.1080/0013838x.2017.1332021"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030464", "title": "Designing Narrative-Focused Role-Playing Games for Visualization Literacy in Young Children", "year": "2020", "conferenceName": "InfoVis", "authors": "Elaine Huynh;Angela Nyhout;Patricia Ganea;Fanny Chevalier", "citationCount": "0", "affiliation": "Huynh, E (Corresponding Author), Univ Toronto, Dept Comp Sci, Toronto, ON, Canada. Huynh, Elaine, Univ Toronto, Dept Comp Sci, Toronto, ON, Canada. Nyhout, Angela; Ganea, Patricia, Univ Toronto, Ontario Inst Studies Educ, Toronto, ON, Canada. Chevalier, Fanny, Univ Toronto, Dept Comp Sci \\& Stat Sci, Toronto, ON, Canada.", "countries": "Canada", "abstract": "Building on game design and education research, this paper introduces narrative-focused role-playing games as a way to promote visualization literacy in young children. Visualization literacy skills are vital in understanding the world around us and constructing meaningful visualizations, yet, how to better develop these skills at an early age remains largely overlooked and understudied. Only recently has the visualization community started to fill this gap, resulting in preliminary studies and development of educational tools for use in early education. We add to these efforts through the exploration of gamification to support learning, and identify an opportunity to apply role-playing game-based designs by leveraging the presence of narratives in data-related problems involving visualizations. We study the effects of including narrative elements on learning through a technology probe, grounded in a set of design considerations stemming from visualization, game design and education science. We create two versions of a game - one with narrative elements and one without - and evaluate our instances on 33 child participants between 11- to 13-years old using a between-subjects study design. Despite participants requiring double the amount of time to complete their game due to additional narrative elements, the inclusion of such elements were found to improve engagement without sacrificing learning; our results indicate no significant differences in development of graph-reading skills, but significant differences in engagement and overall enjoyment of the game. We report observations and qualitative feedback collected, and note areas for improvement and room for future work.", "keywords": "Visualization Literacy,Educational technology,Gamification,Narrative", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030464", "refList": ["10.1007/978-981-13-2694-3\\_2", "10.1145/2702123.2702298", "10.1145/2702123.2702558", "10.1007/978-981-13-2694-3\\_8", "10.1145/2702123.2702245", "10.1109/mis.2012.27", "10.18061/dsq.v21i4.318", "10.1109/tvcg.2013.134", "10.1109/vl.1996.545307", "10.1007/s10708-008-9186-0", "10.1093/cje/ben057", "10.5210/fm.v16i2.3316", "10.1109/tvcg.2019.2934539", "10.1109/tvcg.2016.2598608", "10.1007/978-3-319-94659-7\\_10", "10.1109/mcg.2013.28", "10.17351/ests2017.134", "10.1109/pacificvis.2014.39", "10.1145/3173574.3173728", "10.1145/2598784.2598806", "10.1145/2491500.2491501", "10.1145/1993060.1993065", "10.1109/tvcg.2018.2802520", "10.1145/3025453.3025667", "10.1145/2598510.2598566", "10.1080/15710882.2015.1081240", "10.17351/ests2017.133", "10.1109/tvcg.2014.2346431", "10.1016/j.ijhcs.2015.02.005", "10.1145/2702123.2702180", "10.1109/tvcg.2007.70577", "10.1109/mcg.2019.2923483", "10.5931/djim.v12.i1.6449", "10.1145/3240167.3240206", "10.1145/2468356.2468739", "10.1109/tvcg.2012.213", "10.1145/3025453.3025751", "10.4018/978-1-4666-6497-5.ch003"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030355", "title": "Guidelines For Pursuing and Revealing Data Abstractions", "year": "2020", "conferenceName": "InfoVis", "authors": "Alex Bigelow;Katy Williams;Katherine E. Isaacs", "citationCount": "0", "affiliation": "Bigelow, A (Corresponding Author), Univ Arizona, Tucson, AZ 85721 USA. Bigelow, Alex; Williams, Katy; Isaacs, Katherine E., Univ Arizona, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "Many data abstraction types, such as networks or set relationships, remain unfamiliar to data workers beyond the visualization research community. We conduct a survey and series of interviews about how people describe their data, either directly or indirectly. We refer to the latter as latent data abstractions. We conduct a Grounded Theory analysis that (1) interprets the extent to which latent data abstractions exist, (2) reveals the far-reaching effects that the interventionist pursuit of such abstractions can have on data workers, (3) describes why and when data workers may resist such explorations, and (4) suggests how to take advantage of opportunities and mitigate risks through transparency about visualization research perspectives and agendas. We then use the themes and codes discovered in the Grounded Theory analysis to develop guidelines for data abstraction in visualization projects. To continue the discussion, we make our dataset open along with a visual interface for further exploration.", "keywords": "Data abstraction,Grounded theory,Survey design,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030355", "refList": ["10.1080/2159676x.2016.1251701", "10.1109/infvis.2000.885092", "10.1145/2702123.2702298", "10.4135/9781848607941.n14", "10.1007/978-1-4939", "10.1109/tvcg.2014.2346331", "10.1109/tvcg.2017.2744843", "10.1177/1473871613510429", "10.1007/978-1-4939-0378-8\\_2", "10.1145/2598153.2598175", "10.1109/tvcg.2019.2934285", "10.1177/1473871613488591", "10.1145/2501105.2501106", "10.1109/tvcg.2019.2934538", "10.1109/tvcg.2019.2934539", "10.1017/s1049096510990781", "10.1145/3025453.3025837", "10.1145/3290605.3300474", "10.1145/3290605.3300356", "10.1002/nur.1025", "10.1145/2993901.2993916", "10.1145/3392826", "10.1086/269268", "10.1109/tvcg.2018.2865241", "10.1145/2998181.2998331", "10.1145/291224.291229", "10.1057/ivs.2009.13", "10.1145/2047196.2047205", "10.1109/tvcg.2012.213", "10.1145/3274405", "10.1109/tvcg.2013.145", "10.1016/0040-6031(92)85160-w", "10.1109/iv.2013.45", "10.1109/tvcg.2009.111", "10.1109/mcg.2019.2914844", "10.1109/tvcg.2009.116"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030405", "title": "Insights From Experiments With Rigor in an EvoBio Design Study", "year": "2020", "conferenceName": "InfoVis", "authors": "Jennifer Rogers;Austin H. Patton;Luke Harmon;Alexander Lex;Miriah D. Meyer", "citationCount": "0", "affiliation": "Rogers, J (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Rogers, Jen; Lex, Alexander; Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Patton, Austin H., Washington State Univ, Pullman, WA 99164 USA. Harmon, Luke, Univ Idaho, Moscow, ID 83843 USA.", "countries": "USA", "abstract": "Design study is an established approach of conducting problem-driven visualization research. The academic visualization community has produced a large body of work for reporting on design studies, informed by a handful of theoretical frameworks, and applied to a broad range of application areas. The result is an abundance of reported insights into visualization design, with an emphasis on novel visualization techniques and systems as the primary contribution of these studies. In recent work we proposed a new, interpretivist perspective on design study and six companion criteria for rigor that highlight the opportunities for researchers to contribute knowledge that extends beyond visualization idioms and software. In this work we conducted a year-long collaboration with evolutionary biologists to develop an interactive tool for visual exploration of multivariate datasets and phylogenetic trees. During this design study we experimented with methods to support three of the rigor criteria: ABUNDANT, REFLEXIVE, and TRANSPARENT. As a result we contribute two novel visualization techniques for the analysis of multivariate phylogenetic datasets, three methodological recommendations for conducting design studies drawn from reflections over our process of experimentation, and two writing devices for reporting interpretivist design study. We offer this work as an example for implementing the rigor criteria to produce a diverse range of knowledge contributions.", "keywords": "Methodologies,Application Motivated Visualization,Guidelines,Life Sciences Visualization,Health,Medicine,Biology,Bioinformatics,Genomics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030405", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1080/17493460802276893", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1109/tvcg.2012.213", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030462", "title": "Table Scraps: An Actionable Framework for Multi-Table Data Wrangling From An Artifact Study of Computational Journalism", "year": "2020", "conferenceName": "InfoVis", "authors": "Stephen Kasica;Charles Berret;Tamara Munzner", "citationCount": "0", "affiliation": "Kasica, S (Corresponding Author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada. Kasica, Stephen; Munzner, Tamara, Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada. Berret, Charles, Univ British Columbia, Sch Journalism Writing \\& Media, Vancouver, BC, Canada.", "countries": "Canada", "abstract": "For the many journalists who use data and computation to report the news, data wrangling is an integral part of their work. Despite an abundance of literature on data wrangling in the context of enterprise data analysis, little is known about the specific operations, processes, and pain points journalists encounter while performing this tedious, time-consuming task. To better understand the needs of this user group, we conduct a technical observation study of 50 public repositories of data and analysis code authored by 33 professional journalists at 26 news organizations. We develop two detailed and cross-cutting taxonomies of data wrangling in computational journalism, for actions and for processes. We observe the extensive use of multiple tables, a notable gap in previous wrangling analyses. We develop a concise, actionable framework for general multi-table data wrangling that includes wrangling operations documented in our taxonomy that are without clear parallels in other work. This framework, the first to incorporate tables as first-class objects, will support future interactive wrangling tools for both computational journalism and general-purpose use. We assess the generative and descriptive power of our framework through discussion of its relationship to our set of taxonomies.", "keywords": "Computational journalism,Data journalism,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030462", "refList": ["10.1145/1378773.1378792", "10.1109/tvcg.2012.219", "10.1109/vast47406.2019.8986909", "10.1145/1084805.1084812", "10.1007/s00778-008-0098-x", "10.1016/j.websem.2008.09.005", "10.18637/jss.v040.i01", "10.1145/989863.989865", "10.1109/tvcg.2015.2467551", "10.5281/zenodo.3509134", "10.1109/tvcg.2019.2934539", "10.1109/tvcg.2019.2934593", "10.1109/tse.2018.2796554", "10.17349/jmc117309", "10.1109/2945.981851", "10.1109/vast.2011.6102440", "10.1177/1473871611415994", "10.1145/2001269.2001288"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13964", "year": "2020", "title": "Reading Traces: Scalable Exploration in Elastic Visualizations of Cultural Heritage Data", "conferenceName": "EuroVis", "authors": "Mark{-}Jan Bludau;Viktoria Br{\\\"{u}}ggemann;Anna Busch;Marian D{\\\"{o}}rk", "citationCount": "1", "affiliation": "Bludau, MJ (Corresponding Author), Univ Appl Sci Potsdam, UCLAB, Potsdam, Germany.\nBludau, M. -J.; Brueggemann, V.; Doerk, M., Univ Appl Sci Potsdam, UCLAB, Potsdam, Germany.\nBusch, A., Univ Potsdam, Theodor Fontane Archiv, Potsdam, Germany.", "countries": "Germany", "abstract": "Through a design study, we develop an approach to data exploration that utilizes elastic visualizations designed to support varying degrees of detail and abstraction. Examining the notions of scalability and elasticity in interactive visualizations, we introduce a visualization of personal reading traces such as marginalia or markings inside the reference library of German realist author Theodor Fontane. To explore such a rich and extensive collection, meaningful visual forms of abstraction and detail are as important as the transitions between those states. Following a growing research interest in the role of fluid interactivity and animations between views, we are particularly interested in the potential of carefully designed transitions and consistent representations across scales. The resulting prototype addresses humanistic research questions about the interplay of distant and close reading with visualization research on continuous navigation along several granularity levels, using scrolling as one of the main interaction mechanisms. In addition to presenting the design process and resulting prototype, we present findings from a qualitative evaluation of the tool, which suggest that bridging between distant and close views can enhance exploration, but that transitions between views need to be crafted very carefully to facilitate comprehension.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13964", "refList": ["10.1007/s41244-017-0048-4", "10.1109/tvcg.2009.108", "10.1145/1456650.1456652", "10.1109/tvcg.2014.2346424", "10.1177/1473871611416549", "10.1111/cgf.13195", "10.1145/2207676.2208607", "10.1145/1376616.1376618", "10.1006/ijhc.2002.1017", "10.1109/tvcg.2011.185", "10.1177/1473871611413180", "10.1109/vl.1996.545307", "10.1006/ijhc.1017", "10.1109/tvcg.2019.2934539", "10.1109/tvcg.2018.2830759", "10.1145/1556262.1556300", "10.1109/tvcg.2014.2346677", "10.1145/2909132.2909255", "10.1109/tvcg.2007.70539", "10.1145/2396636.2396675", "10.1145/1978942.1979124", "10.1016/j.ijhcs.2003.08.005", "10.2312/eurovisstar.20151113", "10.1109/infvis.2005.1532127"], "wos": 1, "children": [], "len": 1}], "len": 17}, {"doi": "10.1109/tvcg.2019.2934283", "title": "What is Interaction for Data Visualization?", "year": "2019", "conferenceName": "InfoVis", "authors": "Evanthia Dimara;Charles Perin", "citationCount": "1", "affiliation": "Dimara, E (Corresponding Author), Sorbonne Univ, Paris, France. Dimara, Evanthia, Sorbonne Univ, Paris, France. Perin, Charles, Univ Victoria, Victoria, BC, Canada.", "countries": "Canada;France", "abstract": "Interaction is fundamental to data visualization, but what \u201cinteraction\u201d means in the context of visualization is ambiguous and confusing. We argue that this confusion is due to a lack of consensual definition. To tackle this problem, we start by synthesizing an inclusive view of interaction in the visualization community \u2013 including insights from information visualization, visual analytics and scientific visualization, as well as the input of both senior and junior visualization researchers. Once this view takes shape, we look at how interaction is defined in the field of human-computer interaction (HCI). By extracting commonalities and differences between the views of interaction in visualization and in HCI, we synthesize a definition of interaction for visualization. Our definition is meant to be a thinking tool and inspire novel and bolder interaction design practices. We hope that by better understanding what interaction in visualization is and what it can be, we will enrich the quality of interaction in visualization systems and empower those who use them.", "keywords": "interaction,visualization,data,definition,human-computer interaction", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934283", "refList": ["10.1057/ivs.2009.22", "10.1515/icom-2017-0027", "10.1145/2493102.2493104", "10.1007/978-3-319-06793-3\\_6", "10.1080/03640210801898177", "10.1109/mcg.2010.30", "10.1109/tvcg.2013.134", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2018.2865237", "10.1109/2945.981847", "10.1145/2598784.2598806", "10.1109/tvcg.2013.130", "10.1109/tvcg.2007.70577", "10.1109/mic.2015.129", "10.1145/2909132.2909267", "10.1080/01449290500330331", "10.1109/tvcg.2018.2865233", "10.1109/tvcg.2015.2467831", "10.1109/iv.2015.34", "10.1109/tvcg.2009.111", "10.1145/3290605.3300565", "10.1145/3173574.3173797", "10.1145/948496.948514", "10.1145/3025453.3025765", "10.1109/tvcg.2015.2467613", "10.1145/2659796", "10.1109/tvcg.2014.2346311", "10.1145/3025453.3025524", "10.1080/17452759.2011.558588", "10.1145/3027063.3053113", "10.1109/infvis.2005.1532136", "10.1145/2470654.2481307", "10.1109/tvcg.2018.2864913", "10.1145/345513.345267", "10.3102/00028312005004437", "10.1109/tvcg.2007.70436", "10.1037/0033-295x.106.4.643", "10.1145/2133416.2146416", "10.1109/tvcg.2013.191", "10.1109/tvcg.2010.177", "10.1145/960201.957206", "10.1109/tvcg.2007.70515", "10.1109/infvis.2000.885092", "10.1145/2636240.2636844", "10.1037/h0055392", "10.1177/1473871611413180", "10.1109/vl.1996.545307", "10.1111/j.1471-1842.2009.00848.x", "10.1145/989863.989865", "10.1109/tvcg.2013.124", "10.1109/tvcg.2008.109", "10.1145/1166253.1166265", "10.1145/2702123.2702180", "10.1109/tvcg.2016.2598620", "10.1080/07370024.2016.1226139", "10.1145/3173574.3173909", "10.1109/pacificvis.2010.5429613", "10.1111/j.1467-6478.2006.00368.x", "10.1109/tvcg.2012.204", "10.1109/tvcg.2013.120", "10.1179/1743277412y.0000000019", "10.1145/1936652.1936684", "10.2307/1269768", "10.1109/infvis.1996.559213", "10.1109/tvcg.2016.2598839", "10.1145/2642918.2647360", "10.1057/palgrave.ivs.9500099", "10.1016/j.cag.2009.06.004", "10.1109/mc.2013.178", "10.1109/tvcg.2007.70541", "10.1109/vast.2011.6102473", "10.1145/358886.358895", "10.1109/tvcg.2014.2346573", "10.1109/tvcg.2018.2865159", "10.1145/2207676.2207741", "10.1109/tvcg.2015.2396062", "10.1145/2207676.2208572", "10.1109/tvcg.2016.2598608", "10.1057/ivs.2008.31", "10.1177/001316446002000104", "10.1109/tvcg.2017.2680452", "10.1109/tvcg.2006.80", "10.1145/2598510.2598566", "10.1037/0003-066x.51.4.355", "10.7146/dpb.v16i224.7586", "10.1109/infvis.1998.729560", "10.1162/leon\\_a\\_00011", "10.1109/tvcg.2010.157", "10.1109/tvcg.2014.2359887"], "wos": 1, "children": [], "len": 1}], "len": 21}, {"doi": "10.1109/tvcg.2018.2865025", "title": "An Information-Theoretic Approach to the Cost-benefit Analysis of Visualization in Virtual Environments", "year": "2018", "conferenceName": "VAST", "authors": "Min Chen;Kelly P. Gaither;Nigel W. John;Brian McCann", "citationCount": "1", "affiliation": "Chen, M (Corresponding Author), Univ Oxford, Oxford, England. Chen, Min, Univ Oxford, Oxford, England. Gaither, Kelly; McCann, Brian, Univ Texas Austin, Austin, TX 78712 USA. John, Nigel W., Univ Chester, Chester, Cheshire, England.", "countries": "USA;England", "abstract": "Visualization and virtual environments (VEs) have been two interconnected parallel strands in visual computing for decades. Some VEs have been purposely developed for visualization applications, while many visualization applications are exemplary showcases in general-purpose VEs. Because of the development and operation costs of VEs, the majority of visualization applications in practice have yet to benefit from the capacity of VEs. In this paper, we examine this status quo from an information-theoretic perspective. Our objectives are to conduct cost-benefit analysis on typical VE systems (including augmented and mixed reality, theater-based systems, and large powerwalls), to explain why some visualization applications benefit more from VEs than others, and to sketch out pathways for the future development of visualization applications in VEs. We support our theoretical propositions and analysis using theories and discoveries in the literature of cognitive sciences and the practical evidence reported in the literatures of visualization and VEs.", "keywords": "Theory of visualization,virtual environments,four levels of visualization,virtual reality,augmented reality,mixed reality,cost-benefit analysis,information theory,cognitive sciences,visualization applications,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865025", "refList": ["10.1109/tvcg.2011.231", "10.1007/s00268-007-9307-9", "10.1162/pres.1995.4.1.64", "10.1038/81497", "10.1523/jneurosci.0647-08.2008", "10.1037/a0029856", "10.1038/nature03390", "10.1016/s1364-6613(97)01080-2", "10.1111/cgf.13169", "10.1364/josaa.20.001419", "10.1109/tvcg.2013.127", "10.1037/0033-295x.101.2.343", "10.1109/tvcg.2012.42", "10.1007/s002210100745", "10.1109/vl.1996.545307", "10.1016/s0959-4388(98)80140-2", "10.1007/s00221-006-0804-0", "10.1016/0001-6918(67)90080-7", "10.1523/jneurosci.4319-03.2004", "10.1037/0096-3445.109.2.160", "10.1145/2330667.2330687", "10.1109/tvcg.2010.132", "10.1037/0033-295x.113.4.766", "10.1145/22949.22950", "10.1002/j.1538-7305.1948.tb00917.x", "10.1109/jdt.2008.2001575", "10.1109/tvcg.2008.142", "10.1016/s0079-6123(06)55002-2", "10.1109/mcg.2018.032421653", "10.1038/17953", "10.1162/105474602760204309", "10.1145/253284.253301", "10.1007/978-3-662-43790-2\\_6", "10.1038/nn963", "10.1067/mob.2002.127361", "10.1037/a0033101", "10.1109/ldav.2012.6378981", "10.1113/jphysiol.1964.sp007485", "10.1016/j.tics.2005.02.009", "10.1146/annurev.ne.18.030195.001205", "10.1109/infvis.2004.59", "10.1111/j.1460-2466.1992.tb00812.x", "10.1089/109493101300117884", "10.1109/tvcg.2010.131", "10.1016/s0896-6273(02)01003-6", "10.1016/s0042-6989(01)00102-x", "10.1016/0010-0285(80)90005-5", "10.1109/mcg.2014.18", "10.1109/38.963459", "10.1109/tvcg.2012.133", "10.1007/978-1-4899-5379-7\\_8", "10.1145/2556288.2557020", "10.1109/mcg.2013.37", "10.1109/tvcg.2014.20", "10.3758/bf03200774", "10.1097/sla.0b013e318288c40b", "10.1016/j.cub.2009.12.014", "10.1016/j.cag.2012.04.007", "10.1109/mcg.2014.80", "10.1111/j.1467-8659.2012.03114.x", "10.1111/j.1600-0412.2012.01482.x", "10.1109/tvcg.2006.184", "10.1007/s11548-013-0929-0", "10.1007/978-1-4899-5379-7", "10.1016/0042-6989(84)90041-5", "10.1109/tvcg.2014.2346325", "10.1145/2702123.2702406", "10.1162/pres.1992.1.1.120", "10.1038/36846", "10.1167/7.5.6", "10.1146/annurev.psych.53.100901.135125", "10.1145/1128923.1128948"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934264", "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;David S. Ebert;Arif Ghafoor", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Karimzadeh, Morteza; Ebert, David S.; Ghafoor, Arif, Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.", "keywords": "Summative evaluation,usefulness,evaluation process,taxonomy,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934264", "refList": ["10.1109/tvcg.2017.2744478", "10.1109/tvcg.2018.2865025", "10.1109/tvcg.2006.85", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2017.2745181", "10.1111/cgf.13677", "10.1109/tvcg.2018.2864844", "10.1109/tvcg.2013.126", "10.1109/tvcg.2018.2864811", "10.1109/infvis.2005.1532147", "10.1177/0956797613504966", "10.1145/2669557.2669579", "10.1109/mcg.2005.102", "10.1109/visual.2003.1250426", "10.1136/bmj.39489.470347.ad", "10.1109/tvcg.2017.2744080", "10.1109/mcg.2009.53", "10.1111/j.1467-8527.2005.00307.x", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2864886", "10.1109/tvcg.2018.2864843", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2018.2865051", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2018.2865026", "10.1007/978-3-540-71080-6\\_6", "10.1109/tvcg.2018.2865020", "10.1177/1473871611407399", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2018.2864526", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2864905", "10.1049/sej.1991.0040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2711030", "10.1109/tvcg.2011.279", "10.1109/vast.2017.8585505", "10.1147/jrd.2010.2042914", "10.1016/s0378-7206(98)00044-5", "10.1145/2993901.2993913", "10.1109/tvcg.2018.2865041", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744758", "10.1145/1168149.1168158", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2012.213", "10.1109/tvcg.2017.2744738", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2018.2864826", "10.1145/1377966.1377974", "10.1109/apec.2009.4802646", "10.1145/1168149.1168152", "10.1016/j.jss.2008.03.059", "10.1109/vast.2017.8585484", "10.1109/tvcg.2017.2744818", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2865042", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030388", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "year": "2020", "conferenceName": "SciVis", "authors": "Pepe Eulzer;Sabine Bauer;Francis Kilian;Kai Lawonn", "citationCount": "0", "affiliation": "Eulzer, P (Corresponding Author), Univ Jena, Jena, Germany. Eulzer, Pepe; Lawonn, Kai, Univ Jena, Jena, Germany. Bauer, Sabine, Univ Koblenz Landau, Koblenz, Germany. Kilian, Francis, Cath Clin Koblenz Montabaur, Dept Spine Surg, Koblenz, Germany.", "countries": "Germany", "abstract": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. $\\mathrm{By}$ linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. $\\mathrm{in}$ a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.", "keywords": "Medical visualization,bioinformatics,coordinated views,focus and context,biomechanical simulation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030388", "refList": ["10.1109/tvcg.2018.2864903", "10.1177/1473871613510429", "10.1093/ehjqcco/qcz052", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2018.2865076", "10.1055/s-0039-1687862", "10.1109/visual.1990.146375", "10.1109/tvcg.2017.2744198", "10.1016/j.ijmedinf.2014.10.001", "10.1109/tvcg.2013.124", "10.1016/j.jacc", "10.1111/cgf.13167", "10.17705/1thci.00055", "10.1136/bmjqs.2009.037895", "10.1109/tvcg.2013.238", "10.1109/tvcg.2018.2865240", "10.1186/1471-2261-6-34", "10.1109/tvcg.2019.2934264", "10.1109/tvcg.2013.200", "10.1109/tvcg.2011.209", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467091", "10.1136/bmjopen-2019-033208", "10.1109/beliv.2018.8634027", "10.1109/tvcg.2012.213", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2015.2467325", "10.1145/2133806.2133821", "10.1145/1806799.1806866", "10.1108/02635570610688869", "10.1002/hbm.20701", "10.1561/1100000039", "10.1145/3025453.3025645", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 5}], "len": 35}], "len": 105}, "index": 603, "embedding": [1.7700388431549072, 2.6685378551483154, -1.0222129821777344, -2.389355182647705, -0.630656361579895, 0.16650904715061188, -0.7150075435638428, 3.702998399734497, -0.4060705304145813, 4.204902172088623, -0.10305803269147873, 1.238906741142273, -0.1321265697479248, 0.5245018601417542, 1.2753777503967285, 4.231391429901123, 1.1034841537475586, 8.908039093017578, -0.6329802870750427, 4.108813762664795, -0.06630208343267441, 4.9261698722839355, 1.1372828483581543, 4.570624828338623, -1.9411299228668213, 4.072731971740723, -0.5019459128379822, 2.189319372177124, 3.4911305904388428, 3.1686134338378906, 0.5867303013801575, 3.5114214420318604], "projection": [-0.19553343951702118, 11.29512882232666], "size": 53, "height": 5, "width": 17}