{"data": {"doi": "10.1109/tvcg.2016.2598608", "title": "Embedded Data Representations", "year": "2016", "conferenceName": "InfoVis", "authors": "Wesley Willett;Yvonne Jansen;Pierre Dragicevic", "citationCount": "36", "affiliation": "Willett, W (Corresponding Author), Univ Calgary, Calgary, AB T2N 1N4, Canada. Willett, Wesley, Univ Calgary, Calgary, AB T2N 1N4, Canada. Jansen, Yvonne, Univ Copenhagen, DK-1168 Copenhagen, Denmark. Dragicevic, Pierre, INRIA, Rocquencourt, France.", "countries": "Canada;France;Denmark", "abstract": "We introduce embedded data representations, the use of visual and physical representations of data that are deeply integrated with the physical spaces, objects, and entities to which the data refers. Technologies like lightweight wireless displays, mixed reality hardware, and autonomous vehicles are making it increasingly easier to display data in-context. While researchers and artists have already begun to create embedded data representations, the benefits, trade-offs, and even the language necessary to describe and compare these approaches remain unexplored. In this paper, we formalize the notion of physical data referents - the real-world entities and spaces to which data corresponds - and examine the relationship between referents and the visual and physical representations of their data. We differentiate situated representations, which display data in proximity to data referents, and embedded representations, which display data so that it spatially coincides with data referents. Drawing on examples from visualization, ubiquitous computing, and art, we explore the role of spatial indirection, scale, and interaction for embedded representations. We also examine the tradeoffs between non-situated, situated, and embedded data displays, including both visualizations and physicalizations. Based on our observations, we identify a variety of design challenges for embedded data representation, and suggest opportunities for future research and applications.", "keywords": "augmented reality;Information visualization;data physicalization;ambient displays;ubiquitous computing", "link": "http://dx.doi.org/10.1109/TVCG.2016.2598608", "refList": ["10.1109/ie.2012.9", "10.1145/2702123.2702263", "10.1145/2682884.2682886", "10.1145/1013115.1013171", "10.1109/vr.2015.7223352", "10.1109/tpami.2002.1017615", "10.1109/tvcg.2013.134", "10.1145/2858036.2858059", "10.1109/iv.2008.84", "10.1155/2011/721827", "10.1016/j.cag.2013.10.003", "10.1145/1133265.1133277", "10.1145/1349026.1349037", "10.1145/2807442.2807488", "10.1145/1015706.1015738", "10.1016/j.comnet.2010.05.010", "10.1109/iswc.1997.629931", "10.1080/10929080601169930", "10.1145/634067.634135", "10.1162/105474602317343677", "10.1145/2702123.2702180", "10.1016/j.str.2005.01.009", "10.1109/infvis.1998.729560", "10.1145/2814347.2814350", "10.1145/2491568.2491597", "10.1080/10630732.2012.698065", "10.1007/s10514-013-9349-9", "10.1145/2470654.2481359", "10.1145/1376616.1376772"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865152", "title": "DXR: A Toolkit for Building Immersive Data Visualizations", "year": "2018", "conferenceName": "InfoVis", "authors": "Ronell Sicat;Jiabao Li;Junyoung Choi;Maxime Cordeil;Won-Ki Jeong;Benjamin Bach;Hanspeter Pfister", "citationCount": "12", "affiliation": "Sicat, R (Corresponding Author), Harvard Visual Comp Grp, Cambridge, MA USA. Sicat, Ronell; Pfister, Hanspeter, Harvard Visual Comp Grp, Cambridge, MA USA. Li, Jiabao, Harvard Grad Sch Design, Cambridge, MA USA. Bach, Benjamin, Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland. Cordeil, Maxime, Monash Univ, Immers Analyt Lab, Clayton, Vic, Australia. Choi, JunYoung; Jeong, Won-Ki, Ulsan Natl Inst Sci \\& Technol, Ulsan, South Korea.", "countries": "Scotland;USA;Korea;Australia", "abstract": "This paper presents DXR, a toolkit for building immersive data visualizations based on the Unity development platform. Over the past years, immersive data visualizations in augmented and virtual reality (AR, VR) have been emerging as a promising medium for data sense-making beyond the desktop. However, creating immersive visualizations remains challenging, and often require complex low-level programming and tedious manual encoding of data attributes to geometric and visual properties. These can hinder the iterative idea-to-prototype process, especially for developers without experience in 3D graphics, AR, and VR programming. With DXR, developers can efficiently specify visualization designs using a concise declarative visualization grammar inspired by Vega-Lite. DXR further provides a GUI for easy and quick edits and previews of visualization designs in-situ, i.e., while immersed in the virtual world. DXR also provides reusable templates and customizable graphical marks, enabling unique and engaging visualizations. We demonstrate the flexibility of DXR through several examples spanning a wide range of applications.", "keywords": "Augmented Reality,Virtual Reality,Immersive Visualization,Immersive Analytics,Visualization Toolkit", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865152", "refList": ["10.1145/2642918.2647369", "10.1109/tvcg.2015.2467449", "10.1109/tvcg.2011.185", "10.1145/2598153.2598175", "10.1109/tvcg.2016.2598609", "10.1145/1980462.1980470", "10.1109/tvcg.2014.2346318", "10.1145/3009939.3009947", "10.2312/conf/eg2013/stars/039-063", "10.1145/3013971.3014006", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2016.2599107", "10.1145/3173574.3173664", "10.1109/tvcg.2017.2745941", "10.1145/2556288.2557010", "10.1145/3126594.3126613", "10.1016/j.visinf.2017.11.002", "10.1111/cgf.12391", "10.1109/tvcg.2015.2467091", "10.1109/bigdata.2014.7004282", "10.1109/bdva.2015.7314302", "10.1109/tvcg.2010.144", "10.1109/tvcg.2009.174", "10.1109/infvis.2004.64", "10.1145/2133416.2146416", "10.1109/tvcg.2014.2346322", "10.1109/tvcg.2017.2744079", "10.1111/cgf.12804", "10.1371/journal.pone.0057990", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934788", "title": "Data by Proxy \u2014 Material Traces as Autographic Visualizations", "year": "2019", "conferenceName": "InfoVis", "authors": "Dietmar Offenhuber", "citationCount": "3", "affiliation": "Offenhuber, D (Corresponding Author), Northeastern Univ, Boston, MA 02115 USA. Offenhuber, Dietmar, Northeastern Univ, Boston, MA 02115 USA.", "countries": "USA", "abstract": "Information visualization limits itself, per definition, to the domain of symbolic information. This paper discusses arguments why the field should also consider forms of data that are not symbolically encoded, including physical traces and material indicators. Continuing a provocation presented by Pat Hanrahan in his 2004 IEEE Vis capstone address, this paper compares physical traces to visualizations and describes the techniques and visual practices for producing, revealing, and interpreting them. By contrasting information visualization with a speculative counter model of autographic visualization, this paper examines the design principles for material data. Autographic visualization addresses limitations of information visualization, such as the inability to directly reflect the material circumstances of data generation. The comparison between the two models allows probing the epistemic assumptions behind information visualization and uncovers linkages with the rich history of scientific visualization and trace reading. The paper begins by discussing the gap between data visualizations and their corresponding phenomena and proceeds by investigating how material visualizations can bridge this gap. It contextualizes autographic visualization with paradigms such as data physicalization and indexical visualization and grounds it in the broader theoretical literature of semiotics, science and technology studies (STS), and the history of scientific representation. The main section of the paper proposes a foundational design vocabulary for autographic visualization and offers examples of how citizen scientists already use autographic principles in their displays, which seem to violate the canonical principles of information visualization but succeed at fulfilling other rhetorical purposes in evidence construction. The paper concludes with a discussion of the limitations of autographic visualization, a roadmap for the empirical investigation of trace perception, and thoughts about how information visualization and autographic visualization techniques can contribute to each other.", "keywords": "Traces,indexicality,data physicalization,proxy data sources,data materiality", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934788", "refList": ["10.1002/ad.2135", "10.1038/scientificamerican0991-94", "10.1177/030631285015001002", "10.1145/2998181.2998239", "10.1109/visual.2004.12", "10.1109/vl.1996.545307", "10.1145/258549.258715", "10.1145/3027063.3053165", "10.1063/1.881071", "10.1145/258519.258715", "10.1002/ad.1710", "10.1109/tvcg.2016.2598608", "10.1126/science.aag0822", "10.1093/acprof:oso/9780199671533.003.0005", "10.2307/2981473", "10.1524/9783486755183", "10.17351/ests2017.123", "10.1145/2702123.2702180", "10.7551/mitpress/9780262525381.001.0001", "10.1177/0309132507077082", "10.1038/nature05058", "10.1109/infvis.1997.636792", "10.1177/007327531104900306", "10.1198/jcgs.2009.07098"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934282", "title": "Designing for Mobile and Immersive Visual Analytics in the Field", "year": "2019", "conferenceName": "InfoVis", "authors": "Matt Whitlock;Keke Wu;Danielle Albers Szafir", "citationCount": "0", "affiliation": "Whitlock, M (Corresponding Author), Univ Colorado, Boulder, CO 80309 USA. Whitlock, Matt; Wu, Keke; Szafir, Danielle, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Data collection and analysis in the field is critical for operations in domains such as environmental science and public safety. However, field workers currently face data- and platform-oriented issues in efficient data collection and analysis in the field, such as limited connectivity, screen space, and attentional resources. In this paper, we explore how visual analytics tools might transform field practices by more deeply integrating data into these operations. We use a design probe coupling mobile, cloud, and immersive analytics components to guide interviews with ten experts from five domains to explore how visual analytics could support data collection and analysis needs in the field. The results identify shortcomings of current approaches and target scenarios and design considerations for future field analysis systems. We embody these findings in FieldView, an extensible, open-source prototype designed to support critical use cases for situated field analysis. Our findings suggest the potential for integrating mobile and immersive technologies to enhance data's utility for various field operations and new directions for visual analytics tools to transform fieldwork.", "keywords": "Immersive Analytics,Augmented Reality,Mobile Visualization,Outdoor Visualization,Emergency Response", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934282", "refList": ["10.1007/978-3-319-45853-3\\_15", "10.1145/355324.355329", "10.1186/1472-6947-9-51", "10.1007/978-3-540-69878-4\\_12", "10.1109/tvcg.2018.2868584", "10.1007/978-3-319-45853-315", "10.1007/s10606-015-9235-4", "10.1109/futuretech.2010.5482712", "10.1016/s0001-4575(99)00018-4", "10.1130/ges00503.1", "10.1109/tvcg.2017.2744019", "10.1145/3173574.3173593", "10.1109/mcom.2011.6069707", "10.1145/2750858.2804266", "10.1145/2785830.2785876", "10.1109/tvcg.2018.2865234", "10.1016/s0097-8493(01)00090-5", "10.1109/tvcg.2016.2598608", "10.1145/2851581.2892322", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2017.2745941", "10.1518/155534308x284381", "10.1057/palgrave.ivs.9500168", "10.1144/0016-764905-017", "10.3390/mti1040029", "10.1007/978-3-319-40397-7\\_2", "10.1109/mc.2006.109", "10.1145/2535597.2535608", "10.1109/mcom.2010.5560598", "10.1518/001872095779049543", "10.1145/3025453.3025860", "10.1145/3025453.3025752", "10.1016/b978-0-12-809477-8.00009-1", "10.1126/science.aag2579", "10.1109/mc.2013.147", "10.1145/1050491.1050499", "10.1186/s41039-016-0028-2", "10.1007/978-3-030-01388-22", "10.1002/esp.1417", "10.1177/0193841x02250527", "10.1111/1467-9671.00157", "10.1007/s11069-017-2929-9", "10.1109/tvcg.2018.2864914", "10.1016/j.autcon.2012.09.002"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934283", "title": "What is Interaction for Data Visualization?", "year": "2019", "conferenceName": "InfoVis", "authors": "Evanthia Dimara;Charles Perin", "citationCount": "1", "affiliation": "Dimara, E (Corresponding Author), Sorbonne Univ, Paris, France. Dimara, Evanthia, Sorbonne Univ, Paris, France. Perin, Charles, Univ Victoria, Victoria, BC, Canada.", "countries": "Canada;France", "abstract": "Interaction is fundamental to data visualization, but what \u201cinteraction\u201d means in the context of visualization is ambiguous and confusing. We argue that this confusion is due to a lack of consensual definition. To tackle this problem, we start by synthesizing an inclusive view of interaction in the visualization community \u2013 including insights from information visualization, visual analytics and scientific visualization, as well as the input of both senior and junior visualization researchers. Once this view takes shape, we look at how interaction is defined in the field of human-computer interaction (HCI). By extracting commonalities and differences between the views of interaction in visualization and in HCI, we synthesize a definition of interaction for visualization. Our definition is meant to be a thinking tool and inspire novel and bolder interaction design practices. We hope that by better understanding what interaction in visualization is and what it can be, we will enrich the quality of interaction in visualization systems and empower those who use them.", "keywords": "interaction,visualization,data,definition,human-computer interaction", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934283", "refList": ["10.1057/ivs.2009.22", "10.1515/icom-2017-0027", "10.1145/2493102.2493104", "10.1007/978-3-319-06793-3\\_6", "10.1080/03640210801898177", "10.1109/mcg.2010.30", "10.1109/tvcg.2013.134", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2018.2865237", "10.1109/2945.981847", "10.1145/2598784.2598806", "10.1109/tvcg.2013.130", "10.1109/tvcg.2007.70577", "10.1109/mic.2015.129", "10.1145/2909132.2909267", "10.1080/01449290500330331", "10.1109/tvcg.2018.2865233", "10.1109/tvcg.2015.2467831", "10.1109/iv.2015.34", "10.1109/tvcg.2009.111", "10.1145/3290605.3300565", "10.1145/3173574.3173797", "10.1145/948496.948514", "10.1145/3025453.3025765", "10.1109/tvcg.2015.2467613", "10.1145/2659796", "10.1109/tvcg.2014.2346311", "10.1145/3025453.3025524", "10.1080/17452759.2011.558588", "10.1145/3027063.3053113", "10.1109/infvis.2005.1532136", "10.1145/2470654.2481307", "10.1109/tvcg.2018.2864913", "10.1145/345513.345267", "10.3102/00028312005004437", "10.1109/tvcg.2007.70436", "10.1037/0033-295x.106.4.643", "10.1145/2133416.2146416", "10.1109/tvcg.2013.191", "10.1109/tvcg.2010.177", "10.1145/960201.957206", "10.1109/tvcg.2007.70515", "10.1109/infvis.2000.885092", "10.1145/2636240.2636844", "10.1037/h0055392", "10.1177/1473871611413180", "10.1109/vl.1996.545307", "10.1111/j.1471-1842.2009.00848.x", "10.1145/989863.989865", "10.1109/tvcg.2013.124", "10.1109/tvcg.2008.109", "10.1145/1166253.1166265", "10.1145/2702123.2702180", "10.1109/tvcg.2016.2598620", "10.1080/07370024.2016.1226139", "10.1145/3173574.3173909", "10.1109/pacificvis.2010.5429613", "10.1111/j.1467-6478.2006.00368.x", "10.1109/tvcg.2012.204", "10.1109/tvcg.2013.120", "10.1179/1743277412y.0000000019", "10.1145/1936652.1936684", "10.2307/1269768", "10.1109/infvis.1996.559213", "10.1109/tvcg.2016.2598839", "10.1145/2642918.2647360", "10.1057/palgrave.ivs.9500099", "10.1016/j.cag.2009.06.004", "10.1109/mc.2013.178", "10.1109/tvcg.2007.70541", "10.1109/vast.2011.6102473", "10.1145/358886.358895", "10.1109/tvcg.2014.2346573", "10.1109/tvcg.2018.2865159", "10.1145/2207676.2207741", "10.1109/tvcg.2015.2396062", "10.1145/2207676.2208572", "10.1109/tvcg.2016.2598608", "10.1057/ivs.2008.31", "10.1177/001316446002000104", "10.1109/tvcg.2017.2680452", "10.1109/tvcg.2006.80", "10.1145/2598510.2598566", "10.1037/0003-066x.51.4.355", "10.7146/dpb.v16i224.7586", "10.1109/infvis.1998.729560", "10.1162/leon\\_a\\_00011", "10.1109/tvcg.2010.157", "10.1109/tvcg.2014.2359887"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030464", "title": "Designing Narrative-Focused Role-Playing Games for Visualization Literacy in Young Children", "year": "2020", "conferenceName": "InfoVis", "authors": "Elaine Huynh;Angela Nyhout;Patricia Ganea;Fanny Chevalier", "citationCount": "0", "affiliation": "Huynh, E (Corresponding Author), Univ Toronto, Dept Comp Sci, Toronto, ON, Canada. Huynh, Elaine, Univ Toronto, Dept Comp Sci, Toronto, ON, Canada. Nyhout, Angela; Ganea, Patricia, Univ Toronto, Ontario Inst Studies Educ, Toronto, ON, Canada. Chevalier, Fanny, Univ Toronto, Dept Comp Sci \\& Stat Sci, Toronto, ON, Canada.", "countries": "Canada", "abstract": "Building on game design and education research, this paper introduces narrative-focused role-playing games as a way to promote visualization literacy in young children. Visualization literacy skills are vital in understanding the world around us and constructing meaningful visualizations, yet, how to better develop these skills at an early age remains largely overlooked and understudied. Only recently has the visualization community started to fill this gap, resulting in preliminary studies and development of educational tools for use in early education. We add to these efforts through the exploration of gamification to support learning, and identify an opportunity to apply role-playing game-based designs by leveraging the presence of narratives in data-related problems involving visualizations. We study the effects of including narrative elements on learning through a technology probe, grounded in a set of design considerations stemming from visualization, game design and education science. We create two versions of a game - one with narrative elements and one without - and evaluate our instances on 33 child participants between 11- to 13-years old using a between-subjects study design. Despite participants requiring double the amount of time to complete their game due to additional narrative elements, the inclusion of such elements were found to improve engagement without sacrificing learning; our results indicate no significant differences in development of graph-reading skills, but significant differences in engagement and overall enjoyment of the game. We report observations and qualitative feedback collected, and note areas for improvement and room for future work.", "keywords": "Visualization Literacy,Educational technology,Gamification,Narrative", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030464", "refList": ["10.1007/978-981-13-2694-3\\_2", "10.1145/2702123.2702298", "10.1145/2702123.2702558", "10.1007/978-981-13-2694-3\\_8", "10.1145/2702123.2702245", "10.1109/mis.2012.27", "10.18061/dsq.v21i4.318", "10.1109/tvcg.2013.134", "10.1109/vl.1996.545307", "10.1007/s10708-008-9186-0", "10.1093/cje/ben057", "10.5210/fm.v16i2.3316", "10.1109/tvcg.2019.2934539", "10.1109/tvcg.2016.2598608", "10.1007/978-3-319-94659-7\\_10", "10.1109/mcg.2013.28", "10.17351/ests2017.134", "10.1109/pacificvis.2014.39", "10.1145/3173574.3173728", "10.1145/2598784.2598806", "10.1145/2491500.2491501", "10.1145/1993060.1993065", "10.1109/tvcg.2018.2802520", "10.1145/3025453.3025667", "10.1145/2598510.2598566", "10.1080/15710882.2015.1081240", "10.17351/ests2017.133", "10.1109/tvcg.2014.2346431", "10.1016/j.ijhcs.2015.02.005", "10.1145/2702123.2702180", "10.1109/tvcg.2007.70577", "10.1109/mcg.2019.2923483", "10.5931/djim.v12.i1.6449", "10.1145/3240167.3240206", "10.1145/2468356.2468739", "10.1109/tvcg.2012.213", "10.1145/3025453.3025751", "10.4018/978-1-4666-6497-5.ch003"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2019.00019", "year": "2019", "title": "Defamiliarization, Representation Granularity, and User Experience: A Qualitative Study with Two Situated Visualizations", "conferenceName": "PacificVis", "authors": "Luiz Augusto de Mac{\\^{e}}do Morais;Nazareno Andrade;Dandara Maria Costa de Sousa;Lesandro Ponciano", "citationCount": "0", "affiliation": "Morais, LAD (Corresponding Author), Univ Fed Campina Grande, Campina Grande, Paraiba, Brazil.\nde Macedo Morais, Luiz Augusto; Andrade, Nazareno; Costa de Sousa, Dandara Maria, Univ Fed Campina Grande, Campina Grande, Paraiba, Brazil.\nPonciano, Lesandro, Pontificia Univ Catolica Minas Gerais, Belo Horizonte, MG, Brazil.", "countries": "Brazil", "abstract": "This work explores the user experience with two situated visualizations that lie on different points of design space. The first visualization - the Activity Clock - displays the aggregate presence of laboratory members into a wall clock. The second - Personal Activities - represents the same persons individually, in a conventional poster media. We interviewed 17 participants and leverage a theoretical lens of Continuous Engagement and Sense-Making to study how design decisions impact the user experience with respect to (1) which design factors attract users, (2) how design features affect users' understanding of the visualization, and (3) what kind of reflections are evoked by design. We discuss how the defamiliarizing effect of the Activity Clock plays a dual role in attracting users while also hindering their understanding of the data. We also consider the evidence that fine representation granularity in the Personal Activities evokes deeper reflections.", "keywords": "Situated visualization; user experience; defamiliarization; representation granularity", "link": "https://doi.org/10.1109/PacificVis.2019.00019", "refList": ["10.1007/pl00000019", "10.1145/642611.642653", "10.1145/2702123.2702558", "10.1145/1013115.1013153", "10.1145/2470654.2481306", "10.1145/2702123.2702545", "10.1109/tvcg.2007.70541", "10.1145/2421076.2421086", "10.1109/tvcg.2008.127", "10.1068/d7910", "10.1145/3064857.3079123", "10.1109/tvcg.2016.2598608", "10.1002/asi.20801", "10.1145/2611009.2611018", "10.1145/3025453.3025512", "10.1109/tvcg.2011.279", "10.1007/978-3-642-23771-3\\_17", "10.1145/1460355.1460360", "10.1109/tvcg.2015.2511718", "10.1177/1473871611415996", "10.1109/infvis.2004.8", "10.1145/2491568.2491597", "10.1016/j.jenvp.2010.10.001", "10.1080/10630732.2012.698065", "10.1145/2993901.2993903", "10.1145/1067860.1067862", "10.1145/2677199.2680588"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030427", "title": "Embodied Navigation in Immersive Abstract Data Visualization: Is Overview+Detail or Zooming Better for 3D Scatterplots?", "year": "2020", "conferenceName": "InfoVis", "authors": "Yalong Yang;Maxime Cordeil;Johanna Beyer;Tim Dwyer;Kim Marriott;Hanspeter Pfister", "citationCount": "1", "affiliation": "Yang, YL (Corresponding Author), Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Yang, Yalong; Beyer, Johanna; Pfister, Hanspeter, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Cordeil, Maxime; Dwyer, Tim; Marriott, Kim, Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia.", "countries": "USA;Australia", "abstract": "Abstract data has no natural scale and so interactive data visualizations must provide techniques to allow the user to choose their viewpoint and scale. Such techniques are well established in desktop visualization tools. The two most common techniques are zoom+pan and overview+detail. However, how best to enable the analyst to navigate and view abstract data at different levels of scale in immersive environments has not previously been studied. We report the findings of the first systematic study of immersive navigation techniques for 3D scatterplots. We tested four conditions that represent our best attempt to adapt standard 2D navigation techniques to data visualization in an immersive environment while still providing standard immersive navigation techniques through physical movement and teleportation. We compared room-sized visualization versus a zooming interface, each with and without an overview. We find significant differences in participants' response times and accuracy for a number of standard visual analysis tasks. Both zoom and overview provide benefits over standard locomotion support alone (i.e., physical movement and pointer teleportation). However, which variation is superior, depends on the task. We obtain a more nuanced understanding of the results by analyzing them in terms of a time-cost model for the different components of navigation: way-finding, travel, number of travel steps, and context switching.", "keywords": "Immersive Analytics,Information Visualization,Virtual Reality,Navigation,Overview+Detail,Zooming,Scatterplot", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030427", "refList": ["10.1145/2980179.2982416", "10.1109/tvcg.2014.2346979", "10.1049/htl.2019.0062", "10.1109/tvcg.2010.223", "10.1145/1239451.1239496", "10.1145/2971648.2971706", "10.1109/tvcg.2013.126", "10.1145/1778765.1778830", "10.1007/978-3-540-70956-5\\_2", "10.1145/1978942.1978963", "10.1145/3116595.3116596", "10.1145/882262.882352", "10.1145/1182475.1182504", "10.1007/978-3-322-80058-919", "10.1126/science.aal3618", "10.1145/3290605.3300422", "10.1071/ch9490149", "10.1016/j.ijhcs.2016.09.014", "10.1109/tvcg.2017.2709746", "10.1109/mcg.2019.2897927", "10.1145/3290605.3300737", "10.1007/978-1-4614-0064-63", "10.1109/pacificvis.2019.00019", "10.1109/mprv.2005.93", "10.1109/tvcg.2015.2467671", "10.1145/1964921.1964980", "10.1145/2702123.2702561", "10.1109/ismar.2011.6092386", "10.1145/1852786.1852790", "10.3758/brm.40.2.428", "10.1145/2628363.2628398", "10.1145/2317956.2318010", "10.1145/302979.303114", "10.1007/978-1-4614-0064-6", "10.1145/2702123.2702180", "10.1145/3322276.3322326", "10.1017/xps.2014.5", "10.1109/tlt.2013.37", "10.1007/978-3-322", "10.1145/2540930.2540952", "10.1145/1168149.1168158", "10.1109/tvcg.2011.196", "10.1145/3095814", "10.1145/2750858.2804268", "10.3758/bf03194105", "10.1007/978-3-322-80058-9\\_19", "10.1109/tvcg.2012.213", "10.1145/302979", "10.1016/j.jmp.2012.08.001", "10.1177/0963662514549688", "10.1002/hbm.20701", "10.1145/3025453.3026036", "10.1007/978-3-322-80058-9", "10.1109/tvcg.2019.2903942", "10.1109/tvcg.2009.111", "10.1145/989863.989880", "10.1109/mcg.2010.50", "10.1145/3294109.3295644", "10.1145/2910674.2910679"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030400", "title": "Revisited: Comparison of Empirical Methods to Evaluate Visualizations Supporting Crafting and Assembly Purposes", "year": "2020", "conferenceName": "InfoVis", "authors": "Maximilian Wei\u00df;Katrin Angerbauer;Alexandra Voit;Magdalena Schwarzl;Michael Sedlmair;Sven Mayer", "citationCount": "0", "affiliation": "Weiss, M (Corresponding Author), Univ Stuttgart, Stuttgart, Germany. Weiss, Maximilian; Angerbauer, Katrin; Schwarzl, Magdalena; Sedlmair, Michael, Univ Stuttgart, Stuttgart, Germany. Voit, Alexandra, Adesso SE, Dortmund, Germany. Mayer, Sven, Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.", "countries": "Germany;USA", "abstract": "Ubiquitous, situated, and physical visualizations create entirely new possibilities for tasks contextualized in the real world, such as doctors inserting needles. During the development of situated visualizations, evaluating visualizations is a core requirement. However, performing such evaluations is intrinsically hard as the real scenarios are safety-critical or expensive to test. To overcome these issues, researchers and practitioners adapt classical approaches from ubiquitous computing and use surrogate empirical methods such as Augmented Reality (AR), Virtual Reality (VR) prototypes, or merely online demonstrations. This approach's primary assumption is that meaningful insights can also be gained from different, usually cheaper and less cumbersome empirical methods. Nevertheless, recent efforts in the Human-Computer Interaction (HCI) community have found evidence against this assumption, which would impede the use of surrogate empirical methods. Currently, these insights rely on a single investigation of four interactive objects. The goal of this work is to investigate if these prior findings also hold for situated visualizations. Therefore, we first created a scenario where situated visualizations support users in do-it-yourself (DIY) tasks such as crafting and assembly. We then set up five empirical study methods to evaluate the four tasks using an online survey, as well as VR, AR, laboratory, and in-situ studies. Using this study design, we conducted a new study with 60 participants. Our results show that the situated visualizations we investigated in this study are not prone to the same dependency on the empirical method, as found in previous work. Our study provides the first evidence that analyzing situated visualizations through different empirical (surrogate) methods might lead to comparable results.", "keywords": "Situated visualization,evaluation,comparison", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030400", "refList": ["10.1145/2980179.2982416", "10.1109/tvcg.2014.2346979", "10.1109/tvcg.2010.223", "10.1145/1239451.1239496", "10.1145/2971648.2971706", "10.1109/tvcg.2013.126", "10.1145/1778765.1778830", "10.1145/1978942.1978963", "10.1007/978-3-540-70956-5", "10.1145/3116595.3116596", "10.1145/882262.882352", "10.1145/1182475.1182504", "10.1007/978-3-322-80058-919", "10.1126/science.aal3618", "10.1145/3290605.3300422", "10.1071/ch9490149", "10.1016/j.ijhcs.2016.09.014", "10.1109/tvcg.2017.2709746", "10.1109/mcg.2019.2897927", "10.1007/978-3-540-70956-52", "10.1145/3290605.3300737", "10.1109/pacificvis.2019.00019", "10.1109/mprv.2005.93", "10.1109/tvcg.2015.2467671", "10.1145/1964921.1964980", "10.1145/2702123.2702561", "10.1109/ismar.2011.6092386", "10.1145/1852786.1852790", "10.1007/978-1-4614-0064-6\\_3", "10.3758/brm.40.2.428", "10.1145/2628363.2628398", "10.1145/2317956.2318010", "10.1145/2700648.2809853", "10.1049/ht1.2019.0062", "10.1145/2702123.2702180", "10.1145/3322276.3322326", "10.1017/xps.2014.5", "10.1109/tlt.2013.37", "10.1007/978-3-322", "10.1145/2540930.2540952", "10.1145/1168149.1168158", "10.1109/tvcg.2011.196", "10.1145/3095814", "10.1145/2750858.2804268", "10.3758/bf03194105", "10.1007/978-3-322-80058-9\\_19", "10.1109/tvcg.2012.213", "10.1016/j.jmp.2012.08.001", "10.1145/2851581.2892429", "10.1177/0963662514549688", "10.1145/3025453.3026036", "10.1007/978-3-322-80058-9", "10.1109/tvcg.2019.2903942", "10.1109/tvcg.2009.111", "10.1145/989863.989880", "10.1109/mcg.2010.50", "10.1145/3294109.3295644", "10.1145/2910674.2910679"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 21}, "index": 574, "embedding": [3.6840431690216064, 2.632714033126831, 0.6940076947212219, 6.5590338706970215, -0.6539726853370667, 0.16650904715061188, -0.7323254346847534, 3.471275806427002, -0.4441397190093994, -0.8419216275215149, 2.3718740940093994, 1.9447790384292603, -0.14601923525333405, 4.236797332763672, 0.9733353853225708, 4.187725067138672, 0.4326697885990143, 2.279871940612793, -0.7047576308250427, 6.357198715209961, -0.06630208343267441, 4.680258750915527, 2.3087527751922607, 1.5260350704193115, 1.5503880977630615, 0.857613205909729, -0.5452967882156372, 0.8517097234725952, 4.251620769500732, 2.593095064163208, 2.41774845123291, 3.4786715507507324], "projection": [-3.2131733894348145, 7.996520519256592], "size": 11, "height": 3, "width": 8}