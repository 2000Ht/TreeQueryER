{"data": {"doi": "10.1109/tvcg.2020.3030354", "title": "Explainable Matrix - Visualization for Global and Local Interpretability of Random Forest Classification Ensembles", "year": "2020", "conferenceName": "VAST", "authors": "M\u00e1rio Popolin Neto;Fernando Vieira Paulovich", "citationCount": "0", "affiliation": "Neto, MP (Corresponding Author), Fed Inst Sao Paulo IFSP, Sao Paulo, SP, Brazil. Neto, MP (Corresponding Author), Univ Sao Paulo, Sao Paulo, SP, Brazil. Neto, Mario Popolin, Fed Inst Sao Paulo IFSP, Sao Paulo, SP, Brazil. Neto, Mario Popolin; Paulovich, Fernando V., Univ Sao Paulo, Sao Paulo, SP, Brazil. Paulovich, Fernando V., Dalhousie Univ, Halifax, NS, Canada.", "countries": "Canada;Brazil", "abstract": "Over the past decades, classification models have proven to be essential machine learning tools given their potential and applicability in various domains. In these years, the north of the majority of the researchers had been to improve quantitative metrics, notwithstanding the lack of information about models' decisions such metrics convey. This paradigm has recently shifted, and strategies beyond tables and numbers to assist in interpreting models' decisions are increasing in importance. Part of this trend, visualization techniques have been extensively used to support classification models' interpretability, with a significant focus on rule-based models. Despite the advances, the existing approaches present limitations in terms of visual scalability, and the visualization of large and complex models, such as the ones produced by the Random Forest (RF) technique, remains a challenge. In this paper, we propose Explainable Matrix (ExMatrix), a novel visualization method for RF interpretability that can handle models with massive quantities of rules. It employs a simple yet powerful matrix-like visual metaphor, where rows are rules, columns are features, and cells are rules predicates, enabling the analysis of entire models and auditing classification results. ExMatrix applicability is confirmed via different examples, showing how it can be used in practice to promote RF models interpretability.", "keywords": "Random forest visualization,logic rules visualization,classification model interpretability,explainable artificial intelligence", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030354", "refList": ["10.1109/tvcg.2016.2598838", "10.1145/956750.956837.1-58113-737-0", "10.1007/s11749-016-0481-7", "10.1016/j.dss.2010.12.003", "10.1109/mcg.2011.103", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934251", "10.1145/2939672.2939874", "10.1109/tvcg.2010.79", "10.1109/tvcg.2018.2864475", "10.1111/cgf.12935", "10.1111/insr.12016", "10.1145/2594473.2594475", "10.1109/vast.2017.8585720", "10.1109/tvcg.2014.2331979", "10.1038/s41586-018-0337-2", "10.1109/tvcg.2018.2864812", "10.1109/vast.2012.6400492", "10.1145/3236009", "10.1057/jors.2008.161", "10.1109/cbms.2006.169", "10.1038/ng.142", "10.1111/j.1469-1809.1936.tb02137.x", "10.1111/cgf.13092", "10.1109/vast.2010.5652443", "10.1155/2018/5145348", "10.1109/tvcg.2017.2744378", "10.1023/a:1010933404324", "10.1109/vast.2010.5652398", "10.1109/vast.2011.6102453"], "wos": 1, "children": [], "len": 1}, "index": 1008, "embedding": [-0.2665758728981018, 0.20552323758602142, -1.0222129821777344, -2.0736677646636963, -0.630656361579895, 0.16650904715061188, -0.7150075435638428, 0.39565515518188477, -0.4060705304145813, -0.7935177087783813, 0.16464713215827942, -0.852477490901947, -0.1321265697479248, -0.9479813575744629, 0.321649968624115, 0.4277951717376709, -0.023383229970932007, 0.07000205665826797, -0.012375885620713234, 0.5907588005065918, -0.06630208343267441, -0.08446534723043442, -0.1776050627231598, -0.1290702521800995, -2.458800792694092, 0.09817054867744446, -0.5019459128379822, -0.024586889892816544, -0.10487377643585205, 1.2322404384613037, -0.22355535626411438, 0.08610816299915314], "projection": [3.222749948501587, 3.7869951725006104], "size": 1, "height": 1, "width": 1}