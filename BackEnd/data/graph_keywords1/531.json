{"data": {"title": "A Study of Layout, Rendering, and Interaction Methods for Immersive Graph Visualization", "doi": "10.1109/tvcg.2016.2520921", "year": "2016", "conferenceName": "TVCG", "authors": "Oh-Hyun Kwon;Chris Muelder;Kyungwon Lee;Kwan-Liu Ma", "citationCount": "54", "affiliation": "Kwon, OH (Corresponding Author), Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA.\nKwon, OH (Corresponding Author), Ajou Univ, Suwon 441749, South Korea.\nKwon, Oh-Hyun; Muelder, Chris; Ma, Kwan-Liu, Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA.\nKwon, Oh-Hyun, Ajou Univ, Suwon 441749, South Korea.\nLee, Kyungwon, Ajou Univ, Dept Digital Media, Suwon 441749, South Korea.", "countries": "USA;Korea", "abstract": "Information visualization has traditionally limited itself to 2D representations, primarily due to the prevalence of 2D displays and report formats. However, there has been a recent surge in popularity of consumer grade 3D displays and immersive head-mounted displays (HMDs). The ubiquity of such displays enables the possibility of immersive, stereoscopic visualization environments. While techniques that utilize such immersive environments have been explored extensively for spatial and scientific visualizations, contrastingly very little has been explored for information visualization. In this paper, we present our considerations of layout, rendering, and interaction methods for visualizing graphs in an immersive environment. We conducted a user study to evaluate our techniques compared to traditional 2D graph visualization. The results show that participants answered significantly faster with a fewer number of interactions using our techniques, especially for more difficult tasks. While the overall correctness rates are not significantly different, we found that participants gave significantly more correct answers using our techniques for larger graphs.", "keywords": "Graph visualization; virtual reality; immersive environments; head-mounted display", "refList": ["10.1109/tvcg.2011.233", "10.1109/38.799723", "10.1109/pacificvis.2015.7156357", "10.1109/38.888006", "10.1109/3dvis.2014.7160095", "10.1145/325165.325242", "10.1109/tvcg.2009.138", "10.1145/1279640.1279642", "10.1109/3dvis.2014.7160105", "10.1109/infvis.1997.636718", "10.1109/tvcg.2006.147", "10.1007/978-3-540-88564-1\\_38", "10.1109/tvcg.2008.158", "10.1016/b978-012240530-3/50005-5", "10.1073/pnas.122653799", "10.1109/3dvis.2014.7160103", "10.1145/502122.502124", "10.1145/234972.234975", "10.1109/3dvis.2014.7160104", "10.1145/1080402.1080411", "10.1038/30918", "10.1186/1471-2105-5-48", "10.1109/tvcg.2012.192", "10.1109/38.689657", "10.1109/ipdps.2003.1213486", "10.1109/tvcg.2012.142", "10.1145/229459.229467", "10.1109/tvcg.2011.234", "10.1093/ptj/72.11.770", "10.1086/jar.33.4.3629752", "10.1109/tvcg.2007.70433", "10.1109/tvcg.2008.86", "10.1111/j.1467-8659.2009.01450.x", "10.1016/0021-9045(72)90080-9", "10.1109/2945.841119", "10.1109/3dvis.2014.7160093", "10.1007/978-3-540-72630-2\\_10", "10.1109/tvcg.2005.103", "10.1145/1385569.1385639", "10.1109/3dvis.2014.7160096"], "wos": "", "children": [{"doi": "10.1109/tvcg.2016.2599049", "title": "GlyphLens: View-Dependent Occlusion Management in the Interactive Glyph Visualization", "year": "2016", "conferenceName": "SciVis", "authors": "Xin Tong;Cheng Li;Han-Wei Shen", "citationCount": "13", "affiliation": "Tong, X (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA. Tong, Xin; Li, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Glyph as a powerful multivariate visualization technique is used to visualize data through its visual channels. To visualize 3D volumetric dataset, glyphs are usually placed on 2D surface, such as the slicing plane or the feature surface, to avoid occluding each other. However, the 3D spatial structure of some features may be missing. On the other hand, placing large number of glyphs over the entire 3D space results in occlusion and visual clutter that make the visualization ineffective. To avoid the occlusion, we propose a view-dependent interactive 3D lens that removes the occluding glyphs by pulling the glyphs aside through the animation. We provide two space deformation models and two lens shape models to displace the glyphs based on their spatial distributions. After the displacement, the glyphs around the user-interested region are still visible as the context information, and their spatial structures are preserved. Besides, we attenuate the brightness of the glyphs inside the lens based on their depths to provide more depth cue. Furthermore, we developed an interactive glyph visualization system to explore different glyph-based visualization applications. In the system, we provide a few lens utilities that allows users to pick a glyph or a feature and look at it from different view directions. We compare different display/interaction techniques to visualize/manipulate our lens and glyphs.", "keywords": "View-dependent visualization;focus + context techniques;manipulation and deformation;glyph-based techniques;human-computer interaction", "link": "http://dx.doi.org/10.1109/TVCG.2016.2599049", "refList": ["10.1109/visual.2003.1250400", "10.1145/989863.989871", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2013.100", "10.1145/988834.988870", "10.1111/j.1467-8659.2006.01000.x", "10.1109/visual.1993.398849", "10.1109/pacificvis.2015.7156349", "10.1109/pacificvis.2009.4906851", "10.1145/2766890", "10.1109/pacificvis.2015.7156385", "10.1145/1239451.1239482", "10.2312/vissym/vissym04/147-154", "10.1109/tvcg.2015.2443804", "10.1109/tvcg.2006.167", "10.1145/166117.166126", "10.1007/bf01897116", "10.1111/1467-8659.t01-3-00700", "10.1111/j.1467-8659.2012.03115.x", "10.3390/s130506380", "10.1109/mcg.1984.275995", "10.1145/237091.237098", "10.1109/tmi.2009.2016561", "10.1145/1618452.1618504", "10.1109/tvcg.2010.199", "10.1109/tvcg.2010.127", "10.1109/tvcg.2013.121", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2007.1059", "10.1109/ismar.2004.36", "10.1111/cgf.12099", "10.1109/tvcg.2010.157", "10.1109/infvis.1996.559215"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2864690", "title": "Interactive obstruction-free lensing for volumetric data visualization", "year": "2018", "conferenceName": "SciVis", "authors": "Michael Traor\u00e9;Christophe Hurter;Alexandru Telea", "citationCount": "1", "affiliation": "Traore, M (Corresponding Author), French Civil Aviat Univ, ENAC, Toulouse, France. Traore, Michael; Hurter, Christophe, French Civil Aviat Univ, ENAC, Toulouse, France. Telea, Alexandru, Univ Groningen, Inst Johan Bernoulli, Groningen, Netherlands.", "countries": "France;Netherlands", "abstract": "Occlusion is an issue in volumetric visualization as it prevents direct visualization of the region of interest. While many techniques such as transfer functions, volume segmentation or view distortion have been developed to address this, there is still room for improvement to better support the understanding of objects' vicinity. However, most existing Focus+Context fail to solve partial occlusion in datasets where the target and the occluder are very similar density-wise. For these reasons, we investigate a new technique which maintains the general structure of the investigated volumetric dataset while addressing occlusion issues. With our technique, the user interactively defines an area of interest where an occluded region or object is partially visible. Then our lens starts pushing at its border occluding objects, thus revealing hidden volumetric data. Next, the lens is modified with an extended field of view (fish-eye deformation) to better see the vicinity of the selected region. Finally, the user can freely explore the surroundings of the area under investigation within the lens. To provide real-time exploration, we implemented our lens using a GPU accelerated ray-casting framework to handle ray deformations, local lighting, and local viewpoint manipulation. We illustrate our technique with five application scenarios in baggage inspection, 3D fluid flow visualization, chest radiology, air traffic planning, and DTI fiber exploration.", "keywords": "Interaction techniques,focus + context,volume visualization,volume rendering,raycasting", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864690", "refList": ["10.1109/visual.1999.809865", "10.1145/1409060.1409107", "10.1109/visual.2003.1250400", "10.1145/2598153.2598200", "10.1007/978-1-4614-7657-3\\_19", "10.1109/tvcg.2008.59", "10.1145/989863.989871", "10.1109/tvcg.2016.2599049", "10.1145/345513.345271", "10.1016/s1470-2045(17)30438-2", "10.1109/pacificvis.2014.61", "10.1111/j.1467-8659.2006.00979.x", "10.1109/tvcg.2010.35", "10.2312/conf/eg2012/stars/075-094", "10.1111/cgf.12927", "10.1111/cgf.12871", "10.1145/2024156.2024165", "10.1007/978-3-540-85412-8\\_16", "10.1057/ivs.2009.32", "10.1109/tvcg.2006.140", "10.1109/tvcg.2007.70565", "10.1109/tvcg.2015.2403323", "10.1109/visual.2004.32", "10.2200/s00688ed1v01y201512vis006", "10.1109/tvcg.2015.2443804", "10.1109/tvcg.2006.144", "10.1109/tvcg.2016.2515611", "10.1109/tvcg.2007.48", "10.1109/tvcg.2010.193", "10.1109/tvcg.2007.1051", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.223", "10.1109/38.595268", "10.1111/j.1467-8659.2012.03079.x", "10.1109/tvcg.2009.145", "10.1109/pacificvis.2017.8031594", "10.1145/2425296.2425325", "10.1109/tvcg.2012.265", "10.1109/mcg.2017.10", "10.1007/s12031-007-0029-0", "10.1109/tvcg.2010.127", "10.1016/j.trc.2014.03.005", "10.1109/tvcg.2009.138", "10.1109/tvcg.2006.124"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934332", "title": "LassoNet: Deep Lasso-Selection of 3D Point Clouds", "year": "2019", "conferenceName": "SciVis", "authors": "Zhutian Chen;Wei Zeng 0004;Zhiguang Yang;Lingyun Yu;Chi-Wing Fu;Huamin Qu", "citationCount": "4", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Zhutian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei; Yang, Zhiguang, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yu, Lingyun, Univ Groningen, Groningen, Netherlands. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China.", "countries": "China;Netherlands", "abstract": "Selection is a fundamental task in exploratory analysis and visualization of 3D point clouds. Prior researches on selection methods were developed mainly based on heuristics such as local point density, thus limiting their applicability in general data. Specific challenges root in the great variabilities implied by point clouds (e.g., dense vs. sparse), viewpoint (e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this work, we introduce LassoNet, a new deep neural network for lasso selection of 3D point clouds, attempting to learn a latent mapping from viewpoint and lasso to point cloud regions. To achieve this, we couple user-target points with viewpoint and lasso information through 3D coordinate transform and naive selection, and improve the method scalability via an intention filtering and farthest point sampling. A hierarchical network is trained using a dataset with over 30K lasso-selection records on two different point cloud data. We conduct a formal user study to compare LassoNet with two state-of-the-art lasso-selection methods. The evaluations confirm that our approach improves the selection effectiveness and efficiency across different combinations of 3D point clouds, viewpoints, and lasso selections. Project Website: https://LassoNet.github.io", "keywords": "Point Clouds,Lasso Selection,Deep Learning", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934332", "refList": ["10.1111/j.1467-8659.2009.01515.x", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2016.2599049", "10.1109/iccv.2015.114", "10.1109/vppc.2018.8604993", "10.1111/cgf.13405", "10.1109/tvcg.2018.2843369", "10.1109/cvpr.2018.00278", "10.1145/3025453.3025957", "10.1109/mc.2013.178", "10.1145/2980179.2980238", "10.1109/msp.2017.2693418", "10.1145/237091.237105", "10.1145/2835487", "10.1109/cvpr.2010.5539838", "10.1016/j.visinf.2017.01.006", "10.1109/tcst.2018.2819965", "10.1109/tvcg.2018.2865138", "10.1109/83.623193", "10.1145/3072959.3073608", "10.1145/1053427.1053445", "10.1016/s0039-9140(96)02179-0", "10.1109/cvpr.2015.7298801", "10.1109/cvpr.2017.693", "10.1007/s12650-014-0206-5", "10.1145/3272127.3275110", "10.1109/cvpr.2015.7298845", "10.1109/iccvw.2015.112", "10.1109/iros.2015.7353481", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2012.217", "10.1109/tridui.2006.1618279", "10.1109/tvcg.2012.292", "10.1109/cvpr.2016.609", "10.1016/j.cag.2012.12.003"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028947", "title": "A Fluid Flow Data Set for Machine Learning and its Application to Neural Flow Map Interpolation", "year": "2020", "conferenceName": "SciVis", "authors": "Jakob Jakob;Markus H. Gross;Tobias G\u00fcnther", "citationCount": "0", "affiliation": "Jakob, J (Corresponding Author), Swiss Fed Inst Technol, Zurich, Switzerland. Jakob, Jakob; Gross, Markus; Guenther, Tobias, Swiss Fed Inst Technol, Zurich, Switzerland.", "countries": "Switzerland", "abstract": "In recent years, deep learning has opened countless research opportunities across many different disciplines. At present, visualization is mainly applied to explore and explain neural networks. Its counterpart-the application of deep learning to visualization problems-requires us to share data more openly in order to enable more scientists to engage in data-driven research. In this paper, we construct a large fluid flow data set and apply it to a deep learning problem in scientific visualization. Parameterized by the Reynolds number, the data set contains a wide spectrum of laminar and turbulent fluid flow regimes. The full data set was simulated on a high-performance compute cluster and contains 8000 time-dependent 2D vector fields, accumulating to more than 16 TB in size. Using our public fluid data set, we trained deep convolutional neural networks in order to set a benchmark for an improved post-hoc Lagrangian fluid flow analysis. In in-situ settings, flow maps are exported and interpolated in order to assess the transport characteristics of time-dependent fluids. Using deep learning, we improve the accuracy of flow map interpolations, allowing a more precise flow analysis at a reduced memory IO footprint.", "keywords": "Scientific visualization,deep learning,flow maps", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028947", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1145/3355089.3356560", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1007/978-3-030-00533-7\\_36", "10.3390/rs11161921", "10.1007/978-3-030-48457-6\\_1", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/mcg.2018.2881523", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1145/3072959.3073643", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/tvcg.2013.128", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1007/978-3-319-46475-6\\_43", "10.1126/science.1127647", "10.1007/978-3-319-46475-6\\_25", "10.1111/cgf.13689"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030346", "title": "V2V: A Deep Learning Approach to Variable-to-Variable Selection and Translation for Multivariate Time-Varying Data", "year": "2020", "conferenceName": "SciVis", "authors": "Jun Han;Hao Zheng 0006;Yunhao Xing;Danny Ziyi Chen;Chaoli Wang", "citationCount": "0", "affiliation": "Han, J (Corresponding Author), Univ Notre Dame, Notre Dame, IN 46556 USA. Han, Jun; Zheng, Hao; Chen, Danny Z.; Wang, Chaoli, Univ Notre Dame, Notre Dame, IN 46556 USA. Xing, Yunhao, Sichuan Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China;USA", "abstract": "We present V2V, a novel deep learning framework, as a general-purpose solution to the variable-to-variable (V2V) selection and translation problem for multivariate time-varying data (MTVD) analysis and visualization. V2V leverages a representation learning algorithm to identify transferable variables and utilizes Kullback-Leibler divergence to determine the source and target variables. It then uses a generative adversarial network (GAN) to learn the mapping from the source variable to the target variable via the adversarial, volumetric, and feature losses. V2V takes the pairs of time steps of the source and target variable as input for training, Once trained, it can infer unseen time steps of the target variable given the corresponding time steps of the source variable. Several multivariate time-varying data sets of different characteristics are used to demonstrate the effectiveness of V2V, both quantitatively and qualitatively. We compare V2V against histogram matching and two other deep learning solutions (Pix2Pix and CycleGAN).", "keywords": "Multivariate time-varying data,variable selection and translation,generative adversarial network,data extrapolation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030346", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/cvpr.2017.19", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1146/annurev", "10.3390/rs11161921", "10.1109/tvcg.2007.70551", "10.1145/3309993", "10.1007/978-3-030-48457-6\\_1", "10.1109/igarss.2018.8518411", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1146/annurev-fluid-010313-141322", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1126/science.1127647"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030374", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data", "year": "2020", "conferenceName": "SciVis", "authors": "Yifan Wang;Guoli Yan;Haikuan Zhu;Sagar Buch;Ying Wang;E. Mark Haacke;Jing Hua;Zichun Zhong", "citationCount": "0", "affiliation": "Wang, YF (Corresponding Author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Wang, Yifan; Yan, Guoli; Zhu, Haikuan; Hua, Jing; Zhong, Zichun, Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Buch, Sagar; Wang, Ying; Haacke, Ewart Mark, Wayne State Univ, Dept Radiol, Detroit, MI 48201 USA.", "countries": "USA", "abstract": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.", "keywords": "Deep neural network,3D cerebrovascular segmentation and visualization,maximum intensity projection (MIP),joint embedding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030374", "refList": ["10.1109/cluster.2018.00036", "10.1109/iccv.2015.123", "10.1109/tvcg.2013.133", "10.1109/cvpr.2017.19", "10.1109/tvcg.2019.2934312", "10.1007/978-3-319-46487-9\\_40", "10.1109/tvcg.2006.175", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2018.2880207", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/tvcg.2018.2816059", "10.1109/pacificvis.2009.4906852", "10.1109/tvcg.2018.2864808", "10.1109/mcg.2018.2881523", "10.1109/tvcg.2015.2467431", "10.1109/tpami.2015.2439281", "10.1109/pacificvis48177.2020.8737", "10.1109/cvpr.2019.00244", "10.1109/tvcg.2006.165", "10.1109/bigdata.2018.8622520", "10.1007/978-3-319-46466-4\\_29", "10.1109/visual.2019.8933759", "10.1145/3197517.3201304", "10.1109/pacificvis.2011.5742378", "10.1109/cvpr.2006.91", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1007/978-3-319-24574-4\\_28", "10.1109/pacificvis.2019.00041", "10.1109/tvcg.2019.2934332", "10.1109/cvpr.2018.00916", "10.1007/978-3-319-46475-6\\_43"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2018.00035", "year": "2018", "title": "An Automatic Deformation Approach for Occlusion Free Egocentric Data Exploration", "conferenceName": "PacificVis", "authors": "Cheng Li;Joachim Moortgat;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Moortgat, Joachim; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Occlusion management is an important task for three dimension data exploration. For egocentric data exploration, the occlusion problems, caused by the camera being too close to opaque data elements, have not been well addressed by previous studies. In this paper, we propose an automatic approach to resolve these problems and provide an occlusion free egocentric data exploration. Our system utilizes a state transition model to monitor both the camera and the data, and manages the initiation, duration, and termination of deformation with animation. Our method can be applied to multiple types of scientific datasets, including volumetric data, polygon mesh data, and particle data. We demonstrate our method with different exploration tasks, including camera navigation, isovalue adjustment, transfer function adjustment, and time varying exploration. We have collaborated with a domain expert and received positive feedback.", "keywords": "Data Deformation; Occlusion Management; Data Exploration; Egocentric Visualization", "link": "https://doi.org/10.1109/PacificVis.2018.00035", "refList": ["10.1145/989863.989871", "10.1109/tvcg.2008.59", "10.1111/j.1467-8659.2008.01332.x", "10.1109/tvcg.2016.2599049", "10.1109/pacificvis.2014.14", "10.1109/tvcg.2007.70433", "10.1007/s10596-015-9501-z", "10.1109/38.610209", "10.1109/tvcg.2003.1207447", "10.1111/j.1467-8659.2008.01181.x", "10.1109/tvcg.2012.42", "10.1109/tvcg.2016.2599217", "10.1002/fld.3764", "10.1109/pacificvis.2013.6596123", "10.1109/tvcg.2006.140", "10.1109/tvcg.2012.143", "10.1109/tvcg.2007.70565", "10.1109/scivis.2015.7429485", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.167", "10.1145/505008.505039", "10.1109/tvcg.2006.144", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/38.595268", "10.1109/tvcg.2016.2518338", "10.1109/tvcg.2014.20", "10.1145/1980462.1980487", "10.1111/cgf.12466", "10.2312/compaesth/compaesth05/209-216", "10.1109/tvcg.2010.127", "10.1016/j.advwatres.2016.01.002", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2009.144", "10.1145/1462055.1462056"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/pacificvis.2017.8031579", "year": "2017", "title": "Virtual retractor: An interactive data exploration system using physically based deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Xin Tong;Han{-}Wei Shen", "citationCount": "2", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Tong, Xin; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Interactive data exploration plays a fundamental role in analyzing three dimensional scientific data. Occlusion management and context preservation are among the key factors to ensure effective identification and extraction of three-dimensional features. In this paper, we present an interactive data exploration system that utilizes a physically based deformation method to investigate hidden structures of data in three dimensional data sets. While non-physically based methods are popular for visual analytic applications due to their lower computational cost, physically based deformation methods can often better preserve features and their context. Our physically based deformation method preserves data features by setting the mesh properties according to interesting data attributes. We design effective and intuitive interfaces by using a metaphor of virtual retractor, which reflects the cutting and splitting of data that our system is simulating. We demonstrate case studies on multiple particle datasets and volume datasets, and present feedback from a domain user.", "keywords": "K.6.1 {[}Management of Computing and Information Systems]: Project and People Management-Life Cycle; K.7.m {[}The Computing Profession]: Miscellaneous-Ethics", "link": "https://doi.org/10.1109/PACIFICVIS.2017.8031579", "refList": ["10.1145/1409060.1409107", "10.1109/visual.2003.1250400", "10.1145/989863.989871", "10.1109/tvcg.2008.59", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1145/988834.988870", "10.1109/pacificvis.2014.61", "10.1111/j.1467-8659.2006.01000.x", "10.1111/j.1467-8659.2006.00979.x", "10.1145/1239451.1239482", "10.1109/tvcg.2006.140", "10.1109/tvcg.2007.70565", "10.1088/1742-6596/125/1/012076", "10.1109/tvcg.2015.2443804", "10.1109/tvcg.2006.144", "10.1111/j.1467-8659.2012.03115.x", "10.3390/s130506380", "10.1145/237091.237098", "10.1109/tvcg.2015.2502583", "10.1016/j.cag.2007.09.006", "10.1109/38.595268", "10.1145/1618452.1618504", "10.1145/1980462.1980487", "10.3171/2012.5.jns112334", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.1109/tvcg.2010.127", "10.1109/tvcg.2013.121", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.1109/ismar.2004.36", "10.1109/tvcg.2010.157"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2018.00035", "year": "2018", "title": "An Automatic Deformation Approach for Occlusion Free Egocentric Data Exploration", "conferenceName": "PacificVis", "authors": "Cheng Li;Joachim Moortgat;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Moortgat, Joachim; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Occlusion management is an important task for three dimension data exploration. For egocentric data exploration, the occlusion problems, caused by the camera being too close to opaque data elements, have not been well addressed by previous studies. In this paper, we propose an automatic approach to resolve these problems and provide an occlusion free egocentric data exploration. Our system utilizes a state transition model to monitor both the camera and the data, and manages the initiation, duration, and termination of deformation with animation. Our method can be applied to multiple types of scientific datasets, including volumetric data, polygon mesh data, and particle data. We demonstrate our method with different exploration tasks, including camera navigation, isovalue adjustment, transfer function adjustment, and time varying exploration. We have collaborated with a domain expert and received positive feedback.", "keywords": "Data Deformation; Occlusion Management; Data Exploration; Egocentric Visualization", "link": "https://doi.org/10.1109/PacificVis.2018.00035", "refList": ["10.1145/989863.989871", "10.1109/tvcg.2008.59", "10.1111/j.1467-8659.2008.01332.x", "10.1109/tvcg.2016.2599049", "10.1109/pacificvis.2014.14", "10.1109/tvcg.2007.70433", "10.1007/s10596-015-9501-z", "10.1109/38.610209", "10.1109/tvcg.2003.1207447", "10.1111/j.1467-8659.2008.01181.x", "10.1109/tvcg.2012.42", "10.1109/tvcg.2016.2599217", "10.1002/fld.3764", "10.1109/pacificvis.2013.6596123", "10.1109/tvcg.2006.140", "10.1109/tvcg.2012.143", "10.1109/tvcg.2007.70565", "10.1109/scivis.2015.7429485", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.167", "10.1145/505008.505039", "10.1109/tvcg.2006.144", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/38.595268", "10.1109/tvcg.2016.2518338", "10.1109/tvcg.2014.20", "10.1145/1980462.1980487", "10.1111/cgf.12466", "10.2312/compaesth/compaesth05/209-216", "10.1109/tvcg.2010.127", "10.1016/j.advwatres.2016.01.002", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2009.144", "10.1145/1462055.1462056"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 7}], "len": 27}, {"doi": "10.1109/tvcg.2018.2865191", "title": "FiberClay: Sculpting Three Dimensional Trajectories to Reveal Structural Insights", "year": "2018", "conferenceName": "InfoVis", "authors": "Christophe Hurter;Nathalie Henry Riche;Steven Mark Drucker;Maxime Cordeil;Richard Alligier;Romain Vuillemot", "citationCount": "11", "affiliation": "Hurter, C (Corresponding Author), French Civil Aviat Univ, ENAC, Toulouse, France. Hurter, C (Corresponding Author), Toulouse Univ, Toulouse, France. Hurter, Christophe ll; Alligier, Richard, French Civil Aviat Univ, ENAC, Toulouse, France. Hurter, Christophe ll; Alligier, Richard, Toulouse Univ, Toulouse, France. Riche, Nathalie Henry; Drucker, Steven M., Microsoft Res, Redmond, WA USA. Cordeil, Maxime, Monash Univ, Clayton, Vic, Australia. Vuillemot, Romain, Univ Lyon, Ecole Cent Lyon, CNRS, UMR 5205,LIRIS, F-69134 Lyon, France.", "countries": "USA;France;Australia", "abstract": "Visualizing 3D trajectories to extract insights about their similarities and spatial configuration is a critical task in several domains. Air traffic controllers for example deal with large quantities of aircrafts routes to optimize safety in airspace and neuroscientists attempt to understand neuronal pathways in the human brain by visualizing bundles of fibers from DTI images. Extracting insights from masses of 3D trajectories is challenging as the multiple three dimensional lines have complex geometries, may overlap, cross or even merge with each other, making it impossible to follow individual ones in dense areas. As trajectories are inherently spatial and three dimensional, we propose FiberClay: a system to display and interact with 3D trajectories in immersive environments. FiberClay renders a large quantity of trajectories in real time using GP-GPU techniques. FiberClay also introduces a new set of interactive techniques for composing complex queries in 3D space leveraging immersive environment controllers and user position. These techniques enable an analyst to select and compare sets of trajectories with specific geometries and data properties. We conclude by discussing insights found using FiberClay with domain experts in air traffic control and neurology.", "keywords": "Immersive Analytics,3D Visualization,Dynamic Queries,Bimanual Interaction,Multidimensional Data", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865191", "refList": ["10.1016/b978-0-08-051574-8.50047-9", "10.1016/j.ijhcs.2013.03.003", "10.1057/palgrave.ivs.9500097", "10.1109/tvcg.2008.153", "10.2307/1269768", "10.1109/tvcg.2013.226", "10.1109/tvcg.2005.59", "10.1016/j.compenvurbsys.2014.01.005", "10.1017/cbo9781139128926.013", "10.1109/pacificvis.2014.61", "10.1145/1279640.1279642", "10.1109/pacificvis.2017.8031577", "10.1109/tvcg.2016.2599217", "10.1109/visual.1991.175794", "10.1007/978-1-4615-1177-9\\_27", "10.1007/978-0-387-35504-714", "10.1109/tvcg.2011.233", "10.3390/informatics4030026", "10.1109/tvcg.2011.192", "10.1109/tvcg.2015.2403323", "10.1145/800186.810616", "10.1145/3013971.3014006", "10.1016/j.ijhcs.2005.02.001", "10.1109/3dvis.2014.7160096", "10.1111/j.1467-8659.2012.03115.x", "10.1109/38.946631", "10.1145/3126594.3126613", "10.1109/pacificvis.2011.5742390", "10.4230/dagrep.6.6.1", "10.1111/j.1467-8659.2012.03079.x", "10.1057/palgrave.ivs.9500061", "10.1109/tvcg.2017.2744338", "10.1109/tvcg.2009.145", "10.1109/tvcg.2012.265", "10.1007/s12031-007-0029-0", "10.1145/3025453.3025566", "10.1109/tvcg.2013.153", "10.1109/tvcg.2016.2520921", "10.1016/j.trc.2014.03.005", "10.1109/3dvis.2014.7160093", "10.1109/ipsn.2014.6846743", "10.1109/tvcg.2015.2467112", "10.1109/tvcg.2017.2744079", "10.1109/tvcg.2011.224", "10.1111/cgf.12804", "10.1111/j.1467-8659.2009.01687.x", "10.1016/s1088-467x(99)00013-x", "10.1109/tvcg.2012.217", "10.1145/2992154.2996365"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934415", "title": "Evaluating an Immersive Space-Time Cube Geovisualization for Intuitive Trajectory Data Exploration", "year": "2019", "conferenceName": "InfoVis", "authors": "Jorge A. Wagner Filho;Wolfgang Stuerzlinger;Luciana Porcher Nedel", "citationCount": "4", "affiliation": "Wagner, JA (Corresponding Author), Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil. Wagner, JA (Corresponding Author), Simon Fraser Univ, Burnaby, BC, Canada. Wagner Filho, Jorge A.; Nedel, Luciana, Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil. Wagner Filho, Jorge A.; Stuerzlinger, Wolfgang, Simon Fraser Univ, Burnaby, BC, Canada.", "countries": "Canada;Brazil", "abstract": "A Space-Time Cube enables analysts to clearly observe spatio-temporal features in movement trajectory datasets in geovisualization. However, its general usability is impacted by a lack of depth cues, a reported steep learning curve, and the requirement for efficient 3D navigation. In this work, we investigate a Space-Time Cube in the Immersive Analytics domain. Based on a review of previous work and selecting an appropriate exploration metaphor, we built a prototype environment where the cube is coupled to a virtual representation of the analyst's real desk, and zooming and panning in space and time are intuitively controlled using mid-air gestures. We compared our immersive environment to a desktop-based implementation in a user study with 20 participants across 7 tasks of varying difficulty, which targeted different user interface features. To investigate how performance is affected in the presence of clutter, we explored two scenarios with different numbers of trajectories. While the quantitative performance was similar for the majority of tasks, large differences appear when we analyze the patterns of interaction and consider subjective metrics. The immersive version of the Space-Time Cube received higher usability scores, much higher user preference, and was rated to have a lower mental workload, without causing participants discomfort in 25-minute-long VR sessions.", "keywords": "Space-time cube,Trajectory visualization,Immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934415", "refList": ["10.1109/mcg.2006.74", "10.1080/13658816.2015.1058386", "10.1145/1456650.1456652", "10.1111/cgf.13430", "10.1145/1944745.1944777", "10.1016/b978-155860819-1/50001-7", "10.1109/bdva.2018.8534024", "10.1109/pacificvis.2017.8031578", "10.1111/j.1467-8306.1994.tb01869.x", "10.2307/3001968", "10.1007/978-3-642-15300-6\\_21", "10.1080/13658816.2010.511223", "10.1007/978-3-030-01388-2", "10.1080/00087041.2018.1495898", "10.1145/1773965.1773970", "10.1002/9780470987643.ch15", "10.1109/glocom.2015.7417476", "10.2312/eurovisstar.20141171", "10.1080/17489725.2015.1074736", "10.1109/bdva.2016.7787050", "10.1109/wevr.2017.7957707", "10.1016/s1045-926x(03)00046-6", "10.1109/iv.2018.00026", "10.1179/1743277413y.0000000061", "10.1109/mcg.2019.2898856", "10.1109/iv.2004.1320137", "10.1111/j.1467-8659.2011.01929.x", "10.1109/tvcg.2008.194", "10.1016/j.jtrangeo.2010.11.002", "10.1177/154193120605000909", "10.1109/bigdata.2015.7364040", "10.1111/cgf.12466", "10.1109/icsens.2015.7370446", "10.1080/13658816.2010.508043", "10.1057/ivs.2009.8", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.14236/ewic/hci2016.22", "10.1145/2655691", "10.1007/bf01936872", "10.1109/infvis.2004.27", "10.1109/tvcg.2014.2329308", "10.1111/j.1435-5597.1970.tb01464.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934332", "title": "LassoNet: Deep Lasso-Selection of 3D Point Clouds", "year": "2019", "conferenceName": "SciVis", "authors": "Zhutian Chen;Wei Zeng 0004;Zhiguang Yang;Lingyun Yu;Chi-Wing Fu;Huamin Qu", "citationCount": "4", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Zhutian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei; Yang, Zhiguang, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yu, Lingyun, Univ Groningen, Groningen, Netherlands. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China.", "countries": "China;Netherlands", "abstract": "Selection is a fundamental task in exploratory analysis and visualization of 3D point clouds. Prior researches on selection methods were developed mainly based on heuristics such as local point density, thus limiting their applicability in general data. Specific challenges root in the great variabilities implied by point clouds (e.g., dense vs. sparse), viewpoint (e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this work, we introduce LassoNet, a new deep neural network for lasso selection of 3D point clouds, attempting to learn a latent mapping from viewpoint and lasso to point cloud regions. To achieve this, we couple user-target points with viewpoint and lasso information through 3D coordinate transform and naive selection, and improve the method scalability via an intention filtering and farthest point sampling. A hierarchical network is trained using a dataset with over 30K lasso-selection records on two different point cloud data. We conduct a formal user study to compare LassoNet with two state-of-the-art lasso-selection methods. The evaluations confirm that our approach improves the selection effectiveness and efficiency across different combinations of 3D point clouds, viewpoints, and lasso selections. Project Website: https://LassoNet.github.io", "keywords": "Point Clouds,Lasso Selection,Deep Learning", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934332", "refList": ["10.1111/j.1467-8659.2009.01515.x", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2016.2599049", "10.1109/iccv.2015.114", "10.1109/vppc.2018.8604993", "10.1111/cgf.13405", "10.1109/tvcg.2018.2843369", "10.1109/cvpr.2018.00278", "10.1145/3025453.3025957", "10.1109/mc.2013.178", "10.1145/2980179.2980238", "10.1109/msp.2017.2693418", "10.1145/237091.237105", "10.1145/2835487", "10.1109/cvpr.2010.5539838", "10.1016/j.visinf.2017.01.006", "10.1109/tcst.2018.2819965", "10.1109/tvcg.2018.2865138", "10.1109/83.623193", "10.1145/3072959.3073608", "10.1145/1053427.1053445", "10.1016/s0039-9140(96)02179-0", "10.1109/cvpr.2015.7298801", "10.1109/cvpr.2017.693", "10.1007/s12650-014-0206-5", "10.1145/3272127.3275110", "10.1109/cvpr.2015.7298845", "10.1109/iccvw.2015.112", "10.1109/iros.2015.7353481", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2012.217", "10.1109/tridui.2006.1618279", "10.1109/tvcg.2012.292", "10.1109/cvpr.2016.609", "10.1016/j.cag.2012.12.003"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028947", "title": "A Fluid Flow Data Set for Machine Learning and its Application to Neural Flow Map Interpolation", "year": "2020", "conferenceName": "SciVis", "authors": "Jakob Jakob;Markus H. Gross;Tobias G\u00fcnther", "citationCount": "0", "affiliation": "Jakob, J (Corresponding Author), Swiss Fed Inst Technol, Zurich, Switzerland. Jakob, Jakob; Gross, Markus; Guenther, Tobias, Swiss Fed Inst Technol, Zurich, Switzerland.", "countries": "Switzerland", "abstract": "In recent years, deep learning has opened countless research opportunities across many different disciplines. At present, visualization is mainly applied to explore and explain neural networks. Its counterpart-the application of deep learning to visualization problems-requires us to share data more openly in order to enable more scientists to engage in data-driven research. In this paper, we construct a large fluid flow data set and apply it to a deep learning problem in scientific visualization. Parameterized by the Reynolds number, the data set contains a wide spectrum of laminar and turbulent fluid flow regimes. The full data set was simulated on a high-performance compute cluster and contains 8000 time-dependent 2D vector fields, accumulating to more than 16 TB in size. Using our public fluid data set, we trained deep convolutional neural networks in order to set a benchmark for an improved post-hoc Lagrangian fluid flow analysis. In in-situ settings, flow maps are exported and interpolated in order to assess the transport characteristics of time-dependent fluids. Using deep learning, we improve the accuracy of flow map interpolations, allowing a more precise flow analysis at a reduced memory IO footprint.", "keywords": "Scientific visualization,deep learning,flow maps", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028947", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1145/3355089.3356560", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1007/978-3-030-00533-7\\_36", "10.3390/rs11161921", "10.1007/978-3-030-48457-6\\_1", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/mcg.2018.2881523", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1145/3072959.3073643", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/tvcg.2013.128", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1007/978-3-319-46475-6\\_43", "10.1126/science.1127647", "10.1007/978-3-319-46475-6\\_25", "10.1111/cgf.13689"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030346", "title": "V2V: A Deep Learning Approach to Variable-to-Variable Selection and Translation for Multivariate Time-Varying Data", "year": "2020", "conferenceName": "SciVis", "authors": "Jun Han;Hao Zheng 0006;Yunhao Xing;Danny Ziyi Chen;Chaoli Wang", "citationCount": "0", "affiliation": "Han, J (Corresponding Author), Univ Notre Dame, Notre Dame, IN 46556 USA. Han, Jun; Zheng, Hao; Chen, Danny Z.; Wang, Chaoli, Univ Notre Dame, Notre Dame, IN 46556 USA. Xing, Yunhao, Sichuan Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China;USA", "abstract": "We present V2V, a novel deep learning framework, as a general-purpose solution to the variable-to-variable (V2V) selection and translation problem for multivariate time-varying data (MTVD) analysis and visualization. V2V leverages a representation learning algorithm to identify transferable variables and utilizes Kullback-Leibler divergence to determine the source and target variables. It then uses a generative adversarial network (GAN) to learn the mapping from the source variable to the target variable via the adversarial, volumetric, and feature losses. V2V takes the pairs of time steps of the source and target variable as input for training, Once trained, it can infer unseen time steps of the target variable given the corresponding time steps of the source variable. Several multivariate time-varying data sets of different characteristics are used to demonstrate the effectiveness of V2V, both quantitatively and qualitatively. We compare V2V against histogram matching and two other deep learning solutions (Pix2Pix and CycleGAN).", "keywords": "Multivariate time-varying data,variable selection and translation,generative adversarial network,data extrapolation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030346", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/cvpr.2017.19", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1146/annurev", "10.3390/rs11161921", "10.1109/tvcg.2007.70551", "10.1145/3309993", "10.1007/978-3-030-48457-6\\_1", "10.1109/igarss.2018.8518411", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1146/annurev-fluid-010313-141322", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1126/science.1127647"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030374", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data", "year": "2020", "conferenceName": "SciVis", "authors": "Yifan Wang;Guoli Yan;Haikuan Zhu;Sagar Buch;Ying Wang;E. Mark Haacke;Jing Hua;Zichun Zhong", "citationCount": "0", "affiliation": "Wang, YF (Corresponding Author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Wang, Yifan; Yan, Guoli; Zhu, Haikuan; Hua, Jing; Zhong, Zichun, Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Buch, Sagar; Wang, Ying; Haacke, Ewart Mark, Wayne State Univ, Dept Radiol, Detroit, MI 48201 USA.", "countries": "USA", "abstract": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.", "keywords": "Deep neural network,3D cerebrovascular segmentation and visualization,maximum intensity projection (MIP),joint embedding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030374", "refList": ["10.1109/cluster.2018.00036", "10.1109/iccv.2015.123", "10.1109/tvcg.2013.133", "10.1109/cvpr.2017.19", "10.1109/tvcg.2019.2934312", "10.1007/978-3-319-46487-9\\_40", "10.1109/tvcg.2006.175", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2018.2880207", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/tvcg.2018.2816059", "10.1109/pacificvis.2009.4906852", "10.1109/tvcg.2018.2864808", "10.1109/mcg.2018.2881523", "10.1109/tvcg.2015.2467431", "10.1109/tpami.2015.2439281", "10.1109/pacificvis48177.2020.8737", "10.1109/cvpr.2019.00244", "10.1109/tvcg.2006.165", "10.1109/bigdata.2018.8622520", "10.1007/978-3-319-46466-4\\_29", "10.1109/visual.2019.8933759", "10.1145/3197517.3201304", "10.1109/pacificvis.2011.5742378", "10.1109/cvpr.2006.91", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1007/978-3-319-24574-4\\_28", "10.1109/pacificvis.2019.00041", "10.1109/tvcg.2019.2934332", "10.1109/cvpr.2018.00916", "10.1007/978-3-319-46475-6\\_43"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934803", "title": "There Is No Spoon: Evaluating Performance, Space Use, and Presence with Expert Domain Users in Immersive Analytics", "year": "2019", "conferenceName": "InfoVis", "authors": "Andrea Batch;Andrew Cunningham;Maxime Cordeil;Niklas Elmqvist;Tim Dwyer;Bruce H. Thomas;Kim Marriott", "citationCount": "6", "affiliation": "Batch, A (Corresponding Author), Univ Maryland, College Pk, MD 20742 USA. Batch, Andrea; Elmqvist, Niklas, Univ Maryland, College Pk, MD 20742 USA. Cunningham, Andrew; Thomas, Bruce H., Univ South Australia Adelaide, Adelaide, SA, Australia. Cordeil, Maxime; Dwyer, Tim; Marriott, Kim, Univ Melbourne, Melbourne, Vic, Australia.", "countries": "USA;Australia", "abstract": "Immersive analytics turns the very space surrounding the user into a canvas for data analysis, supporting human cognitive abilities in myriad ways. We present the results of a design study, contextual inquiry, and longitudinal evaluation involving professional economists using a Virtual Reality (VR) system for multidimensional visualization to explore actual economic data. Results from our preregistered evaluation highlight the varied use of space depending on context (exploration vs. presentation), the organization of space to support work, and the impact of immersion on navigation and orientation in the 3D analysis space.", "keywords": "Design study,evaluation,economic analysis,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934803", "refList": ["10.1145/3139131.3139141", "10.1109/2.60882", "10.1007/978-3-030-01388-2\\_1", "10.1109/38.250911", "10.1145/302979.303166", "10.1073/pnas.1306779110", "10.1109/tvcg.2012.219", "10.1162/105474698565686", "10.1016/0364-0213(94)90007-8", "10.1145/642611.642650", "10.1162/pres.1992.1.4.482", "10.1007/978-3-030-01388-2", "10.1109/2.19829", "10.1111/1467-8284.00096", "10.1007/978-3-030-01388-2\\_7", "10.1109/hicss.2011.339", "10.1109/tvcg.2016.2599107", "10.1145/3173574.3173664", "10.1145/1008653.1008669", "10.1109/tvcg.2017.2743990", "10.1145/642611", "10.1145/3126594.3126613", "10.1007/978-94-007-6833-89", "10.1162/pres\\_a\\_00124", "10.1016/0004-3702(94)00017-u", "10.1109/tvcg.2014.20", "10.1145/1168149.1168158", "10.1109/visual.1995.480800", "10.1109/icsens.2015.7370446", "10.1207/s15516709cog1701\\_1", "10.1162/105474601300343603", "10.1109/tvcg.2016.2518135", "10.1109/tvcg.2012.213", "10.1016/s0097-8493(02)00113-9", "10.1162/pres.1994.3.2.130", "10.1109/tvcg.2018.2865191", "10.1109/visual.1990.146402", "10.1162/105474601300343612", "10.1145/2702123.2702406", "10.1162/pres.1992.1.1.120", "10.1145/3009939.3009955", "10.1109/glocom.2015.7417476", "10.1162/pres\\_a\\_00261"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030450", "title": "Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment", "year": "2020", "conferenceName": "InfoVis", "authors": "Benjamin Lee;Xiaoyun Hu;Maxime Cordeil;Arnaud Prouzeau;Bernhard Jenny;Tim Dwyer", "citationCount": "0", "affiliation": "Lee, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Lee, Benjamin; Hu, Xiaoyun; Cordeil, Maxime; Prouzeau, Arnaud; Jenny, Bernhard; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia.", "countries": "Australia", "abstract": "Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA: the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner's personal workspace.", "keywords": "Immersive analytics,collaboration,virtual reality,qualitative study,multivariate data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030450", "refList": ["10.1109/tvcg.2008.153", "10.1007/s10606-004-5062-8", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2019.2914677", "10.1016/0020-7373(91)90039-a", "10.1109/tvcg.2011.287", "10.1117/12.2005484", "10.1109/mmul.2009.35", "10.1109/tvcg.2019.2934803", "10.1145/3359996.3364242", "10.1109/immersive.2016.7932384", "10.1016/j.future.2008.07.015", "10.1057/palgrave.ivs.9500167", "10.1109/mcg.2019.2898941", "10.1145/3343055.3360746", "10.1007/978-3-319-45853-3\\_8", "10.1145/2576099", "10.1145/2858036.2858039", "10.1109/tvcg.2016.2599107", "10.1145/3173574.3173664", "10.1007/978-3-030-01388-2\\_2", "10.1109/vr.2019.8797978", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1007/978-3-030-01388-2\\_8", "10.1023/a:1021271517844", "10.1109/bigdata.2014.7004282", "10.1109/tvcg.2019.2934395", "10.1109/ismar.2010.5643530", "10.1007/978-3-030-01388-22", "10.1145/2133806.2133821", "10.1109/3dvis.2014.7160093", "10.1145/2556288.2557058", "10.1109/vr.2019.8797845"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030392", "title": "ShuttleSpace: Exploring and Analyzing Movement Trajectory in Immersive Visualization", "year": "2020", "conferenceName": "InfoVis", "authors": "Shuainan Ye;Zhutian Chen;Xiangtong Chu;Yifan Wang;Siwei Fu;Lejun Shen;Kun Zhou;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD CG, Hangzhou, Peoples R China. Ye, Shuainan; Chu, Xiangtong; Wang, Yifan; Zhou, Kun; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD CG, Hangzhou, Peoples R China. Chen, Zhutian, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Fu, Siwei, Zhejiang Lab, Hangzhou, Zhejiang, Peoples R China. Shen, Lejun, Chengdu Sports Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China", "abstract": "We present ShuttleSpace, an immersive analytics system to assist experts in analyzing trajectory data in badminton. Trajectories in sports, such as the movement of players and balls, contain rich information on player behavior and thus have been widely analyzed by coaches and analysts to improve the players' performance. However, existing visual analytics systems often present the trajectories in court diagrams that are abstractions of reality, thereby causing difficulty for the experts to imagine the situation on the court and understand why the player acted in a certain way. With recent developments in immersive technologies, such as virtual reality (VR), experts gradually have the opportunity to see, feel, explore, and understand these 3D trajectories from the player's perspective. Yet, few research has studied how to support immersive analysis of sports data from such a perspective. Specific challenges are rooted in data presentation (e.g., how to seamlessly combine 2D and 3D visualizations) and interaction (e.g., how to naturally interact with data without keyboard and mouse) in VR. To address these challenges, we have worked closely with domain experts who have worked for a top national badminton team to design ShuttleSpace. Our system leverages 1) the peripheral vision to combine the 2D and 3D visualizations and 2) the VR controller to support natural interactions via a stroke metaphor. We demonstrate the effectiveness of ShuttleSpace through three case studies conducted by the experts with useful insights. We further conduct interviews with the experts whose feedback confirms that our first-person immersive analytics system is suitable and useful for analyzing badminton data.", "keywords": "Movement trajectory,badminton analytics,virtual reality", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030392", "refList": ["10.1016/j.eswa.2012.01.204", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1142/s1005386719000178", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/978-981-10-3994-2\\_1", "10.1007/978-3-319-07812-0\\_12", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1016/j.eswa.2017.07.027", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 35}, {"doi": "10.1109/tvcg.2018.2865192", "title": "Origin-Destination Flow Maps in Immersive Environments", "year": "2018", "conferenceName": "InfoVis", "authors": "Yalong Yang;Tim Dwyer;Bernhard Jenny;Kim Marriott;Maxime Cordeil;Haohui Chen", "citationCount": "5", "affiliation": "Yang, YL (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Yang, YL (Corresponding Author), CSIRO, Data61, Canberra, ACT, Australia. Yang, Yalong; Dwyer, Tim; Jenny, Bernhard; Marriott, Kim; Cordeil, Maxime, Monash Univ, Clayton, Vic, Australia. Yang, Yalong; Chen, Haohui, CSIRO, Data61, Canberra, ACT, Australia.", "countries": "Australia", "abstract": "Immersive virtual- and augmented-reality headsets can overlay a flat image against any surface or hang virtual objects in the space around the user. The technology is rapidly improving and may, in the long term, replace traditional flat panel displays in many situations. When displays are no longer intrinsically flat, how should we use the space around the user for abstract data visualisation? In this paper, we ask this question with respect to origin-destination flow data in a global geographic context. We report on the findings of three studies exploring different spatial encodings for flow maps. The first experiment focuses on different 2D and 3D encodings for flows on flat maps. We find that participants are significantly more accurate with raised flow paths whose height is proportional to flow distance but fastest with traditional straight line 2D flows. In our second and third experiment we compared flat maps, 3D globes and a novel interactive design we call<i>MapsLink</i>, involving a pair of linked flat maps. We find that participants took significantly more time with MapsLink than other flow maps while the 3D globe with raised flows was the fastest, most accurate, and most preferred method. Our work suggests that<i>careful</i>use of the third spatial dimension can resolve visual clutter in complex flow maps.", "keywords": "Origin-destination,Flow Map,Virtual Reality,Cartographic Information Visualisation,Immersive Analytics", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865192", "refList": ["10.1111/j.1467-8659.2011.01946.x", "10.1145/2254556.2254652", "10.1080/13658816.2017.1307378", "10.1179/caj.1971.8.2.139", "10.1080/17445647.2017.1313788", "10.1007/s00168-008-0256-5", "10.1016/j.compenvurbsys.2009.01.007", "10.1145/1080402.1080411", "10.1109/infvis.2003.1249008", "10.1109/infvis.1995.528697", "10.1080/15230406.2016.1262280", "10.1093/comjnl/bxx117", "10.1145/159544.159587", "10.1109/glocom.2015.7417476", "10.1145/3013971.3013983", "10.1126/science.1248676", "10.1109/eitt.2017.18", "10.1109/infvis.2005.1532150", "10.1080/03085696708592302", "10.1145/2702123.2702172", "10.1109/tvcg.2011.181", "10.1145/3126594.3126613", "10.1109/pacificvis.2011.5742390", "10.1109/tvcg.2016.2598958", "10.4230/dagrep.6.6.1", "10.1109/infvis.1996.559226", "10.1007/978-3-319-03841-4\\_34", "10.1111/cgf.13431", "10.1109/tvcg.2014.2346441", "10.1179/000870410x12658023467367", "10.1109/tvcg.2007.70521", "10.1109/icsens.2015.7370446", "10.2307/1791753", "10.1109/38.486685", "10.1109/tvcg.2016.2520921", "10.1559/152304087783875273", "10.1007/bf01936872", "10.1016/j.jvlc.2016.07.006", "10.1109/3dvis.2014.7160094", "10.1111/j.0033-0124.1981.00419.x", "10.1109/tvcg.2016.2598885", "10.1111/j.1435-5597.1970.tb01464.x", "10.1177/1473871616681375", "10.1109/tvcg.2011.202", "10.1016/j.jvlc.2014.03.001"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030392", "title": "ShuttleSpace: Exploring and Analyzing Movement Trajectory in Immersive Visualization", "year": "2020", "conferenceName": "InfoVis", "authors": "Shuainan Ye;Zhutian Chen;Xiangtong Chu;Yifan Wang;Siwei Fu;Lejun Shen;Kun Zhou;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD CG, Hangzhou, Peoples R China. Ye, Shuainan; Chu, Xiangtong; Wang, Yifan; Zhou, Kun; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD CG, Hangzhou, Peoples R China. Chen, Zhutian, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Fu, Siwei, Zhejiang Lab, Hangzhou, Zhejiang, Peoples R China. Shen, Lejun, Chengdu Sports Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China", "abstract": "We present ShuttleSpace, an immersive analytics system to assist experts in analyzing trajectory data in badminton. Trajectories in sports, such as the movement of players and balls, contain rich information on player behavior and thus have been widely analyzed by coaches and analysts to improve the players' performance. However, existing visual analytics systems often present the trajectories in court diagrams that are abstractions of reality, thereby causing difficulty for the experts to imagine the situation on the court and understand why the player acted in a certain way. With recent developments in immersive technologies, such as virtual reality (VR), experts gradually have the opportunity to see, feel, explore, and understand these 3D trajectories from the player's perspective. Yet, few research has studied how to support immersive analysis of sports data from such a perspective. Specific challenges are rooted in data presentation (e.g., how to seamlessly combine 2D and 3D visualizations) and interaction (e.g., how to naturally interact with data without keyboard and mouse) in VR. To address these challenges, we have worked closely with domain experts who have worked for a top national badminton team to design ShuttleSpace. Our system leverages 1) the peripheral vision to combine the 2D and 3D visualizations and 2) the VR controller to support natural interactions via a stroke metaphor. We demonstrate the effectiveness of ShuttleSpace through three case studies conducted by the experts with useful insights. We further conduct interviews with the experts whose feedback confirms that our first-person immersive analytics system is suitable and useful for analyzing badminton data.", "keywords": "Movement trajectory,badminton analytics,virtual reality", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030392", "refList": ["10.1016/j.eswa.2012.01.204", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1142/s1005386719000178", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/978-981-10-3994-2\\_1", "10.1007/978-3-319-07812-0\\_12", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1016/j.eswa.2017.07.027", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2019.00022", "year": "2019", "title": "What-Why Analysis of Expert Interviews: Analysing Geographically-Embedded Flow Data", "conferenceName": "PacificVis", "authors": "Yalong Yang;Sarah Goodwin", "citationCount": "0", "affiliation": "Yang, YL (Corresponding Author), Monash Univ, Caulfield Sch Informat Technol, Clayton, Vic, Australia.\nYang, Yalong; Goodwin, Sarah, Monash Univ, Caulfield Sch Informat Technol, Clayton, Vic, Australia.", "countries": "Australia", "abstract": "In this paper, we present our analysis of five expert interviews, each from a different application domain. Such analysis is crucial to understanding the real-world scenarios of analysing geographically-embedded flow data. The results of our analysis show that similar high-level tasks were conducted in different domains. To better describe the targets of these tasks, we proposed three flow-targets for analysing geographically-embedded flow data: single flow, total flow and regional flow.", "keywords": "Human-centered computing; Visualization; Visualization design and evaluation methods", "link": "https://doi.org/10.1109/PacificVis.2019.00022", "refList": ["10.1109/dsn-w.2018.00014", "10.1109/tvcg.2012.219", "10.1109/tvcg.2015.2500225", "10.1109/tvcg.2018.2865192", "10.1016/j.compenvurbsys.2013.10.007", "10.1109/tvcg.2013.130", "10.1145/1168149.1168168", "10.1109/tvcg.2016.2598885", "10.1145/3009939", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2865040", "10.1017/s0950268816002053", "10.1109/infvis.2005.1532136"], "wos": 1, "children": [], "len": 1}], "len": 11}, {"doi": "10.1109/tvcg.2019.2934370", "title": "Deadeye Visualization Revisited: Investigation of Preattentiveness and Applicability in Virtual Environments", "year": "2019", "conferenceName": "SciVis", "authors": "Andrey Krekhov;Sebastian Cmentowski;Andre Waschk;Jens H. Kr\u00fcger", "citationCount": "1", "affiliation": "Krekhov, A (Corresponding Author), Univ Duisburg Essen, Ctr Visual Data Anal \\& Comp Graph COVIDAG, Duisburg, Germany. Krekhov, Andrey; Cmentowski, Sebastian; Waschk, Andre; Kruger, Jens, Univ Duisburg Essen, Ctr Visual Data Anal \\& Comp Graph COVIDAG, Duisburg, Germany. Kruger, Jens, Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA.", "countries": "Germany;USA", "abstract": "Visualizations rely on highlighting to attract and guide our attention. To make an object of interest stand out independently from a number of distractors, the underlying visual cue, e.g., color, has to be preattentive. In our prior work, we introduced Deadeye as an instantly recognizable highlighting technique that works by rendering the target object for one eye only. In contrast to prior approaches, Deadeye excels by not modifying any visual properties of the target. However, in the case of 2D visualizations, the method requires an additional setup to allow dichoptic presentation, which is a considerable drawback. As a follow-up to requests from the community, this paper explores Deadeye as a highlighting technique for 3D visualizations, because such stereoscopic scenarios support dichoptic presentation out of the box. Deadeye suppresses binocular disparities for the target object, so we cannot assume the applicability of our technique as a given fact. With this motivation, the paper presents quantitative evaluations of Deadeye in VR, including configurations with multiple heterogeneous distractors as an important robustness challenge. After confirming the preserved preattentiveness (all average accuracies above 90%) under such real-world conditions, we explore VR volume rendering as an example application scenario for Deadeye. We depict a possible workflow for integrating our technique, conduct an exploratory survey to demonstrate benefits and limitations, and finally provide related design implications.", "keywords": "Popout,virtual reality,preattentive vision,volume rendering,dichoptic presentation,binocular rivalry", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934370", "refList": ["10.1016/s0893-6080(99)00019-2", "10.3758/bf03207480", "10.1002/j.1538-7305.1960.tb03954.x", "10.1109/tvcg.2011.127", "10.2312/egwr/egsr06/377-387", "10.3758/bf03199301", "10.1109/5.662875", "10.1016/0042-6989(90)90161-d", "10.1371/journal.pone.0129101", "10.1167/8.5.1", "10.1037/0033-295x.95.1.15", "10.1038/332154a0", "10.1016/j.media.2017.07.005", "10.1167/12.12.2", "10.1109/vis.2003.10001", "10.3758/bf03211889", "10.1038/290091a0", "10.1037/0033-295x.114.3.599", "10.1145/3242671.3242704", "10.1016/0042-6989(94)90311-5", "10.1016/j.visres.2007.07.003", "10.1109/tvcg.2011.234", "10.1109/tvcg.2019.2898796", "10.1145/2207676.2208638", "10.1037/0033-2909.83.5.880", "10.1016/s0042-6989(97)00167-3", "10.1037/0033-295x.96.1.145", "10.1115/1.2830851", "10.1109/medivis.2008.10", "10.1038/35058500", "10.1016/s0042-6989(96)00156-3", "10.1037/0096-1523.15.3.419", "10.1109/tvcg.2018.2864498", "10.1038/s41562-017-0058", "10.1109/tvcg.2014.2346352", "10.1167/iovs.03-0878", "10.1109/tvcg.2016.2520921", "10.1111/cgf.12936", "10.1109/mcg.2018.032421652", "10.1163/156856805774406756", "10.1145/234972.234975", "10.1111/j.1467-9280.1990.tb00067.x", "10.1109/tvcg.2012.42", "10.1037/0096-1523.12.1.3", "10.1016/j.visres.2009.06.021", "10.1007/978-1-4684-6775-8\\_9", "10.3758/bf03211045", "10.1007/978-1-4684-6775-8", "10.1016/0042-6989(90)90128-8", "10.1016/j.visres.2009.05.001", "10.1016/j.visres.2004.04.017", "10.1145/1502800.1502805", "10.1016/0042-6989(71)90213-6", "10.1109/tpami.2012.89", "10.1038/333265a0", "10.1109/tvcg.2016.2518338", "10.1145/3025453.3025984", "10.1038/320264a0", "10.1115/imece2007-43781", "10.1145/1970378.1970384", "10.1111/j.1467-9280.1990.tb00227.x", "10.1111/tra.12538", "10.3758/s13414-011-0256-x", "10.1109/tale.2018.8615248", "10.3758/s13414-011-0100-3", "10.1080/135062899395037", "10.1098/rspb.1979.0029", "10.1038/380621a0", "10.1016/0010-0285(80)90005-5", "10.1007/978-1-4899-5379-7\\_8", "10.1016/0042-6989(96)00099-5", "10.1109/34.865184", "10.1068/p3456", "10.1109/pacificvis.2015.7156357", "10.3758/s13414-016-1247-8", "10.1007/978-1-4899-5379-7", "10.1016/s0042-6989(98)00052-2", "10.1126/science.2300824"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934415", "title": "Evaluating an Immersive Space-Time Cube Geovisualization for Intuitive Trajectory Data Exploration", "year": "2019", "conferenceName": "InfoVis", "authors": "Jorge A. Wagner Filho;Wolfgang Stuerzlinger;Luciana Porcher Nedel", "citationCount": "4", "affiliation": "Wagner, JA (Corresponding Author), Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil. Wagner, JA (Corresponding Author), Simon Fraser Univ, Burnaby, BC, Canada. Wagner Filho, Jorge A.; Nedel, Luciana, Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil. Wagner Filho, Jorge A.; Stuerzlinger, Wolfgang, Simon Fraser Univ, Burnaby, BC, Canada.", "countries": "Canada;Brazil", "abstract": "A Space-Time Cube enables analysts to clearly observe spatio-temporal features in movement trajectory datasets in geovisualization. However, its general usability is impacted by a lack of depth cues, a reported steep learning curve, and the requirement for efficient 3D navigation. In this work, we investigate a Space-Time Cube in the Immersive Analytics domain. Based on a review of previous work and selecting an appropriate exploration metaphor, we built a prototype environment where the cube is coupled to a virtual representation of the analyst's real desk, and zooming and panning in space and time are intuitively controlled using mid-air gestures. We compared our immersive environment to a desktop-based implementation in a user study with 20 participants across 7 tasks of varying difficulty, which targeted different user interface features. To investigate how performance is affected in the presence of clutter, we explored two scenarios with different numbers of trajectories. While the quantitative performance was similar for the majority of tasks, large differences appear when we analyze the patterns of interaction and consider subjective metrics. The immersive version of the Space-Time Cube received higher usability scores, much higher user preference, and was rated to have a lower mental workload, without causing participants discomfort in 25-minute-long VR sessions.", "keywords": "Space-time cube,Trajectory visualization,Immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934415", "refList": ["10.1109/mcg.2006.74", "10.1080/13658816.2015.1058386", "10.1145/1456650.1456652", "10.1111/cgf.13430", "10.1145/1944745.1944777", "10.1016/b978-155860819-1/50001-7", "10.1109/bdva.2018.8534024", "10.1109/pacificvis.2017.8031578", "10.1111/j.1467-8306.1994.tb01869.x", "10.2307/3001968", "10.1007/978-3-642-15300-6\\_21", "10.1080/13658816.2010.511223", "10.1007/978-3-030-01388-2", "10.1080/00087041.2018.1495898", "10.1145/1773965.1773970", "10.1002/9780470987643.ch15", "10.1109/glocom.2015.7417476", "10.2312/eurovisstar.20141171", "10.1080/17489725.2015.1074736", "10.1109/bdva.2016.7787050", "10.1109/wevr.2017.7957707", "10.1016/s1045-926x(03)00046-6", "10.1109/iv.2018.00026", "10.1179/1743277413y.0000000061", "10.1109/mcg.2019.2898856", "10.1109/iv.2004.1320137", "10.1111/j.1467-8659.2011.01929.x", "10.1109/tvcg.2008.194", "10.1016/j.jtrangeo.2010.11.002", "10.1177/154193120605000909", "10.1109/bigdata.2015.7364040", "10.1111/cgf.12466", "10.1109/icsens.2015.7370446", "10.1080/13658816.2010.508043", "10.1057/ivs.2009.8", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.14236/ewic/hci2016.22", "10.1145/2655691", "10.1007/bf01936872", "10.1109/infvis.2004.27", "10.1109/tvcg.2014.2329308", "10.1111/j.1435-5597.1970.tb01464.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934208", "title": "Evaluating Perceptual Bias During Geometric Scaling of Scatterplots", "year": "2019", "conferenceName": "VAST", "authors": "Yating Wei;Honghui Mei;Ying Zhao;Shuyue Zhou;Bingru Lin;Haojing Jiang;Wei Chen", "citationCount": "5", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou 310058, Zhejiang, Peoples R China. Zhao, Y (Corresponding Author), Cent South Univ, Sch Comp Sci \\& Engn, Changsha 410083, Hunan, Peoples R China. Wei, Yating; Mei, Honghui; Zhou, Shuyue; Lin, Bingru; Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou 310058, Zhejiang, Peoples R China. Zhao, Ying; Jiang, Haojing, Cent South Univ, Sch Comp Sci \\& Engn, Changsha 410083, Hunan, Peoples R China.", "countries": "China", "abstract": "Scatterplots are frequently scaled to fit display areas in multi-view and multi-device data analysis environments. A common method used for scaling is to enlarge or shrink the entire scatterplot together with the inside points synchronously and proportionally. This process is called geometric scaling. However, geometric scaling of scatterplots may cause a perceptual bias, that is, the perceived and physical values of visual features may be dissociated with respect to geometric scaling. For example, if a scatterplot is projected from a laptop to a large projector screen, then observers may feel that the scatterplot shown on the projector has fewer points than that viewed on the laptop. This paper presents an evaluation study on the perceptual bias of visual features in scatterplots caused by geometric scaling. The study focuses on three fundamental visual features (i.e., numerosity, correlation, and cluster separation) and three hypotheses that are formulated on the basis of our experience. We carefully design three controlled experiments by using well-prepared synthetic data and recruit participants to complete the experiments on the basis of their subjective experience. With a detailed analysis of the experimental results, we obtain a set of instructive findings. First, geometric scaling causes a bias that has a linear relationship with the scale ratio. Second, no significant difference exists between the biases measured from normally and uniformly distributed scatterplots. Third, changing the point radius can correct the bias to a certain extent. These findings can be used to inspire the design decisions of scatterplots in various scenarios.", "keywords": "Evaluation,scatterplot,geometric scaling,bias,perceptual consistency", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934208", "refList": ["10.2307/2288843", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2014.2346979", "10.1126/science.216.4550.1138", "10.1109/tvcg.2017.2744138", "10.1111/j.1467-8659.2009.01467.x", "10.1145/2491568.2491577", "10.1109/mwc.2018.1700325", "10.1145/2449396.2449439", "10.1109/tvcg.2011.127", "10.1109/tvcg.2011.229", "10.1167/15.5.4", "10.1073/pnas.1113195108", "10.1145/2702123.2702545", "10.1109/vast.2009.5332628", "10.1109/tvcg.2018.2800013", "10.1007/s12650-018-0530-2", "10.1016/s0042-6989(97)00340-4", "10.1109/vast.2010.5652460", "10.1109/mcg.2018.2879067", "10.1109/tvcg.2018.2864912", "10.1109/pacificvis.2010.5429604", "10.1109/tcst.2018.2819965", "10.1167/10.2.10", "10.1145/2470654.2481318", "10.1145/1842993.1843002", "10.1167/12.6.8", "10.1109/tvcg.2013.124", "10.1057/ivs.2008.13", "10.1109/tvcg.2007.70596", "10.1109/tvcg.2018.2865266", "10.1145/3173574.3173664", "10.1109/tvcg.2015.2467671", "10.1016/j.cag.2017.07.004", "10.1177/0956797613501520", "10.1109/tvcg.2018.2865020", "10.1111/j.1467-8659.2012.03125.x", "10.3758/bf03205986", "10.3758/s13423-016-1174-7", "10.1109/tvcg.2017.2680452", "10.1109/mc.2006.109", "10.1109/tvcg.2017.2744098", "10.1038/srep32810", "10.1016/j.visres.2013.06.006", "10.1002/jhbs.20078", "10.1109/tvcg.2006.163", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1016/j.cognition.2007.10.009", "10.1109/tvcg.2018.2865142", "10.3758/app.72.7.1839", "10.1109/tvcg.2018.2810918", "10.1016/j.jvlc.2017.10.001", "10.1145/2682623", "10.1109/tvcg.2018.2864884", "10.1145/3025453.3025984", "10.1109/tvcg.2006.184", "10.1109/tvcg.2013.153", "10.1016/j.jvlc.2018.08.003", "10.1109/tvcg.2016.2520921", "10.1111/cgf.13446", "10.1017/s0022381612000187", "10.1145/2702123.2702406", "10.1109/vast.2012.6400487", "10.1109/tvcg.2013.183", "10.1177/1473871611415997", "10.1145/2702123.2702585", "10.1145/2993901.2993903", "10.1109/tvcg.2013.120", "10.1111/cgf.12632", "10.1145/1385569.1385602", "10.1109/tvcg.2017.2754480", "10.1111/j.1467-8659.2009.01694.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934395", "title": "The Impact of Immersion on Cluster Identification Tasks", "year": "2019", "conferenceName": "InfoVis", "authors": "Matthias Kraus;Niklas Weiler;Daniela Oelke;Johannes Kehrer;Daniel A. Keim;Johannes Fuchs", "citationCount": "4", "affiliation": "Kraus, M (Corresponding Author), Univ Konstanz, Constance, Germany. Kraus, M.; Weiler, N.; Keim, D. A.; Fuchs, J., Univ Konstanz, Constance, Germany. Oelke, D.; Kehrer, J., Siemens Corp Technol, Munich, Germany.", "countries": "Germany", "abstract": "Recent developments in technology encourage the use of head-mounted displays (HMDs) as a medium to explore visualizations in virtual realities (VRs). VR environments (VREs) enable new, more immersive visualization design spaces compared to traditional computer screens. Previous studies in different domains, such as medicine, psychology, and geology, report a positive effect of immersion, e.g., on learning performance or phobia treatment effectiveness. Our work presented in this paper assesses the applicability of those findings to a common task from the information visualization (InfoVis) domain. We conducted a quantitative user study to investigate the impact of immersion on cluster identification tasks in scatterplot visualizations. The main experiment was carried out with 18 participants in a within-subjects setting using four different visualizations, (1) a 2D scatterplot matrix on a screen, (2) a 3D scatterplot on a screen, (3) a 3D scatterplot miniature in a VRE and (4) a fully immersive 3D scatterplot in a VRE. The four visualization design spaces vary in their level of immersion, as shown in a supplementary study. The results of our main study indicate that task performance differs between the investigated visualization design spaces in terms of accuracy, efficiency, memorability, sense of orientation, and user preference. In particular, the 2D visualization on the screen performed worse compared to the 3D visualizations with regard to the measured variables. The study shows that an increased level of immersion can be a substantial benefit in the context of 3D data and cluster detection.", "keywords": "Virtual reality,evaluation,visual analytics,clustering", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934395", "refList": ["10.1111/j.1467-9671.2010.01194.x", "10.1177/1473871614556393", "10.1109/tvcg.2008.153", "10.1111/cgf.13430", "10.1109/tvcg.2004.17", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2007.70433", "10.1089/109493103322011641", "10.2307/2290001", "10.2307/2289444", "10.1016/j.ijhcs.2008.04.004", "10.1109/tvcg.2012.42", "10.1109/vr.2018.8447558", "10.1162/105474602760204318", "10.2307/2986199", "10.3758/bf03200735", "10.1097/opx.0b013e31825da430", "10.1109/infvis.1999.801851", "10.1109/vast.2008.4677350", "10.1162/105474698565686", "10.1109/iv.2013.51", "10.1109/tvcg.2017.2745941", "10.1109/infvis.1998.729555", "10.1111/j.1467-8659.2012.03125.x", "10.1109/2945.506223", "10.1016/s1045-926x(03)00046-6", "10.1097/00042871-200701010-00099", "10.1109/iv.2004.1320137", "10.1109/visual.2002.1183816", "10.1109/tvcg.2018.2864477", "10.1109/vast.2007.4389000", "10.1016/j.ijms.2006.06.015", "10.1007/pl00022704", "10.1111/cgf.13072", "10.1145/3290605.3300555", "10.1109/bdva.2016.7787042", "10.1109/hicss.2013.197", "10.2312/vissym/vissym04/255-260", "10.1115/imece2007-43781", "10.1007/978-4-431-68057-43", "10.1109/tvcg.2016.2520921", "10.1109/mcg.2004.1255801", "10.1109/tvcg.2013.153", "10.1162/pres.1996.5.3.274", "10.1198/106186004x12425", "10.1162/105474601300343612", "10.1162/pres.1997.6.6.603", "10.1111/cgf.12804", "10.1111/j.1467-8659.2009.01666.x", "10.1109/vr.2004.1310069", "10.1109/vr.1999.756938", "10.1007/978-3-658-02897-8\\_16", "10.1162/pres\\_a\\_00016", "10.2307/2288711"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030450", "title": "Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment", "year": "2020", "conferenceName": "InfoVis", "authors": "Benjamin Lee;Xiaoyun Hu;Maxime Cordeil;Arnaud Prouzeau;Bernhard Jenny;Tim Dwyer", "citationCount": "0", "affiliation": "Lee, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Lee, Benjamin; Hu, Xiaoyun; Cordeil, Maxime; Prouzeau, Arnaud; Jenny, Bernhard; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia.", "countries": "Australia", "abstract": "Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA: the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner's personal workspace.", "keywords": "Immersive analytics,collaboration,virtual reality,qualitative study,multivariate data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030450", "refList": ["10.1109/tvcg.2008.153", "10.1007/s10606-004-5062-8", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2019.2914677", "10.1016/0020-7373(91)90039-a", "10.1109/tvcg.2011.287", "10.1117/12.2005484", "10.1109/mmul.2009.35", "10.1109/tvcg.2019.2934803", "10.1145/3359996.3364242", "10.1109/immersive.2016.7932384", "10.1016/j.future.2008.07.015", "10.1057/palgrave.ivs.9500167", "10.1109/mcg.2019.2898941", "10.1145/3343055.3360746", "10.1007/978-3-319-45853-3\\_8", "10.1145/2576099", "10.1145/2858036.2858039", "10.1109/tvcg.2016.2599107", "10.1145/3173574.3173664", "10.1007/978-3-030-01388-2\\_2", "10.1109/vr.2019.8797978", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1007/978-3-030-01388-2\\_8", "10.1023/a:1021271517844", "10.1109/bigdata.2014.7004282", "10.1109/tvcg.2019.2934395", "10.1109/ismar.2010.5643530", "10.1007/978-3-030-01388-22", "10.1145/2133806.2133821", "10.1109/3dvis.2014.7160093", "10.1145/2556288.2557058", "10.1109/vr.2019.8797845"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.4722", "year": "2020", "title": "A Study of Mental Maps in Immersive Network Visualization", "conferenceName": "PacificVis", "authors": "Joseph Kotlarek;Oh{-}Hyun Kwon;Kwan{-}Liu Ma;Peter Eades;Andreas Kerren;Karsten Klein;Falk Schreiber", "citationCount": "0", "affiliation": "Kotlarek, J (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA.\nKotlarek, Joseph; Kwon, Oh-Hyun; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.\nEades, Peter, Univ Sydney, Sydney, NSW, Australia.\nKerren, Andreas, Linnaeus Univ, Vaxjo, Sweden.\nKlein, Karsten; Schreiber, Falk, Univ Konstanz, Constance, Germany.", "countries": "Sweden;Germany;USA;Australia", "abstract": "The visualization of a network influences the quality of the mental map that the viewer develops to understand the network. In this study, we investigate the effects of a 3D immersive visualization environment compared to a traditional 2D desktop environment on the comprehension of a network's structure. We compare the two visualization environments using three tasks-interpreting network structure, memorizing a set of nodes, and identifying the structural changes-commonly used for evaluating the quality of a mental map in network visualization. The results show that participants were able to interpret network structure more accurately when viewing the network in an immersive environment, particularly for larger networks. However, we found that 2D visualizations performed better than immersive visualization for tasks that required spatial memory.", "keywords": "Human-centered computing; Visualization; Visualization techniques; Graph drawings; Human-centered computing; Visualization; Empirical studies in visualization", "link": "https://doi.org/10.1109/PacificVis48177.2020.4722", "refList": ["10.1103/physreve.74.036104", "10.1109/tvcg.2019.2934396", "10.1007/978-3-540-87730-1\\_9", "10.1117/12.2005484", "10.1177/1473871612455749", "10.1109/pacificvis.2017.8031577", "10.1007/978-3-319-73207-7", "10.1109/38.888006", "10.1109/2945.841119", "10.1109/mc.2005.297", "10.1007/s10055-018-0346-3", "10.1109/tvcg.2016.2599107", "10.1109/icsmc.1992.271688", "10.1038/30918", "10.1006/jvlc.1995.1010", "10.1089/109493101300117938", "10.1109/vrais.1998.658488", "10.1109/pacificvis.2015.7156357", "10.1109/tvcg.2010.78", "10.1109/tvcg.2016.2520921", "10.1007/978-3-030-01388-22", "10.1145/229459.229467", "10.1145/1056808.1056875", "10.1109/tvcg.2017.2744079", "10.1109/bdva.2015.7314293", "10.1086/jar.33.4.3629752", "10.1016/j.ijhcs.2013.08.004"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2017.8031577", "year": "2017", "title": "A gesture system for graph visualization in virtual reality environments", "conferenceName": "PacificVis", "authors": "Yi{-}Jheng Huang;Takanori Fujiwara;Yun{-}Xuan Lin;Wen{-}Chieh Lin;Kwan{-}Liu Ma", "citationCount": "7", "affiliation": "Huang, YJ (Corresponding Author), Natl Chiao Tung Univ, Hsinchu, Taiwan.\nHuang, Yi-Jheng; Lin, Yun-Xuan; Lin, Wen-Chieh, Natl Chiao Tung Univ, Hsinchu, Taiwan.\nFujiwara, Takanori; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "Taiwan;USA", "abstract": "As virtual reality (VR) hardware technology becomes more mature and affordable, it is timely to develop visualization applications making use of such technology. How to interact with data in an immersive 3D space is both an interesting and challenging problem, demanding more research investigations. In this paper, we present a gesture input system for graph visualization in a stereoscopic 3D space. We compare desktop mouse input with gesture input with bare hands for performing a set of tasks on graphs. Our study results indicate that users are able to effortlessly manipulate and analyze graphs using gesture input. Furthermore, the results also show that using gestures is more efficient when exploring the complicated graph.", "keywords": "H.5.2 {[}Information Systems]: Information Interfaces and Presentation-User Interfaces", "link": "https://doi.org/10.1109/PACIFICVIS.2017.8031577", "refList": ["10.1109/tvcg.2012.142", "10.1007/978-3-540-88564-1\\_38", "10.1145/234972.234975", "10.1145/1279640.1279642", "10.1145/129888.129892", "10.1007/s00170-016-9461-z", "10.1109/tvcg.2011.234", "10.1109/38.888006", "10.1145/1080402.1080411", "10.1145/971478.971522", "10.1109/3dvis.2014.7160095", "10.1109/3dvis.2014.7160096", "10.1002/(sici)1521-4036(200001)42:1", "10.1007/978-3-319-06793-3\\_5", "10.1016/j.cag.2009.06.002", "10.1109/tsmcc.2007.893280", "10.1109/tvcg.2013.130", "10.1145/1168149.1168168", "10.1109/tvcg.2008.86", "10.1109/3dvis.2014.7160104", "10.1109/mcg.2009.115", "10.1109/tvcg.2013.114", "10.1109/tvcg.2016.2520921", "10.1145/229459.229467", "10.1109/3dvis.2014.7160093", "10.1145/2541016.2541072", "10.1016/j.cag.2012.12.003", "10.1007/s10462-012-9356-9", "10.1109/3dvis.2014.7160105"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865191", "title": "FiberClay: Sculpting Three Dimensional Trajectories to Reveal Structural Insights", "year": "2018", "conferenceName": "InfoVis", "authors": "Christophe Hurter;Nathalie Henry Riche;Steven Mark Drucker;Maxime Cordeil;Richard Alligier;Romain Vuillemot", "citationCount": "11", "affiliation": "Hurter, C (Corresponding Author), French Civil Aviat Univ, ENAC, Toulouse, France. Hurter, C (Corresponding Author), Toulouse Univ, Toulouse, France. Hurter, Christophe ll; Alligier, Richard, French Civil Aviat Univ, ENAC, Toulouse, France. Hurter, Christophe ll; Alligier, Richard, Toulouse Univ, Toulouse, France. Riche, Nathalie Henry; Drucker, Steven M., Microsoft Res, Redmond, WA USA. Cordeil, Maxime, Monash Univ, Clayton, Vic, Australia. Vuillemot, Romain, Univ Lyon, Ecole Cent Lyon, CNRS, UMR 5205,LIRIS, F-69134 Lyon, France.", "countries": "USA;France;Australia", "abstract": "Visualizing 3D trajectories to extract insights about their similarities and spatial configuration is a critical task in several domains. Air traffic controllers for example deal with large quantities of aircrafts routes to optimize safety in airspace and neuroscientists attempt to understand neuronal pathways in the human brain by visualizing bundles of fibers from DTI images. Extracting insights from masses of 3D trajectories is challenging as the multiple three dimensional lines have complex geometries, may overlap, cross or even merge with each other, making it impossible to follow individual ones in dense areas. As trajectories are inherently spatial and three dimensional, we propose FiberClay: a system to display and interact with 3D trajectories in immersive environments. FiberClay renders a large quantity of trajectories in real time using GP-GPU techniques. FiberClay also introduces a new set of interactive techniques for composing complex queries in 3D space leveraging immersive environment controllers and user position. These techniques enable an analyst to select and compare sets of trajectories with specific geometries and data properties. We conclude by discussing insights found using FiberClay with domain experts in air traffic control and neurology.", "keywords": "Immersive Analytics,3D Visualization,Dynamic Queries,Bimanual Interaction,Multidimensional Data", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865191", "refList": ["10.1016/b978-0-08-051574-8.50047-9", "10.1016/j.ijhcs.2013.03.003", "10.1057/palgrave.ivs.9500097", "10.1109/tvcg.2008.153", "10.2307/1269768", "10.1109/tvcg.2013.226", "10.1109/tvcg.2005.59", "10.1016/j.compenvurbsys.2014.01.005", "10.1017/cbo9781139128926.013", "10.1109/pacificvis.2014.61", "10.1145/1279640.1279642", "10.1109/pacificvis.2017.8031577", "10.1109/tvcg.2016.2599217", "10.1109/visual.1991.175794", "10.1007/978-1-4615-1177-9\\_27", "10.1007/978-0-387-35504-714", "10.1109/tvcg.2011.233", "10.3390/informatics4030026", "10.1109/tvcg.2011.192", "10.1109/tvcg.2015.2403323", "10.1145/800186.810616", "10.1145/3013971.3014006", "10.1016/j.ijhcs.2005.02.001", "10.1109/3dvis.2014.7160096", "10.1111/j.1467-8659.2012.03115.x", "10.1109/38.946631", "10.1145/3126594.3126613", "10.1109/pacificvis.2011.5742390", "10.4230/dagrep.6.6.1", "10.1111/j.1467-8659.2012.03079.x", "10.1057/palgrave.ivs.9500061", "10.1109/tvcg.2017.2744338", "10.1109/tvcg.2009.145", "10.1109/tvcg.2012.265", "10.1007/s12031-007-0029-0", "10.1145/3025453.3025566", "10.1109/tvcg.2013.153", "10.1109/tvcg.2016.2520921", "10.1016/j.trc.2014.03.005", "10.1109/3dvis.2014.7160093", "10.1109/ipsn.2014.6846743", "10.1109/tvcg.2015.2467112", "10.1109/tvcg.2017.2744079", "10.1109/tvcg.2011.224", "10.1111/cgf.12804", "10.1111/j.1467-8659.2009.01687.x", "10.1016/s1088-467x(99)00013-x", "10.1109/tvcg.2012.217", "10.1145/2992154.2996365"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934415", "title": "Evaluating an Immersive Space-Time Cube Geovisualization for Intuitive Trajectory Data Exploration", "year": "2019", "conferenceName": "InfoVis", "authors": "Jorge A. Wagner Filho;Wolfgang Stuerzlinger;Luciana Porcher Nedel", "citationCount": "4", "affiliation": "Wagner, JA (Corresponding Author), Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil. Wagner, JA (Corresponding Author), Simon Fraser Univ, Burnaby, BC, Canada. Wagner Filho, Jorge A.; Nedel, Luciana, Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil. Wagner Filho, Jorge A.; Stuerzlinger, Wolfgang, Simon Fraser Univ, Burnaby, BC, Canada.", "countries": "Canada;Brazil", "abstract": "A Space-Time Cube enables analysts to clearly observe spatio-temporal features in movement trajectory datasets in geovisualization. However, its general usability is impacted by a lack of depth cues, a reported steep learning curve, and the requirement for efficient 3D navigation. In this work, we investigate a Space-Time Cube in the Immersive Analytics domain. Based on a review of previous work and selecting an appropriate exploration metaphor, we built a prototype environment where the cube is coupled to a virtual representation of the analyst's real desk, and zooming and panning in space and time are intuitively controlled using mid-air gestures. We compared our immersive environment to a desktop-based implementation in a user study with 20 participants across 7 tasks of varying difficulty, which targeted different user interface features. To investigate how performance is affected in the presence of clutter, we explored two scenarios with different numbers of trajectories. While the quantitative performance was similar for the majority of tasks, large differences appear when we analyze the patterns of interaction and consider subjective metrics. The immersive version of the Space-Time Cube received higher usability scores, much higher user preference, and was rated to have a lower mental workload, without causing participants discomfort in 25-minute-long VR sessions.", "keywords": "Space-time cube,Trajectory visualization,Immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934415", "refList": ["10.1109/mcg.2006.74", "10.1080/13658816.2015.1058386", "10.1145/1456650.1456652", "10.1111/cgf.13430", "10.1145/1944745.1944777", "10.1016/b978-155860819-1/50001-7", "10.1109/bdva.2018.8534024", "10.1109/pacificvis.2017.8031578", "10.1111/j.1467-8306.1994.tb01869.x", "10.2307/3001968", "10.1007/978-3-642-15300-6\\_21", "10.1080/13658816.2010.511223", "10.1007/978-3-030-01388-2", "10.1080/00087041.2018.1495898", "10.1145/1773965.1773970", "10.1002/9780470987643.ch15", "10.1109/glocom.2015.7417476", "10.2312/eurovisstar.20141171", "10.1080/17489725.2015.1074736", "10.1109/bdva.2016.7787050", "10.1109/wevr.2017.7957707", "10.1016/s1045-926x(03)00046-6", "10.1109/iv.2018.00026", "10.1179/1743277413y.0000000061", "10.1109/mcg.2019.2898856", "10.1109/iv.2004.1320137", "10.1111/j.1467-8659.2011.01929.x", "10.1109/tvcg.2008.194", "10.1016/j.jtrangeo.2010.11.002", "10.1177/154193120605000909", "10.1109/bigdata.2015.7364040", "10.1111/cgf.12466", "10.1109/icsens.2015.7370446", "10.1080/13658816.2010.508043", "10.1057/ivs.2009.8", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.14236/ewic/hci2016.22", "10.1145/2655691", "10.1007/bf01936872", "10.1109/infvis.2004.27", "10.1109/tvcg.2014.2329308", "10.1111/j.1435-5597.1970.tb01464.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934332", "title": "LassoNet: Deep Lasso-Selection of 3D Point Clouds", "year": "2019", "conferenceName": "SciVis", "authors": "Zhutian Chen;Wei Zeng 0004;Zhiguang Yang;Lingyun Yu;Chi-Wing Fu;Huamin Qu", "citationCount": "4", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Zhutian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei; Yang, Zhiguang, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yu, Lingyun, Univ Groningen, Groningen, Netherlands. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China.", "countries": "China;Netherlands", "abstract": "Selection is a fundamental task in exploratory analysis and visualization of 3D point clouds. Prior researches on selection methods were developed mainly based on heuristics such as local point density, thus limiting their applicability in general data. Specific challenges root in the great variabilities implied by point clouds (e.g., dense vs. sparse), viewpoint (e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this work, we introduce LassoNet, a new deep neural network for lasso selection of 3D point clouds, attempting to learn a latent mapping from viewpoint and lasso to point cloud regions. To achieve this, we couple user-target points with viewpoint and lasso information through 3D coordinate transform and naive selection, and improve the method scalability via an intention filtering and farthest point sampling. A hierarchical network is trained using a dataset with over 30K lasso-selection records on two different point cloud data. We conduct a formal user study to compare LassoNet with two state-of-the-art lasso-selection methods. The evaluations confirm that our approach improves the selection effectiveness and efficiency across different combinations of 3D point clouds, viewpoints, and lasso selections. Project Website: https://LassoNet.github.io", "keywords": "Point Clouds,Lasso Selection,Deep Learning", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934332", "refList": ["10.1111/j.1467-8659.2009.01515.x", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2016.2599049", "10.1109/iccv.2015.114", "10.1109/vppc.2018.8604993", "10.1111/cgf.13405", "10.1109/tvcg.2018.2843369", "10.1109/cvpr.2018.00278", "10.1145/3025453.3025957", "10.1109/mc.2013.178", "10.1145/2980179.2980238", "10.1109/msp.2017.2693418", "10.1145/237091.237105", "10.1145/2835487", "10.1109/cvpr.2010.5539838", "10.1016/j.visinf.2017.01.006", "10.1109/tcst.2018.2819965", "10.1109/tvcg.2018.2865138", "10.1109/83.623193", "10.1145/3072959.3073608", "10.1145/1053427.1053445", "10.1016/s0039-9140(96)02179-0", "10.1109/cvpr.2015.7298801", "10.1109/cvpr.2017.693", "10.1007/s12650-014-0206-5", "10.1145/3272127.3275110", "10.1109/cvpr.2015.7298845", "10.1109/iccvw.2015.112", "10.1109/iros.2015.7353481", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2012.217", "10.1109/tridui.2006.1618279", "10.1109/tvcg.2012.292", "10.1109/cvpr.2016.609", "10.1016/j.cag.2012.12.003"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028947", "title": "A Fluid Flow Data Set for Machine Learning and its Application to Neural Flow Map Interpolation", "year": "2020", "conferenceName": "SciVis", "authors": "Jakob Jakob;Markus H. Gross;Tobias G\u00fcnther", "citationCount": "0", "affiliation": "Jakob, J (Corresponding Author), Swiss Fed Inst Technol, Zurich, Switzerland. Jakob, Jakob; Gross, Markus; Guenther, Tobias, Swiss Fed Inst Technol, Zurich, Switzerland.", "countries": "Switzerland", "abstract": "In recent years, deep learning has opened countless research opportunities across many different disciplines. At present, visualization is mainly applied to explore and explain neural networks. Its counterpart-the application of deep learning to visualization problems-requires us to share data more openly in order to enable more scientists to engage in data-driven research. In this paper, we construct a large fluid flow data set and apply it to a deep learning problem in scientific visualization. Parameterized by the Reynolds number, the data set contains a wide spectrum of laminar and turbulent fluid flow regimes. The full data set was simulated on a high-performance compute cluster and contains 8000 time-dependent 2D vector fields, accumulating to more than 16 TB in size. Using our public fluid data set, we trained deep convolutional neural networks in order to set a benchmark for an improved post-hoc Lagrangian fluid flow analysis. In in-situ settings, flow maps are exported and interpolated in order to assess the transport characteristics of time-dependent fluids. Using deep learning, we improve the accuracy of flow map interpolations, allowing a more precise flow analysis at a reduced memory IO footprint.", "keywords": "Scientific visualization,deep learning,flow maps", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028947", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1145/3355089.3356560", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1007/978-3-030-00533-7\\_36", "10.3390/rs11161921", "10.1007/978-3-030-48457-6\\_1", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/mcg.2018.2881523", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1145/3072959.3073643", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/tvcg.2013.128", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1007/978-3-319-46475-6\\_43", "10.1126/science.1127647", "10.1007/978-3-319-46475-6\\_25", "10.1111/cgf.13689"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030346", "title": "V2V: A Deep Learning Approach to Variable-to-Variable Selection and Translation for Multivariate Time-Varying Data", "year": "2020", "conferenceName": "SciVis", "authors": "Jun Han;Hao Zheng 0006;Yunhao Xing;Danny Ziyi Chen;Chaoli Wang", "citationCount": "0", "affiliation": "Han, J (Corresponding Author), Univ Notre Dame, Notre Dame, IN 46556 USA. Han, Jun; Zheng, Hao; Chen, Danny Z.; Wang, Chaoli, Univ Notre Dame, Notre Dame, IN 46556 USA. Xing, Yunhao, Sichuan Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China;USA", "abstract": "We present V2V, a novel deep learning framework, as a general-purpose solution to the variable-to-variable (V2V) selection and translation problem for multivariate time-varying data (MTVD) analysis and visualization. V2V leverages a representation learning algorithm to identify transferable variables and utilizes Kullback-Leibler divergence to determine the source and target variables. It then uses a generative adversarial network (GAN) to learn the mapping from the source variable to the target variable via the adversarial, volumetric, and feature losses. V2V takes the pairs of time steps of the source and target variable as input for training, Once trained, it can infer unseen time steps of the target variable given the corresponding time steps of the source variable. Several multivariate time-varying data sets of different characteristics are used to demonstrate the effectiveness of V2V, both quantitatively and qualitatively. We compare V2V against histogram matching and two other deep learning solutions (Pix2Pix and CycleGAN).", "keywords": "Multivariate time-varying data,variable selection and translation,generative adversarial network,data extrapolation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030346", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/cvpr.2017.19", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1146/annurev", "10.3390/rs11161921", "10.1109/tvcg.2007.70551", "10.1145/3309993", "10.1007/978-3-030-48457-6\\_1", "10.1109/igarss.2018.8518411", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1146/annurev-fluid-010313-141322", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1126/science.1127647"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030374", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data", "year": "2020", "conferenceName": "SciVis", "authors": "Yifan Wang;Guoli Yan;Haikuan Zhu;Sagar Buch;Ying Wang;E. Mark Haacke;Jing Hua;Zichun Zhong", "citationCount": "0", "affiliation": "Wang, YF (Corresponding Author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Wang, Yifan; Yan, Guoli; Zhu, Haikuan; Hua, Jing; Zhong, Zichun, Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Buch, Sagar; Wang, Ying; Haacke, Ewart Mark, Wayne State Univ, Dept Radiol, Detroit, MI 48201 USA.", "countries": "USA", "abstract": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.", "keywords": "Deep neural network,3D cerebrovascular segmentation and visualization,maximum intensity projection (MIP),joint embedding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030374", "refList": ["10.1109/cluster.2018.00036", "10.1109/iccv.2015.123", "10.1109/tvcg.2013.133", "10.1109/cvpr.2017.19", "10.1109/tvcg.2019.2934312", "10.1007/978-3-319-46487-9\\_40", "10.1109/tvcg.2006.175", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2018.2880207", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/tvcg.2018.2816059", "10.1109/pacificvis.2009.4906852", "10.1109/tvcg.2018.2864808", "10.1109/mcg.2018.2881523", "10.1109/tvcg.2015.2467431", "10.1109/tpami.2015.2439281", "10.1109/pacificvis48177.2020.8737", "10.1109/cvpr.2019.00244", "10.1109/tvcg.2006.165", "10.1109/bigdata.2018.8622520", "10.1007/978-3-319-46466-4\\_29", "10.1109/visual.2019.8933759", "10.1145/3197517.3201304", "10.1109/pacificvis.2011.5742378", "10.1109/cvpr.2006.91", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1007/978-3-319-24574-4\\_28", "10.1109/pacificvis.2019.00041", "10.1109/tvcg.2019.2934332", "10.1109/cvpr.2018.00916", "10.1007/978-3-319-46475-6\\_43"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934803", "title": "There Is No Spoon: Evaluating Performance, Space Use, and Presence with Expert Domain Users in Immersive Analytics", "year": "2019", "conferenceName": "InfoVis", "authors": "Andrea Batch;Andrew Cunningham;Maxime Cordeil;Niklas Elmqvist;Tim Dwyer;Bruce H. Thomas;Kim Marriott", "citationCount": "6", "affiliation": "Batch, A (Corresponding Author), Univ Maryland, College Pk, MD 20742 USA. Batch, Andrea; Elmqvist, Niklas, Univ Maryland, College Pk, MD 20742 USA. Cunningham, Andrew; Thomas, Bruce H., Univ South Australia Adelaide, Adelaide, SA, Australia. Cordeil, Maxime; Dwyer, Tim; Marriott, Kim, Univ Melbourne, Melbourne, Vic, Australia.", "countries": "USA;Australia", "abstract": "Immersive analytics turns the very space surrounding the user into a canvas for data analysis, supporting human cognitive abilities in myriad ways. We present the results of a design study, contextual inquiry, and longitudinal evaluation involving professional economists using a Virtual Reality (VR) system for multidimensional visualization to explore actual economic data. Results from our preregistered evaluation highlight the varied use of space depending on context (exploration vs. presentation), the organization of space to support work, and the impact of immersion on navigation and orientation in the 3D analysis space.", "keywords": "Design study,evaluation,economic analysis,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934803", "refList": ["10.1145/3139131.3139141", "10.1109/2.60882", "10.1007/978-3-030-01388-2\\_1", "10.1109/38.250911", "10.1145/302979.303166", "10.1073/pnas.1306779110", "10.1109/tvcg.2012.219", "10.1162/105474698565686", "10.1016/0364-0213(94)90007-8", "10.1145/642611.642650", "10.1162/pres.1992.1.4.482", "10.1007/978-3-030-01388-2", "10.1109/2.19829", "10.1111/1467-8284.00096", "10.1007/978-3-030-01388-2\\_7", "10.1109/hicss.2011.339", "10.1109/tvcg.2016.2599107", "10.1145/3173574.3173664", "10.1145/1008653.1008669", "10.1109/tvcg.2017.2743990", "10.1145/642611", "10.1145/3126594.3126613", "10.1007/978-94-007-6833-89", "10.1162/pres\\_a\\_00124", "10.1016/0004-3702(94)00017-u", "10.1109/tvcg.2014.20", "10.1145/1168149.1168158", "10.1109/visual.1995.480800", "10.1109/icsens.2015.7370446", "10.1207/s15516709cog1701\\_1", "10.1162/105474601300343603", "10.1109/tvcg.2016.2518135", "10.1109/tvcg.2012.213", "10.1016/s0097-8493(02)00113-9", "10.1162/pres.1994.3.2.130", "10.1109/tvcg.2018.2865191", "10.1109/visual.1990.146402", "10.1162/105474601300343612", "10.1145/2702123.2702406", "10.1162/pres.1992.1.1.120", "10.1145/3009939.3009955", "10.1109/glocom.2015.7417476", "10.1162/pres\\_a\\_00261"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030450", "title": "Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment", "year": "2020", "conferenceName": "InfoVis", "authors": "Benjamin Lee;Xiaoyun Hu;Maxime Cordeil;Arnaud Prouzeau;Bernhard Jenny;Tim Dwyer", "citationCount": "0", "affiliation": "Lee, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Lee, Benjamin; Hu, Xiaoyun; Cordeil, Maxime; Prouzeau, Arnaud; Jenny, Bernhard; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia.", "countries": "Australia", "abstract": "Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA: the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner's personal workspace.", "keywords": "Immersive analytics,collaboration,virtual reality,qualitative study,multivariate data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030450", "refList": ["10.1109/tvcg.2008.153", "10.1007/s10606-004-5062-8", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2019.2914677", "10.1016/0020-7373(91)90039-a", "10.1109/tvcg.2011.287", "10.1117/12.2005484", "10.1109/mmul.2009.35", "10.1109/tvcg.2019.2934803", "10.1145/3359996.3364242", "10.1109/immersive.2016.7932384", "10.1016/j.future.2008.07.015", "10.1057/palgrave.ivs.9500167", "10.1109/mcg.2019.2898941", "10.1145/3343055.3360746", "10.1007/978-3-319-45853-3\\_8", "10.1145/2576099", "10.1145/2858036.2858039", "10.1109/tvcg.2016.2599107", "10.1145/3173574.3173664", "10.1007/978-3-030-01388-2\\_2", "10.1109/vr.2019.8797978", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1007/978-3-030-01388-2\\_8", "10.1023/a:1021271517844", "10.1109/bigdata.2014.7004282", "10.1109/tvcg.2019.2934395", "10.1109/ismar.2010.5643530", "10.1007/978-3-030-01388-22", "10.1145/2133806.2133821", "10.1109/3dvis.2014.7160093", "10.1145/2556288.2557058", "10.1109/vr.2019.8797845"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030392", "title": "ShuttleSpace: Exploring and Analyzing Movement Trajectory in Immersive Visualization", "year": "2020", "conferenceName": "InfoVis", "authors": "Shuainan Ye;Zhutian Chen;Xiangtong Chu;Yifan Wang;Siwei Fu;Lejun Shen;Kun Zhou;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD CG, Hangzhou, Peoples R China. Ye, Shuainan; Chu, Xiangtong; Wang, Yifan; Zhou, Kun; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD CG, Hangzhou, Peoples R China. Chen, Zhutian, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Fu, Siwei, Zhejiang Lab, Hangzhou, Zhejiang, Peoples R China. Shen, Lejun, Chengdu Sports Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China", "abstract": "We present ShuttleSpace, an immersive analytics system to assist experts in analyzing trajectory data in badminton. Trajectories in sports, such as the movement of players and balls, contain rich information on player behavior and thus have been widely analyzed by coaches and analysts to improve the players' performance. However, existing visual analytics systems often present the trajectories in court diagrams that are abstractions of reality, thereby causing difficulty for the experts to imagine the situation on the court and understand why the player acted in a certain way. With recent developments in immersive technologies, such as virtual reality (VR), experts gradually have the opportunity to see, feel, explore, and understand these 3D trajectories from the player's perspective. Yet, few research has studied how to support immersive analysis of sports data from such a perspective. Specific challenges are rooted in data presentation (e.g., how to seamlessly combine 2D and 3D visualizations) and interaction (e.g., how to naturally interact with data without keyboard and mouse) in VR. To address these challenges, we have worked closely with domain experts who have worked for a top national badminton team to design ShuttleSpace. Our system leverages 1) the peripheral vision to combine the 2D and 3D visualizations and 2) the VR controller to support natural interactions via a stroke metaphor. We demonstrate the effectiveness of ShuttleSpace through three case studies conducted by the experts with useful insights. We further conduct interviews with the experts whose feedback confirms that our first-person immersive analytics system is suitable and useful for analyzing badminton data.", "keywords": "Movement trajectory,badminton analytics,virtual reality", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030392", "refList": ["10.1016/j.eswa.2012.01.204", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1142/s1005386719000178", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/978-981-10-3994-2\\_1", "10.1007/978-3-319-07812-0\\_12", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1016/j.eswa.2017.07.027", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 35}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030392", "title": "ShuttleSpace: Exploring and Analyzing Movement Trajectory in Immersive Visualization", "year": "2020", "conferenceName": "InfoVis", "authors": "Shuainan Ye;Zhutian Chen;Xiangtong Chu;Yifan Wang;Siwei Fu;Lejun Shen;Kun Zhou;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD CG, Hangzhou, Peoples R China. Ye, Shuainan; Chu, Xiangtong; Wang, Yifan; Zhou, Kun; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD CG, Hangzhou, Peoples R China. Chen, Zhutian, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Fu, Siwei, Zhejiang Lab, Hangzhou, Zhejiang, Peoples R China. Shen, Lejun, Chengdu Sports Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China", "abstract": "We present ShuttleSpace, an immersive analytics system to assist experts in analyzing trajectory data in badminton. Trajectories in sports, such as the movement of players and balls, contain rich information on player behavior and thus have been widely analyzed by coaches and analysts to improve the players' performance. However, existing visual analytics systems often present the trajectories in court diagrams that are abstractions of reality, thereby causing difficulty for the experts to imagine the situation on the court and understand why the player acted in a certain way. With recent developments in immersive technologies, such as virtual reality (VR), experts gradually have the opportunity to see, feel, explore, and understand these 3D trajectories from the player's perspective. Yet, few research has studied how to support immersive analysis of sports data from such a perspective. Specific challenges are rooted in data presentation (e.g., how to seamlessly combine 2D and 3D visualizations) and interaction (e.g., how to naturally interact with data without keyboard and mouse) in VR. To address these challenges, we have worked closely with domain experts who have worked for a top national badminton team to design ShuttleSpace. Our system leverages 1) the peripheral vision to combine the 2D and 3D visualizations and 2) the VR controller to support natural interactions via a stroke metaphor. We demonstrate the effectiveness of ShuttleSpace through three case studies conducted by the experts with useful insights. We further conduct interviews with the experts whose feedback confirms that our first-person immersive analytics system is suitable and useful for analyzing badminton data.", "keywords": "Movement trajectory,badminton analytics,virtual reality", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030392", "refList": ["10.1016/j.eswa.2012.01.204", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1142/s1005386719000178", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/978-981-10-3994-2\\_1", "10.1007/978-3-319-07812-0\\_12", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1016/j.eswa.2017.07.027", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.4722", "year": "2020", "title": "A Study of Mental Maps in Immersive Network Visualization", "conferenceName": "PacificVis", "authors": "Joseph Kotlarek;Oh{-}Hyun Kwon;Kwan{-}Liu Ma;Peter Eades;Andreas Kerren;Karsten Klein;Falk Schreiber", "citationCount": "0", "affiliation": "Kotlarek, J (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA.\nKotlarek, Joseph; Kwon, Oh-Hyun; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.\nEades, Peter, Univ Sydney, Sydney, NSW, Australia.\nKerren, Andreas, Linnaeus Univ, Vaxjo, Sweden.\nKlein, Karsten; Schreiber, Falk, Univ Konstanz, Constance, Germany.", "countries": "Sweden;Germany;USA;Australia", "abstract": "The visualization of a network influences the quality of the mental map that the viewer develops to understand the network. In this study, we investigate the effects of a 3D immersive visualization environment compared to a traditional 2D desktop environment on the comprehension of a network's structure. We compare the two visualization environments using three tasks-interpreting network structure, memorizing a set of nodes, and identifying the structural changes-commonly used for evaluating the quality of a mental map in network visualization. The results show that participants were able to interpret network structure more accurately when viewing the network in an immersive environment, particularly for larger networks. However, we found that 2D visualizations performed better than immersive visualization for tasks that required spatial memory.", "keywords": "Human-centered computing; Visualization; Visualization techniques; Graph drawings; Human-centered computing; Visualization; Empirical studies in visualization", "link": "https://doi.org/10.1109/PacificVis48177.2020.4722", "refList": ["10.1103/physreve.74.036104", "10.1109/tvcg.2019.2934396", "10.1007/978-3-540-87730-1\\_9", "10.1117/12.2005484", "10.1177/1473871612455749", "10.1109/pacificvis.2017.8031577", "10.1007/978-3-319-73207-7", "10.1109/38.888006", "10.1109/2945.841119", "10.1109/mc.2005.297", "10.1007/s10055-018-0346-3", "10.1109/tvcg.2016.2599107", "10.1109/icsmc.1992.271688", "10.1038/30918", "10.1006/jvlc.1995.1010", "10.1089/109493101300117938", "10.1109/vrais.1998.658488", "10.1109/pacificvis.2015.7156357", "10.1109/tvcg.2010.78", "10.1109/tvcg.2016.2520921", "10.1007/978-3-030-01388-22", "10.1145/229459.229467", "10.1145/1056808.1056875", "10.1109/tvcg.2017.2744079", "10.1109/bdva.2015.7314293", "10.1086/jar.33.4.3629752", "10.1016/j.ijhcs.2013.08.004"], "wos": 1, "children": [], "len": 1}], "len": 43}, {"doi": "10.1111/cgf.13430", "year": "2018", "title": "VirtualDesk: A Comfortable and Efficient Immersive Information Visualization Approach", "conferenceName": "EuroVis", "authors": "Jorge A. Wagner Filho;Carla Maria Dal Sasso Freitas;Luciana P. Nedel", "citationCount": "5", "affiliation": "Wagner, JA (Corresponding Author), Univ Fed Rio Grande do Sul, Inst Informat, Porto Alegre, RS, Brazil.\nWagner Filho, J. A.; Freitas, C. M. D. S.; Nedel, L., Univ Fed Rio Grande do Sul, Inst Informat, Porto Alegre, RS, Brazil.", "countries": "Brazil", "abstract": "3D representations are potentially useful under many circumstances, but suffer from long known perception and interaction challenges. Current immersive technologies, which combine stereoscopic displays and natural interaction, are being progressively seen as an opportunity to tackle this issue, but new guidelines and studies are still needed, especially regarding information visualization. Many proposed approaches are impractical for actual usage, resulting in user discomfort or requiring too much time or space. In this work, we implement and evaluate an alternative data exploration metaphor where the user remains seated and viewpoint change is only realisable through physical movements. All manipulation is done directly by natural mid-air gestures, with the data being rendered at arm's reach. The virtual reproduction of the analyst's desk aims to increase immersion and enable tangible interaction with controls and two dimensional associated information. A comparative user study was carried out against a desktop-based equivalent, exploring a set of 9 perception and interaction tasks based on previous literature and a multidimensional projection use case. We demonstrate that our prototype setup, named VirtualDesk, presents excellent results regarding user comfort and immersion, and performs equally or better in all analytical tasks, while adding minimal or no time overhead and amplifying user subjective perceptions of efficiency and engagement. Results are also contrasted to a previous experiment employing artificial flying navigation, with significant observed improvements.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13430", "refList": ["10.1177/1473871614556393", "10.1109/mcg.2014.97", "10.1007/978-3-540-88564-1\\_38", "10.1145/1944745.1944777", "10.1109/3dui.2016.7460040", "10.1145/1279640.1279642", "10.1109/sibgrapi.2015.34", "10.1111/j.1467-8659.2011.01960.x", "10.1037/h0071325", "10.1111/j.1477-7053.2006.00204.x", "10.1109/tvcg.2016.2599107", "10.1109/iv.2013.51", "10.1109/wevr.2017.7957707", "10.1201/9781498710411", "10.1145/3126594.3126613", "10.1177/154193120605000909", "10.1109/bigdata.2014.7004282", "10.1186/s12859-016-1446-2", "10.1111/cgf.12466", "10.1162/105474601300343603", "10.1145/2702123.2702201", "10.1007/3-540-44491-2\\_3", "10.1109/tvcg.2016.2520921", "10.1002/hbm.20701", "10.1007/978-3-319-16766-4\\_20", "10.1109/aero.2016.7500608", "10.1109/bigdata.2015.7364040", "10.1016/j.cag.2012.12.003"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934415", "title": "Evaluating an Immersive Space-Time Cube Geovisualization for Intuitive Trajectory Data Exploration", "year": "2019", "conferenceName": "InfoVis", "authors": "Jorge A. Wagner Filho;Wolfgang Stuerzlinger;Luciana Porcher Nedel", "citationCount": "4", "affiliation": "Wagner, JA (Corresponding Author), Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil. Wagner, JA (Corresponding Author), Simon Fraser Univ, Burnaby, BC, Canada. Wagner Filho, Jorge A.; Nedel, Luciana, Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil. Wagner Filho, Jorge A.; Stuerzlinger, Wolfgang, Simon Fraser Univ, Burnaby, BC, Canada.", "countries": "Canada;Brazil", "abstract": "A Space-Time Cube enables analysts to clearly observe spatio-temporal features in movement trajectory datasets in geovisualization. However, its general usability is impacted by a lack of depth cues, a reported steep learning curve, and the requirement for efficient 3D navigation. In this work, we investigate a Space-Time Cube in the Immersive Analytics domain. Based on a review of previous work and selecting an appropriate exploration metaphor, we built a prototype environment where the cube is coupled to a virtual representation of the analyst's real desk, and zooming and panning in space and time are intuitively controlled using mid-air gestures. We compared our immersive environment to a desktop-based implementation in a user study with 20 participants across 7 tasks of varying difficulty, which targeted different user interface features. To investigate how performance is affected in the presence of clutter, we explored two scenarios with different numbers of trajectories. While the quantitative performance was similar for the majority of tasks, large differences appear when we analyze the patterns of interaction and consider subjective metrics. The immersive version of the Space-Time Cube received higher usability scores, much higher user preference, and was rated to have a lower mental workload, without causing participants discomfort in 25-minute-long VR sessions.", "keywords": "Space-time cube,Trajectory visualization,Immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934415", "refList": ["10.1109/mcg.2006.74", "10.1080/13658816.2015.1058386", "10.1145/1456650.1456652", "10.1111/cgf.13430", "10.1145/1944745.1944777", "10.1016/b978-155860819-1/50001-7", "10.1109/bdva.2018.8534024", "10.1109/pacificvis.2017.8031578", "10.1111/j.1467-8306.1994.tb01869.x", "10.2307/3001968", "10.1007/978-3-642-15300-6\\_21", "10.1080/13658816.2010.511223", "10.1007/978-3-030-01388-2", "10.1080/00087041.2018.1495898", "10.1145/1773965.1773970", "10.1002/9780470987643.ch15", "10.1109/glocom.2015.7417476", "10.2312/eurovisstar.20141171", "10.1080/17489725.2015.1074736", "10.1109/bdva.2016.7787050", "10.1109/wevr.2017.7957707", "10.1016/s1045-926x(03)00046-6", "10.1109/iv.2018.00026", "10.1179/1743277413y.0000000061", "10.1109/mcg.2019.2898856", "10.1109/iv.2004.1320137", "10.1111/j.1467-8659.2011.01929.x", "10.1109/tvcg.2008.194", "10.1016/j.jtrangeo.2010.11.002", "10.1177/154193120605000909", "10.1109/bigdata.2015.7364040", "10.1111/cgf.12466", "10.1109/icsens.2015.7370446", "10.1080/13658816.2010.508043", "10.1057/ivs.2009.8", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.14236/ewic/hci2016.22", "10.1145/2655691", "10.1007/bf01936872", "10.1109/infvis.2004.27", "10.1109/tvcg.2014.2329308", "10.1111/j.1435-5597.1970.tb01464.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934395", "title": "The Impact of Immersion on Cluster Identification Tasks", "year": "2019", "conferenceName": "InfoVis", "authors": "Matthias Kraus;Niklas Weiler;Daniela Oelke;Johannes Kehrer;Daniel A. Keim;Johannes Fuchs", "citationCount": "4", "affiliation": "Kraus, M (Corresponding Author), Univ Konstanz, Constance, Germany. Kraus, M.; Weiler, N.; Keim, D. A.; Fuchs, J., Univ Konstanz, Constance, Germany. Oelke, D.; Kehrer, J., Siemens Corp Technol, Munich, Germany.", "countries": "Germany", "abstract": "Recent developments in technology encourage the use of head-mounted displays (HMDs) as a medium to explore visualizations in virtual realities (VRs). VR environments (VREs) enable new, more immersive visualization design spaces compared to traditional computer screens. Previous studies in different domains, such as medicine, psychology, and geology, report a positive effect of immersion, e.g., on learning performance or phobia treatment effectiveness. Our work presented in this paper assesses the applicability of those findings to a common task from the information visualization (InfoVis) domain. We conducted a quantitative user study to investigate the impact of immersion on cluster identification tasks in scatterplot visualizations. The main experiment was carried out with 18 participants in a within-subjects setting using four different visualizations, (1) a 2D scatterplot matrix on a screen, (2) a 3D scatterplot on a screen, (3) a 3D scatterplot miniature in a VRE and (4) a fully immersive 3D scatterplot in a VRE. The four visualization design spaces vary in their level of immersion, as shown in a supplementary study. The results of our main study indicate that task performance differs between the investigated visualization design spaces in terms of accuracy, efficiency, memorability, sense of orientation, and user preference. In particular, the 2D visualization on the screen performed worse compared to the 3D visualizations with regard to the measured variables. The study shows that an increased level of immersion can be a substantial benefit in the context of 3D data and cluster detection.", "keywords": "Virtual reality,evaluation,visual analytics,clustering", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934395", "refList": ["10.1111/j.1467-9671.2010.01194.x", "10.1177/1473871614556393", "10.1109/tvcg.2008.153", "10.1111/cgf.13430", "10.1109/tvcg.2004.17", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2007.70433", "10.1089/109493103322011641", "10.2307/2290001", "10.2307/2289444", "10.1016/j.ijhcs.2008.04.004", "10.1109/tvcg.2012.42", "10.1109/vr.2018.8447558", "10.1162/105474602760204318", "10.2307/2986199", "10.3758/bf03200735", "10.1097/opx.0b013e31825da430", "10.1109/infvis.1999.801851", "10.1109/vast.2008.4677350", "10.1162/105474698565686", "10.1109/iv.2013.51", "10.1109/tvcg.2017.2745941", "10.1109/infvis.1998.729555", "10.1111/j.1467-8659.2012.03125.x", "10.1109/2945.506223", "10.1016/s1045-926x(03)00046-6", "10.1097/00042871-200701010-00099", "10.1109/iv.2004.1320137", "10.1109/visual.2002.1183816", "10.1109/tvcg.2018.2864477", "10.1109/vast.2007.4389000", "10.1016/j.ijms.2006.06.015", "10.1007/pl00022704", "10.1111/cgf.13072", "10.1145/3290605.3300555", "10.1109/bdva.2016.7787042", "10.1109/hicss.2013.197", "10.2312/vissym/vissym04/255-260", "10.1115/imece2007-43781", "10.1007/978-4-431-68057-43", "10.1109/tvcg.2016.2520921", "10.1109/mcg.2004.1255801", "10.1109/tvcg.2013.153", "10.1162/pres.1996.5.3.274", "10.1198/106186004x12425", "10.1162/105474601300343612", "10.1162/pres.1997.6.6.603", "10.1111/cgf.12804", "10.1111/j.1467-8659.2009.01666.x", "10.1109/vr.2004.1310069", "10.1109/vr.1999.756938", "10.1007/978-3-658-02897-8\\_16", "10.1162/pres\\_a\\_00016", "10.2307/2288711"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030450", "title": "Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment", "year": "2020", "conferenceName": "InfoVis", "authors": "Benjamin Lee;Xiaoyun Hu;Maxime Cordeil;Arnaud Prouzeau;Bernhard Jenny;Tim Dwyer", "citationCount": "0", "affiliation": "Lee, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Lee, Benjamin; Hu, Xiaoyun; Cordeil, Maxime; Prouzeau, Arnaud; Jenny, Bernhard; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia.", "countries": "Australia", "abstract": "Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA: the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner's personal workspace.", "keywords": "Immersive analytics,collaboration,virtual reality,qualitative study,multivariate data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030450", "refList": ["10.1109/tvcg.2008.153", "10.1007/s10606-004-5062-8", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2019.2914677", "10.1016/0020-7373(91)90039-a", "10.1109/tvcg.2011.287", "10.1117/12.2005484", "10.1109/mmul.2009.35", "10.1109/tvcg.2019.2934803", "10.1145/3359996.3364242", "10.1109/immersive.2016.7932384", "10.1016/j.future.2008.07.015", "10.1057/palgrave.ivs.9500167", "10.1109/mcg.2019.2898941", "10.1145/3343055.3360746", "10.1007/978-3-319-45853-3\\_8", "10.1145/2576099", "10.1145/2858036.2858039", "10.1109/tvcg.2016.2599107", "10.1145/3173574.3173664", "10.1007/978-3-030-01388-2\\_2", "10.1109/vr.2019.8797978", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1007/978-3-030-01388-2\\_8", "10.1023/a:1021271517844", "10.1109/bigdata.2014.7004282", "10.1109/tvcg.2019.2934395", "10.1109/ismar.2010.5643530", "10.1007/978-3-030-01388-22", "10.1145/2133806.2133821", "10.1109/3dvis.2014.7160093", "10.1145/2556288.2557058", "10.1109/vr.2019.8797845"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 15}, {"doi": "10.1111/cgf.13431", "year": "2018", "title": "Maps and Globes in Virtual Reality", "conferenceName": "EuroVis", "authors": "Yalong Yang;Bernhard Jenny;Tim Dwyer;Kim Marriott;Haohui Chen;Maxime Cordeil", "citationCount": "13", "affiliation": "Yang, YL (Corresponding Author), Monash Univ, Fac Informat Technol, Clayton, Vic, Australia.\nYang, YL (Corresponding Author), CSIRO, Data61, Canberra, ACT, Australia.\nYang, Yalong; Jenny, Bernhard; Dwyer, Tim; Marriott, Kim; Cordeil, Maxime, Monash Univ, Fac Informat Technol, Clayton, Vic, Australia.\nYang, Yalong; Chen, Haohui, CSIRO, Data61, Canberra, ACT, Australia.", "countries": "Australia", "abstract": "This paper explores different ways to render world-wide geographic maps in virtual reality (VR). We compare: (a) a 3D exocentric globe, where the user's viewpoint is outside the globe; (b) a flat map (rendered to a plane in VR); (c) an egocentric 3D globe, with the viewpoint inside the globe; and (d) a curved map, created by projecting the map onto a section of a sphere which curves around the user. In all four visualisations the geographic centre can be smoothly adjusted with a standard handheld VR controller and the user, through a head-tracked headset, can physically move around the visualisation. For distance comparison exocentric globe is more accurate than egocentric globe and flat map. For area comparison more time is required with exocentric and egocentric globes than with flat and curved maps. For direction estimation, the exocentric globe is more accurate and faster than the other visual presentations. Our study participants had a weak preference for the exocentric globe. Generally the curved map had benefits over the flat map. In almost all cases the egocentric globe was found to be the least effective visualisation. Overall, our results provide support for the use of exocentric globes for geographic visualisation in mixed-reality.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13431", "refList": ["10.1037/0096-3445.129.2.193", "10.1016/s0377-2217(03)00037-7", "10.1073/pnas.0407994102", "10.1016/j.cognition.2010.03.009", "10.1553/giscience2016\\_01\\_s314", "10.1037/0278-7393.32.3.333", "10.1080/00045608.2015.1022091", "10.1016/j.compenvurbsys.2015.05.001", "10.1007/978-3-319-51835-0\\_9", "10.2307/1420573", "10.1098/rsif.2009.0495", "10.1179/0008704113z.00000000078", "10.1145/159544.159587", "10.1145/3013971.3013983", "10.1007/978-3-642-80328-4\\_13", "10.1029/2002jd003179", "10.1145/2702123.2702172", "10.1038/nature10082", "10.1080/00087041.2015.1131938", "10.1126/science.1244693", "10.1109/bdva.2015.7314302", "10.2307/1421904", "10.1080/17538947.2014.1002867", "10.1109/tvcg.2016.2520921", "10.1109/3dvis.2014.7160094", "10.1080/00045600802683734", "10.1093/oxfordjournals.epirev.a017993", "10.2307/622344"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865192", "title": "Origin-Destination Flow Maps in Immersive Environments", "year": "2018", "conferenceName": "InfoVis", "authors": "Yalong Yang;Tim Dwyer;Bernhard Jenny;Kim Marriott;Maxime Cordeil;Haohui Chen", "citationCount": "5", "affiliation": "Yang, YL (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Yang, YL (Corresponding Author), CSIRO, Data61, Canberra, ACT, Australia. Yang, Yalong; Dwyer, Tim; Jenny, Bernhard; Marriott, Kim; Cordeil, Maxime, Monash Univ, Clayton, Vic, Australia. Yang, Yalong; Chen, Haohui, CSIRO, Data61, Canberra, ACT, Australia.", "countries": "Australia", "abstract": "Immersive virtual- and augmented-reality headsets can overlay a flat image against any surface or hang virtual objects in the space around the user. The technology is rapidly improving and may, in the long term, replace traditional flat panel displays in many situations. When displays are no longer intrinsically flat, how should we use the space around the user for abstract data visualisation? In this paper, we ask this question with respect to origin-destination flow data in a global geographic context. We report on the findings of three studies exploring different spatial encodings for flow maps. The first experiment focuses on different 2D and 3D encodings for flows on flat maps. We find that participants are significantly more accurate with raised flow paths whose height is proportional to flow distance but fastest with traditional straight line 2D flows. In our second and third experiment we compared flat maps, 3D globes and a novel interactive design we call<i>MapsLink</i>, involving a pair of linked flat maps. We find that participants took significantly more time with MapsLink than other flow maps while the 3D globe with raised flows was the fastest, most accurate, and most preferred method. Our work suggests that<i>careful</i>use of the third spatial dimension can resolve visual clutter in complex flow maps.", "keywords": "Origin-destination,Flow Map,Virtual Reality,Cartographic Information Visualisation,Immersive Analytics", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865192", "refList": ["10.1111/j.1467-8659.2011.01946.x", "10.1145/2254556.2254652", "10.1080/13658816.2017.1307378", "10.1179/caj.1971.8.2.139", "10.1080/17445647.2017.1313788", "10.1007/s00168-008-0256-5", "10.1016/j.compenvurbsys.2009.01.007", "10.1145/1080402.1080411", "10.1109/infvis.2003.1249008", "10.1109/infvis.1995.528697", "10.1080/15230406.2016.1262280", "10.1093/comjnl/bxx117", "10.1145/159544.159587", "10.1109/glocom.2015.7417476", "10.1145/3013971.3013983", "10.1126/science.1248676", "10.1109/eitt.2017.18", "10.1109/infvis.2005.1532150", "10.1080/03085696708592302", "10.1145/2702123.2702172", "10.1109/tvcg.2011.181", "10.1145/3126594.3126613", "10.1109/pacificvis.2011.5742390", "10.1109/tvcg.2016.2598958", "10.4230/dagrep.6.6.1", "10.1109/infvis.1996.559226", "10.1007/978-3-319-03841-4\\_34", "10.1111/cgf.13431", "10.1109/tvcg.2014.2346441", "10.1179/000870410x12658023467367", "10.1109/tvcg.2007.70521", "10.1109/icsens.2015.7370446", "10.2307/1791753", "10.1109/38.486685", "10.1109/tvcg.2016.2520921", "10.1559/152304087783875273", "10.1007/bf01936872", "10.1016/j.jvlc.2016.07.006", "10.1109/3dvis.2014.7160094", "10.1111/j.0033-0124.1981.00419.x", "10.1109/tvcg.2016.2598885", "10.1111/j.1435-5597.1970.tb01464.x", "10.1177/1473871616681375", "10.1109/tvcg.2011.202", "10.1016/j.jvlc.2014.03.001"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030392", "title": "ShuttleSpace: Exploring and Analyzing Movement Trajectory in Immersive Visualization", "year": "2020", "conferenceName": "InfoVis", "authors": "Shuainan Ye;Zhutian Chen;Xiangtong Chu;Yifan Wang;Siwei Fu;Lejun Shen;Kun Zhou;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD CG, Hangzhou, Peoples R China. Ye, Shuainan; Chu, Xiangtong; Wang, Yifan; Zhou, Kun; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD CG, Hangzhou, Peoples R China. Chen, Zhutian, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Fu, Siwei, Zhejiang Lab, Hangzhou, Zhejiang, Peoples R China. Shen, Lejun, Chengdu Sports Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China", "abstract": "We present ShuttleSpace, an immersive analytics system to assist experts in analyzing trajectory data in badminton. Trajectories in sports, such as the movement of players and balls, contain rich information on player behavior and thus have been widely analyzed by coaches and analysts to improve the players' performance. However, existing visual analytics systems often present the trajectories in court diagrams that are abstractions of reality, thereby causing difficulty for the experts to imagine the situation on the court and understand why the player acted in a certain way. With recent developments in immersive technologies, such as virtual reality (VR), experts gradually have the opportunity to see, feel, explore, and understand these 3D trajectories from the player's perspective. Yet, few research has studied how to support immersive analysis of sports data from such a perspective. Specific challenges are rooted in data presentation (e.g., how to seamlessly combine 2D and 3D visualizations) and interaction (e.g., how to naturally interact with data without keyboard and mouse) in VR. To address these challenges, we have worked closely with domain experts who have worked for a top national badminton team to design ShuttleSpace. Our system leverages 1) the peripheral vision to combine the 2D and 3D visualizations and 2) the VR controller to support natural interactions via a stroke metaphor. We demonstrate the effectiveness of ShuttleSpace through three case studies conducted by the experts with useful insights. We further conduct interviews with the experts whose feedback confirms that our first-person immersive analytics system is suitable and useful for analyzing badminton data.", "keywords": "Movement trajectory,badminton analytics,virtual reality", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030392", "refList": ["10.1016/j.eswa.2012.01.204", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1142/s1005386719000178", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/978-981-10-3994-2\\_1", "10.1007/978-3-319-07812-0\\_12", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1016/j.eswa.2017.07.027", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2019.00022", "year": "2019", "title": "What-Why Analysis of Expert Interviews: Analysing Geographically-Embedded Flow Data", "conferenceName": "PacificVis", "authors": "Yalong Yang;Sarah Goodwin", "citationCount": "0", "affiliation": "Yang, YL (Corresponding Author), Monash Univ, Caulfield Sch Informat Technol, Clayton, Vic, Australia.\nYang, Yalong; Goodwin, Sarah, Monash Univ, Caulfield Sch Informat Technol, Clayton, Vic, Australia.", "countries": "Australia", "abstract": "In this paper, we present our analysis of five expert interviews, each from a different application domain. Such analysis is crucial to understanding the real-world scenarios of analysing geographically-embedded flow data. The results of our analysis show that similar high-level tasks were conducted in different domains. To better describe the targets of these tasks, we proposed three flow-targets for analysing geographically-embedded flow data: single flow, total flow and regional flow.", "keywords": "Human-centered computing; Visualization; Visualization design and evaluation methods", "link": "https://doi.org/10.1109/PacificVis.2019.00022", "refList": ["10.1109/dsn-w.2018.00014", "10.1109/tvcg.2012.219", "10.1109/tvcg.2015.2500225", "10.1109/tvcg.2018.2865192", "10.1016/j.compenvurbsys.2013.10.007", "10.1109/tvcg.2013.130", "10.1145/1168149.1168168", "10.1109/tvcg.2016.2598885", "10.1145/3009939", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2865040", "10.1017/s0950268816002053", "10.1109/infvis.2005.1532136"], "wos": 1, "children": [], "len": 1}], "len": 11}, {"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}], "len": 15}, {"doi": "10.1111/cgf.13675", "year": "2019", "title": "CV3: Visual Exploration, Assessment, and Comparison of CVs", "conferenceName": "EuroVis", "authors": "Velitchko Andreev Filipov;Alessio Arleo;Paolo Federico;Silvia Miksch", "citationCount": "0", "affiliation": "Filipov, V (Corresponding Author), Vienna Univ Technol, TU Wien, Vienna, Austria.\nFilipov, V; Arleo, A.; Miksch, S., Vienna Univ Technol, TU Wien, Vienna, Austria.\nFederico, P., Nokia, Vimercate, Italy.", "countries": "Italy;Austria", "abstract": "The Curriculum Vitae (CV, also referred to as resume) is an established representation of a person's academic and professional history. A typical CV is comprised of multiple sections associated with spatio-temporal, nominal, hierarchical, and ordinal data. The main task of a recruiter is, given a job application with specific requirements, to compare and assess CVs in order to build a short list of promising candidates to interview. Commonly, this is done by viewing CVs in a side-by-side fashion. This becomes challenging when comparing more than two CVs, because the reader is required to switch attention between them. Furthermore, there is no guarantee that the CVs are structured similarly, thus making the overview cluttered and significantly slowing down the comparison process. In order to address these challenges, in this paper we propose CV3, an interactive exploration environment offering users a new way to explore, assess, and compare multiple CVs, to suggest suitable candidates for specific job requirements. We validate our system by means of domain expert feedback whose results highlight both the efficacy of our approach and its limitations. We learned that CV3 eases the overall burden of recruiters thereby assisting them in the selection process.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13675", "refList": ["10.1145/245129.245134", "10.1109/tvcg.2015.2467971", "10.1145/3025453.3025628", "10.1109/cmv.2007.20", "10.1007/978-3-319-66610-5\\_26", "10.1109/2945.841119", "10.1016/j.cag.2013.11.002", "10.1093/comjnl/bxx117", "10.1109/38.689657", "10.1007/3-540-31190-4", "10.1007/bf01954291", "10.1145/3230707", "10.1007/s10961-006-7203-3", "10.1145/102377.115768", "10.1007/978-0-85729-079-3\\_8", "10.1145/1056018.1056041", "10.1109/infvis.2004.1", "10.17697/ibmrd/2014/v3i1/46706", "10.1109/infvis.2001.963285", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2015.2467620", "10.1007/bf01187665", "10.1109/tvcg.2009.111", "10.1016/j.respol.2004.01.005", "10.1109/tvcg.2016.2614803"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13169", "year": "2017", "title": "Empirically Measuring Soft Knowledge in Visualization", "conferenceName": "EuroVis", "authors": "Natchaya Kijmongkolchai;Alfie Abdul{-}Rahman;Min Chen", "citationCount": "3", "affiliation": "Kijmongkolchai, N (Corresponding Author), Univ Oxford, Oxford, England.\nKijmongkolchai, Natchaya; Abdul-Rahman, Alfie; Chen, Min, Univ Oxford, Oxford, England.", "countries": "England", "abstract": "In this paper, we present an empirical study designed to evaluate the hypothesis that humans' soft knowledge can enhance the cost-benefit ratio of a visualization process by reducing the potential distortion. In particular, we focused on the impact of three classes of soft knowledge: (i) knowledge about application contexts, (ii) knowledge about the patterns to be observed (i.e., in relation to visualization task), and (iii) knowledge about statistical measures. We mapped these classes into three control variables, and used real-world time series data to construct stimuli. The results of the study confirmed the positive contribution of each class of knowledge towards the reduction of the potential distortion, while the knowledge about the patterns prevents distortion more effectively than the other two classes.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13169", "refList": ["10.1111/cgf.12887", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2006.194", "10.1109/tvcg.2013.166", "10.1109/tvcg.2014.2346420", "10.1111/cgf.12634", "10.1111/cgf.12889", "10.1109/tvcg.2015.2467759", "10.1109/tvcg.2013.187", "10.1109/tvcg.2012.197", "10.1109/tvcg.2012.180", "10.1109/tvcg.2015.2467322", "10.1109/tvcg.2016.2598862", "10.1111/cgf.12127", "10.1111/cgf.12104", "10.1109/tvcg.2015.2513410", "10.1161/01.cir.101.23.e215", "10.1109/tvcg.2012.163", "10.1109/tvcg.2013.234", "10.1109/tvcg.2015.2467758", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346428", "10.1109/tvcg.2012.233", "10.1109/tvcg.2014.2346979", "10.1109/tvcg.2016.2598898", "10.1111/cgf.12638", "10.1111/cgf.12880", "10.1111/j.1467-8659.2012.03129.x", "10.1111/cgf.12092", "10.1109/tvcg.2014.2346320", "10.1109/tvcg.2010.150", "10.1109/tvcg.2016.2598466", "10.1111/cgf.12633", "10.1109/tvcg.2016.2598594", "10.1109/tvcg.2015.2467752", "10.1109/tvcg.2015.2502587", "10.1111/j.1467-8659.2012.03092.x", "10.1109/tvcg.2016.2520921", "10.1145/2858036.2858272", "10.1109/tvcg.2012.215", "10.1109/tvcg.2012.230", "10.1109/tvcg.2014.2371858", "10.1109/visual.2001.964505", "10.1109/tvcg.2015.2467201", "10.1109/tvcg.2014.2346422", "10.1111/cgf.12656", "10.1057/ivs.2008.13", "10.1109/tvcg.2015.2424872", "10.1109/tvcg.2012.279", "10.1109/tvcg.2014.2346998", "10.1109/tvcg.2012.220", "10.1109/tvcg.2015.2467951", "10.1109/tvcg.2013.183", "10.1109/tvcg.2012.223", "10.1109/tvcg.2012.189", "10.1109/tvcg.2012.222", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2012.196", "10.1109/tvcg.2016.2599106", "10.1109/tvcg.2014.2346424", "10.1109/tvcg.2012.199", "10.1109/tvcg.2016.2518158", "10.1109/tvcg.2012.221", "10.1111/cgf.13009", "10.1109/tvcg.2014.2346983", "10.1111/j.1467-8659.2012.03093.x", "10.1109/tvcg.2013.170", "10.1111/cgf.12888", "10.1109/tvcg.2012.245", "10.1109/tvcg.2014.2346426", "10.1109/tvcg.2016.2598544", "10.1109/tvcg.2015.2467751", "10.1109/tvcg.2014.2346298", "10.1109/tvcg.2012.144", "10.1109/tvcg.2012.251", "10.1109/tvcg.2016.2598885"], "wos": 1, "children": [{"doi": "10.1109/vast.2017.8585498", "title": "The Role of Explicit Knowledge: A Conceptual Model of Knowledge-Assisted Visual Analytics", "year": "2017", "conferenceName": "VAST", "authors": "Paolo Federico;Markus Wagner 0008;Alexander Rind;Albert Amor-Amoros;Silvia Miksch;Wolfgang Aigner", "citationCount": "3", "affiliation": "Federico, P (Corresponding Author), TU Wien, Vienna, Austria. Federico, Paolo; Wagner, Markus; Rind, Alexander; Amor-Amoros, Albert; Miksch, Silvia; Aigner, Wolfgang, TU Wien, Vienna, Austria. Wagner, Markus; Rind, Alexander; Aigner, Wolfgang, St Poelten Univ Appl Sci, Sankt Polten, Austria.", "countries": "Austria", "abstract": "Visual Analytics (VA) aims to combine the strengths of humans and computers for effective data analysis. In this endeavor, humans' tacit knowledge from prior experience is an important asset that can be leveraged by both human and computer to improve the analytic process. While VA environments are starting to include features to formalize, store, and utilize such knowledge, the mechanisms and degree in which these environments integrate explicit knowledge varies widely. Additionally, this important class of VA environments has never been elaborated on by existing work on VA theory. This paper proposes a conceptual model of Knowledge-assisted VA conceptually grounded on the visualization model by van Wijk. We apply the model to describe various examples of knowledge-assisted VA from the literature and elaborate on three of them in finer detail. Moreover, we illustrate the utilization of the model to compare different design alternatives and to evaluate existing approaches with respect to their use of knowledge. Finally, the model can inspire designers to generate novel VA environments using explicit knowledge effectively.", "keywords": "Automated analysis,tacit knowledge,explicit knowledge,visual analytics,information visualization,theory and model", "link": "http://dx.doi.org/10.1109/VAST.2017.8585498", "refList": ["10.1016/j.artmed.2006.03.001", "10.1057/ivs.2009.22", "10.1109/infvis.2000.885092", "10.2312/pe/eurovast/eurova11/009-012", "10.1109/tvcg.2016.2598839", "10.1109/tvcg.2014.2346575", "10.1111/cgf.13169", "10.1186/1471-2105-13-s8-s3", "10.1007/s00371-015-1132-9", "10.1109/tvcg.2013.146", "10.1109/tvcg.2016.2598471", "10.1016/j.cag.2009.06.004", "10.1145/2993901.2993915", "10.1111/cgf.12090", "10.1177/1473871611412817", "10.1145/2598153.2598172", "10.1016/j.cag.2009.06.006", "10.1109/tvcg.2016.2598460", "10.1016/j.autcon.2014.03.012", "10.1145/989863.989865", "10.1109/21.44068", "10.1057/palgrave.ivs.9500045", "10.2312/eurova.20151108", "10.1002/9781444303179.ch3", "10.1109/hicss.2016.183", "10.1177/0165551506070706", "10.1145/2494188.2494202", "10.1145/1556262.1556327", "10.1016/j.artmed.2006.04.002", "10.1007/978-3-540-71080-6\\_6", "10.1111/j.1467-8659.2008.01230.x", "10.1109/pacificvis.2011.5742371", "10.1109/vast.2014.7042530", "10.1109/mcg.2010.8", "10.1109/mcg.2015.25", "10.1109/vast.2012.6400555", "10.1109/tvcg.2014.2346481", "10.1145/2836034.2836040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2008.109", "10.1016/j.artmed.2005.10.003", "10.1109/mcg.2014.73", "10.1016/s0004-3702(96)00025-2", "10.1093/intqhc/mzm007", "10.1016/j.dss.2012.06.009", "10.1016/j.cose.2017.02.003", "10.1109/infvis.1998.729560", "10.1109/vast.2010.5654451", "10.1109/tvcg.2016.2598829", "10.1109/vast.2007.4389021", "10.1145/1562849.1562851", "10.1145/302979.303030", "10.1145/985692.985706", "10.1109/infvis.1997.636792", "10.1111/j.1467-8659.2012.03092.x", "10.1109/mcg.2009.6", "10.1057/ivs.2008.28", "10.1109/mcg.2005.91", "10.1016/j.artmed.2010.02.001", "10.1177/0272989x14565822", "10.1109/mcg.2010.15", "10.1007/s10844-014-0304-9", "10.2312/pe.eurovast.eurova13.043-047", "10.1109/mcg.2014.33", "10.1109/tvcg.2014.2346574", "10.1111/j.1467-8659.2009.01708.x", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934654", "title": "Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections", "year": "2019", "conferenceName": "VAST", "authors": "Mennatallah El-Assady;Rebecca Kehlbeck;Christopher Collins;Daniel A. Keim;Oliver Deussen", "citationCount": "3", "affiliation": "El-Assady, M (Corresponding Author), Univ Konstanz, Constance, Germany. El-Assady, M (Corresponding Author), Ontario Tech Univ, Oshawa, ON, Canada. El-Assady, Mennatallah; Kehlbeck, Rebecca; Keim, Daniel; Deussen, Oliver, Univ Konstanz, Constance, Germany. El-Assady, Mennatallah; Collins, Christopher, Ontario Tech Univ, Oshawa, ON, Canada.", "countries": "Canada;Germany", "abstract": "We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decision-making process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.", "keywords": "Topic Model Optimization,Word Embedding,Mixed-Initiative Refinement,Guided Visual Analytics,Semantic Mapping", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934654", "refList": ["10.1109/vast.2014.7042493", "10.1145/2133806.2133826", "10.1016/j.visinf.2018.09.003", "10.1007/978-3-319-67008-9\\_26", "10.1109/tvcg.2013.126", "10.1162/tacl\\_a\\_00140", "10.1007/s13202-018-0509-5", "10.1109/bdva.2018.8534018", "10.1108/eb026526", "10.1007/s10994-013-5413-0", "10.1145/564376.564421", "10.1016/j.visinf.2017.01.006", "10.3115/v1/p14-2050", "10.1007/bf00288933", "10.1109/tvcg.2013.212", "10.1145/2207676.2207741", "10.1109/tvcg.2013.162", "10.1109/tvcg.2016.2515592", "10.1109/tvcg.2017.2745080", "10.1109/mcg.2013.53", "10.1109/tvcg.2017.2744199", "10.1145/3091108", "10.18653/v1/p17-4009", "10.1162/jmlr.2003.3.4-5.951", "10.1109/vast.2017.8585498", "10.1109/tvcg.2017.2723397", "10.1109/tvcg.2018.2864769", "10.1007/s10618-005-0361-3", "10.3115/v1/d14-1167", "10.1007/bf01840357", "10.1162/jmlr.2003.3.4-5.993", "10.1145/2678025.2701370", "10.1016/j.ins.2016.06.040", "10.1109/tvcg.2017.2746018", "10.1109/vast.2011.6102461", "10.1111/cgf.13092", "10.3115/1117729.1117730", "10.1109/mcg.2015.91", "10.1145/2669557.2669572"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13959", "year": "2020", "title": "Knowledge-Assisted Comparative Assessment of Breast Cancer using Dynamic Contrast-Enhanced Magnetic Resonance Imaging", "conferenceName": "EuroVis", "authors": "Kai Nie;Pascal A. Baltzer;Bernhard Preim;Gabriel Mistelbauer", "citationCount": "0", "affiliation": "Nie, K (Corresponding Author), Otto von Guericke Univ, Dept Simulat \\& Graph, Magdeburg, Germany.\nNie, K.; Preim, B.; Mistelbauer, G., Otto von Guericke Univ, Dept Simulat \\& Graph, Magdeburg, Germany.\nBaltzer, P., Med Univ Vienna, Dept Biomed Imaging \\& Image Guided Therapy, Vienna, Austria.", "countries": "Germany;Austria", "abstract": "Breast perfusion data are dynamic medical image data that depict perfusion characteristics of the investigated tissue. These data consist of a series of static datasets that are acquired at different time points and aggregated into time intensity curves (TICs) for each voxel. The characteristics of these TICs provide important information about a lesion's composition, but their analysis is time-consuming due to their large number. Subsequently, these TICs are used to classify a lesion as benign or malignant. This lesion scoring is commonly done manually by physicians and may therefore be subject to bias. We propose an approach that addresses both of these problems by combining an automated lesion classification with a visual confirmatory analysis, especially for uncertain cases. Firstly, we cluster the TICs of a lesion using ordering points to identify the clustering structure (OPTICS) and then visualize these clusters. Together with their relative size, they are added to a library. We then model fuzzy inference rules by using the lesion's TIC clusters as antecedents and its score as consequent. Using a fuzzy scoring system, we can suggest a score for a new lesion. Secondly, to allow physicians to confirm the suggestion in uncertain cases, we display the TIC clusters together with their spatial distribution and allow them to compare two lesions side by side. With our knowledge-assisted comparative visual analysis, physicians can explore and classify breast lesions. The true positive prediction accuracy of our scoring system achieved 71.4\\% in one-fold cross-validation using 14 lesions.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13959", "refList": ["10.1016/j.patcog.2017.08.004", "10.1109/icinfa.2017.8078962", "10.1109/iccv.2013.222", "10.1002/jmri.1123", "10.1016/j.ejrad.2017.01.020", "10.1007/s00330-016-4612-z", "10.1002/widm.30", "10.1002/jmri.26721", "10.1109/tvcg.2008.95", "10.1007/s00330-015-4075-7", "10.1007/s00330-007-0762-3", "10.1148/radiology.213.3.r99dc01881", "10.1007/s10278-010-9298-1", "10.1016/j.datak.2006.01.013", "10.1016/j.cag.2010.05.016", "10.5121/mlaij.2016.3103", "10.1089/10665270360688057", "10.1117/1.jmi.5.1.014502", "10.1002/mp.12408", "10.1198/jcgs.2011.09224", "10.1109/cbms.2013.6627768", "10.1109/vast.2017.8585498", "10.1118/1.4937787", "10.1016/j.acra.2009.03.017", "10.1145/2503210.2503255", "10.1007/978-3-319-68548-9\\_44", "10.1109/tvcg.2007.70569", "10.1016/j.compmedimag.2007.02.007", "10.1109/tmi.2013.2281984", "10.1148/radiol.2442051620", "10.1016/j.jacr.2009.07.023", "10.1002/j.1538-7305.1957.tb01515.x", "10.1148/radiol.14121031", "10.3322/caac.21492", "10.1148/radiology.211.1.r99ap38101", "10.1016/j.compbiomed.2014.10.006", "10.1016/j.eswa.2016.01.004", "10.3238/arztebl.2018.0316", "10.1559/152304003100010929", "10.1016/s0002-9610(01)00726-7", "10.1016/j.ejrad.2020.108819", "10.1109/tmi.2012.2191302"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/tvcg.2018.2864913", "title": "A Framework for Externalizing Implicit Error Using Visualization", "year": "2018", "conferenceName": "InfoVis", "authors": "Nina McCurdy;Julie Gerdes;Miriah D. Meyer", "citationCount": "8", "affiliation": "McCurdy, N (Corresponding Author), Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA. McCurdy, Nina; Meyer, Miriah, Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA. Gerdes, Julie, Texas Tech Univ, Coll Arts \\& Sci, Lubbock, TX 79409 USA.", "countries": "USA", "abstract": "This paper presents a framework for externalizing and analyzing expert knowledge about discrepancies in data through the use of visualization. Grounded in an 18-month design study with global health experts, the framework formalizes the notion of data discrepancies as implicit error, both in global health data and more broadly. We use the term implicit error to describe measurement error that is inherent to and pervasive throughout a dataset, but that isn't explicitly accounted for or defined. Instead, implicit error exists in the minds of experts, is mainly qualitative, and is accounted for subjectively during expert interpretation of the data. Externalizing knowledge surrounding implicit error can assist in synchronizing, validating, and enhancing interpretation, and can inform error analysis and mitigation. The framework consists of a description of implicit error components that are important for downstream analysis, along with a process model for externalizing and analyzing implicit error using visualization. As a second contribution, we provide a rich, reflective, and verifiable description of our research process as an exemplar summary toward the ongoing inquiry into ways of increasing the validity and transferability of design study research.", "keywords": "implicit error,knowledge externalization,design study", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864913", "refList": ["10.1197/jamia.m2342", "10.1109/tvcg.2013.132", "10.1111/cgf.13169", "10.1179/1743277414y.0000000099", "10.1016/j.cag.2009.06.004", "10.1111/j.1467-8659.2009.01678.x", "10.1111/j.0361-3666.2003.00237.x", "10.1111/cgf.12392", "10.1016/j.jvlc.2011.04.002", "10.1186/1471-2334-11-37", "10.1371/journal.pmed.1000376", "10.1109/infvis.2005.1532134", "10.1109/pacificvis.2017.8031599", "10.1109/tvcg.2017.2743898", "10.1197/jamia.m2544", "10.1145/3025453.3025592", "10.1109/tvcg.2015.2468151", "10.1007/978-1-4471-6497-5\\_1", "10.3233/978-1-60750-533-4-23", "10.1177/0165551506070706", "10.1145/642611.642616", "10.6064/2012/875253", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2015.2465151", "10.1111/j.1467-9604.2007.00468.x", "10.1109/mcg.2012.31", "10.1109/tvcg.2007.70589", "10.1145/2993901.2993916", "10.1109/vast.2010.5652885", "10.1145/3025453.3025738", "10.1109/38.689662", "10.1016/s0925-7535(97)00052-0", "10.9745/ghsp-d-15-00207", "10.1518/001872095779049543", "10.1080/15323269.2011.587100", "10.1117/12.587254", "10.1016/j.cie.2014.11.025", "10.1109/tvcg.2017.2745240", "10.1109/tvcg.2012.213", "10.1109/vast.2011.6102457", "10.1109/mcg.2015.50", "10.1145/1385569.1385582", "10.1016/j.jbi.2014.04.006", "10.1136/amiajnl-2011-000486"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934539", "title": "Criteria for Rigor in Visualization Design Study", "year": "2019", "conferenceName": "InfoVis", "authors": "Miriah D. Meyer;Jason Dykes", "citationCount": "16", "affiliation": "Meyer, M (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Dykes, Jason, City Univ London, London, England.", "countries": "USA;England", "abstract": "We develop a new perspective on research conducted through visualization design study that emphasizes design as a method of inquiry and the broad range of knowledge-contributions achieved through it as multiple, subjective, and socially constructed. From this interpretivist position we explore the nature of visualization design study and develop six criteria for rigor. We propose that rigor is established and judged according to the extent to which visualization design study research and its reporting are INFORMED, REFLEXIVE, ABUNDANT, PLAUSIBLE, RESONANT, and TRANSPARENT. This perspective and the criteria were constructed through a four-year engagement with the discourse around rigor and the nature of knowledge in social science, information systems, and design. We suggest methods from cognate disciplines that can support visualization researchers in meeting these criteria during the planning, execution, and reporting of design study. Through a series of deliberately provocative questions, we explore implications of this new perspective for design study research in visualization, concluding that as a discipline, visualization is not yet well positioned to embrace, nurture, and fully benefit from a rigorous, interpretivist approach to design study. The perspective and criteria we present are intended to stimulate dialogue and debate around the nature of visualization design study and the broader underpinnings of the discipline.", "keywords": "design study,relativism,interpretivism,knowledge construction,qualitative research,research through design", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934539", "refList": ["10.1007/978-1-4939-0378-8\\_8", "10.1177/1049732315588501", "10.1177/146879410200200205", "10.2307/1177100", "10.1016/0142-694x(82)90040-0", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1145/2362364.2362371", "10.1080/2159676x.2017.1393221", "10.1177/1473871613510429", "10.1145/2212877.2212889", "10.1080/09650790802011973", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2015.2467195", "10.1109/mcse.2007.106", "10.1080/1750984x.2017.1317357", "10.1109/tvcg.2017.2745958", "10.1177/1525822x0101300203", "10.1080/23265507.2017.1300068", "10.1111/j.1540-4560.1946.tb02295.x", "10.1177/1468794108098034", "10.1007/978-3-7643-8472-2\\_6", "10.1145/2317956.2317968", "10.2307/1511837", "10.1177/104973202129120052", "10.3233/efi-2004-22201", "10.1109/beliv.2018.8634427", "10.1109/beliv.2018.8634261", "10.1016/s0142-694x(01)00009-6", "10.1007/978-3-7643-8472-2\\_3", "10.1145/642611.642616", "10.1177/1468794107085301", "10.1177/1077800410383121", "10.1109/tvcg.2010.137", "10.1145/2405716.2405725", "10.1145/2702123.2702172", "10.1109/tvcg.2014.2346248", "10.1109/tvcg.2011.209", "10.1111/j.1467-8659.2009.01710.x", "10.1145/3173574.3173775", "10.1145/1993060.1993065", "10.1007/978-1-4419-5653-8\\_2", "10.1177/107780049900500402", "10.1109/tvcg.2018.2864905", "10.2307/2288400", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1177/160940690400300403", "10.1145/2993901.2993916", "10.1145/1879831.1879836", "10.2307/3178066", "10.1109/tvcg.2018.2864913", "10.3102/0013189x022004016", "10.1109/tvcg.2015.2511718", "10.1109/tvcg.2018.2865241", "10.1016/j.ijnurstu.2010.06.004", "10.1109/tvcg.2012.213", "10.1111/0735-2751.00040", "10.1109/tvcg.2013.145", "10.1002/ev.1427", "10.1109/tvcg.2018.2811488", "10.1075/idj.23.1.07thu", "10.1109/tvcg.2009.111", "10.1109/mcg.2018.2874523", "10.1111/cgf.13184", "10.1111/cgf.13595"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3029413", "title": "A Design Space of Vision Science Methods for Visualization Research", "year": "2020", "conferenceName": "InfoVis", "authors": "Madison A. Elliott;Christine Nothelfer;Cindy Xiong;Danielle Albers Szafir", "citationCount": "0", "affiliation": "Elliott, MA (Corresponding Author), Univ British Columbia, Vancouver, BC, Canada. Elliott, Madison A., Univ British Columbia, Vancouver, BC, Canada. Nothelfer, Christine, Northwestern Univ, Evanston, IL 60208 USA. Xiong, Cindy, Univ Massachusetts, Amherst, MA 01003 USA. Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "Canada;USA", "abstract": "A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.", "keywords": "Perception,human vision,empirical research,evaluation,HCI", "link": "http://dx.doi.org/10.1109/TVCG.2020.3029413", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1177/0886109909354981", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1093/bioinformatics/btq110", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1177/1744987107081254", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1177/174498710501000305", "10.1017/s1049096513001789", "10.1109/tvcg.2012.213", "10.1093/nar/gkz239", "10.1093/sysbio/sys062", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030438", "title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening", "year": "2020", "conferenceName": "SciVis", "authors": "Mar\u00eda Virginia Sabando;Pavol Ulbrich;Mat\u00edas N. Selzer;Jan Byska;Jan Mican;Ignacio Ponzoni;Axel J. Soto;Maria Luj\u00e1n Ganuza;Barbora Kozl\u00edkov\u00e1", "citationCount": "0", "affiliation": "Sabando, MV (Corresponding Author), Univ Nacl Sur, Inst Comp Sci \\& Engn UNS CONICET, Bahia Blanca, Buenos Aires, Argentina. Sabando, MV (Corresponding Author), Univ Nacl Sur, Dept Comp Sci \\& Engn, Bahia Blanca, Buenos Aires, Argentina. Sabando, Maria Virginia; Selzer, Matias; Ponzoni, Ignacio; Soto, Axel J.; Ganuza, Maria Lujan, Univ Nacl Sur, Inst Comp Sci \\& Engn UNS CONICET, Bahia Blanca, Buenos Aires, Argentina. Sabando, Maria Virginia; Ponzoni, Ignacio; Soto, Axel J., Univ Nacl Sur, Dept Comp Sci \\& Engn, Bahia Blanca, Buenos Aires, Argentina. Ulbrich, Pavol; Byska, Jan; Kozlikova, Barbora, Masaryk Univ, Fac Informat, Visitlab, Brno, Czech Republic. Selzer, Matias; Ganuza, Maria Lujan, Univ Nacl Sur, VyGLab Res Lab UNS CICPBA, Dept Comp Sci \\& Engn, Bahia Blanca, Buenos Aires, Argentina. Mican, Jan, Masaryk Univ, Dept Expt Biol, Loschmidt Labs, Brno, Czech Republic. Mican, Jan, Masaryk Univ, RECETOX, Brno, Czech Republic. Mican, Jan, Masaryk Univ, Fac Med, Brno, Czech Republic.", "countries": "Argentina;Republic", "abstract": "In the modern drug discovery process, medicinal chemists deal with the complexity of analysis of large ensembles of candidate molecules. Computational tools, such as dimensionality reduction (DR) and classification, are commonly used to efficiently process the multidimensional space of features. These underlying calculations often hinder interpretability of results and prevent experts from assessing the impact of individual molecular features on the resulting representations. To provide a solution for scrutinizing such complex data, we introduce ChemVA, an interactive application for the visual exploration of large molecular ensembles and their features. Our tool consists of multiple coordinated views: Hexagonal view, Detail view, 3D view, Table view, and a newly proposed Difference view designed for the comparison of DR projections. These views display DR projections combined with biological activity, selected molecular features, and confidence scores for each of these projections. This conjunction of views allows the user to drill down through the dataset and to efficiently select candidate compounds. Our approach was evaluated on two case studies of finding structurally similar ligands with similar binding affinity to a target protein, as well as on an external qualitative evaluation. The results suggest that our system allows effective visual inspection and comparison of different high-dimensional molecular representations. Furthermore, ChemVA assists in the identification of candidate compounds while providing information on the certainty behind different molecular representations.", "keywords": "Virtual screening,visual analysis,dimensionality reduction,coordinated views,cheminformatics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030438", "refList": ["10.1109/tvcg.2008.137", "10.1057/ivs.2009.10", "10.2312/eurovisstar.20151110", "10.1109/eisic.2015.35", "10.1109/pacificvis.2014.44", "10.1145/1142473.1142574", "10.1109/tvcg.2013.223", "10.1109/tvcg.2014.2346573", "10.1109/tvcg.2019.2934539", "10.1111/cgf.13717", "10.1109/vizsec.2009.5375536", "10.1111/cgf.12925", "10.1109/tvcg.2015.2467551", "10.1109/mcg.2015.99", "10.1007/978-3-319", "10.1109/tvcg.2012.255", "10.1145/1064830.1064834", "10.1177/1473871611433713", "10.1207/s1532690xci0804\\_2", "10.1145/1168149.1168168", "10.1016/j.chb.2006.10.002", "10.1109/tvcg.2014.2346441", "10.1109/eisic.2017.15", "10.1111/1467-8721.00160", "10.1109/tvcg.2018.2865024", "10.1109/infvis.2004.2"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030435", "title": "Data Visceralization: Enabling Deeper Understanding of Data Using Virtual Reality", "year": "2020", "conferenceName": "InfoVis", "authors": "Benjamin Lee;Dave Brown;Bongshin Lee;Christophe Hurter;Steven Mark Drucker;Tim Dwyer", "citationCount": "1", "affiliation": "Lee, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Lee, Benjamin; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Brown, Dave; Lee, Bongshin; Drucker, Steven, Microsoft Res, Redmond, WA USA. Hurter, Christophe, French Civil Aviat Univ, ENAC, Toulouse, France.", "countries": "USA;France;Australia", "abstract": "A fundamental part of data visualization is transforming data to map abstract information onto visual attributes. While this abstraction is a powerful basis for data visualization, the connection between the representation and the original underlying data (i.e., what the quantities and measurements actually correspond with in reality) can be lost. On the other hand, virtual reality (VR) is being increasingly used to represent real and abstract models as natural experiences to users. In this work, we explore the potential of using VR to help restore the basic understanding of units and measures that are often abstracted away in data visualization in an approach we call data visceralization. By building VR prototypes as design probes, we identify key themes and factors for data visceralization. We do this first through a critical reflection by the authors, then by involving external participants. We find that data visceralization is an engaging way of understanding the qualitative aspects of physical measures and their real-life form, which complements analytical and quantitative understanding commonly gained from data visualization. However, data visceralization is most effective when there is a one-to-one mapping between data and representation, with transformations such as scaling affecting this understanding. We conclude with a discussion of future directions for data visceralization.", "keywords": "Data visceralization,virtual reality,exploratory study", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030435", "refList": ["10.1080/01973762.2013.761106", "10.1109/tvcg.2015.2467811", "10.1109/tvcg.2012.221", "10.1145/3284179.3284326", "10.1109/tvcg.2019.2934287", "10.1109/2945.841119", "10.1109/tvcg.2019.2934539", "10.1109/mcg.2013.101", "10.1109/tvcg.2011.175", "10.1109/tvcg.2018.2830759", "10.1109/3dvis.2014.7160096", "10.1109/mcg.2018.2878900", "10.1109/iv.2011.32", "10.1109/tvcg.2013.196", "10.1109/tvcg.2018.2865241", "10.1515/abitech-2017-0002", "10.1145/2468356.2468739", "10.16995/olh.280", "10.1080/15230406.2018.1513343", "10.1109/iv.2004.1320189", "10.1109/tvcg.2012.213", "10.1109/mcg.2006.120", "10.1109/icdar.2017.286", "10.1371/journal.pone.0146368", "10.1080/1472586x.2011.548488", "10.1109/tvcg.2014.2346574", "10.1080/0013838x.2017.1332021"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030464", "title": "Designing Narrative-Focused Role-Playing Games for Visualization Literacy in Young Children", "year": "2020", "conferenceName": "InfoVis", "authors": "Elaine Huynh;Angela Nyhout;Patricia Ganea;Fanny Chevalier", "citationCount": "0", "affiliation": "Huynh, E (Corresponding Author), Univ Toronto, Dept Comp Sci, Toronto, ON, Canada. Huynh, Elaine, Univ Toronto, Dept Comp Sci, Toronto, ON, Canada. Nyhout, Angela; Ganea, Patricia, Univ Toronto, Ontario Inst Studies Educ, Toronto, ON, Canada. Chevalier, Fanny, Univ Toronto, Dept Comp Sci \\& Stat Sci, Toronto, ON, Canada.", "countries": "Canada", "abstract": "Building on game design and education research, this paper introduces narrative-focused role-playing games as a way to promote visualization literacy in young children. Visualization literacy skills are vital in understanding the world around us and constructing meaningful visualizations, yet, how to better develop these skills at an early age remains largely overlooked and understudied. Only recently has the visualization community started to fill this gap, resulting in preliminary studies and development of educational tools for use in early education. We add to these efforts through the exploration of gamification to support learning, and identify an opportunity to apply role-playing game-based designs by leveraging the presence of narratives in data-related problems involving visualizations. We study the effects of including narrative elements on learning through a technology probe, grounded in a set of design considerations stemming from visualization, game design and education science. We create two versions of a game - one with narrative elements and one without - and evaluate our instances on 33 child participants between 11- to 13-years old using a between-subjects study design. Despite participants requiring double the amount of time to complete their game due to additional narrative elements, the inclusion of such elements were found to improve engagement without sacrificing learning; our results indicate no significant differences in development of graph-reading skills, but significant differences in engagement and overall enjoyment of the game. We report observations and qualitative feedback collected, and note areas for improvement and room for future work.", "keywords": "Visualization Literacy,Educational technology,Gamification,Narrative", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030464", "refList": ["10.1007/978-981-13-2694-3\\_2", "10.1145/2702123.2702298", "10.1145/2702123.2702558", "10.1007/978-981-13-2694-3\\_8", "10.1145/2702123.2702245", "10.1109/mis.2012.27", "10.18061/dsq.v21i4.318", "10.1109/tvcg.2013.134", "10.1109/vl.1996.545307", "10.1007/s10708-008-9186-0", "10.1093/cje/ben057", "10.5210/fm.v16i2.3316", "10.1109/tvcg.2019.2934539", "10.1109/tvcg.2016.2598608", "10.1007/978-3-319-94659-7\\_10", "10.1109/mcg.2013.28", "10.17351/ests2017.134", "10.1109/pacificvis.2014.39", "10.1145/3173574.3173728", "10.1145/2598784.2598806", "10.1145/2491500.2491501", "10.1145/1993060.1993065", "10.1109/tvcg.2018.2802520", "10.1145/3025453.3025667", "10.1145/2598510.2598566", "10.1080/15710882.2015.1081240", "10.17351/ests2017.133", "10.1109/tvcg.2014.2346431", "10.1016/j.ijhcs.2015.02.005", "10.1145/2702123.2702180", "10.1109/tvcg.2007.70577", "10.1109/mcg.2019.2923483", "10.5931/djim.v12.i1.6449", "10.1145/3240167.3240206", "10.1145/2468356.2468739", "10.1109/tvcg.2012.213", "10.1145/3025453.3025751", "10.4018/978-1-4666-6497-5.ch003"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030355", "title": "Guidelines For Pursuing and Revealing Data Abstractions", "year": "2020", "conferenceName": "InfoVis", "authors": "Alex Bigelow;Katy Williams;Katherine E. Isaacs", "citationCount": "0", "affiliation": "Bigelow, A (Corresponding Author), Univ Arizona, Tucson, AZ 85721 USA. Bigelow, Alex; Williams, Katy; Isaacs, Katherine E., Univ Arizona, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "Many data abstraction types, such as networks or set relationships, remain unfamiliar to data workers beyond the visualization research community. We conduct a survey and series of interviews about how people describe their data, either directly or indirectly. We refer to the latter as latent data abstractions. We conduct a Grounded Theory analysis that (1) interprets the extent to which latent data abstractions exist, (2) reveals the far-reaching effects that the interventionist pursuit of such abstractions can have on data workers, (3) describes why and when data workers may resist such explorations, and (4) suggests how to take advantage of opportunities and mitigate risks through transparency about visualization research perspectives and agendas. We then use the themes and codes discovered in the Grounded Theory analysis to develop guidelines for data abstraction in visualization projects. To continue the discussion, we make our dataset open along with a visual interface for further exploration.", "keywords": "Data abstraction,Grounded theory,Survey design,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030355", "refList": ["10.1080/2159676x.2016.1251701", "10.1109/infvis.2000.885092", "10.1145/2702123.2702298", "10.4135/9781848607941.n14", "10.1007/978-1-4939", "10.1109/tvcg.2014.2346331", "10.1109/tvcg.2017.2744843", "10.1177/1473871613510429", "10.1007/978-1-4939-0378-8\\_2", "10.1145/2598153.2598175", "10.1109/tvcg.2019.2934285", "10.1177/1473871613488591", "10.1145/2501105.2501106", "10.1109/tvcg.2019.2934538", "10.1109/tvcg.2019.2934539", "10.1017/s1049096510990781", "10.1145/3025453.3025837", "10.1145/3290605.3300474", "10.1145/3290605.3300356", "10.1002/nur.1025", "10.1145/2993901.2993916", "10.1145/3392826", "10.1086/269268", "10.1109/tvcg.2018.2865241", "10.1145/2998181.2998331", "10.1145/291224.291229", "10.1057/ivs.2009.13", "10.1145/2047196.2047205", "10.1109/tvcg.2012.213", "10.1145/3274405", "10.1109/tvcg.2013.145", "10.1016/0040-6031(92)85160-w", "10.1109/iv.2013.45", "10.1109/tvcg.2009.111", "10.1109/mcg.2019.2914844", "10.1109/tvcg.2009.116"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030405", "title": "Insights From Experiments With Rigor in an EvoBio Design Study", "year": "2020", "conferenceName": "InfoVis", "authors": "Jennifer Rogers;Austin H. Patton;Luke Harmon;Alexander Lex;Miriah D. Meyer", "citationCount": "0", "affiliation": "Rogers, J (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Rogers, Jen; Lex, Alexander; Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Patton, Austin H., Washington State Univ, Pullman, WA 99164 USA. Harmon, Luke, Univ Idaho, Moscow, ID 83843 USA.", "countries": "USA", "abstract": "Design study is an established approach of conducting problem-driven visualization research. The academic visualization community has produced a large body of work for reporting on design studies, informed by a handful of theoretical frameworks, and applied to a broad range of application areas. The result is an abundance of reported insights into visualization design, with an emphasis on novel visualization techniques and systems as the primary contribution of these studies. In recent work we proposed a new, interpretivist perspective on design study and six companion criteria for rigor that highlight the opportunities for researchers to contribute knowledge that extends beyond visualization idioms and software. In this work we conducted a year-long collaboration with evolutionary biologists to develop an interactive tool for visual exploration of multivariate datasets and phylogenetic trees. During this design study we experimented with methods to support three of the rigor criteria: ABUNDANT, REFLEXIVE, and TRANSPARENT. As a result we contribute two novel visualization techniques for the analysis of multivariate phylogenetic datasets, three methodological recommendations for conducting design studies drawn from reflections over our process of experimentation, and two writing devices for reporting interpretivist design study. We offer this work as an example for implementing the rigor criteria to produce a diverse range of knowledge contributions.", "keywords": "Methodologies,Application Motivated Visualization,Guidelines,Life Sciences Visualization,Health,Medicine,Biology,Bioinformatics,Genomics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030405", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1080/17493460802276893", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1109/tvcg.2012.213", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030462", "title": "Table Scraps: An Actionable Framework for Multi-Table Data Wrangling From An Artifact Study of Computational Journalism", "year": "2020", "conferenceName": "InfoVis", "authors": "Stephen Kasica;Charles Berret;Tamara Munzner", "citationCount": "0", "affiliation": "Kasica, S (Corresponding Author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada. Kasica, Stephen; Munzner, Tamara, Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada. Berret, Charles, Univ British Columbia, Sch Journalism Writing \\& Media, Vancouver, BC, Canada.", "countries": "Canada", "abstract": "For the many journalists who use data and computation to report the news, data wrangling is an integral part of their work. Despite an abundance of literature on data wrangling in the context of enterprise data analysis, little is known about the specific operations, processes, and pain points journalists encounter while performing this tedious, time-consuming task. To better understand the needs of this user group, we conduct a technical observation study of 50 public repositories of data and analysis code authored by 33 professional journalists at 26 news organizations. We develop two detailed and cross-cutting taxonomies of data wrangling in computational journalism, for actions and for processes. We observe the extensive use of multiple tables, a notable gap in previous wrangling analyses. We develop a concise, actionable framework for general multi-table data wrangling that includes wrangling operations documented in our taxonomy that are without clear parallels in other work. This framework, the first to incorporate tables as first-class objects, will support future interactive wrangling tools for both computational journalism and general-purpose use. We assess the generative and descriptive power of our framework through discussion of its relationship to our set of taxonomies.", "keywords": "Computational journalism,Data journalism,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030462", "refList": ["10.1145/1378773.1378792", "10.1109/tvcg.2012.219", "10.1109/vast47406.2019.8986909", "10.1145/1084805.1084812", "10.1007/s00778-008-0098-x", "10.1016/j.websem.2008.09.005", "10.18637/jss.v040.i01", "10.1145/989863.989865", "10.1109/tvcg.2015.2467551", "10.5281/zenodo.3509134", "10.1109/tvcg.2019.2934539", "10.1109/tvcg.2019.2934593", "10.1109/tse.2018.2796554", "10.17349/jmc117309", "10.1109/2945.981851", "10.1109/vast.2011.6102440", "10.1177/1473871611415994", "10.1145/2001269.2001288"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13964", "year": "2020", "title": "Reading Traces: Scalable Exploration in Elastic Visualizations of Cultural Heritage Data", "conferenceName": "EuroVis", "authors": "Mark{-}Jan Bludau;Viktoria Br{\\\"{u}}ggemann;Anna Busch;Marian D{\\\"{o}}rk", "citationCount": "1", "affiliation": "Bludau, MJ (Corresponding Author), Univ Appl Sci Potsdam, UCLAB, Potsdam, Germany.\nBludau, M. -J.; Brueggemann, V.; Doerk, M., Univ Appl Sci Potsdam, UCLAB, Potsdam, Germany.\nBusch, A., Univ Potsdam, Theodor Fontane Archiv, Potsdam, Germany.", "countries": "Germany", "abstract": "Through a design study, we develop an approach to data exploration that utilizes elastic visualizations designed to support varying degrees of detail and abstraction. Examining the notions of scalability and elasticity in interactive visualizations, we introduce a visualization of personal reading traces such as marginalia or markings inside the reference library of German realist author Theodor Fontane. To explore such a rich and extensive collection, meaningful visual forms of abstraction and detail are as important as the transitions between those states. Following a growing research interest in the role of fluid interactivity and animations between views, we are particularly interested in the potential of carefully designed transitions and consistent representations across scales. The resulting prototype addresses humanistic research questions about the interplay of distant and close reading with visualization research on continuous navigation along several granularity levels, using scrolling as one of the main interaction mechanisms. In addition to presenting the design process and resulting prototype, we present findings from a qualitative evaluation of the tool, which suggest that bridging between distant and close views can enhance exploration, but that transitions between views need to be crafted very carefully to facilitate comprehension.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13964", "refList": ["10.1007/s41244-017-0048-4", "10.1109/tvcg.2009.108", "10.1145/1456650.1456652", "10.1109/tvcg.2014.2346424", "10.1177/1473871611416549", "10.1111/cgf.13195", "10.1145/2207676.2208607", "10.1145/1376616.1376618", "10.1006/ijhc.2002.1017", "10.1109/tvcg.2011.185", "10.1177/1473871611413180", "10.1109/vl.1996.545307", "10.1006/ijhc.1017", "10.1109/tvcg.2019.2934539", "10.1109/tvcg.2018.2830759", "10.1145/1556262.1556300", "10.1109/tvcg.2014.2346677", "10.1145/2909132.2909255", "10.1109/tvcg.2007.70539", "10.1145/2396636.2396675", "10.1145/1978942.1979124", "10.1016/j.ijhcs.2003.08.005", "10.2312/eurovisstar.20151113", "10.1109/infvis.2005.1532127"], "wos": 1, "children": [], "len": 1}], "len": 17}, {"doi": "10.1109/tvcg.2019.2934283", "title": "What is Interaction for Data Visualization?", "year": "2019", "conferenceName": "InfoVis", "authors": "Evanthia Dimara;Charles Perin", "citationCount": "1", "affiliation": "Dimara, E (Corresponding Author), Sorbonne Univ, Paris, France. Dimara, Evanthia, Sorbonne Univ, Paris, France. Perin, Charles, Univ Victoria, Victoria, BC, Canada.", "countries": "Canada;France", "abstract": "Interaction is fundamental to data visualization, but what \u201cinteraction\u201d means in the context of visualization is ambiguous and confusing. We argue that this confusion is due to a lack of consensual definition. To tackle this problem, we start by synthesizing an inclusive view of interaction in the visualization community \u2013 including insights from information visualization, visual analytics and scientific visualization, as well as the input of both senior and junior visualization researchers. Once this view takes shape, we look at how interaction is defined in the field of human-computer interaction (HCI). By extracting commonalities and differences between the views of interaction in visualization and in HCI, we synthesize a definition of interaction for visualization. Our definition is meant to be a thinking tool and inspire novel and bolder interaction design practices. We hope that by better understanding what interaction in visualization is and what it can be, we will enrich the quality of interaction in visualization systems and empower those who use them.", "keywords": "interaction,visualization,data,definition,human-computer interaction", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934283", "refList": ["10.1057/ivs.2009.22", "10.1515/icom-2017-0027", "10.1145/2493102.2493104", "10.1007/978-3-319-06793-3\\_6", "10.1080/03640210801898177", "10.1109/mcg.2010.30", "10.1109/tvcg.2013.134", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2018.2865237", "10.1109/2945.981847", "10.1145/2598784.2598806", "10.1109/tvcg.2013.130", "10.1109/tvcg.2007.70577", "10.1109/mic.2015.129", "10.1145/2909132.2909267", "10.1080/01449290500330331", "10.1109/tvcg.2018.2865233", "10.1109/tvcg.2015.2467831", "10.1109/iv.2015.34", "10.1109/tvcg.2009.111", "10.1145/3290605.3300565", "10.1145/3173574.3173797", "10.1145/948496.948514", "10.1145/3025453.3025765", "10.1109/tvcg.2015.2467613", "10.1145/2659796", "10.1109/tvcg.2014.2346311", "10.1145/3025453.3025524", "10.1080/17452759.2011.558588", "10.1145/3027063.3053113", "10.1109/infvis.2005.1532136", "10.1145/2470654.2481307", "10.1109/tvcg.2018.2864913", "10.1145/345513.345267", "10.3102/00028312005004437", "10.1109/tvcg.2007.70436", "10.1037/0033-295x.106.4.643", "10.1145/2133416.2146416", "10.1109/tvcg.2013.191", "10.1109/tvcg.2010.177", "10.1145/960201.957206", "10.1109/tvcg.2007.70515", "10.1109/infvis.2000.885092", "10.1145/2636240.2636844", "10.1037/h0055392", "10.1177/1473871611413180", "10.1109/vl.1996.545307", "10.1111/j.1471-1842.2009.00848.x", "10.1145/989863.989865", "10.1109/tvcg.2013.124", "10.1109/tvcg.2008.109", "10.1145/1166253.1166265", "10.1145/2702123.2702180", "10.1109/tvcg.2016.2598620", "10.1080/07370024.2016.1226139", "10.1145/3173574.3173909", "10.1109/pacificvis.2010.5429613", "10.1111/j.1467-6478.2006.00368.x", "10.1109/tvcg.2012.204", "10.1109/tvcg.2013.120", "10.1179/1743277412y.0000000019", "10.1145/1936652.1936684", "10.2307/1269768", "10.1109/infvis.1996.559213", "10.1109/tvcg.2016.2598839", "10.1145/2642918.2647360", "10.1057/palgrave.ivs.9500099", "10.1016/j.cag.2009.06.004", "10.1109/mc.2013.178", "10.1109/tvcg.2007.70541", "10.1109/vast.2011.6102473", "10.1145/358886.358895", "10.1109/tvcg.2014.2346573", "10.1109/tvcg.2018.2865159", "10.1145/2207676.2207741", "10.1109/tvcg.2015.2396062", "10.1145/2207676.2208572", "10.1109/tvcg.2016.2598608", "10.1057/ivs.2008.31", "10.1177/001316446002000104", "10.1109/tvcg.2017.2680452", "10.1109/tvcg.2006.80", "10.1145/2598510.2598566", "10.1037/0003-066x.51.4.355", "10.7146/dpb.v16i224.7586", "10.1109/infvis.1998.729560", "10.1162/leon\\_a\\_00011", "10.1109/tvcg.2010.157", "10.1109/tvcg.2014.2359887"], "wos": 1, "children": [], "len": 1}], "len": 21}, {"doi": "10.1109/tvcg.2018.2865025", "title": "An Information-Theoretic Approach to the Cost-benefit Analysis of Visualization in Virtual Environments", "year": "2018", "conferenceName": "VAST", "authors": "Min Chen;Kelly P. Gaither;Nigel W. John;Brian McCann", "citationCount": "1", "affiliation": "Chen, M (Corresponding Author), Univ Oxford, Oxford, England. Chen, Min, Univ Oxford, Oxford, England. Gaither, Kelly; McCann, Brian, Univ Texas Austin, Austin, TX 78712 USA. John, Nigel W., Univ Chester, Chester, Cheshire, England.", "countries": "USA;England", "abstract": "Visualization and virtual environments (VEs) have been two interconnected parallel strands in visual computing for decades. Some VEs have been purposely developed for visualization applications, while many visualization applications are exemplary showcases in general-purpose VEs. Because of the development and operation costs of VEs, the majority of visualization applications in practice have yet to benefit from the capacity of VEs. In this paper, we examine this status quo from an information-theoretic perspective. Our objectives are to conduct cost-benefit analysis on typical VE systems (including augmented and mixed reality, theater-based systems, and large powerwalls), to explain why some visualization applications benefit more from VEs than others, and to sketch out pathways for the future development of visualization applications in VEs. We support our theoretical propositions and analysis using theories and discoveries in the literature of cognitive sciences and the practical evidence reported in the literatures of visualization and VEs.", "keywords": "Theory of visualization,virtual environments,four levels of visualization,virtual reality,augmented reality,mixed reality,cost-benefit analysis,information theory,cognitive sciences,visualization applications,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865025", "refList": ["10.1109/tvcg.2011.231", "10.1007/s00268-007-9307-9", "10.1162/pres.1995.4.1.64", "10.1038/81497", "10.1523/jneurosci.0647-08.2008", "10.1037/a0029856", "10.1038/nature03390", "10.1016/s1364-6613(97)01080-2", "10.1111/cgf.13169", "10.1364/josaa.20.001419", "10.1109/tvcg.2013.127", "10.1037/0033-295x.101.2.343", "10.1109/tvcg.2012.42", "10.1007/s002210100745", "10.1109/vl.1996.545307", "10.1016/s0959-4388(98)80140-2", "10.1007/s00221-006-0804-0", "10.1016/0001-6918(67)90080-7", "10.1523/jneurosci.4319-03.2004", "10.1037/0096-3445.109.2.160", "10.1145/2330667.2330687", "10.1109/tvcg.2010.132", "10.1037/0033-295x.113.4.766", "10.1145/22949.22950", "10.1002/j.1538-7305.1948.tb00917.x", "10.1109/jdt.2008.2001575", "10.1109/tvcg.2008.142", "10.1016/s0079-6123(06)55002-2", "10.1109/mcg.2018.032421653", "10.1038/17953", "10.1162/105474602760204309", "10.1145/253284.253301", "10.1007/978-3-662-43790-2\\_6", "10.1038/nn963", "10.1067/mob.2002.127361", "10.1037/a0033101", "10.1109/ldav.2012.6378981", "10.1113/jphysiol.1964.sp007485", "10.1016/j.tics.2005.02.009", "10.1146/annurev.ne.18.030195.001205", "10.1109/infvis.2004.59", "10.1111/j.1460-2466.1992.tb00812.x", "10.1089/109493101300117884", "10.1109/tvcg.2010.131", "10.1016/s0896-6273(02)01003-6", "10.1016/s0042-6989(01)00102-x", "10.1016/0010-0285(80)90005-5", "10.1109/mcg.2014.18", "10.1109/38.963459", "10.1109/tvcg.2012.133", "10.1007/978-1-4899-5379-7\\_8", "10.1145/2556288.2557020", "10.1109/mcg.2013.37", "10.1109/tvcg.2014.20", "10.3758/bf03200774", "10.1097/sla.0b013e318288c40b", "10.1016/j.cub.2009.12.014", "10.1016/j.cag.2012.04.007", "10.1109/mcg.2014.80", "10.1111/j.1467-8659.2012.03114.x", "10.1111/j.1600-0412.2012.01482.x", "10.1109/tvcg.2006.184", "10.1007/s11548-013-0929-0", "10.1007/978-1-4899-5379-7", "10.1016/0042-6989(84)90041-5", "10.1109/tvcg.2014.2346325", "10.1145/2702123.2702406", "10.1162/pres.1992.1.1.120", "10.1038/36846", "10.1167/7.5.6", "10.1146/annurev.psych.53.100901.135125", "10.1145/1128923.1128948"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934264", "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;David S. Ebert;Arif Ghafoor", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Karimzadeh, Morteza; Ebert, David S.; Ghafoor, Arif, Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.", "keywords": "Summative evaluation,usefulness,evaluation process,taxonomy,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934264", "refList": ["10.1109/tvcg.2017.2744478", "10.1109/tvcg.2018.2865025", "10.1109/tvcg.2006.85", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2017.2745181", "10.1111/cgf.13677", "10.1109/tvcg.2018.2864844", "10.1109/tvcg.2013.126", "10.1109/tvcg.2018.2864811", "10.1109/infvis.2005.1532147", "10.1177/0956797613504966", "10.1145/2669557.2669579", "10.1109/mcg.2005.102", "10.1109/visual.2003.1250426", "10.1136/bmj.39489.470347.ad", "10.1109/tvcg.2017.2744080", "10.1109/mcg.2009.53", "10.1111/j.1467-8527.2005.00307.x", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2864886", "10.1109/tvcg.2018.2864843", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2018.2865051", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2018.2865026", "10.1007/978-3-540-71080-6\\_6", "10.1109/tvcg.2018.2865020", "10.1177/1473871611407399", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2018.2864526", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2864905", "10.1049/sej.1991.0040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2711030", "10.1109/tvcg.2011.279", "10.1109/vast.2017.8585505", "10.1147/jrd.2010.2042914", "10.1016/s0378-7206(98)00044-5", "10.1145/2993901.2993913", "10.1109/tvcg.2018.2865041", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744758", "10.1145/1168149.1168158", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2012.213", "10.1109/tvcg.2017.2744738", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2018.2864826", "10.1145/1377966.1377974", "10.1109/apec.2009.4802646", "10.1145/1168149.1168152", "10.1016/j.jss.2008.03.059", "10.1109/vast.2017.8585484", "10.1109/tvcg.2017.2744818", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2865042", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030388", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "year": "2020", "conferenceName": "SciVis", "authors": "Pepe Eulzer;Sabine Bauer;Francis Kilian;Kai Lawonn", "citationCount": "0", "affiliation": "Eulzer, P (Corresponding Author), Univ Jena, Jena, Germany. Eulzer, Pepe; Lawonn, Kai, Univ Jena, Jena, Germany. Bauer, Sabine, Univ Koblenz Landau, Koblenz, Germany. Kilian, Francis, Cath Clin Koblenz Montabaur, Dept Spine Surg, Koblenz, Germany.", "countries": "Germany", "abstract": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. $\\mathrm{By}$ linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. $\\mathrm{in}$ a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.", "keywords": "Medical visualization,bioinformatics,coordinated views,focus and context,biomechanical simulation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030388", "refList": ["10.1109/tvcg.2018.2864903", "10.1177/1473871613510429", "10.1093/ehjqcco/qcz052", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2018.2865076", "10.1055/s-0039-1687862", "10.1109/visual.1990.146375", "10.1109/tvcg.2017.2744198", "10.1016/j.ijmedinf.2014.10.001", "10.1109/tvcg.2013.124", "10.1016/j.jacc", "10.1111/cgf.13167", "10.17705/1thci.00055", "10.1136/bmjqs.2009.037895", "10.1109/tvcg.2013.238", "10.1109/tvcg.2018.2865240", "10.1186/1471-2261-6-34", "10.1109/tvcg.2019.2934264", "10.1109/tvcg.2013.200", "10.1109/tvcg.2011.209", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467091", "10.1136/bmjopen-2019-033208", "10.1109/beliv.2018.8634027", "10.1109/tvcg.2012.213", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2015.2467325", "10.1145/2133806.2133821", "10.1145/1806799.1806866", "10.1108/02635570610688869", "10.1002/hbm.20701", "10.1561/1100000039", "10.1145/3025453.3025645", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 5}], "len": 35}, {"doi": "10.1111/cgf.13178", "year": "2017", "title": "Stardust: Accessible and Transparent GPU Support for Information Visualization Rendering", "conferenceName": "EuroVis", "authors": "Donghao Ren;Bongshin Lee;Tobias H{\\\"{o}}llerer", "citationCount": "7", "affiliation": "Ren, DH (Corresponding Author), Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.\nRen, Donghao; Hollerer, Tobias, Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.\nLee, Bongshin, Microsoft Res, Redmond, WA USA.", "countries": "USA", "abstract": "Web-based visualization libraries are in wide use, but performance bottlenecks occur when rendering, and especially animating, a large number of graphical marks. While GPU-based rendering can drastically improve performance, that paradigm has a steep learning curve, usually requiring expertise in the computer graphics pipeline and shader programming. In addition, the recent growth of virtual and augmented reality poses a challenge for supporting multiple display environments beyond regular canvases, such as a Head Mounted Display (HMD) and Cave Automatic Virtual Environment (CAVE). In this paper, we introduce a new web-based visualization library called Stardust, which provides a familiar API while leveraging GPU's processing power. Stardust also enables developers to create both 2D and 3D visualizations for diverse display environments using a uniform API. To demonstrate Stardust's expressiveness and portability, we present five example visualizations and a coding playground for four display environments. We also evaluate its performance by comparing it against the standard HTML5 Canvas, D3, and Vega.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13178", "refList": ["10.1145/1015706.1015800", "10.1111/cgf.12129", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2011.185", "10.1145/129888.129892", "10.1038/324446a0", "10.2307/2686111", "10.3402/qhw.v6i2.5918", "10.1145/1015706.1015801", "10.1109/tvcg.2009.191", "10.1007/s00146-006-0050-9", "10.1109/34.910880", "10.1109/tvcg.2009.174", "10.1109/icsens.2015.7370446", "10.1109/infvis.2004.64", "10.1016/j.cag.2013.12.004", "10.1109/tvcg.2016.2520921", "10.1057/palgrave/ivs/9500003", "10.1177/1473871611415997", "10.1109/glocom.2015.7417476", "10.1109/tvcg.2016.2599030", "10.1109/tvcg.2016.2614803"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934537", "title": "P5: Portable Progressive Parallel Processing Pipelines for Interactive Data Analysis and Visualization", "year": "2019", "conferenceName": "InfoVis", "authors": "Jianping Kelvin Li;Kwan-Liu Ma", "citationCount": "2", "affiliation": "Li, JPK (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Li, Jianping Kelvin; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "We present P5, a web-based visualization toolkit that combines declarative visualization grammar and GPU computing for progressive data analysis and visualization. To interactively analyze and explore big data, progressive analytics and visualization methods have recently emerged. Progressive visualizations of incrementally refining results have the advantages of allowing users to steer the analysis process and make early decisions. P5 leverages declarative grammar for specifying visualization designs and exploits GPU computing to accelerate progressive data processing and rendering. The declarative specifications can be modified during progressive processing to create different visualizations for analyzing the intermediate results. To enable user interactions for progressive data analysis, P5 utilizes the GPU to automatically aggregate and index data based on declarative interaction specifications to facilitate effective interactive visualization. We demonstrate the effectiveness and usefulness of P5 through a variety of example applications and several performance benchmark tests.", "keywords": "Information visualization,progressive analytics,visualization software,GPU computing,data exploration", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934537", "refList": ["10.1109/tvcg.2014.2346578", "10.1111/cgf.13205", "10.1109/tvcg.2009.110", "10.1109/cluster.2017.26", "10.4230/dagrep.8.10.1", "10.1111/cgf.12129", "10.1109/tvcg.2014.2346452", "10.1109/tvcg.2011.185", "10.1109/tvcg.2014.2346319", "10.1023/a:1009726021843", "10.1145/2465351.2465355", "10.1016/s0743-7315(02)00004-7", "10.1111/cgf.13178", "10.1145/775047.775109", "10.1109/tvcg.2015.2467091", "10.1109/tvcg.2003.1196005", "10.1109/tvcg.2016.2598470", "10.1109/tvcg.2015.2462356", "10.1145/2020408.2020579", "10.1109/tvcg.2013.179", "10.1109/tvcg.2009.191", "10.1109/tvcg.2010.144", "10.1109/tvcg.2016.2607714", "10.1109/tvcg.2014.2346574", "10.1109/tvcg.2017.2744358", "10.1016/j.visinf.2018.04.011", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030453", "title": "P6: A Declarative Language for Integrating Machine Learning in Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Jianping Kelvin Li;Kwan-Liu Ma", "citationCount": "0", "affiliation": "Li, JK (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Li, Jianping Kelvin; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "We present P6, a declarative language for building high performance visual analytics systems through its support for specifying and integrating machine learning and interactive visualization methods. As data analysis methods based on machine learning and artificial intelligence continue to advance, a visual analytics solution can leverage these methods for better exploiting large and complex data. However, integrating machine learning methods with interactive visual analysis is challenging. Existing declarative programming libraries and toolkits for visualization lack support for coupling machine learning methods. By providing a declarative language for visual analytics, P6 can empower more developers to create visual analytics applications that combine machine learning and visualization methods for data analysis and problem solving. Through a variety of example applications, we demonstrate P6's capabilities and show the benefits of using declarative specifications to build visual analytics systems. We also identify and discuss the research opportunities and challenges for declarative visual analytics.", "keywords": "visual analytics,interactive visualization,machine learning,toolkits,declarative specification", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030453", "refList": ["10.1145/1809400.1809403", "10.1057/ivs.2009.22", "10.1145/2882903.2912565", "10.1109/mcse.2011.37", "10.1007/978-0-387-98141-3\\_1", "10.1145/3173574.3173606", "10.1111/cgf.12129", "10.1109/tvcg.2014.2346452", "10.1109/tvcg.2011.185", "10.1109/tvcg.2007.70515", "10.1016/j.sigpro.2019.107299", "10.3233/978-1-61499-649-1-87", "10.1145/1502650.1502695", "10.1109/vl.1996.545307", "10.1111/j.1095-8649.2005.00662.x", "10.1145/108360.108361", "10.1109/tvcg.2018.2871139", "10.1109/icde.2011.5767930", "10.1016/j.visinf.2018.04.010", "10.1109/tvcg.2017.2744684", "10.1109/infvis.2002.1173156", "10.1109/vast.2008.4677357", "10.1109/bigdata.2014.7004255", "10.1109/tvcg.2014.2346481", "10.1111/cgf.13178", "10.1109/tvcg.2008.109", "10.1109/tvcg.2015.2467091", "10.1109/mc.2013.120", "10.1109/tvcg.2016.2534558", "10.1145/3126594.3126642", "10.1023/a:1011368926479", "10.21105/joss.01057", "10.1109/tvcg.2010.144", "10.1109/mcg.2012.87", "10.1145/2133806.2133821", "10.1111/cgf.13092", "10.1109/tvcg.2019.2934537", "10.1111/cgf.12903", "10.1109/tvcg.2009.111", "10.1109/tvcg.2016.2599030", "10.1007/s12650-018-0531-1"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030361", "title": "PipelineProfiler: A Visual Analytics Tool for the Exploration of AutoML Pipelines", "year": "2020", "conferenceName": "VAST", "authors": "Jorge Henrique Piazentin Ono;Sonia Castelo;Roque Lopez;Enrico Bertini;Juliana Freire;Cl\u00e1udio T. Silva", "citationCount": "0", "affiliation": "Ono, JP (Corresponding Author), NYU, New York, NY 10003 USA. Ono, Jorge Piazentin; Castelo, Sonia; Lopez, Roque; Bertini, Enrico; Freire, Juliana; Silva, Claudio, NYU, New York, NY 10003 USA.", "countries": "USA", "abstract": "In recent years, a wide variety of automated machine learning (AutoML) methods have been proposed to generate end-to-end ML pipelines. While these techniques facilitate the creation of models, given their black-box nature, the complexity of the underlying algorithms, and the large number of pipelines they derive, they are difficult for developers to debug. It is also challenging for machine learning experts to select an AutoML system that is well suited for a given problem. In this paper, we present the Pipeline Profiler, an interactive visualization tool that allows the exploration and comparison of the solution space of machine learning (ML) pipelines produced by AutoML systems. PipelineProfiler is integrated with Jupyter Notebook and can be combined with common data science tools to enable a rich set of analyses of the ML pipelines, providing users a better understanding of the algorithms that generated them as well as insights into how they can be improved. We demonstrate the utility of our tool through use cases where PipelineProfiler is used to better understand and improve a real-world AutoML system. Furthermore, we validate our approach by presenting a detailed analysis of a think-aloud experiment with six data scientists who develop and evaluate AutoML tools.", "keywords": "Automatic Machine Learning,Pipeline Visualization,Model Evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030361", "refList": ["10.1145/1809400.1809403", "10.1057/ivs.2009.22", "10.1109/mcse.2011.37", "10.1007/978-0-387-98141-3\\_1", "10.1145/3173574.3173606", "10.1111/cgf.12129", "10.1109/tvcg.2014.2346452", "10.1109/tvcg.2011.185", "10.1109/tvcg.2007.70515", "10.1016/j.sigpro.2019.107299", "10.3233/978-1-61499-649-1-87", "10.1109/vl.1996.545307", "10.1111/j.1095-8649.2005.00662.x", "10.1109/tvcg.2018.2871139", "10.1109/icde.2011.5767930", "10.1016/j.visinf.2018.04.010", "10.1109/pacificvis.2017.8031587", "10.1109/infvis.2002.1173156", "10.1109/vast.2008.4677357", "10.1109/bigdata.2014.7004255", "10.1109/tvcg.2014.2346481", "10.14778/3007263.3007279", "10.1111/cgf.13178", "10.1109/tvcg.2008.109", "10.1109/tvcg.2015.2467091", "10.1109/mc.2013.120", "10.1109/tvcg.2016.2534558", "10.1145/3126594.3126642", "10.21105/joss.01057", "10.1109/infvis.1998.729560", "10.1109/tvcg.2010.144", "10.1109/mcg.2012.87", "10.1145/2133806.2133821", "10.1111/cgf.13092", "10.1109/tvcg.2019.2934537", "10.1111/cgf.12903", "10.1109/tvcg.2009.111", "10.1109/tvcg.2016.2599030", "10.1007/s12650-018-0531-1"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/tvcg.2020.3030453", "title": "P6: A Declarative Language for Integrating Machine Learning in Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Jianping Kelvin Li;Kwan-Liu Ma", "citationCount": "0", "affiliation": "Li, JK (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Li, Jianping Kelvin; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "We present P6, a declarative language for building high performance visual analytics systems through its support for specifying and integrating machine learning and interactive visualization methods. As data analysis methods based on machine learning and artificial intelligence continue to advance, a visual analytics solution can leverage these methods for better exploiting large and complex data. However, integrating machine learning methods with interactive visual analysis is challenging. Existing declarative programming libraries and toolkits for visualization lack support for coupling machine learning methods. By providing a declarative language for visual analytics, P6 can empower more developers to create visual analytics applications that combine machine learning and visualization methods for data analysis and problem solving. Through a variety of example applications, we demonstrate P6's capabilities and show the benefits of using declarative specifications to build visual analytics systems. We also identify and discuss the research opportunities and challenges for declarative visual analytics.", "keywords": "visual analytics,interactive visualization,machine learning,toolkits,declarative specification", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030453", "refList": ["10.1145/1809400.1809403", "10.1057/ivs.2009.22", "10.1145/2882903.2912565", "10.1109/mcse.2011.37", "10.1007/978-0-387-98141-3\\_1", "10.1145/3173574.3173606", "10.1111/cgf.12129", "10.1109/tvcg.2014.2346452", "10.1109/tvcg.2011.185", "10.1109/tvcg.2007.70515", "10.1016/j.sigpro.2019.107299", "10.3233/978-1-61499-649-1-87", "10.1145/1502650.1502695", "10.1109/vl.1996.545307", "10.1111/j.1095-8649.2005.00662.x", "10.1145/108360.108361", "10.1109/tvcg.2018.2871139", "10.1109/icde.2011.5767930", "10.1016/j.visinf.2018.04.010", "10.1109/tvcg.2017.2744684", "10.1109/infvis.2002.1173156", "10.1109/vast.2008.4677357", "10.1109/bigdata.2014.7004255", "10.1109/tvcg.2014.2346481", "10.1111/cgf.13178", "10.1109/tvcg.2008.109", "10.1109/tvcg.2015.2467091", "10.1109/mc.2013.120", "10.1109/tvcg.2016.2534558", "10.1145/3126594.3126642", "10.1023/a:1011368926479", "10.21105/joss.01057", "10.1109/tvcg.2010.144", "10.1109/mcg.2012.87", "10.1145/2133806.2133821", "10.1111/cgf.13092", "10.1109/tvcg.2019.2934537", "10.1111/cgf.12903", "10.1109/tvcg.2009.111", "10.1109/tvcg.2016.2599030", "10.1007/s12650-018-0531-1"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030361", "title": "PipelineProfiler: A Visual Analytics Tool for the Exploration of AutoML Pipelines", "year": "2020", "conferenceName": "VAST", "authors": "Jorge Henrique Piazentin Ono;Sonia Castelo;Roque Lopez;Enrico Bertini;Juliana Freire;Cl\u00e1udio T. Silva", "citationCount": "0", "affiliation": "Ono, JP (Corresponding Author), NYU, New York, NY 10003 USA. Ono, Jorge Piazentin; Castelo, Sonia; Lopez, Roque; Bertini, Enrico; Freire, Juliana; Silva, Claudio, NYU, New York, NY 10003 USA.", "countries": "USA", "abstract": "In recent years, a wide variety of automated machine learning (AutoML) methods have been proposed to generate end-to-end ML pipelines. While these techniques facilitate the creation of models, given their black-box nature, the complexity of the underlying algorithms, and the large number of pipelines they derive, they are difficult for developers to debug. It is also challenging for machine learning experts to select an AutoML system that is well suited for a given problem. In this paper, we present the Pipeline Profiler, an interactive visualization tool that allows the exploration and comparison of the solution space of machine learning (ML) pipelines produced by AutoML systems. PipelineProfiler is integrated with Jupyter Notebook and can be combined with common data science tools to enable a rich set of analyses of the ML pipelines, providing users a better understanding of the algorithms that generated them as well as insights into how they can be improved. We demonstrate the utility of our tool through use cases where PipelineProfiler is used to better understand and improve a real-world AutoML system. Furthermore, we validate our approach by presenting a detailed analysis of a think-aloud experiment with six data scientists who develop and evaluate AutoML tools.", "keywords": "Automatic Machine Learning,Pipeline Visualization,Model Evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030361", "refList": ["10.1145/1809400.1809403", "10.1057/ivs.2009.22", "10.1109/mcse.2011.37", "10.1007/978-0-387-98141-3\\_1", "10.1145/3173574.3173606", "10.1111/cgf.12129", "10.1109/tvcg.2014.2346452", "10.1109/tvcg.2011.185", "10.1109/tvcg.2007.70515", "10.1016/j.sigpro.2019.107299", "10.3233/978-1-61499-649-1-87", "10.1109/vl.1996.545307", "10.1111/j.1095-8649.2005.00662.x", "10.1109/tvcg.2018.2871139", "10.1109/icde.2011.5767930", "10.1016/j.visinf.2018.04.010", "10.1109/pacificvis.2017.8031587", "10.1109/infvis.2002.1173156", "10.1109/vast.2008.4677357", "10.1109/bigdata.2014.7004255", "10.1109/tvcg.2014.2346481", "10.14778/3007263.3007279", "10.1111/cgf.13178", "10.1109/tvcg.2008.109", "10.1109/tvcg.2015.2467091", "10.1109/mc.2013.120", "10.1109/tvcg.2016.2534558", "10.1145/3126594.3126642", "10.21105/joss.01057", "10.1109/infvis.1998.729560", "10.1109/tvcg.2010.144", "10.1109/mcg.2012.87", "10.1145/2133806.2133821", "10.1111/cgf.13092", "10.1109/tvcg.2019.2934537", "10.1111/cgf.12903", "10.1109/tvcg.2009.111", "10.1109/tvcg.2016.2599030", "10.1007/s12650-018-0531-1"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14005", "year": "2020", "title": "Canis: A High-Level Language for Data-Driven Chart Animations", "conferenceName": "EuroVis", "authors": "T. Ge;Y. Zhao;B. Lee;D. Ren;B. Chen;Y. Wang", "citationCount": "0", "affiliation": "Wang, Y (Corresponding Author), Shandong Univ, Qingdao, Peoples R China.\nGe, T.; Zhao, Y.; Wang, Y., Shandong Univ, Qingdao, Peoples R China.\nLee, B., Microsoft Res, Redmond, WA USA.\nRen, D., Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.\nChen, B., Peking Univ, Beijing, Peoples R China.", "countries": "USA;China", "abstract": "In this paper, we introduce Canis, a high-level domain-specific language that enables declarative specifications of data-driven chart animations. By leveraging data-enriched SVG charts, its grammar of animations can be applied to the charts created by existing chart construction tools. With Canis, designers can select marks from the charts, partition the selected marks into mark units based on data attributes, and apply animation effects to the mark units, with the control of when the effects start. The Canis compiler automatically synthesizes the Lottie animation JSON files {[}Aira], which can be rendered natively across multiple platforms. To demonstrate Canis' expressiveness, we present a wide range of chart animations. We also evaluate its scalability by showing the effectiveness of our compiler in reducing the output specification size and comparing its performance on different platforms against D3.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14005", "refList": ["10.1109/tvcg.2016.2598647", "10.1198/jcgs.2009.07098", "10.1111/cgf.13709", "10.1109/tvcg.2010.78", "10.1109/tvcg.2014.2346424", "10.1145/3173574.3173697", "10.1111/cgf.13178", "10.1006/ijhc.1017", "10.1145/3025453.3025942", "10.1145/2702123.2702431", "10.1109/tvcg.2008.125", "10.1006/ijhc.2002.1017", "10.1109/tvcg.2011.185", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2016.2599030.2", "10.1145/2642918.2647411", "10.1109/tvcg.2009.174"], "wos": 1, "children": [], "len": 1}], "len": 13}], "len": 233}, "index": 531, "embedding": [-0.15067017078399658, 7.321950435638428, -1.0222129821777344, -2.389355182647705, -0.630656361579895, 0.16650904715061188, 6.578383922576904, 6.213881969451904, -0.4060705304145813, 9.003061294555664, -1.364963173866272, 4.25940465927124, -0.1321265697479248, 3.9393715858459473, 14.079000473022461, 11.813533782958984, 2.0474815368652344, 10.848711967468262, -0.5624300837516785, 2.1799986362457275, -0.06630208343267441, 11.316211700439453, 9.303775787353516, 10.992363929748535, -2.458800792694092, 11.941276550292969, -0.5019459128379822, 7.77477502822876, 6.304133415222168, 10.451327323913574, 3.2635722160339355, 10.385138511657715], "projection": [0.8968336582183838, 12.375913619995117], "size": 117, "height": 5, "width": 41}