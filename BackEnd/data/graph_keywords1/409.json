{"data": {"doi": "10.1109/tvcg.2014.2346913", "title": "VAET: A Visual Analytics Approach for E-Transactions Time-Series", "year": "2014", "conferenceName": "VAST", "authors": "Cong Xie;Wei Chen;Xinxin Huang;Yueqi Hu;Scott Barlowe;Jing Yang", "citationCount": "35", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Zhejiang, Peoples R China. Xie, Cong; Chen, Wei; Huang, Xinxin, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Zhejiang, Peoples R China. Chen, Wei, Zhejiang Univ, Cyber Innovat Joint Res Ctr, Hangzhou, Zhejiang, Peoples R China. Hu, Yueqi; Yang, Jing, Univ N Carolina, Dept Comp Sci, Charlotte, NC 28223 USA. Barlowe, Scott, Western Carolina Univ, Cullowhee, NC USA.", "countries": "USA;China", "abstract": "Previous studies on E-transaction time-series have mainly focused on finding temporal trends of transaction behavior. Interesting transactions that are time-stamped and situation-relevant may easily be obscured in a large amount of information. This paper proposes a visual analytics system, Visual Analysis of E-transaction Time-Series (VAET), that allows the analysts to interactively explore large transaction datasets for insights about time-varying transactions. With a set of analyst-determined training samples, VAET automatically estimates the saliency of each transaction in a large time-series using a probabilistic decision tree learner. It provides an effective time-of-saliency (TOS) map where the analysts can explore a large number of transactions at different time granularities. Interesting transactions are further encoded with KnotLines, a compact visual representation that captures both the temporal variations and the contextual connection of transactions. The analysts can thus explore, select, and investigate knotlines of interest. A case study and user study with a real E-transactions dataset (26 million records) demonstrate the effectiveness of VAET.", "keywords": "Time-Series, Visual Analytics, E-transaction", "link": "http://dx.doi.org/10.1109/TVCG.2014.2346913", "refList": ["10.1109/tvcg.2009.123", "10.1111/j.1467-8659.2008.01222.x", "10.1007/978-0-85729-079-3\\_1", "10.1109/wsc.2003.1261490", "10.1109/vast.2007.4389009", "10.1145/985692.985765", "10.1145/1656274.1656278", "10.1109/tvcg.2010.162", "10.1023/a:1024099825458", "10.1109/tvcg.2006.84", "10.1109/2945.722299", "10.1109/tvcg.2009.180", "10.1145/1345448.1345454", "10.1057/palgrave.ivs.9500061"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2015.2467991", "title": "Exploring Evolving Media Discourse Through Event Cueing", "year": "2015", "conferenceName": "VAST", "authors": "Yafeng Lu;Michael Steptoe;Sarah Burke;Hong Wang;Jiun-Yi Tsai;Hasan Davulcu;Douglas C. Montgomery;Steven R. Corman;Ross Maciejewski", "citationCount": "20", "affiliation": "Lu, YF (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Lu, Yafeng; Steptoe, Michael; Burke, Sarah; Wang, Hong; Tsai, Jiun-Yi; Davulcu, Hasan; Montgomery, Douglas; Corman, Steven R.; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA.", "countries": "USA", "abstract": "Online news, microblogs and other media documents all contain valuable insight regarding events and responses to events. Underlying these documents is the concept of framing, a process in which communicators act (consciously or unconsciously) to construct a point of view that encourages facts to be interpreted by others in a particular manner. As media discourse evolves, how topics and documents are framed can undergo change, shifting the discussion to different viewpoints or rhetoric. What causes these shifts can be difficult to determine directly; however, by linking secondary datasets and enabling visual exploration, we can enhance the hypothesis generation process. In this paper, we present a visual analytics framework for event cueing using media data. As discourse develops over time, our framework applies a time series intervention model which tests to see if the level of framing is different before or after a given date. If the model indicates that the times before and after are statistically significantly different, this cues an analyst to explore related datasets to help enhance their understanding of what (if any) events may have triggered these changes in discourse. Our framework consists of entity extraction and sentiment analysis as lenses for data exploration and uses two different models for intervention analysis. To demonstrate the usage of our framework, we present a case study on exploring potential relationships between climate change framing and conflicts in Africa.", "keywords": "Media Analysis, Time Series Analysis, Event Detection", "link": "http://dx.doi.org/10.1109/TVCG.2015.2467991", "refList": ["10.1093/biomet/34.1-2.28", "10.1109/vast.2008.4677364", "10.1145/2556288.2557228", "10.1177/1473871613493996", "10.1016/j.dss.2014.07.003", "10.1111/j.1460-2466.1993.tb01304.x", "10.1109/vast.2014.7042484", "10.1146/annurev.polisci.10.072805.103054", "10.1145/2598153.2598172", "10.1109/infvis.1999.801851", "10.3115/v1/p14-5010", "10.1109/tvcg.2013.162", "10.1109/vast.2011.6102488", "10.1109/tvcg.2010.225", "10.1109/vast.2009.5333919", "10.1109/vast.2012.6400557", "10.1109/tvcg.2012.225", "10.1109/pacificvis.2012.6183572", "10.1145/2470654.2481374", "10.1109/vast.2014.7042494", "10.1109/tvcg.2014.2346682", "10.1109/vast.2012.6400491", "10.1146/annurev.soc.26.1.611", "10.1073/pnas.1411899111", "10.1109/vast.2012.6400485", "10.1109/tvcg.2013.222", "10.1111/j.1460-2466.1999.tb02784.x", "10.1145/1541880.1541882", "10.1109/tvcg.2011.179", "10.1109/tvcg.2014.2346913", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2016.2598590", "title": "Visualizing Social Media Content with SentenTree", "year": "2016", "conferenceName": "InfoVis", "authors": "Mengdie Hu;Krist Wongsuphasawat;John T. Stasko", "citationCount": "20", "affiliation": "Hu, MD (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA. Hu, Mengdie; Stasko, John, Georgia Inst Technol, Atlanta, GA 30332 USA. Hu, Mengdie; Wongsuphasawat, Krist, Twitter Inc, San Francisco, CA USA.", "countries": "USA", "abstract": "We introduce SentenTree, a novel technique for visualizing the content of unstructured social media text. SentenTree displays frequent sentence patterns abstracted from a corpus of social media posts. The technique employs design ideas from word clouds and the Word Tree, but overcomes a number of limitations of both those visualizations. SentenTree displays a node-link diagram where nodes are words and links indicate word co-occurrence within the same sentence. The spatial arrangement of nodes gives cues to the syntactic ordering of words while the size of nodes gives cues to their frequency of occurrence. SentenTree can help people gain a rapid understanding of key concepts and opinions in a large social media text collection. It is implemented as a lightweight application that runs in the browser.", "keywords": "text visualization;social media;natural language processing;word cloud;Twitter", "link": "http://dx.doi.org/10.1109/TVCG.2016.2598590", "refList": ["10.1017/s1368980010003733", "10.1109/iv.2010.24", "10.1177/1473871613493996", "10.1109/tvcg.2013.221", "10.1111/j.1467-8659.2011.01923.x", "10.1109/pacificvis.2015.7156366", "10.1109/tvcg.2009.165", "10.1109/vl.1996.545307", "10.1007/978-3-319-07959-2\\_21", "10.1145/2207676.2207738", "10.1145/1835804.1835827", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1145/1963405.1963504", "10.1109/tvcg.2012.96", "10.1109/vast.2011.6102488", "10.1109/tvcg.2015.2467991", "10.1109/2945.981848", "10.1109/infvis.1995.528686", "10.1109/mcg.2010.102", "10.1109/tvcg.2010.194", "10.1145/1374489.1374501", "10.1109/tvcg.2009.171", "10.1145/2207676.2208672", "10.1111/j.1467-8659.2012.03107.x", "10.1109/vast.2012.6400485", "10.3366/cor.2015.0067", "10.1145/2362364.2362367", "10.1109/pacificvis.2011.5742382", "10.1007/978", "10.1109/tvcg.2010.154", "10.1109/tvcg.2006.156", "10.1007/978-3-319-07821-2\\_11", "10.1109/tvcg.2012.324", "10.1109/tvcg.2011.179", "10.1109/tvcg.2008.172"], "wos": 1, "children": [{"doi": "10.1109/vast.2017.8585638", "title": "E-Map: A Visual Analytics Approach for Exploring Significant Event Evolutions in Social Media", "year": "2017", "conferenceName": "VAST", "authors": "Siming Chen;Shuai Chen;Lijing Lin;Xiaoru Yuan;Jie Liang 0004;Xiaolong Zhang", "citationCount": "10", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing, Peoples R China. Yuan, XR (Corresponding Author), Peking Univ, Sch EECS, Beijing, Peoples R China. Chen, Siming; Chen, Shuai; Lin, Lijing; Yuan, Xiaoru, Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing, Peoples R China. Chen, Siming; Chen, Shuai; Lin, Lijing; Yuan, Xiaoru, Peking Univ, Sch EECS, Beijing, Peoples R China. Liang, Jie, Univ Technol, Fac Engn \\& Informat Technol, Sydney, NSW, Australia. Zhang, Xiaolong, Penn State Univ, Coll Informat Sci \\& Technol, University Pk, PA 16802 USA.", "countries": "USA;China;Australia", "abstract": "Significant events are often discussed and spread through social media, involving many people. Reposting activities and opinions expressed in social media offer good opportunities to understand the evolution of events. However, the dynamics of reposting activities and the diversity of user comments pose challenges to understand event-related social media data. We propose E-Map, a visual analytics approach that uses map-like visualization tools to help multi-faceted analysis of social media data on a significant event and in-depth understanding of the development of the event. E-Map transforms extracted keywords, messages, and reposting behaviors into map features such as cities, towns, and rivers to build a structured and semantic space for users to explore. It also visualizes complex posting and reposting behaviors as simple trajectories and connections that can be easily followed. By supporting multi-level spatial temporal exploration, E-Map helps to reveal the patterns of event development and key players in an event, disclosing the ways they shape and affect the development of the event. Two cases analysing real-world events confirm the capacities of E-Map in facilitating the analysis of event evolution with social media data.", "keywords": "Social Media,Event Analysis,Map-like Visual Metaphor,Spatial Temporal Visual Analytics", "link": "http://dx.doi.org/10.1109/VAST.2017.8585638", "refList": ["10.1109/tvcg.2007.70582", "10.1109/tvcg.2016.2598919", "10.1109/pacificvis.2010.5429590", "10.1057/ivs.2008.23", "10.1109/vast.2014.7042496", "10.1016/j.cag.2013.11.003", "10.1109/vast.2011.6102456", "10.1109/tvcg.2013.221", "10.1109/tvcg.2011.185", "10.1109/vast.2016.7883510", "10.1111/j.1467-8659.2012.03120.x", "10.1109/tvcg.2013.186", "10.1109/tmm.2016.2614220", "10.7155/jgaa.00302", "10.1109/mc.2012.430", "10.1109/tvcg.2015.2467619", "10.1109/tvcg.2010.129", "10.1016/s0341-8162(01)00164-3", "10.1109/tvcg.2016.2598590", "10.1145/2065023.2065041", "10.1111/cgf.13211", "10.1109/tvcg.2013.162", "10.1109/tvcg.2011.288", "10.1145/1963405.1963504", "10.5670/oceanog.2016.66", "10.1109/tvcg.2015.2467554", "10.1109/tvcg.2015.2467691", "10.1016/j.cag.2013.10.008", "10.1109/mcg.2015.73", "10.1109/vast.2012.6400557", "10.1109/tvcg.2016.2539960", "10.1109/tit.1982.1056489", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2014.2346922", "10.1109/tmm.2014.2384912", "10.1109/vast.2016.7883511", "10.1002/spe.4380211102", "10.1145/2488388.2488504", "10.2312/eurovisstar.20141176", "10.1109/bigdata.2013.6691714", "10.1109/tvcg.2009.171", "10.1109/tvcg.2013.196", "10.1109/vast.2008.4677356", "10.1145/2207676.2208672", "10.1111/j.1467-8659.2011.01955.x", "10.1109/pacificvis.2014.38", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2015.7156376", "10.1145/1348549.1348556", "10.1109/vast.2015.7347632", "10.1109/tvcg.2014.2346919", "10.1109/tvcg.2014.2346433"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934263", "title": "R-Map: A Map Metaphor for Visualizing Information Reposting Process in Social Media", "year": "2019", "conferenceName": "VAST", "authors": "Shuai Chen;Sihang Li;Siming Chen;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Chen, Shuai; Li, Sihang, Peking Univ, Sch EECS, Minist Educ, Key Lab Machine Petrept, Beijing, Peoples R China. Yuan, Xiaoru, Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Chen, Siming, Fraunhofer Inst IAIS, St Augustin, Germany. Chen, Siming, Univ Bonn, Bonn, Germany.", "countries": "Germany;China", "abstract": "We propose R-Map (Reposting Map), a visual analytical approach with a map metaphor to support interactive exploration and analysis of the information reposting process in social media. A single original social media post can cause large cascades of repostings (i.e., retweets) on online networks, involving thousands, even millions of people with different opinions. Such reposting behaviors form the reposting tree, in which a node represents a message and a link represents the reposting relation. In R-Map, the reposting tree structure can be spatialized with highlighted key players and tiled nodes. The important reposting behaviors, the following relations and the semantics relations are represented as rivers, routes and bridges, respectively, in a virtual geographical space. R-Map supports a scalable overview of a large number of information repostings with semantics. Additional interactions on the map are provided to support the investigation of temporal patterns and user behaviors in the information diffusion process. We evaluate the usability and effectiveness of our system with two use cases and a formal user study.", "keywords": "Social Media,Information Diffusion,Map-like Visual Metaphor", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934263", "refList": ["10.1109/infvis.2000.885091", "10.1109/pacificvis.2010.5429590", "10.1111/j.0020-2754.1998.00269.x", "10.1109/vast.2017.8585638", "10.1145/2700398", "10.1109/tmm.2016.2614229", "10.1109/visual.1991.175815", "10.1145/3183347", "10.1109/infvis.2001.963290", "10.1109/vast.2016.7883510", "10.1109/access.2016.2605009", "10.1109/mcg.2011.103", "10.1145/1124772.1124851", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2010.79", "10.1109/infvis.2000.885095", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1002/(sici)1097-0266(199606)17:6", "10.5670/oceanog.2016.66", "10.1559/152304003100011081", "10.1109/tit.1982.1056489", "10.1559/152304098782383034", "10.1109/tvcg.2014.2346922", "10.1007/978-3-540-85567-5\\_9", "10.1145/2488388.2488504", "10.1109/38.974518", "10.1109/bigdata.2013.6691714", "10.1109/pacificvis.2014.38", "10.1109/tvcg.2012.291", "10.1109/infvis.2005.1532128", "10.2307/2685881", "10.1007/1-4020-4179-9\\_91", "10.1109/infvis.1999.801860", "10.1109/asonam.2011.37"], "wos": 1, "children": [{"doi": "10.1111/cgf.14031", "year": "2020", "title": "The State of the Art in Map-Like Visualization", "conferenceName": "EuroVis", "authors": "Marius Hogr{\\\"{a}}fer;Magnus Heitzler;Hans{-}J{\\\"{o}}rg Schulz", "citationCount": "0", "affiliation": "Hografer, M (Corresponding Author), Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHografer, Marius; Schulz, Hans-Jorg, Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHeitzler, Magnus, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Denmark", "abstract": "Cartographic maps have been shown to provide cognitive benefits when interpreting data in relation to a geographic location. In visualization, the term map-like describes techniques that incorporate characteristics of cartographic maps in their representation of abstract data. However, the field of map-like visualization is vast and currently lacks a clear classification of the existing techniques. Moreover, choosing the right technique to support a particular visualization task is further complicated, as techniques are scattered across different domains, with each considering different characteristics as map-like. In this paper, we give an overview of the literature on map-like visualization and provide a hierarchical classification of existing techniques along two general perspectives: imitation and schematization of cartographic maps. Each perspective is further divided into four principal categories that group common map-like techniques along the visual primitives they affect. We further discuss this classification from a task-centered view and highlight open research questions.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14031", "refList": ["10.1111/j.0020-2754.1998.00269.x.21", "10.1109/iv.2005.26", "10.2307/2980460", "10.1109/tvcg.2017.2743959", "10.1016/j.jvlc.2011.11.004", "10.1145/2501988.2502046", "10.1559/1523040042742402", "10.1007/s00779-011-0500-3", "10.1111/cgf.12932", "10.1109/tvcg.2010.89", "10.1111/cgf.13200", "10.1080/15230406.2016.1160797", "10.1179/000870410x12825500202896", "10.1109/tvcg.2007.70596", "10.5167/80972.19uzh-80972", "10.1007/978-3-319-11593-1\\_2", "10.1037/aca0000175", "10.1080/17538947.2014.923942", "10.1109/tvcg.2015.2467321", "10.1109/vissof.2005.1684299", "10.1559/152304098782383034", "10.1007/978-3-642-36763-2\\_38", "10.1109/tvcg.2013.130", "10.1109/tvcg.2014.2346274", "10.1111/cgf.12648", "10.1111/0004-5608.00242", "10.1177/1473871617724212", "10.1109/iv.2004.1320123", "10.1371/journal.pcbi.1006907", "10.1109/tvcg.2016.2599030", "10.1559/1523040042742411", "10.3390/informatics5030031", "10.1111/cgf.13079", "10.1109/tvcg.2015.2467811", "10.1016/j.tics.2003.12.004", "10.1007/s00799-016-0168-4", "10.1109/infvis.2004.57", "10.1007/s10021-007-9038-7", "10.1109/pacificvis.2015.7156366", "10.1007/978-3-319-27261-0\\_1", "10.1007/978-1-4471-2804-5\\_6", "10.1109/access.2019.2939977", "10.1109/tvcg.2017.2747545", "10.1109/icdm.2003.1250978", "10.1111/cgf.13167", "10.1016/0010-0285(78)90006-3", "10.3138/nj8v-8514-871t-221k", "10.1016/j.cag.2009.06.002", "10.1111/j.1467-8659.2011.01937.x", "10.1016/b978-044451020-4/50035-9", "10.1111/1467-8659.00566", "10.5220/0006618101080119", "10.1109/pacificvis.2012.6183571", "10.1145/2038558.2038579", "10.1109/tvcg.2010.191", "10.1111/j.0033-0124.1985.00075.x", "10.1109/tvcg.2004.1260761", "10.1007/978-3-642-34848-8\\_6", "10.1145/2968220.2968239", "10.1145/3002151.3002160", "10.1109/tst.2013.6509098", "10.1109/tvcg.2013.91", "10.1177/1473871615597077", "10.1016/j.jvlc.2011.02.001", "10.1080/17445647.2014.935502", "10.1177/1687814017740710", "10.1111/1744-7917.12601", "10.1073/pnas.0400280101", "10.1111/cgf.13672", "10.1109/mcg.2006.90", "10.1002/asi.21712", "10.1007/978-3-642-33024-7\\_3", "10.1145/2801040.2801056", "10.1109/mcg.2010.101", "10.1179/003962607x165041", "10.1057/palgrave.ivs.9500039", "10.1109/vast.2016.7883510", "10.1559/152304009788988288", "10.1080/23729333.2017.1301346", "10.1109/tvcg.2013.66", "10.1111/j.1467-8659.2012.03085.x", "10.1109/tvcg.2011.288", "10.3390/ijgi9040253", "10.1109/infvis.2005.1532150", "10.1145/2254556.2254636", "10.20382//jocg.v4i1a9", "10.1016/0010-0285(81)90016-5", "10.1145/2556288.2557224", "10.1109/iv.2001.942043", "10.1021/ed1000203", "10.1016/0169-7439(87)80084-9", "10.1109/tvcg.2010.154", "10.1016/j.jvlc.2015.10.003", "10.1109/tvcg.2019.2903945", "10.1109/tvcg.2013.120", "10.1109/tvcg.2019.2934263", "10.1146/annurev-ecolsys-102209-144718", "10.1109/tvcg.2008.165", "10.3138/a477-3202-7876-n514", "10.1080/23729333.2017.1288535", "10.1111/j.0020-2754.1998.00269.x", "10.1109/mcg.2004.41", "10.1109/vast.2009.5332593", "10.1002/smr.414", "10.1007/s12650-019-00584-3", "10.1145/22949.22950", "10.1179/1743277413y.0000000036", "10.1080/15230406.2016.1262280", "10.1016/s0341-8162(01)00164-3", "10.22224/gistbok/2017.3.8", "10.1109/tvcg.2008.155", "10.1057/ivs.2008.31", "10.1016/j.cag.2004.03.012", "10.1179/1743277412y.0000000007", "10.1016/j.soncn.2011.02.001", "10.1179/caj.1987.24.1.27", "10.3138/carto.48.3.1691", "10.5220/0004267205150524", "10.1109/38.974518", "10.1145/102377.115768", "10.1057/palgrave.ivs.9500186", "10.1109/5.58325", "10.5167/80972.uzh-80972", "10.1177/030913339602000204", "10.1007/978-3-642-22300-6\\_14", "10.1109/iv.2004.1320189", "10.1111/cgf.13447", "10.1007/s11192-017-2596-3", "10.1559/1523040053722150", "10.1007/978-3-662-45803-7\\_34"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1111/cgf.13446", "year": "2018", "title": "Quality Metrics for Information Visualization", "conferenceName": "EuroVis", "authors": "Michael Behrisch;Michael Blumenschein;Nam Wook Kim;Lin Shao;Mennatallah El{-}Assady;Johannes Fuchs;Daniel Seebacher;Alexandra Diehl;Ulrik Brandes;Hanspeter Pfister;Tobias Schreck;Daniel Weiskopf;Daniel A. Keim", "citationCount": "25", "affiliation": "Behrisch, M (Corresponding Author), Harvard Univ, Cambridge, MA 02138 USA.\nBehrisch, M.; Kim, N. W.; Pfister, H., Harvard Univ, Cambridge, MA 02138 USA.\nBlumenschein, M.; El-Assady, M.; Fuchs, J.; Seebacher, D.; Diehl, A.; Keim, D. A., Univ Konstanz, Constance, Germany.\nShao, L.; Schreck, T., Graz Univ Technol, Graz, Austria.\nBrandes, U., Swiss Fed Inst Technol, Zurich, Switzerland.\nWeiskopf, D., Univ Stuttgart, Stuttgart, Germany.", "countries": "Switzerland;Germany;USA;Austria", "abstract": "The visualization community has developed to date many intuitions and understandings of how to judge the quality of views in visualizing data. The computation of a visualization's quality and usefulness ranges from measuring clutter and overlap, up to the existence and perception of specific (visual) patterns. This survey attempts to report, categorize and unify the diverse understandings and aims to establish a common vocabulary that will enable a wide audience to understand their differences and subtleties. For this purpose, we present a commonly applicable quality metric formalization that should detail and relate all constituting parts of a quality metric. We organize our corpus of reviewed research papers along the data types established in the information visualization community: multi- and high-dimensional, relational, sequential, geospatial and text data. For each data type, we select the visualization subdomains in which quality metrics are an active research field and report their findings, reason on the underlying concepts, describe goals and outline the constraints and requirements. One central goal of this survey is to provide guidance on future research opportunities for the field and outline how different visualization communities could benefit from each other by applying or transferring knowledge to their respective subdomain. Additionally, we aim to motivate the visualization community to compare computed measures to the perception of humans.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13446", "refList": ["10.2307/276978", "10.1057/ivs.2009.10", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2014.2346420", "10.1016/j.cag.2018.01.010", "10.1109/tvcg.2017.2743959", "10.1109/tvcg.2011.127", "10.1109/tvcg.2015.2467759", "10.1109/tvcg.2013.187", "10.1109/tvcg.2007.70594", "10.1186/1471-2105-9-155", "10.1145/2702123.2702545", "10.1145/1645953.1646023", "10.1093/bioinformatics/btm312", "10.1109/pacificvis.2016.7465245", "10.1145/3038462.3038463", "10.1007/978-3-540-70956-5", "10.1109/tvcg.2017.2743939", "10.1111/cgf.12932", "10.1111/j.1467-8659.2012.03106.x", "10.1145/1133265.1133318", "10.1109/tvcg.2010.186", "10.1109/iv.2013.101", "10.1109/tvcg.2016.2598590", "10.2312/conf/eg2013/stars/039-063", "10.3758/bf03201236", "10.1111/cgf.12935", "10.1145/1056808.1056914", "10.1007/bf01898350", "10.2307/1390686", "10.1109/2945.981848", "10.1109/hicss.2008.422", "10.1109/tvcg.2011.201", "10.1109/pacificvis.2011.5742390", "10.2307/2288400", "10.1111/cgf.12872", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2006.163", "10.1109/tvcg.2013.150", "10.1109/tvcg.2009.171", "10.1109/tvcg.2017.2723397", "10.1117/12.2079841", "10.1109/pacificvis.2009.4906838", "10.1109/infvis.2004.1", "10.1109/tvcg.2008.166", "10.1287/opre.20.5.993", "10.7155/jgaa.00370", "10.1111/j.1467-8659.2008.01240.x", "10.1109/inf0vis.2005.14", "10.1109/tvcg.2009.23", "10.1109/infvis.1998.729559", "10.1177/0165551506078083", "10.1109/t-c.1974.224051", "10.1109/tvcg.2017.2744339", "10.1109/tvcg.2014.2346433", "10.1145/989863.989940", "10.1109/tvcg.2015.2509990", "10.2307/2288843", "10.1016/j.neucom.2017.01.105", "10.1073/pnas.43.10.923", "10.1111/cgf.12647", "10.1016/j.rse.2017.06.031", "10.1109/infvis.2005.1532145", "10.1109/2945.841121", "10.1109/tvcg.2011.229", "10.2312/eurova.20141140", "10.1111/j.2044-8317.1974.tb00534.x", "10.1109/pacificvis.2015.7156366", "10.1109/vast.2009.5332628", "10.1109/vast.2014.7042480", "10.1145/2993901.2993907", "10.1177/0951692899011001004", "10.1007/978-3-642-03655-2\\_43", "10.1198/106186008x320465", "10.1109/visual.1997.663916", "10.1111/cgf.12125", "10.1109/infvis.2003.1249009", "10.1109/icdm.2003.1250978", "10.1109/tvcg.2017.2745919", "10.1109/tvcg.2012.128", "10.1109/vast.2010.5652450", "10.1109/tvcg.2006.161", "10.1145/571647.571649", "10.1109/tvcg.2011.193", "10.1111/cgf.12633", "10.1109/tvcg.2017.2674978", "10.1109/vast.2010.5652433", "10.1109/tvcg.2017.2745978", "10.1515/itit-2014-1070", "10.1145/1374489.1374501", "10.1109/tvcg.2009.153", "10.2312/eurovisshort.20171128", "10.2312/eurovisshort.20151130", "10.1016/0925-7721(94)00014-x", "10.1057/palgrave.ivs.95000/3", "10.1145/302979.303030", "10.1002/widm.1071", "10.1109/pacificvis.2014.42", "10.1109/tvcg.2016.2598467", "10.1111/j.1467-8659.2009.01667.x", "10.2307/2287708", "10.1177/1473871613477091", "10.1145/967900.968153", "10.1109/tvcg.2010.162", "10.1145/568522.568523", "10.1016/j.cag.2004.03.022", "10.1109/tvcg.2015.2466992", "10.1145/2993901.2993903", "10.1147/jrd.2015.2411412", "10.1177/154193120504900508", "10.1109/tvcg.2006.138", "10.1111/cgf.13168", "10.1111/cgf.12380", "10.2312/conf/eg2013/stars/095-116", "10.1109/tvcg.2017.2745859", "10.1109/tvcg.2017.2701829", "10.1109/tvcg.2007.70529", "10.1007/bf01199431", "10.1117/12.697548", "10.1109/tvcg.2017.2653106", "10.1207/s15327906mbr2701\\_4", "10.1109/vl.1996.545307", "10.1109/tvcg.2017.2745140", "10.1111/j.1467-8659.2012.03069.x", "10.1111/j.1467-8659.2011.01961.x", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.124", "10.1057/ivs.2008.13", "10.2312/vissym/eurovis06/195-202", "10.2312/vissym/eurovis07/163-170", "10.1109/tvcg.2015.2467324", "10.1109/tvcg.2011.167", "10.1109/pacificvis.2014.40", "10.1109/pacificvis.2012.6183570", "10.1111/j.1467-8659.2012.03125.x", "10.1075/idj.20.1.02bei", "10.1111/1467-8306.00061", "10.1145/2858036.2858155", "10.1109/tvcg.2012.108", "10.2312/vmv.20171261", "10.1109/tvcg.2015.2467531", "10.1145/1379092.1379130", "10.1109/tpami.1979.4766909", "10.1080/13875860903039172", "10.1109/mcg.2006.70", "10.1109/tvcg.2010.242", "10.1109/vast.2007.4389004", "10.1109/tvcg.2015.2467191", "10.2312/cgvc.20171276", "10.1016/0169-7439(87)80084-9", "10.2312/pe/eurovisshort/eurovisshort2012/097-101", "10.1016/j.neucom.2014.07.073", "10.2307/2685263", "10.1145/996546.996554", "10.1111/j.1467-8659.2009.01666.x", "10.1117/12.2083444", "10.1109/tvcg.2014.2346572", "10.1007/978-0-387-39940-9\\_262", "10.1007/bf00988593", "10.1145/1054972.1055078", "10.1111/j.1467-8659.2009.01467.x", "10.1111/cgf.13181", "10.1145/502512.502530", "10.1109/pacificvis.2010.5429600", "10.1111/j.1467-8659.2011.01923.x", "10.1109/mcg.2004.41", "10.1109/tvcg.2015.2467971", "10.1111/j.1467-8659.2011.01919.x", "10.1002/sam.10071", "10.1109/tvcg.2010.184", "10.1007/978-3-642-27848-8\\_648-1", "10.1109/tvcg.2010.132", "10.1145/22949.22950", "10.1145/1842993.1843002", "10.1109/infvis.2000.885096", "10.1016/j.ins.2015.04.017", "10.1109/iv.2009.43", "10.1016/j.visinf.2017.11.001", "10.1109/tvcg.2011.239", "10.1109/infovis.2005.40", "10.1111/cgf.12641", "10.1109/pacificvis.2016.7465262", "10.1145/2470654.2466443", "10.1007/s10844-011-0157-4", "10.1111/cgf.12919", "10.1016/j.jvlc.2016.07.003", "10.1109/tvcg.2016.2549018", "10.1057/palgrave.ivs.9500166", "10.3406/colan.1981.1409.1", "10.1093/bioinformatics/bti141", "10.1111/j.1467-8306.2004.09401004.x", "10.1145/1168149.1168168", "10.1109/tvcg.2017.2744184", "10.1117/12.2079420", "10.1109/iv.2005.62", "10.1145/102377.115768", "10.1109/tvcg.2014.2346677", "10.1109/tvcg.2014.2346426", "10.1111/j.1467-8659.2012.03107.x", "10.5220/0006097400400051", "10.2991/978-94-6239-186-4", "10.2307/2284077", "10.1109/iv.2008.89", "10.1016/j.jvlc.2015.12.001", "10.1109/pacificvis.2013.6596147", "10.1145/1242572.1242826", "10.1016/j.cag.2007.01.030", "10.1111/cgf.12632", "10.2312/eurovisstar.20151113", "10.1559/152304009788188808"], "wos": 1, "children": [{"doi": "10.1109/vast.2018.8802486", "title": "SMARTexplore: Simplifying High-Dimensional Data Analysis through a Table-Based Visual Analytics Approach", "year": "2018", "conferenceName": "VAST", "authors": "Michael Blumenschein;Michael Behrisch;Stefanie Schmid;Simon Butscher;Deborah Wahl;Karoline Villinger;Britta Renner;Harald Reiterer;Daniel A. Keim", "citationCount": "0", "affiliation": "Blumenschein, M (Corresponding Author), Univ Konstanz, Constance, Germany. Blumenschein, Michael; Schmid, Stefanie; Butscher, Simon; Wahl, Deborah R.; Villinger, Karoline; Renner, Britta; Reiterer, Harald; Keim, Daniel A., Univ Konstanz, Constance, Germany. Behrisch, Michael, Harvard Univ, Cambridge, MA 02138 USA.", "countries": "Germany;USA", "abstract": "We present SMARTEXPLORE, a novel visual analytics technique that simplifies the identification and understanding of clusters, correlations, and complex patterns in high-dimensional data. The analysis is integrated into an interactive table-based visualization that maintains a consistent and familiar representation throughout the analysis. The visualization is tightly coupled with pattern matching, subspace analysis, reordering, and layout algorithms. To increase the analyst's trust in the revealed patterns, SMARTEXPLORE automatically selects and computes statistical measures based on dimension and data properties. While existing approaches to analyzing high-dimensional data (e.g., planar projections and Parallel coordinates) have proven effective, they typically have steep learning curves for non-visualization experts. Our evaluation, based on three expert case studies, confirms that non-visualization experts successfully reveal patterns in high-dimensional data when using SMARTEXPLORE.", "keywords": "High-dimensional data,visual exploration,pattern-driven analysis,tabular visualization,subspace,aggregation", "link": "http://dx.doi.org/10.1109/VAST.2018.8802486", "refList": ["10.1177/1473871612460526", "10.1109/tvcg.2015.2489649", "10.1111/j.1467-8659.2008.01241.x", "10.1007/978-3-319-25087-8\\_29", "10.1007/b98835", "10.13140/rg.2.2.16570.90567", "10.1109/tvcg.2014.2346260", "10.1109/vast.2009.5332628", "10.2307/2528823", "10.1109/tvcg.2010.184", "10.1145/1133265.1133318", "10.1057/palgrave.ivs.9500072", "10.1111/j.1467-8659.2012.03110.x", "10.1145/1007730.1007731", "10.1057/palgrave.ivs.9500086", "10.1111/cgf.12935", "10.1109/tvcg.2014.2346279", "10.1007/bf01898350", "10.1109/tvcg.2013.173", "10.1109/tvcg.2017.2672987", "10.1109/tvcg.2014.2346248", "10.1109/infvis.2004.46", "10.1109/vast.2010.5652433", "10.1109/vast.2009.5332611", "10.1109/tvcg.2015.2467553", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2013.150", "10.1109/tvcg.2016.2640960", "10.1111/cgf.12630", "10.1007/978-1-4757-1904-8", "10.1109/tvcg.2011.188", "10.1109/tvcg.2010.138", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2017.2743978", "10.1111/cgf.12879", "10.1109/iv.2008.33", "10.1111/cgf.13446", "10.1007/s00371-018-1483-0", "10.1109/infvis.1998.729559", "10.1145/2669557.2669572", "10.1111/j.1467-8659.2008.01239.x"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934541", "title": "A Recursive Subdivision Technique for Sampling Multi-class Scatterplots", "year": "2019", "conferenceName": "InfoVis", "authors": "Xin Chen;Tong Ge;Jian Zhang 0070;Baoquan Chen;Chi-Wing Fu;Oliver Deussen;Yunhai Wang", "citationCount": "5", "affiliation": "Chen, X (Corresponding Author), Shandong Univ, Jinan, Shandong, Peoples R China. Chen, Xin; Ge, Tong; Wang, Yunhai, Shandong Univ, Jinan, Shandong, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China. Fu, Chi-Wing, SIAT, Guangdong Prov Key Lab CV \\& VR Tech, Shenzhen, Guangdong, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Deussen, Oliver, Konstanz Univ, Constance, Germany. Deussen, Oliver, SIAT, Shenzhen VIsuCA Key Lab, Shenzhen, Guangdong, Peoples R China.", "countries": "Germany;China", "abstract": "We present a non-uniform recursive sampling technique for multi-class scatterplots, with the specific goal of faithfully presenting relative data and class densities, while preserving major outliers in the plots. Our technique is based on a customized binary kd-tree, in which leaf nodes are created by recursively subdividing the underlying multi-class density map. By backtracking, we merge leaf nodes until they encompass points of all classes for our subsequently applied outlier-aware multi-class sampling strategy. A quantitative evaluation shows that our approach can better preserve outliers and at the same time relative densities in multi-class scatterplots compared to the previous approaches, several case studies demonstrate the effectiveness of our approach in exploring complex and real world data.", "keywords": "Scatterplot,multi-class sampling,kd-tree,outlier,relative density", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934541", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2006.170", "10.1109/tvcg.2008.119", "10.1109/tvcg.2007.70580", "10.2307/2289444", "10.1145/1964921.1964943", "10.1109/tvcg.2013.65", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/itoec.2018.8740621", "10.1145/1778765.1778816", "10.1109/tvcg.2010.197", "10.1201/b17511", "10.1109/tvcg.2018.2864912", "10.1007/0-387-28695-0", "10.1109/5.726791", "10.1109/visual.1998.745301", "10.1016/j.physa.2011.12.004", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2004.1272729", "10.1145/335191.335388", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1109/iv.2002.1028760", "10.1145/1842993.1842999", "10.1109/tvcg.2018.2869149", "10.1038/nmeth.2490", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2010.176", "10.1145/2702123.2702585", "10.1057/ivs.2009.34"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14001", "year": "2020", "title": "Sunspot Plots: Model-based Structure Enhancement for Dense Scatter Plots", "conferenceName": "EuroVis", "authors": "Thomas Trautner;Fabian Bolte;Sergej Stoppel;Stefan Bruckner", "citationCount": "0", "affiliation": "Trautner, T (Corresponding Author), Univ Bergen, Bergen, Norway.\nTrautner, T.; Bolte, F.; Stoppel, S.; Bruckner, S., Univ Bergen, Bergen, Norway.", "countries": "Norway", "abstract": "Scatter plots are a powerful and well-established technique for visualizing the relationships between two variables as a collection of discrete points. However, especially when dealing with large and dense data, scatter plots often exhibit problems such as overplotting, making the data interpretation arduous. Density plots are able to overcome these limitations in highly populated regions, but fail to provide accurate information of individual data points. This is particularly problematic in sparse regions where the density estimate may not provide a good representation of the underlying data. In this paper, we present sunspot plots, a visualization technique that communicates dense data as a continuous data distribution, while preserving the discrete nature of data samples in sparsely populated areas. We furthermore demonstrate the advantages of our approach on typical failure cases of scatter plots within synthetic and real-world data sets and validate its effectiveness in a user study.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14001", "refList": ["10.1057/palgrave.ivs.9500122", "10.2312/eggh/hpg12/097-103", "10.1109/tvcg.2008.119", "10.1109/pacificvis.2011.5742387", "10.1038/331163a0", "10.1109/visual.2019.8933620", "10.2307/2683294", "10.1201/9781351072304", "10.2307/2289444", "10.1109/tvcg.2019.2934541", "10.1080/00949657508810123", "10.1109/tvcg.2013.65", "10.1109/infvis.1997.636789", "10.1109/2945.841121", "10.1109/mcse.2007.55", "10.1109/tvcg.2010.197", "10.1111/cgf.12871", "10.1109/infvis.2003.1249018", "10.1145/3173574.3173991", "10.1109/tvcg.2012.238", "10.1007/0-387-28695-0", "10.1109/pacificvis.2010.5429604", "10.18637/jss.v008.i03", "10.1109/tvcg.2007.70596", "10.1109/visual.1998.745301", "10.1007/0-387-37977-0\\_3", "10.1145/1556262.1556289", "10.2312/wiced.20161094", "10.1109/tvcg.2007.70535", "10.1145/1056808.1056914", "10.1109/tvcg.2003.1196007", "10.1109/iv.2004.1320190", "10.1111/cgf.12877", "10.1162/leon.2007.40.2.202a", "10.1109/tvcg.2017.2674978", "10.1057/ivs.2010.4", "10.1016/j.cag.2018.02.008", "10.1201/9781315140919", "10.1109/visual.2000.885677", "10.1002/jhbs.20078", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1109/visual.2001.964495", "10.1109/iv.2002.1028760", "10.1111/cgf.13684", "10.1109/hicss.2013.197", "10.1109/tvcg.2019.2903956", "10.1093/mnras/stt961", "10.1109/tvcg.2017.2668409", "10.1111/j.1467-8659.2009.01478.x", "10.1145/2702123.2702585", "10.2307/1418003", "10.2307/1390742", "10.1145/360825.360839", "10.1109/pacificvis.2009.4906843", "10.1057/ivs.2009.34", "10.2307/2288711"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934284", "title": "Color Crafting: Automating the Construction of Designer Quality Color Ramps", "year": "2019", "conferenceName": "InfoVis", "authors": "Stephen Smart;Keke Wu;Danielle Albers Szafir", "citationCount": "3", "affiliation": "Smart, S (Corresponding Author), Univ Colorado, Boulder, CO 80309 USA. Smart, Stephen; Wu, Keke; Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Visualizations often encode numeric data using sequential and diverging color ramps. Effective ramps use colors that are sufficiently discriminable, align well with the data, and are aesthetically pleasing. Designers rely on years of experience to create high-quality color ramps. However, it is challenging for novice visualization developers that lack this experience to craft effective ramps as most guidelines for constructing ramps are loosely defined qualitative heuristics that are often difficult to apply. Our goal is to enable visualization developers to readily create effective color encodings using a single seed color. We do this using an algorithmic approach that models designer practices by analyzing patterns in the structure of designer-crafted color ramps. We construct these models from a corpus of 222 expert-designed color ramps, and use the results to automatically generate ramps that mimic designer practices. We evaluate our approach through an empirical study comparing the outputs of our approach with designer-crafted color ramps. Our models produce ramps that support accurate and aesthetically pleasing visualizations at least as well as designer ramps and that outperform conventional mathematical approaches.", "keywords": "Visualization,Aesthetics in Visualization,Color Perception,Visual Design,Design Mining", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934284", "refList": ["10.1145/3009924", "10.1109/tvcg.2015.2489649", "10.1109/tvcg.2017.2744359", "10.1002/(sici)1098-1098(199622)7:2", "10.1016/s0734-189x(83)80046-2", "10.1109/tvcg.2016.2599106", "10.1364/josaa.29.000313", "10.1109/tvcg.2016.2598918", "10.2307/2683294", "10.1109/tvcg.2017.2653106", "10.1016/j.ijhcs.2010.05.006", "10.1016/0146-664x(81)90006-x", "10.1007/978-3-642-10520-3\\_9", "10.1109/38.135886", "10.1146/annurev-psych-120710-100504", "10.1016/j.csda.2008.11.033", "10.1145/2461912.2461988", "10.1016/j.cub.2007.06.022", "10.1016/s0146-664x(79)80040-4", "10.1109/mcg.2004.1297012", "10.1109/iv.2009.94", "10.1109/tpami.2010.184", "10.3758/s13414-010-0027-0", "10.1145/22949.22950", "10.1109/visual.1995.480803", "10.1109/tvcg.2014.2346277", "10.2307/2684111", "10.1109/tvcg.2018.2865240", "10.1111/cgf.12633", "10.1109/38.7760", "10.1016/j.jspi.2015.04.007", "10.1109/iv.2008.24", "10.1145/2939502.2939506", "10.1111/cgf.12127", "10.1016/j.cag.2010.11.015", "10.1145/3025453.3026041", "10.1109/tvcg.2012.315", "10.2307/2288400", "10.1511/2005.5.436", "10.1016/s0097-8493(96)00072-6", "10.1109/mcg.2018.011461525", "10.1145/2470654.2466420", "10.1109/tvcg.2012.279", "10.1109/tvcg.2014.2346978", "10.1109/tvcg.2017.2743978", "10.1145/3406601.3406602", "10.1117/12.2084548", "10.1109/tvcg.2018.2865147", "10.1109/tvcg.2015.2467191", "10.1111/cgf.13446", "10.1109/tvcg.2017.2744320", "10.1111/j.1467-8659.2008.01203.x", "10.1145/2702613.2702975", "10.1007/978-3-319-26633-6\\_13", "10.1145/2207676.2208547", "10.1109/tvcg.2008.174", "10.1179/000870403235002042"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028891", "title": "A Structured Review of Data Management Technology for Interactive Visualization and Analysis", "year": "2020", "conferenceName": "InfoVis", "authors": "Leilani Battle;Carlos Scheidegger", "citationCount": "0", "affiliation": "Battle, L (Corresponding Author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA. Battle, Leilani, Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA. Scheidegger, Carlos, Univ Arizona, Dept Comp Sci, HDC Lab, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "In the last two decades, interactive visualization and analysis have become a central tool in data-driven decision making. Concurrently to the contributions in data visualization, research in data management has produced technology that directly benefits interactive analysis. Here, we contribute a systematic review of 30 years of work in this adjacent field, and highlight techniques and principles we believe to be underappreciated in visualization work. We structure our review along two axes. First, we use task taxonomies from the visualization literature to structure the space of interactions in usual systems. Second, we created a categorization of data management work that strikes a balance between specificity and generality. Concretely, we contribute a characterization of 131 research papers along these two axes. We find that five notions in data management venues fit interactive visualization systems well: materialized views, approximate query processing, user modeling and query prediction, muiti-query optimization, lineage techniques, and indexing techniques. In addition, we find a preponderance of work in materialized views and approximate query processing, most targeting a limited subset of the interaction tasks in the taxonomy we used. This suggests natural avenues of future research both in visualization and data management. Our categorization both changes how we visualization researchers design and build our systems, and highlights where future work is necessary.", "keywords": "", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028891", "refList": ["10.1109/tvcg.2012.233", "10.1016/s0022-5371(74)80015-0", "10.1109/tvcg.2017.2744359", "10.1037/0096-3445.136.4.623", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2012.196", "10.1037/a0029856", "10.1109/tvcg.2014.2346979", "10.1037/h0030300", "10.1109/tvcg.2016.2598918", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2018.2864909", "10.1111/cgf.13079", "10.3389/fpsyg.2012.00355", "10.1145/2858036.2858465", "10.1080/01621459.1989.10478821", "10.1037/0278-7393.24.3.732", "10.1109/tvcg.2011.127", "10.1145/2858036.2858063", "10.4249/scholarpedia.3325", "10.4324/9781410611949", "10.1111/cgf.13444", "10.1145/2993901.2993909", "10.1037/0033-295x.96.2.267", "10.1006/ijhc.1017", "10.1086/405615", "10.1109/tvcg.2019.2934801", "10.1038/17953", "10.1037/xhp0000314", "10.1109/tvcg.2019.2934400", "10.1145/2470654.2470723", "10.1037/0096-1523.16.2.332", "10.1167/16.5.11", "10.3758/s13423-016-1174-7", "10.3758/bf03207704", "10.1146/annurev.psych.55.090902.141415", "10.2307/2288400", "10.3758/bf03204258", "10.1109/tvcg.2011.279", "10.1109/vissoft.2014.36", "10.3758/s13423-011-0055-3", "10.1145/3025453.3025922", "10.1109/tvcg.2019.2934284", "10.3758/bf03210498", "10.3758/bf03200774", "10.2307/1419876", "10.1038/s41562-017-0058", "10.1109/tvcg.2010.237", "10.1109/pacificvis.2012.6183556", "10.1109/infvis.1997.636792", "10.1093/acprof:oso/9780198523192.003.0005", "10.1073/pnas.1117465109", "10.1109/tvcg.2013.234", "10.1038/nn.3655", "10.1111/cgf.12379", "10.1146/annurev-psych-010416-044232", "10.1111/cgf.13695", "10.1037/0033-295x.107.3.500", "10.1109/tvcg.2013.183", "10.1146/annurev.psych.53.100901.135125", "10.1037//0022-3514.79.6.995", "10.1559/152304003100010929", "10.1109/tvcg.2018.2865147", "10.1037/0096-1523.18.3.849", "10.1111/j.1467-8659.2009.01694.x", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934799", "title": "Data Sampling in Multi-view and Multi-class Scatterplots via Set Cover Optimization", "year": "2019", "conferenceName": "InfoVis", "authors": "Ruizhen Hu;Tingkai Sha;Oliver van Kaick;Oliver Deussen;Hui Huang 0004", "citationCount": "4", "affiliation": "Hu, RZ (Corresponding Author), Shenzhen Univ, Visual Comp Res Ctr, Shenzhen, Guangdong, Peoples R China. Hu, Ruizhen; Sha, Tingkai; Huang, Hui, Shenzhen Univ, Visual Comp Res Ctr, Shenzhen, Guangdong, Peoples R China. van Kaick, Oliver, Carleton Univ, Sch Comp Sci, Ottawa, ON, Canada. Deussen, Oliver, Konstanz Univ, Constance, Germany. Deussen, Oliver, SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Guangdong, Peoples R China.", "countries": "Canada;Germany;China", "abstract": "We present a method for data sampling in scatterplots by jointly optimizing point selection for different views or classes. Our method uses space-filling curves (Z-order curves) that partition a point set into subsets that, when covered each by one sample, provide a sampling or coreset with good approximation guarantees in relation to the original point set. For scatterplot matrices with multiple views, different views provide different space-filling curves, leading to different partitions of the given point set. For multi-class scatterplots, the focus on either per-class distribution or global distribution provides two different partitions of the given point set that need to be considered in the selection of the coreset. For both cases, we convert the coreset selection problem into an Exact Cover Problem (ECP), and demonstrate with quantitative and qualitative evaluations that an approximate solution that solves the ECP efficiently is able to provide high-quality samplings.", "keywords": "Sampling,Scatterplot,SPLOM,Exact Cover Problem", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934799", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2006.170", "10.1109/tvcg.2008.119", "10.1111/j.1467-8659.2009.01467.x", "10.2307/2289444", "10.1109/tvcg.2013.65", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/itoec.2018.8740621", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2864912", "10.1007/0-387-28695-0", "10.1109/visual.1998.745301", "10.1287/moor.4.3.233", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.2307/2284239", "10.1109/tvcg.2017.2674978", "10.1016/j.dss.2009.05.016", "10.1109/tvcg.2014.2346594", "10.1109/vds.2017.8573446", "10.1007/978-1-4684-2001-2\\_9", "10.1109/iv.2002.1028760", "10.1145/1842993.1842999", "10.1038/nmeth.2490", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2010.176", "10.1145/2702123.2702585", "10.1057/ivs.2009.34", "10.1179/000870403235002042"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934432", "title": "Discriminability Tests for Visualization Effectiveness and Scalability", "year": "2019", "conferenceName": "InfoVis", "authors": "Rafael Veras;Christopher Collins", "citationCount": "1", "affiliation": "Veras, R (Corresponding Author), Ontario Tech Univ, Oshawa, ON, Canada. Veras, Rafael; Collins, Christopher, Ontario Tech Univ, Oshawa, ON, Canada.", "countries": "Canada", "abstract": "The scalability of a particular visualization approach is limited by the ability for people to discern differences between plots made with different datasets. Ideally, when the data changes, the visualization changes in perceptible ways. This relation breaks down when there is a mismatch between the encoding and the character of the dataset being viewed. Unfortunately, visualizations are often designed and evaluated without fully exploring how they will respond to a wide variety of datasets. We explore the use of an image similarity measure, the Multi-Scale Structural Similarity Index (MS-SSIM), for testing the discriminability of a data visualization across a variety of datasets. MS-SSIM is able to capture the similarity of two visualizations across multiple scales, including low level granular changes and high level patterns. Significant data changes that are not captured by the MS-SSIM indicate visualizations of low discriminability and effectiveness. The measure's utility is demonstrated with two empirical studies. In the first, we compare human similarity judgments and MS-SSIM scores for a collection of scatterplots. In the second, we compute the discriminability values for a set of basic visualizations and compare them with empirical measurements of effectiveness. In both cases, the analyses show that the computational measure is able to approximate empirical results. Our approach can be used to rank competing encodings on their discriminability and to aid in selecting visualizations for a particular type of data distribution.", "keywords": "Scalability,Discriminability,Simulation,Perception", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934432", "refList": ["10.1109/tvcg.2012.233", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2012.230", "10.1111/cgf.12647", "10.1109/tvcg.2016.2598918", "10.1109/tvcg.2007.70529", "10.1145/2858036.2858435", "10.1109/vast.2009.5332628", "10.1145/3025453.3025912", "10.1145/1133265.1133318", "10.1109/tip.2010.2092435", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2865264", "10.1175/jtech-d-11-00103.1", "10.1145/1190036.1190039", "10.1111/cgf.12127", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1109/mcg.2014.18", "10.1145/3173574.3174172", "10.1109/tvcg.2009.153", "10.1057/palgrave.ivs.9500070", "10.1109/tvcg.2018.2790961", "10.1109/tvcg.2014.2346978", "10.1109/tvcg.2018.2810918", "10.1111/cgf.13409", "10.1109/tip.2003.819861", "10.1145/2642918.2647411", "10.1080/15230406.2016.1140074", "10.1109/jstsp.2009.2015374", "10.1109/tvcg.2010.237", "10.1109/mcg.2009.6", "10.1111/j.1467-8659.2009.01667.x", "10.1109/tvcg.2010.161", "10.1111/cgf.13446", "10.1109/tvcg.2014.2346325", "10.1109/tvcg.2009.111", "10.1147/jrd.2015.2411412"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030394", "title": "Direct Volume Rendering with Nonparametric Models of Uncertainty", "year": "2020", "conferenceName": "SciVis", "authors": "Tushar M. Athawale;Bo Ma 0002;Elham Sakhaee;Christopher R. Johnson;Alireza Entezari", "citationCount": "0", "affiliation": "Athawale, TM (Corresponding Author), Univ Utah, Sci Comp \\& Imaging SCI Inst, Salt Lake City, UT 84112 USA. Athawale, Tushar M.; Johnson, Chris R., Univ Utah, Sci Comp \\& Imaging SCI Inst, Salt Lake City, UT 84112 USA. Ma, Bo; Sakhaee, Elham; Entezari, Alireza, Univ Florida, Dept CISE, Gainesville, FL 32611 USA.", "countries": "USA", "abstract": "We present a nonparametric statistical framework for the quantification, analysis, and propagation of data uncertainty in direct volume rendering (DVR). The state-of-the-art statistical DVR framework allows for preserving the transfer function (TF) of the ground truth function when visualizing uncertain data; however, the existing framework is restricted to parametric models of uncertainty. In this paper, we address the limitations of the existing DVR framework by extending the DVR framework for nonparametric distributions. We exploit the quantile interpolation technique to derive probability distributions representing uncertainty in viewing-ray sample intensities in closed form, which allows for accurate and efficient computation. We evaluate our proposed nonparametric statistical models through qualitative and quantitative comparisons with the mean-field and parametric statistical models, such as uniform and Gaussian, as well as Gaussian mixtures. In addition, we present an extension of the state-of-the-art rendering parametric framework to 2D TFs for improved DVR classifications. We show the applicability of our uncertainty quantification framework to ensemble, downsampled, and bivariate versions of scalar field datasets.", "keywords": "Volumes,uncertainty,nonparametric,2D transfer function", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030394", "refList": ["10.1109/icdm.2012.80", "10.1145/1401890.1401904", "10.1109/tvcg.2017.2745139", "10.18128/d030.v6.0", "10.1111/cgf.12142", "10.1109/tvcg.2009.114", "10.1111/j.1467-8659.2009.01677.x", "10.1109/mcse.2007.55", "10.1109/icde.2012.16", "10.1198/106186008x320465", "10.1145/1835804.1835868", "10.1559/1523040054738936", "10.1109/tvcg.2019.2934432", "10.1109/34.1000236", "10.1109/infvis.2005.1532136", "10.1109/tvcg.2012.128", "10.1109/pacificvis.2017.8031573", "10.1109/tvcg.2016.2598920", "10.1109/tvcg.2018.2865021", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1109/sp.2009.22", "10.1109/tvcg.2017.2744184", "10.1145/773153.773173", "10.1145/2660267.2660348", "10.1109/icdmw.2009.93", "10.1016/j.jtrangeo.2015.09.001", "10.1109/icde.2010.5447831", "10.1007/11681878\\_14", "10.1111/cgf.13409", "10.1007/s13278-014-0205-5", "10.1109/tvcg.2011.163", "10.1145/3035918.3035940", "10.1109/sp.2008.33", "10.1145/2882903.2882931"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934208", "title": "Evaluating Perceptual Bias During Geometric Scaling of Scatterplots", "year": "2019", "conferenceName": "VAST", "authors": "Yating Wei;Honghui Mei;Ying Zhao;Shuyue Zhou;Bingru Lin;Haojing Jiang;Wei Chen", "citationCount": "5", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou 310058, Zhejiang, Peoples R China. Zhao, Y (Corresponding Author), Cent South Univ, Sch Comp Sci \\& Engn, Changsha 410083, Hunan, Peoples R China. Wei, Yating; Mei, Honghui; Zhou, Shuyue; Lin, Bingru; Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou 310058, Zhejiang, Peoples R China. Zhao, Ying; Jiang, Haojing, Cent South Univ, Sch Comp Sci \\& Engn, Changsha 410083, Hunan, Peoples R China.", "countries": "China", "abstract": "Scatterplots are frequently scaled to fit display areas in multi-view and multi-device data analysis environments. A common method used for scaling is to enlarge or shrink the entire scatterplot together with the inside points synchronously and proportionally. This process is called geometric scaling. However, geometric scaling of scatterplots may cause a perceptual bias, that is, the perceived and physical values of visual features may be dissociated with respect to geometric scaling. For example, if a scatterplot is projected from a laptop to a large projector screen, then observers may feel that the scatterplot shown on the projector has fewer points than that viewed on the laptop. This paper presents an evaluation study on the perceptual bias of visual features in scatterplots caused by geometric scaling. The study focuses on three fundamental visual features (i.e., numerosity, correlation, and cluster separation) and three hypotheses that are formulated on the basis of our experience. We carefully design three controlled experiments by using well-prepared synthetic data and recruit participants to complete the experiments on the basis of their subjective experience. With a detailed analysis of the experimental results, we obtain a set of instructive findings. First, geometric scaling causes a bias that has a linear relationship with the scale ratio. Second, no significant difference exists between the biases measured from normally and uniformly distributed scatterplots. Third, changing the point radius can correct the bias to a certain extent. These findings can be used to inspire the design decisions of scatterplots in various scenarios.", "keywords": "Evaluation,scatterplot,geometric scaling,bias,perceptual consistency", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934208", "refList": ["10.2307/2288843", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2014.2346979", "10.1126/science.216.4550.1138", "10.1109/tvcg.2017.2744138", "10.1111/j.1467-8659.2009.01467.x", "10.1145/2491568.2491577", "10.1109/mwc.2018.1700325", "10.1145/2449396.2449439", "10.1109/tvcg.2011.127", "10.1109/tvcg.2011.229", "10.1167/15.5.4", "10.1073/pnas.1113195108", "10.1145/2702123.2702545", "10.1109/vast.2009.5332628", "10.1109/tvcg.2018.2800013", "10.1007/s12650-018-0530-2", "10.1016/s0042-6989(97)00340-4", "10.1109/vast.2010.5652460", "10.1109/mcg.2018.2879067", "10.1109/tvcg.2018.2864912", "10.1109/pacificvis.2010.5429604", "10.1109/tcst.2018.2819965", "10.1167/10.2.10", "10.1145/2470654.2481318", "10.1145/1842993.1843002", "10.1167/12.6.8", "10.1109/tvcg.2013.124", "10.1057/ivs.2008.13", "10.1109/tvcg.2007.70596", "10.1109/tvcg.2018.2865266", "10.1145/3173574.3173664", "10.1109/tvcg.2015.2467671", "10.1016/j.cag.2017.07.004", "10.1177/0956797613501520", "10.1109/tvcg.2018.2865020", "10.1111/j.1467-8659.2012.03125.x", "10.3758/bf03205986", "10.3758/s13423-016-1174-7", "10.1109/tvcg.2017.2680452", "10.1109/mc.2006.109", "10.1109/tvcg.2017.2744098", "10.1038/srep32810", "10.1016/j.visres.2013.06.006", "10.1002/jhbs.20078", "10.1109/tvcg.2006.163", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1016/j.cognition.2007.10.009", "10.1109/tvcg.2018.2865142", "10.3758/app.72.7.1839", "10.1109/tvcg.2018.2810918", "10.1016/j.jvlc.2017.10.001", "10.1145/2682623", "10.1109/tvcg.2018.2864884", "10.1145/3025453.3025984", "10.1109/tvcg.2006.184", "10.1109/tvcg.2013.153", "10.1016/j.jvlc.2018.08.003", "10.1109/tvcg.2016.2520921", "10.1111/cgf.13446", "10.1017/s0022381612000187", "10.1145/2702123.2702406", "10.1109/vast.2012.6400487", "10.1109/tvcg.2013.183", "10.1177/1473871611415997", "10.1145/2702123.2702585", "10.1145/2993901.2993903", "10.1109/tvcg.2013.120", "10.1111/cgf.12632", "10.1145/1385569.1385602", "10.1109/tvcg.2017.2754480", "10.1111/j.1467-8659.2009.01694.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934300", "title": "GUIRO: User-Guided Matrix Reordering", "year": "2019", "conferenceName": "VAST", "authors": "Michael Behrisch;Tobias Schreck;Hanspeter Pfister", "citationCount": "1", "affiliation": "Behrisch, M (Corresponding Author), Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Behrisch, Michael; Pfister, Hanspeter, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Schreck, Tobias, Graz Univ Technol, Graz, Austria.", "countries": "USA;Austria", "abstract": "Matrix representations are one of the main established and empirically proven to be effective visualization techniques for relational (or network) data. However, matrices\u2014similar to node-link diagrams\u2014are most effective if their layout reveals the underlying data topology. Given the many developed algorithms, a practical problem arises: \u201cWhich matrix reordering algorithm should I choose for my dataset at hand?\u201d To make matters worse, different reordering algorithms applied to the same dataset may let significantly different visual matrix patterns emerge. This leads to the question of trustworthiness and explainability of these fully automated, often heuristic, black-box processes. We present GUIRO, a Visual Analytics system that helps novices, network analysts, and algorithm designers to open the black-box. Users can investigate the usefulness and expressiveness of 70 accessible matrix reordering algorithms. For network analysts, we introduce a novel model space representation and two interaction techniques for a user-guided reordering of rows or columns, and especially groups thereof (submatrix reordering). These novel techniques contribute to the understanding of the global and local dataset topology. We support algorithm designers by giving them access to 16 reordering quality metrics and visual exploration means for comparing reordering implementations on a row/column permutation level. We evaluated GUIRO in a guided explorative user study with 12 subjects, a case study demonstrating its usefulness in a real-world scenario, and through an expert study gathering feedback on our design decisions. We found that our proposed methods help even inexperienced users to understand matrix patterns and allow a user-guided steering of reordering algorithms. GUIRO helps to increase the transparency of matrix reordering algorithms, thus helping a broad range of users to get a better insight into the complex reordering process, in turn supporting data and reordering algorithm insights.", "keywords": "Visual Analytics,matrix,black-box algorithms,seriation,ordering,sorting,steerable algorithm,interaction,2D projection", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934300", "refList": ["10.1109/tvcg.2007.70582", "10.2307/276978", "10.1109/tvcg.2008.61", "10.3390/su10040973", "10.1101/121889", "10.1145/1345448.1345453", "10.1186/s12879-014-0695-9", "10.1186/1471-2105-9-155", "10.1145/1124772.1124891", "10.1109/tvcg.2010.159", "10.1111/j.2044-8317.1974.tb00534.x", "10.1109/tpami.2015.2470671", "10.1198/000313005x22770", "10.1109/tvcg.2012.219", "10.1002/sam.10071", "10.1016/j.ejor.2016.08.066", "10.1007/s00265-003-0651-y", "10.1109/biovis.2013.6664342", "10.3115/v1/p14-1062", "10.3389/fpsyg.2017.01349", "10.1109/tpami.2004.1265866", "10.1057/palgrave.ivs.9500086", "10.1007/s004260000031", "10.1111/cgf.12935", "10.1145/800195.805928", "10.1109/tvcg.2014.2346279", "10.2312/eurovisstar.20141174", "10.1109/tvcg.2012.256", "10.1093/bioinformatics/17.suppl\\_1.s22", "10.1109/infvis.2004.46", "10.1007/978-3-319-06793-3\\_5", "10.1109/tvcg.2006.147", "10.1109/tvcg.2015.2468078", "10.1093/bioinformatics/bti141", "10.1145/1168149.1168168", "10.1109/tvcg.2017.2745978", "10.1177/1473871613513228", "10.1109/mcg.2014.62", "10.1109/tvcg.2006.166", "10.1145/2470654.2470724", "10.1177/154193120605000909", "10.1109/sibgrapi.2007.21", "10.1145/3065386", "10.1109/tvcg.2006.160", "10.1287/opre.20.5.993", "10.1111/cgf.13446", "10.1057/palgrave.ivs.9500092", "10.1145/568522.568523", "10.1109/mcg.2013.66", "10.1109/tvcg.2018.2865940", "10.1109/tvcg.2012.208"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934796", "title": "Improving the Robustness of Scagnostics", "year": "2019", "conferenceName": "InfoVis", "authors": "Yunhai Wang;Zeyu Wang 0005;Tingting Liu;Michael Correll;Zhanglin Cheng;Oliver Deussen;Michael Sedlmair", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Jinan, Peoples R China. Wang, Yunhai; Wang, Zeyu; Liu, Tingting, Shandong Univ, Jinan, Peoples R China. Wang, Zeyu; Cheng, Zhanglin; Deussen, Oliver, SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Peoples R China. Correll, Michael, Tableau Res, Seattle, WA USA. Deussen, Oliver, Konstanz Univ, Constance, Germany. Sedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "In this paper, we examine the robustness of scagnostics through a series of theoretical and empirical studies. First, we investigate the sensitivity of scagnostics by employing perturbing operations on more than 60M synthetic and real-world scatterplots. We found that two scagnostic measures, Outlying and Clumpy, are overly sensitive to data binning. To understand how these measures align with human judgments of visual features, we conducted a study with 24 participants, which reveals that i) humans are not sensitive to small perturbations of the data that cause large changes in both measures, and ii) the perception of clumpiness heavily depends on per-cluster topologies and structures. Motivated by these results, we propose Robust Scagnostics (RScag) by combining adaptive binning with a hierarchy-based form of scagnostics. An analysis shows that RScag improves on the robustness of original scagnostics, aligns better with human judgments, and is equally fast as the traditional scagnostic measures.", "keywords": "Scagnostics,scatterplots,sensitivity analysis,Robust Scagnostics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934796", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2346979", "10.1111/j.1467-8659.2009.01467.x", "10.1109/vast.2008.4677368", "10.2307/2289444", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/vast.2009.5332628", "10.1111/insr.12095\\_11", "10.1109/tvcg.2010.184", "10.1109/tvcg.2015.2467323", "10.1109/vast.2010.5652460", "10.1111/j.1467-8659.2012.03069.x", "10.1145/1842993.1843002", "10.1198/106186008x320465", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.20", "10.1111/cgf.12641", "10.1109/tvcg.2012.128", "10.1109/tvcg.2015.2467671", "10.1057/palgrave.ivs.9500091", "10.1007/978-1-4612-4400-4", "10.1111/cgf.13176", "10.1002/0470870958", "10.1109/tvcg.2018.2864907", "10.1111/j.1467-8659.2012.03125.x", "10.1109/vast.2012.6400490", "10.1109/infvis.2005.1532142", "10.1109/vast.2010.5652433", "10.1109/vast.2009.5332611", "10.1201/9781315140919", "10.1145/2858036.2858155", "10.1109/ldav.2013.6675164", "10.1515/itit-2014-1070", "10.1111/cgf.13684", "10.1109/tpami.1979.4766909", "10.2307/2289936", "10.1109/pacificvis.2014.42", "10.1109/tvcg.2016.2598467", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2006.94", "10.1109/tvcg.2014.2346572", "10.1111/cgf.12632", "10.1109/tvcg.2017.2744339"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/vast47406.2019.8986917", "title": "VIANA: Visual Interactive Annotation of Argumentation", "year": "2019", "conferenceName": "VAST", "authors": "Fabian Sperrle;Rita Sevastjanova;Rebecca Kehlbeck;Mennatallah El-Assady", "citationCount": "0", "affiliation": "Sperrle, F (Corresponding Author), Univ Konstanz, Constance, Germany. Sperrle, Fabian; Sevastjanova, Rita; Kehlbeck, Rebecca; El-Assady, Mennatallah, Univ Konstanz, Constance, Germany.", "countries": "Germany", "abstract": "Argumentation Mining addresses the challenging tasks of identifying boundaries of argumentative text fragments and extracting their relationships. Fully automated solutions do not reach satisfactory accuracy due to their insufficient incorporation of semantics and domain knowledge. Therefore, experts currently rely on time-consuming manual annotations. In this paper, we present a visual analytics system that augments the manual annotation process by automatically suggesting which text fragments to annotate next. The accuracy of those suggestions is improved over time by incorporating linguistic knowledge and language modeling to learn a measure of argument similarity from user interactions. Based on a long-term collaboration with domain experts, we identify and model five high-level analysis tasks. We enable close reading and note-taking, annotation of arguments, argument reconstruction, extraction of argument relations, and exploration of argument graphs. To avoid context switches, we transition between all views through seamless morphing, visually anchoring all text- and graph-based layers. We evaluate our system with a two-stage expert user study based on a corpus of presidential debates. The results show that experts prefer our system over existing solutions due to the speedup provided by the automatic suggestions and the tight integration between text and graph views.", "keywords": "Argumentation annotation,machine learning,user interaction,layered interfaces,semantic transitions", "link": "http://dx.doi.org/10.1109/VAST47406.2019.8986917", "refList": ["10.1007/978-3-642-40624-9\\_1", "10.3233/978-1-61499-436-7-185", "10.1145/371920.372071", "10.3233/978-1-61499-906-5-4", "10.1109/tvcg.2015.2467759", "10.18653/v1/d15-1050", "10.2307/2529310", "10.1109/mic.2003.1167344", "10.1145/312624.312682", "10.1145/2850417", "10.18653/v1/p17-2039", "10.1016/j.eswa.2016.02.013", "10.1007/978-0-387-85820-3\\_3", "10.1007/s11412-009-9080-x", "10.1109/tvcg.2008.127", "10.1145/2207676.2207741", "10.1109/cit.2012.217", "10.3233/aac-170022", "10.1109/tvcg.2017.2745080", "10.1007/978-3-319-44039-2\\_6", "10.1145/3290605.3300233", "10.1007/978-94-017-0431-1", "10.3233/978-1-61499-906-5-313", "10.1142/s0218213004001922", "10.1109/tvcg.2012.262", "10.1177/001316446002000104", "10.1109/tvcg.2018.2834341", "10.3233/978-1-61499-436-7-463", "10.1145/1772690.1772773", "10.1109/tvcg.2014.2346677", "10.1145/2523813", "10.1162/153244303322533223", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2015.2467531", "10.1109/tvcg.2018.2864769", "10.1007/978-3-319-90092-6\\_14", "10.1137/1.9781611972801.19", "10.1109/vast.2012.6400485", "10.1007/s10579-013-9215-6", "10.3115/v1/d14-1162", "10.1111/cgf.13446", "10.1109/tvcg.2006.156", "10.1111/cgf.13092", "10.1145/2645710.2645759", "10.1109/bigdata.2017.8258140", "10.1145/2669557.2669572", "10.2312/eurovisstar.20151113"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030376", "title": "Insight Beyond Numbers: The Impact of Qualitative Factors on Visual Data Analysis", "year": "2020", "conferenceName": "VAST", "authors": "Benjamin Karer;Hans Hagen;Dirk J. Lehmann", "citationCount": "0", "affiliation": "Karer, B (Corresponding Author), Fed Criminal Police Off Germany, Wiesbaden, Germany. Karer, Benjamin, Fed Criminal Police Off Germany, Wiesbaden, Germany. Hagen, Hans, TU Kaiserslautern, Kaiserslautern, Germany. Lehmann, Dirk J., Ostfalia Univ Appl Sci, Wolfenbuttel, Germany. Lehmann, Dirk J., IAV GmbH, Berlin, Germany.", "countries": "Germany", "abstract": "As of today, data analysis focuses primarily on the findings to be made inside the data and concentrates less on how those findings relate to the domain of investigation. Contemporary visualization as a field of research shows a strong tendency to adopt this data-centrism. Despite their decisive influence on the analysis result, qualitative aspects of the analysis process such as the structure, soundness, and complexity of the applied reasoning strategy are rarely discussed explicitly. We argue that if the purpose of visualization is the provision of domain insight rather than the depiction of data analysis results, a holistic perspective requires a qualitative component to to be added to the discussion of quantitative and human factors. To support this point, we demonstrate how considerations of qualitative factors in visual analysis can be applied to obtain explanations and possible solutions for a number of practical limitations inherent to the data-centric perspective on analysis. Based on this discussion of what we call qualitative visual analysis, we develop an inside-outside principle of nested levels of context that can serve as a conceptual basis for the development of visualization systems that optimally support the emergence of insight during analysis.", "keywords": "Visualization,Reasoning,Qualitative Aspects", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030376", "refList": ["10.1057/ivs.2009.22", "10.1109/tvcg.2015.2467732", "10.1109/infvis.2000.885092", "10.1109/vast.2014.7042482", "10.1145/1498700.1498704", "10.1109/tvcg.2009.108", "10.1109/tvcg.2018.2829750", "10.1037/0033-295x.111.4.1036", "10.1109/tvcg.2012.199", "10.1109/38.267476", "10.1007/978-3-540-70956-5\\_2", "10.1109/tvcg.2015.2467613", "10.1109/tvcg.2015.2467195", "10.1109/tvcg.2014.2346419", "10.1109/vast.2017.8585669", "10.1109/tvcg.2010.132", "10.1145/22949.22950", "10.1109/tvcg.2014.2346984", "10.1017/cbo9780511816772", "10.1109/mcg.2003.1231171", "10.1111/coin.12227", "10.1109/tvcg.2018.2865138", "10.1109/38.788803", "10.1109/tvcg.2018.2864849", "10.1109/mcg.2012.120", "10.1109/tvcg.2007.70535", "10.1162/neco.2008.12-06-420", "10.1109/tvcg.2018.2865240", "10.1089/tmj.2010.0114", "10.1109/beliv.2018.8634267", "10.1109/tvcg.2014.2346481", "10.1207/s15327809jls0402\\_2", "10.1109/tvcg.2006.80", "10.1145/2858036.2858280", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2011.52", "10.1016/j.cag.2014.03.002", "10.1109/tvcg.2015.2513410", "10.1111/j.1756-8765.2011.01150.x", "10.1109/infvis.2005.1532142", "10.1002/spe.4380211102", "10.1186/s41235-018-0120-9", "10.1073/pnas.1807180116", "10.1109/tvcg.2012.133", "10.1109/tvcg.2012.273", "10.1111/j.1467-8659.2011.01928.x", "10.1145/353485.353486", "10.1109/tvcg.2015.2462356", "10.1109/mcg.2019.2923483", "10.1111/cgf.13264", "10.1057/palgrave.ivs.9500070", "10.1145/1168149.1168158", "10.1109/mcg.2019.2961716", "10.1016/s0167-739x(96)00029-5", "10.1111/j.1467-8659.2009.01667.x", "10.1177/1473871611415989", "10.1109/tvcg.2013.234", "10.1109/iv.2012.33", "10.1111/cgf.13446", "10.1057/ivs.2008.28", "10.1111/cgf.12379", "10.1037/0033-295x.112.1.159", "10.1109/tvcg.2010.161", "10.1109/tvcg.2017.2744319", "10.1145/989863.989880", "10.1007/s11390-016-1663-1", "10.1177/1473871615609787", "10.1016/j.cola.2019.100911"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030395", "title": "Towards Modeling Visualization Processes as Dynamic Bayesian Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Christian Heine 0002", "citationCount": "0", "affiliation": "Heine, C (Corresponding Author), Univ Leipzig, Leipzig, Germany. Heine, Christian, Univ Leipzig, Leipzig, Germany.", "countries": "Germany", "abstract": "Visualization designs typically need to be evaluated with user studies, because their suitability for a particular task is hard to predict. What the field of visualization is currently lacking are theories and models that can be used to explain why certain designs work and others do not. This paper outlines a general framework for modeling visualization processes that can serve as the first step towards such a theory. It surveys related research in mathematical and computational psychology and argues for the use of dynamic Bayesian networks to describe these time-dependent, probabilistic processes. It is discussed how these models could be used to aid in design evaluation. The development of concrete models will be a long process. Thus, the paper outlines a research program sketching how to develop prototypes and their extensions from existing models, controlled experiments, and observational studies.", "keywords": "Visualization,model building,perception,cognition,dynamic Bayesian networks", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030395", "refList": ["10.1057/ivs.2009.22", "10.1109/tvcg.2015.2467732", "10.1109/infvis.2000.885092", "10.1109/vast.2014.7042482", "10.1145/1498700.1498704", "10.1109/tvcg.2009.108", "10.1109/tvcg.2018.2829750", "10.1037/0033-295x.111.4.1036", "10.1109/tvcg.2012.199", "10.1111/cgf.13899", "10.1109/38.267476", "10.1007/978-3-540-70956-5\\_2", "10.1109/tvcg.2015.2467613", "10.1109/tvcg.2015.2467195", "10.1109/tvcg.2014.2346419", "10.1145/3290605.3300562", "10.1109/vl.1996.545307", "10.1109/vast.2017.8585669", "10.1109/infvis.2003.1249004", "10.1109/tvcg.2010.132", "10.1145/22949.22950", "10.1109/tvcg.2014.2346984", "10.1145/3290605.3300418", "10.1017/cbo9780511816772", "10.1109/mcg.2003.1231171", "10.1111/coin.12227", "10.1109/tvcg.2018.2865138", "10.1109/38.788803", "10.1109/tvcg.2018.2864849", "10.1109/mcg.2012.120", "10.1109/tvcg.2007.70535", "10.1162/neco.2008.12-06-420", "10.1109/tvcg.2018.2865240", "10.1089/tmj.2010.0114", "10.1109/beliv.2018.8634267", "10.1109/tvcg.2014.2346481", "10.1207/s15327809jls0402\\_2", "10.1109/tvcg.2006.80", "10.1145/2858036.2858280", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2011.52", "10.1016/j.cag.2014.03.002", "10.1109/tvcg.2015.2513410", "10.1111/j.1756-8765.2011.01150.x", "10.1109/infvis.2005.1532142", "10.1002/spe.4380211102", "10.1186/s41235-018-0120-9", "10.1073/pnas.1807180116", "10.1109/tvcg.2012.133", "10.1109/tvcg.2012.273", "10.1111/j.1467-8659.2011.01928.x", "10.1145/353485.353486", "10.1109/tvcg.2015.2462356", "10.1109/mcg.2019.2923483", "10.1111/cgf.13264", "10.1057/palgrave.ivs.9500070", "10.1145/1168149.1168158", "10.1109/mcg.2019.2961716", "10.1016/s0167-739x(96)00029-5", "10.1111/j.1467-8659.2009.01667.x", "10.1177/1473871611415989", "10.1109/tvcg.2013.234", "10.1109/iv.2012.33", "10.1111/cgf.13446", "10.1057/ivs.2008.28", "10.1111/cgf.12379", "10.1037/0033-295x.112.1.159", "10.1017/cb09781139033916.005", "10.1109/tvcg.2010.161", "10.1109/tvcg.2017.2744319", "10.1145/989863.989880", "10.1007/s11390-016-1663-1", "10.1177/1473871615609787", "10.1016/j.cola.2019.100911", "10.1057/palgrave.ivs.9500025"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.8199", "year": "2020", "title": "Efficient Morphing of Shape-preserving Star Coordinates", "conferenceName": "PacificVis", "authors": "Vladimir Molchanov;Sagad Hamid;Lars Linsen", "citationCount": "0", "affiliation": "Molchanov, V (Corresponding Author), Westfalische Wilhelms Univ Munster, Munster, Germany.\nMolchanov, Vladimir; Hamid, Sagad; Linsen, Lars, Westfalische Wilhelms Univ Munster, Munster, Germany.", "countries": "Germany", "abstract": "Data tours follow an exploratory multi-dimensional data visualization concept that provides animations of projections of the multidimensional data to a 2D visual space. To create an animation, a sequence of key projections is provided and morphings between each pair of consecutive key projections are computed, which then can be stitched together to form the data tour. The morphings should be smooth so that a user can easily follow the transformations, and their computations shall be fast to allow for their integration into an interactive visual exploration process. Moreover, if the key projections are chosen to satisfy additional conditions, it is desirable that these conditions are maintained during morphing. Shape preservation is such a desirable condition, as it avoids shape distortions that may otherwise be caused by a projection. We develop a novel efficient morphing algorithms for computing shape-preserving data tours, i.e., data tours constructed for a sequence of shape-preserving linear projections. We propose a stepping strategy for the morphing to avoid discontinuities in the evolution of the projections, where we represent the linear projections using a star-coordinates system. Our algorithms are less computationally involved, produce smoother morphings, and require less user-defined parameter settings than existing state-of-the-art approaches.", "keywords": "Human-centered computing; Visualization; Visualization techniques", "link": "https://doi.org/10.1109/PacificVis48177.2020.8199", "refList": ["10.2312/eurovisshort.20141152", "10.1109/tvcg.2013.182", "10.1109/tvcg.2008.153", "10.1109/tvcg.2015.2467591", "10.4135/9781412985130", "10.1111/cgf.12878", "10.1016/j.cag.2016.08.007", "10.1080/10618600.1995.10474674", "10.1109/infvis.2003.1249004", "10.1002/9781118445112.stat06472", "10.1109/visual.1997.663916", "10.1111/cgf.12117", "10.1137/0906011", "10.1109/tvcg.2018.2865118", "10.1126/science.290.5500.2319", "10.1111/cgf.12845", "10.1111/j.1467-8659.2012.03125.x", "10.1109/tvcg.2017.2705189", "10.1002/0471725293", "10.1111/cgf.12876", "10.1111/cgf.13404", "10.2307/2289161", "10.1111/cgf.13446", "10.1198/106186008x318440", "10.2307/1390747", "10.1109/tvcg.2012.35", "10.1109/tvcg.2015.2467132", "10.1109/tvcg.2006.94", "10.1109/t-c.1974.224051", "10.1109/pacificvis.2019.00018", "10.1109/tvcg.2017.2744339", "10.1109/tsmcb.2005.850151"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2019.00018", "year": "2019", "title": "Scatterplot Summarization by Constructing Fast and Robust Principal Graphs from Skeletons", "conferenceName": "PacificVis", "authors": "Jos{\\'{e}} Matute;Marcel Fischer;Alexandru C. Telea;Lars Linsen", "citationCount": "1", "affiliation": "Matute, J (Corresponding Author), Univ Munster, Munster, Germany.\nMatute, Jose; Fischer, Marcel; Linsen, Lars, Univ Munster, Munster, Germany.\nTelea, Alexandru C., Univ Groningen, Groningen, Netherlands.", "countries": "Germany;Netherlands", "abstract": "Principal curves are a long-standing and well-known method for summarizing large scatterplots. They are defined as self-consistent curves (or curve sets in the more general case) that locally pass through the middle of the scatterplot data. However, computing principal curves that capture well complex scatterplot topologies and are robust to noise is hard and/or slow for large scatterplots. We present a fast and robust approach for computing principal graphs (a generalization of principal curves for more complex topologies) inspired by the similarity to medial descriptors (curves locally centered in a shape). Compared to state-of-the-art methods for computing principal graphs, we outperform these in terms of computational scalability and robustness to noise and resolution. We also demonstrate the advantages of our method over other scatterplot summarization approaches.", "keywords": "", "link": "https://doi.org/10.1109/PacificVis.2019.00018", "refList": ["10.1016/j.cag.2014.01.006", "10.1016/s0167-8655(02)00032-6", "10.1006/jmva.2000.1917", "10.1111/cgf.12386", "10.1109/vast.2008.4677367", "10.1109/tpami.2004.1261076", "10.1111/cgf.12865", "10.1109/tvcg.2010.213", "10.1109/tvcg.2014.2346455", "10.1109/tvcg.2011.233", "10.1111/j.1467-8659.2009.01680.x", "10.1109/tvcg.2016.2515611", "10.1109/tit.1982.1056489", "10.1109/34.899944", "10.1007/bf01889678", "10.1137/s0036144599352836", "10.1145/2858036.2858155", "10.1002/jhbs.20078", "10.1198/jcgs.2011.09224", "10.1109/tvcg.2017.2744184", "10.1057/ivs.2010.2", "10.1111/j.1467-8659.2012.03107.x", "10.2307/2289936", "10.1109/pacificvis.2014.42", "10.1109/tpami.2008.21", "10.1111/cgf.13446", "10.2307/2290446", "10.1109/34.982884", "10.1109/tvcg.2006.94", "10.2307/2288711", "10.1117/12.304651", "10.5201/ipol.2013.87", "10.1109/mlsp.2008.4685520"], "wos": 1, "children": [{"doi": "10.1109/pacificvis48177.2020.8199", "year": "2020", "title": "Efficient Morphing of Shape-preserving Star Coordinates", "conferenceName": "PacificVis", "authors": "Vladimir Molchanov;Sagad Hamid;Lars Linsen", "citationCount": "0", "affiliation": "Molchanov, V (Corresponding Author), Westfalische Wilhelms Univ Munster, Munster, Germany.\nMolchanov, Vladimir; Hamid, Sagad; Linsen, Lars, Westfalische Wilhelms Univ Munster, Munster, Germany.", "countries": "Germany", "abstract": "Data tours follow an exploratory multi-dimensional data visualization concept that provides animations of projections of the multidimensional data to a 2D visual space. To create an animation, a sequence of key projections is provided and morphings between each pair of consecutive key projections are computed, which then can be stitched together to form the data tour. The morphings should be smooth so that a user can easily follow the transformations, and their computations shall be fast to allow for their integration into an interactive visual exploration process. Moreover, if the key projections are chosen to satisfy additional conditions, it is desirable that these conditions are maintained during morphing. Shape preservation is such a desirable condition, as it avoids shape distortions that may otherwise be caused by a projection. We develop a novel efficient morphing algorithms for computing shape-preserving data tours, i.e., data tours constructed for a sequence of shape-preserving linear projections. We propose a stepping strategy for the morphing to avoid discontinuities in the evolution of the projections, where we represent the linear projections using a star-coordinates system. Our algorithms are less computationally involved, produce smoother morphings, and require less user-defined parameter settings than existing state-of-the-art approaches.", "keywords": "Human-centered computing; Visualization; Visualization techniques", "link": "https://doi.org/10.1109/PacificVis48177.2020.8199", "refList": ["10.2312/eurovisshort.20141152", "10.1109/tvcg.2013.182", "10.1109/tvcg.2008.153", "10.1109/tvcg.2015.2467591", "10.4135/9781412985130", "10.1111/cgf.12878", "10.1016/j.cag.2016.08.007", "10.1080/10618600.1995.10474674", "10.1109/infvis.2003.1249004", "10.1002/9781118445112.stat06472", "10.1109/visual.1997.663916", "10.1111/cgf.12117", "10.1137/0906011", "10.1109/tvcg.2018.2865118", "10.1126/science.290.5500.2319", "10.1111/cgf.12845", "10.1111/j.1467-8659.2012.03125.x", "10.1109/tvcg.2017.2705189", "10.1002/0471725293", "10.1111/cgf.12876", "10.1111/cgf.13404", "10.2307/2289161", "10.1111/cgf.13446", "10.1198/106186008x318440", "10.2307/1390747", "10.1109/tvcg.2012.35", "10.1109/tvcg.2015.2467132", "10.1109/tvcg.2006.94", "10.1109/t-c.1974.224051", "10.1109/pacificvis.2019.00018", "10.1109/tvcg.2017.2744339", "10.1109/tsmcb.2005.850151"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.13684", "year": "2019", "title": "ClustMe: A Visual Quality Measure for Ranking Monochrome Scatterplots based on Cluster Patterns", "conferenceName": "EuroVis", "authors": "Mostafa M. Abbas;Micha{\\\"{e}}l Aupetit;Michael Sedlmair;Halima Bensmail", "citationCount": "4", "affiliation": "Abbas, MM (Corresponding Author), HBKU, QCRI, Doha, Qatar.\nAbbas, Mostafa M.; Aupetit, Michael; Bensmail, Halima, HBKU, QCRI, Doha, Qatar.\nSedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany.", "countries": "Qatar;Germany", "abstract": "We propose ClustMe, a new visual quality measure to rank monochrome scatterplots based on cluster patterns. ClustMe is based on data collected from a human-subjects study, in which 34 participants judged synthetically generated cluster patterns in 1000 scatterplots. We generated these patterns by carefully varying the free parameters of a simple Gaussian Mixture Model with two components, and asked the participants to count the number of clusters they could see (1 or more than 1). Based on the results, we form ClustMe by selecting the model that best predicts these human judgments among 7 different state-of-the-art merging techniques (Demp). To quantitatively evaluate ClustMe, we conducted a second study, in which 31 human subjects ranked 435 pairs of scatterplots of real and synthetic data in terms of cluster patterns complexity. We use this data to compare ClustMe's performance to 4 other state-of-the-art clustering measures, including the well-known Clumpiness scagnostics. We found that of all measures, ClustMe is in strongest agreement with the human rankings.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13684", "refList": ["10.1109/tvcg.2014.2330617", "10.1109/tvcg.2014.2346979", "10.1109/tvcg.2017.2744138", "10.1109/tvcg.2017.2701829", "10.2307/2529310", "10.1109/tvcg.2011.229", "10.1109/vast.2011.6102437", "10.1068/p030033", "10.1016/j.jvcir.2011.01.005", "10.1109/tnn.2005.845141", "10.1145/2669557.2669559", "10.1167/8.7.6", "10.1145/2993901.2993907", "10.1145/2803140.2803143", "10.1145/1842993.1843002", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.124", "10.1057/ivs.2008.13", "10.1109/vast.2012.6400488", "10.1111/j.1467-9574.2008.00412.x", "10.1214/aos/1176344136", "10.1109/tvcg.2015.2467671", "10.3138/y308-2422-8615-1233", "10.1111/j.1467-8659.2012.03125.x", "10.1177/001316446002000104", "10.1145/2858036.2858155", "10.1007/s11634-010-0058-3", "10.1214/009053605000000417", "10.1109/tvcg.2009.153", "10.1023/a:1018510926151", "10.1109/tvcg.2014.2346978", "10.1007/s10816-016-9307-x", "10.1177/0301006615602599", "10.1109/pacificvis.2014.42", "10.1162/neco.1991.3.2.246", "10.1007/3-540-44491-2\\_3", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2018.2846735", "10.2307/2291604", "10.1109/inf0vis.2005.14", "10.1198/016214502760047131", "10.1007/s00180-008-0119-7", "10.1109/t-c.1974.224051", "10.1111/cgf.12632", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934796", "title": "Improving the Robustness of Scagnostics", "year": "2019", "conferenceName": "InfoVis", "authors": "Yunhai Wang;Zeyu Wang 0005;Tingting Liu;Michael Correll;Zhanglin Cheng;Oliver Deussen;Michael Sedlmair", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Jinan, Peoples R China. Wang, Yunhai; Wang, Zeyu; Liu, Tingting, Shandong Univ, Jinan, Peoples R China. Wang, Zeyu; Cheng, Zhanglin; Deussen, Oliver, SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Peoples R China. Correll, Michael, Tableau Res, Seattle, WA USA. Deussen, Oliver, Konstanz Univ, Constance, Germany. Sedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "In this paper, we examine the robustness of scagnostics through a series of theoretical and empirical studies. First, we investigate the sensitivity of scagnostics by employing perturbing operations on more than 60M synthetic and real-world scatterplots. We found that two scagnostic measures, Outlying and Clumpy, are overly sensitive to data binning. To understand how these measures align with human judgments of visual features, we conducted a study with 24 participants, which reveals that i) humans are not sensitive to small perturbations of the data that cause large changes in both measures, and ii) the perception of clumpiness heavily depends on per-cluster topologies and structures. Motivated by these results, we propose Robust Scagnostics (RScag) by combining adaptive binning with a hierarchy-based form of scagnostics. An analysis shows that RScag improves on the robustness of original scagnostics, aligns better with human judgments, and is equally fast as the traditional scagnostic measures.", "keywords": "Scagnostics,scatterplots,sensitivity analysis,Robust Scagnostics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934796", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2346979", "10.1111/j.1467-8659.2009.01467.x", "10.1109/vast.2008.4677368", "10.2307/2289444", "10.1109/tvcg.2011.229", "10.1109/iv.2004.1320207", "10.1109/vast.2009.5332628", "10.1111/insr.12095\\_11", "10.1109/tvcg.2010.184", "10.1109/tvcg.2015.2467323", "10.1109/vast.2010.5652460", "10.1111/j.1467-8659.2012.03069.x", "10.1145/1842993.1843002", "10.1198/106186008x320465", "10.1109/pacificvis.2016.7465244", "10.1109/tvcg.2013.20", "10.1111/cgf.12641", "10.1109/tvcg.2012.128", "10.1109/tvcg.2015.2467671", "10.1057/palgrave.ivs.9500091", "10.1007/978-1-4612-4400-4", "10.1111/cgf.13176", "10.1002/0470870958", "10.1109/tvcg.2018.2864907", "10.1111/j.1467-8659.2012.03125.x", "10.1109/vast.2012.6400490", "10.1109/infvis.2005.1532142", "10.1109/vast.2010.5652433", "10.1109/vast.2009.5332611", "10.1201/9781315140919", "10.1145/2858036.2858155", "10.1109/ldav.2013.6675164", "10.1515/itit-2014-1070", "10.1111/cgf.13684", "10.1109/tpami.1979.4766909", "10.2307/2289936", "10.1109/pacificvis.2014.42", "10.1109/tvcg.2016.2598467", "10.1109/tvcg.2013.153", "10.1111/cgf.13446", "10.1109/tvcg.2006.94", "10.1109/tvcg.2014.2346572", "10.1111/cgf.12632", "10.1109/tvcg.2017.2744339"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.14001", "year": "2020", "title": "Sunspot Plots: Model-based Structure Enhancement for Dense Scatter Plots", "conferenceName": "EuroVis", "authors": "Thomas Trautner;Fabian Bolte;Sergej Stoppel;Stefan Bruckner", "citationCount": "0", "affiliation": "Trautner, T (Corresponding Author), Univ Bergen, Bergen, Norway.\nTrautner, T.; Bolte, F.; Stoppel, S.; Bruckner, S., Univ Bergen, Bergen, Norway.", "countries": "Norway", "abstract": "Scatter plots are a powerful and well-established technique for visualizing the relationships between two variables as a collection of discrete points. However, especially when dealing with large and dense data, scatter plots often exhibit problems such as overplotting, making the data interpretation arduous. Density plots are able to overcome these limitations in highly populated regions, but fail to provide accurate information of individual data points. This is particularly problematic in sparse regions where the density estimate may not provide a good representation of the underlying data. In this paper, we present sunspot plots, a visualization technique that communicates dense data as a continuous data distribution, while preserving the discrete nature of data samples in sparsely populated areas. We furthermore demonstrate the advantages of our approach on typical failure cases of scatter plots within synthetic and real-world data sets and validate its effectiveness in a user study.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14001", "refList": ["10.1057/palgrave.ivs.9500122", "10.2312/eggh/hpg12/097-103", "10.1109/tvcg.2008.119", "10.1109/pacificvis.2011.5742387", "10.1038/331163a0", "10.1109/visual.2019.8933620", "10.2307/2683294", "10.1201/9781351072304", "10.2307/2289444", "10.1109/tvcg.2019.2934541", "10.1080/00949657508810123", "10.1109/tvcg.2013.65", "10.1109/infvis.1997.636789", "10.1109/2945.841121", "10.1109/mcse.2007.55", "10.1109/tvcg.2010.197", "10.1111/cgf.12871", "10.1109/infvis.2003.1249018", "10.1145/3173574.3173991", "10.1109/tvcg.2012.238", "10.1007/0-387-28695-0", "10.1109/pacificvis.2010.5429604", "10.18637/jss.v008.i03", "10.1109/tvcg.2007.70596", "10.1109/visual.1998.745301", "10.1007/0-387-37977-0\\_3", "10.1145/1556262.1556289", "10.2312/wiced.20161094", "10.1109/tvcg.2007.70535", "10.1145/1056808.1056914", "10.1109/tvcg.2003.1196007", "10.1109/iv.2004.1320190", "10.1111/cgf.12877", "10.1162/leon.2007.40.2.202a", "10.1109/tvcg.2017.2674978", "10.1057/ivs.2010.4", "10.1016/j.cag.2018.02.008", "10.1201/9781315140919", "10.1109/visual.2000.885677", "10.1002/jhbs.20078", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1109/visual.2001.964495", "10.1109/iv.2002.1028760", "10.1111/cgf.13684", "10.1109/hicss.2013.197", "10.1109/tvcg.2019.2903956", "10.1093/mnras/stt961", "10.1109/tvcg.2017.2668409", "10.1111/j.1467-8659.2009.01478.x", "10.1145/2702123.2702585", "10.2307/1418003", "10.2307/1390742", "10.1145/360825.360839", "10.1109/pacificvis.2009.4906843", "10.1057/ivs.2009.34", "10.2307/2288711"], "wos": 1, "children": [], "len": 1}], "len": 13}, {"doi": "10.1111/cgf.14000", "year": "2020", "title": "Evaluating Reordering Strategies for Cluster Identification in Parallel Coordinates", "conferenceName": "EuroVis", "authors": "Michael Blumenschein;Xuan Zhang;David Pomerenke;Daniel A. Keim;Johannes Fuchs", "citationCount": "0", "affiliation": "Blumenschein, M (Corresponding Author), Univ Konstanz, Constance, Germany.\nBlumenschein, Michael; Pomerenke, David; Keim, Daniel A.; Fuchs, Johannes, Univ Konstanz, Constance, Germany.\nZhang, Xuan, Rhein Westfal TH Aachen, Aachen, Germany.", "countries": "Germany", "abstract": "The ability to perceive patterns in parallel coordinates plots (PCPs) is heavily influenced by the ordering of the dimensions. While the community has proposed over 30 automatic ordering strategies, we still lack empirical guidance for choosing an appropriate strategy for a given task. In this paper, we first propose a classification of tasks and patterns and analyze which PCP reordering strategies help in detecting them. Based on our classification, we then conduct an empirical user study with 31 participants to evaluate reordering strategies for cluster identification tasks. We particularly measure time, identification quality, and the users' confidence for two different strategies using both synthetic and real-world datasets. Our results show that, somewhat unexpectedly, participants tend to focus on dissimilar rather than similar dimension pairs when detecting clusters, and are more confident in their answers. This is especially true when increasing the amount of clutter in the data. As a result of these findings, we propose a new reordering strategy based on the dissimilarity of neighboring dimension pairs.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14000", "refList": ["10.1111/j.1467-8659.2008.01241.x", "10.2312/conf/eg2013/stars/095-116", "10.1109/tvcg.2014.2346979", "10.1117/12.838819", "10.2307/2290001", "10.1111/cgf.12638", "10.1007/978-3-642-30217-6\\_42", "10.1109/sc.2005.47", "10.1002/wics.145", "10.1109/tvcg.2011.229", "10.1109/vast.2009.5332628", "10.1109/vast.2017.8585613", "10.1145/2993901.2993907", "10.1007/bf00410640", "10.1109/tvcg.2010.184", "10.11591/ijece.v5i6", "10.1111/j.1467-8659.2011.01961.x", "10.1109/visual.1997.663916", "10.1111/j.1467-8659.2012.03129.x", "10.1109/visual.1999.809866", "10.1057/ivs.2008.13", "10.1016/j.visinf.2017.11.001", "10.2307/2282967", "10.1007/978-0-387-68628-8\\_10", "10.5888/pcd9.120082", "10.1109/tvcg.2007.70535", "10.1109/vast.2010.5652450", "10.1007/bf01898350", "10.1016/0010-0285(91)90009-d", "10.1007/978-3-642-24958-7\\_12", "10.1109/infvis.2005.1532142", "10.1057/palgrave.ivs.9500166", "10.1111/j.1467-8659.2008.01239.x", "10.1109/mcse.2015.55", "10.2312/pe/eurovast/eurova12/007-011.7", "10.1109/tvcg.2009.153", "10.1198/jcgs.2010.09136", "10.1145/2463676.2463696", "10.1109/infvis.2005.1532138", "10.1109/tvcg.2010.242", "10.5220/0006097400400051", "10.1109/visual.2019.8933706", "10.1109/atc.2015.7388338", "10.2312/pe/eurovast/eurova12/007-011", "10.5120/5044-7370", "10.1111/cgf.13446", "10.1109/icmla.2012.148", "10.1016/j.jvlc.2015.12.001", "10.1109/tvcg.2015.2466992", "10.1111/j.1467-8659.2009.01666.x", "10.5169/seals-266450", "10.1109/infvis.1998.729559", "10.1016/j.jvlc.2017.10.003", "10.1109/visual.1995.485139", "10.1007/978-0-387-39940-9\\_262", "10.1109/tvcg.2006.138"], "wos": 1, "children": [], "len": 1}], "len": 71}, {"doi": "10.1111/cgf.13401", "year": "2018", "title": "Towards Easy Comparison of Local Businesses Using Online Reviews", "conferenceName": "EuroVis", "authors": "Yong Wang;Hammad Haleem;Conglei Shi;Yanhong Wu;Xun Zhao;Siwei Fu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, Y (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.\nWang, Yong; Haleem, Hammad; Zhao, Xun; Fu, Siwei; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.\nShi, Conglei, Airbnb Inc, San Francisco, CA USA.\nWu, Yanhong, Vis Res, Palo Alto, CA USA.", "countries": "USA;China", "abstract": "With the rapid development of e-commerce, there is an increasing number of online review websites, such as Yelp, to help customers make better purchase decisions. Viewing online reviews, including the rating score and text comments by other customers, and conducting a comparison between different businesses are the key to making an optimal decision. However, due to the massive amount of online reviews, the potential difference of user rating standards, and the significant variance of review time, length, details and quality, it is difficult for customers to achieve a quick and comprehensive comparison. In this paper, we present E-Comp, a carefully-designed visual analytics system based on online reviews, to help customers compare local businesses at different levels of details. More specifically, intuitive glyphs overlaid on maps are designed for quick candidate selection. Grouped Sankey diagram visualizing the rating difference by common customers is chosen for more reliable comparison of two businesses. Augmented word cloud showing adjective-noun word pairs, combined with a temporal view, is proposed to facilitate in-depth comparison of businesses in terms of different time periods, rating scores and features. The effectiveness and usability of E-Comp are demonstrated through a case study and in-depth user interviews.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13401", "refList": ["10.1007/978-3-211-77280-5\\_4", "10.1177/1473871611416549", "10.1145/2702123.2702476", "10.1145/1014052.1014073", "10.1109/tvcg.2014.2346249", "10.1007/978-3-540-33037-08", "10.1109/tvcg.2007.70570", "10.1016/j.ijhm.2008.06.011", "10.1109/mic.2003.1167344", "10.1177/0047287513481274", "10.1109/iv.2013.5", "10.1111/j.1467-8659.2008.01205.x", "10.1561/1500000001", "10.1007/978-3-319-30319-2\\_13", "10.1109/tvcg.2013.254", "10.1007/978-3-540-33037-0\\_8", "10.1109/tvcg.2013.122", "10.1002/acp.2350050106", "10.1109/tvcg.2016.2598590", "10.2312/eurovisshort.20161167", "10.1007/s11002-013-9278-6", "10.1111/cgf.12888", "10.1145/1134707.1134743", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2010.183", "10.1109/vast.2009.5333919", "10.1109/tvcg.2017.2744199", "10.1111/cgf.13217", "10.1145/1060745.1060797", "10.1162/jmlr.2003.3.4-5.951", "10.1109/tvcg.2017.2723397", "10.2753/jec1086-4415170204", "10.1287/mksc.1110.0653", "10.1109/tvcg.2012.110", "10.1086/268567", "10.1109/tvcg.2017.2744738", "10.1007/978-3-540", "10.1109/tvcg.2014.2346912", "10.1109/mis.2013.36", "10.1109/tvcg.2013.173", "10.1109/tvcg.2006.76", "10.1109/tvcg.2017.2745298", "10.1559/152304009788188808"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030419", "title": "Comparative Layouts Revisited: Design Space, Guidelines, and Future Directions", "year": "2020", "conferenceName": "InfoVis", "authors": "Sehi L'Yi;Jaemin Jo;Jinwook Seo", "citationCount": "0", "affiliation": "L'Yi, S (Corresponding Author), Harvard Med Sch, Boston, MA 02115 USA. L'Yi, Sehi, Harvard Med Sch, Boston, MA 02115 USA. Jo, Jaemin, Sungkyunkwan Univ, Seoul, South Korea. Seo, Jinwook, Seoul Natl Univ, Seoul, South Korea.", "countries": "USA;Korea", "abstract": "We present a systematic review on three comparative layouts-juxtaposition, superposition, and explicit-encoding-which are information visualization (InfoVis) layouts designed to support comparison tasks. For the last decade, these layouts have served as fundamental idioms in designing many visualization systems. However, we found that the layouts have been used with inconsistent terms and confusion, and the lessons from previous studies are fragmented. The goal of our research is to distill the results from previous studies into a consistent and reusable framework. We review 127 research papers, including 15 papers with quantitative user studies, which employed comparative layouts. We first alleviate the ambiguous boundaries in the design space of comparative layouts by suggesting lucid terminology (e.g., chart-wise and item-wise juxtaposition). We then identify the diverse aspects of comparative layouts, such as the advantages and concerns of using each layout in the real-world scenarios and researchers' approaches to overcome the concerns. Building our knowledge on top of the initial insights gained from the Gleicher et al.'s survey [19], we elaborate on relevant empirical evidence that we distilled from our survey (e.g., the actual effectiveness of the layouts in different study settings) and identify novel facets that the original work did not cover (e.g., the familiarity of the layouts to people). Finally, we show the consistent and contradictory results on the performance of comparative layouts and offer practical implications for using the layouts by suggesting trade-offs and seven actionable guidelines.", "keywords": "Comparative layout,visual comparison,literature review,juxtaposition,superposition,explicit-encoding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030419", "refList": ["10.1109/tvcg.2013.233", "10.1111/cgf.12380", "10.1177/1473871611416549", "10.1145/2702123.2702419", "10.1109/tvcg.2014.2322363", "10.1111/cgf.12791", "10.1145/2702123.2702130", "10.1145/2702123.2702217", "10.1109/tvcg.2012.237", "10.1177/1473871613480062", "10.1109/mcg.2017.377152546", "10.1109/tvcg.2013.213", "10.1111/cgf.12369", "10.1109/tvcg.2017.2744198", "10.1145/3139295.3139309", "10.1109/tvcg.2019.2934801", "10.1109/tvcg.2013.122", "10.1109/tvcg.2017.2747545", "10.1109/tvcg.2015.2413774", "10.1037/0096-1523.24.3.719", "10.1109/tvcg.2014.2346320", "10.1109/tvcg.2007.70535", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1145/2556288.2557141", "10.1109/tvcg.2013.149", "10.1145/1165734.1165736", "10.5220/0006127502170224", "10.1109/tvcg.2017.2745298", "10.1177/1473871617692841", "10.1190/int-2017-0083.1", "10.1190/int-2014-0283.1", "10.1109/tvcg.2016.2598796", "10.1111/cgf.13401", "10.1016/j.cag.2017.05.005", "10.1177/1473871616667632", "10.1145/3025453.3025882", "10.1145/2470654.2470724", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2010.164", "10.1145/3103010.3103013", "10.1109/pacificvis.2016.7465266", "10.1109/pacificvis.2012.6183556", "10.1109/tvcg.2015.2467751", "10.1109/tvcg.2018.2796557", "10.1111/cgf.13531", "10.1109/tvcg.2013.161", "10.1109/iv.2018.00051", "10.1109/tvcg.2010.162", "10.1109/tvcg.2018.2864510", "10.1109/iv.2017.30", "10.1109/tvcg.2007.70623"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13211", "year": "2017", "title": "Social Media Visual Analytics", "conferenceName": "EuroVis", "authors": "Siming Chen;Lijing Lin;Xiaoru Yuan", "citationCount": "29", "affiliation": "Chen, SM (Corresponding Author), Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.\nChen, SM (Corresponding Author), Peking Univ, Sch EECS, Beijing, Peoples R China.\nChen, Siming; Lin, Lijing; Yuan, Xiaoru, Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.\nChen, Siming; Lin, Lijing; Yuan, Xiaoru, Peking Univ, Sch EECS, Beijing, Peoples R China.", "countries": "China", "abstract": "With the development of social media (e.g. Twitter, Flickr, Foursquare, Sina Weibo, etc.), a large number of people are now using them and post microblogs, messages and multi-media information. The everyday usage of social media results in big open social media data. The data offer fruitful information and reflect social behaviors of people. There is much visualization and visual analytics research on such data. We collect state-of-the-art research and put it into three main categories: social network, spatial temporal information and text analysis. We further summarize the visual analytics pipeline for the social media, combining the above categories and supporting complex tasks. With these techniques, social media analytics can apply to multiple disciplines. We summarize the applications and public tools to further investigate the challenges and trends.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13211", "refList": ["10.1016/j.cag.2013.11.003", "10.1109/vast.2010.5652922", "10.1007/s12650-015-0277-y", "10.1007/978-3-540-70956-5", "10.1007/s00146-014-0549-4", "10.1109/tmm.2014.2340133", "10.1109/tmm.2015.2510329", "10.1057/palgrave.ivs.9500116", "10.1109/tvcg.2010.129", "10.1109/tvcg.2016.2598590", "10.1145/2065023.2065041", "10.1109/tvcg.2013.162", "10.1145/1963405.1963504", "10.1177/1473871613490678", "10.1007/978-0-85729-436-4\\_9", "10.1145/2488388.2488504", "10.1109/bigdata.2015.7363826", "10.1109/tvcg.2009.171", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/tvcg.2014.2371856", "10.1145/2089094.2089102", "10.1109/tvcg.2014.2346919", "10.1109/tvcg.2014.2346433", "10.1109/tvcg.2015.2509990", "10.1109/pacificvis.2015.7156366", "10.1016/j.jocs.2010.12.007", "10.1109/tvcg.2011.169", "10.1109/tvcg.2015.2467619", "10.1016/j.cag.2013.10.008", "10.1007/978-3-319-06793-3\\_3", "10.1109/tmm.2009.2012912", "10.1109/pacificvis.2015.7156367", "10.1109/tvcg.2014.2346922", "10.1371/journal.pone.0101837", "10.1109/tmm.2015.2425143", "10.1145/2493102.2493108", "10.1109/tvcg.2015.2467196", "10.1109/tvcg.2013.196", "10.1109/mcg.2014.61", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2006.160", "10.1057/palgrave.ivs.9500092", "10.1016/j.giq.2012.06.002", "10.1016/b978-0-12-382229-1.00002-3", "10.1109/vast.2016.7883510", "10.1111/j.1467-8659.2012.03120.x", "10.1109/tmm.2016.2614220", "10.7155/jgaa.00302", "10.1109/tvcg.2014.2346920", "10.1109/vast.2015.7347631", "10.1371/journal.pone.0095043", "10.1109/vast.2014.7042495", "10.1511/2001.4.344", "10.1109/vast.2012.6400557", "10.1109/pacificvis.2012.6183572", "10.1145/2567948.2577020", "10.1016/j.joi.2014.07.006", "10.1111/j.1467-8659.2009.01687.x", "10.1145/2801040.2801054", "10.1016/j.bushor.2009.09.003", "10.1109/infvis.2000.885098", "10.1109/ldav.2013.6675163", "10.1007/s13218-012-0177-4", "10.1080/13658816.2013.825724", "10.1109/tvcg.2007.70582", "10.1109/mc.2013.152", "10.1109/vast.2014.7042496", "10.1109/vast.2011.6102456", "10.1145/2733373.2806236", "10.1109/tvcg.2013.221", "10.1177/1473871615576925", "10.1109/vast.2016.7883513", "10.1109/tvcg.2013.186", "10.1109/mc.2012.430", "10.1109/tvcg.2015.2467554", "10.1109/pacificvis.2014.48", "10.1145/2212776.2212796", "10.1109/tvcg.2006.107", "10.1109/pacificvis.2014.38", "10.1109/pacificvis.2015.7156376", "10.1109/vast.2014.7042535", "10.1109/tvcg.2014.2359887"], "wos": 1, "children": [{"doi": "10.1109/vast.2017.8585638", "title": "E-Map: A Visual Analytics Approach for Exploring Significant Event Evolutions in Social Media", "year": "2017", "conferenceName": "VAST", "authors": "Siming Chen;Shuai Chen;Lijing Lin;Xiaoru Yuan;Jie Liang 0004;Xiaolong Zhang", "citationCount": "10", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing, Peoples R China. Yuan, XR (Corresponding Author), Peking Univ, Sch EECS, Beijing, Peoples R China. Chen, Siming; Chen, Shuai; Lin, Lijing; Yuan, Xiaoru, Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing, Peoples R China. Chen, Siming; Chen, Shuai; Lin, Lijing; Yuan, Xiaoru, Peking Univ, Sch EECS, Beijing, Peoples R China. Liang, Jie, Univ Technol, Fac Engn \\& Informat Technol, Sydney, NSW, Australia. Zhang, Xiaolong, Penn State Univ, Coll Informat Sci \\& Technol, University Pk, PA 16802 USA.", "countries": "USA;China;Australia", "abstract": "Significant events are often discussed and spread through social media, involving many people. Reposting activities and opinions expressed in social media offer good opportunities to understand the evolution of events. However, the dynamics of reposting activities and the diversity of user comments pose challenges to understand event-related social media data. We propose E-Map, a visual analytics approach that uses map-like visualization tools to help multi-faceted analysis of social media data on a significant event and in-depth understanding of the development of the event. E-Map transforms extracted keywords, messages, and reposting behaviors into map features such as cities, towns, and rivers to build a structured and semantic space for users to explore. It also visualizes complex posting and reposting behaviors as simple trajectories and connections that can be easily followed. By supporting multi-level spatial temporal exploration, E-Map helps to reveal the patterns of event development and key players in an event, disclosing the ways they shape and affect the development of the event. Two cases analysing real-world events confirm the capacities of E-Map in facilitating the analysis of event evolution with social media data.", "keywords": "Social Media,Event Analysis,Map-like Visual Metaphor,Spatial Temporal Visual Analytics", "link": "http://dx.doi.org/10.1109/VAST.2017.8585638", "refList": ["10.1109/tvcg.2007.70582", "10.1109/tvcg.2016.2598919", "10.1109/pacificvis.2010.5429590", "10.1057/ivs.2008.23", "10.1109/vast.2014.7042496", "10.1016/j.cag.2013.11.003", "10.1109/vast.2011.6102456", "10.1109/tvcg.2013.221", "10.1109/tvcg.2011.185", "10.1109/vast.2016.7883510", "10.1111/j.1467-8659.2012.03120.x", "10.1109/tvcg.2013.186", "10.1109/tmm.2016.2614220", "10.7155/jgaa.00302", "10.1109/mc.2012.430", "10.1109/tvcg.2015.2467619", "10.1109/tvcg.2010.129", "10.1016/s0341-8162(01)00164-3", "10.1109/tvcg.2016.2598590", "10.1145/2065023.2065041", "10.1111/cgf.13211", "10.1109/tvcg.2013.162", "10.1109/tvcg.2011.288", "10.1145/1963405.1963504", "10.5670/oceanog.2016.66", "10.1109/tvcg.2015.2467554", "10.1109/tvcg.2015.2467691", "10.1016/j.cag.2013.10.008", "10.1109/mcg.2015.73", "10.1109/vast.2012.6400557", "10.1109/tvcg.2016.2539960", "10.1109/tit.1982.1056489", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2014.2346922", "10.1109/tmm.2014.2384912", "10.1109/vast.2016.7883511", "10.1002/spe.4380211102", "10.1145/2488388.2488504", "10.2312/eurovisstar.20141176", "10.1109/bigdata.2013.6691714", "10.1109/tvcg.2009.171", "10.1109/tvcg.2013.196", "10.1109/vast.2008.4677356", "10.1145/2207676.2208672", "10.1111/j.1467-8659.2011.01955.x", "10.1109/pacificvis.2014.38", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2015.7156376", "10.1145/1348549.1348556", "10.1109/vast.2015.7347632", "10.1109/tvcg.2014.2346919", "10.1109/tvcg.2014.2346433"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934263", "title": "R-Map: A Map Metaphor for Visualizing Information Reposting Process in Social Media", "year": "2019", "conferenceName": "VAST", "authors": "Shuai Chen;Sihang Li;Siming Chen;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Chen, Shuai; Li, Sihang, Peking Univ, Sch EECS, Minist Educ, Key Lab Machine Petrept, Beijing, Peoples R China. Yuan, Xiaoru, Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Chen, Siming, Fraunhofer Inst IAIS, St Augustin, Germany. Chen, Siming, Univ Bonn, Bonn, Germany.", "countries": "Germany;China", "abstract": "We propose R-Map (Reposting Map), a visual analytical approach with a map metaphor to support interactive exploration and analysis of the information reposting process in social media. A single original social media post can cause large cascades of repostings (i.e., retweets) on online networks, involving thousands, even millions of people with different opinions. Such reposting behaviors form the reposting tree, in which a node represents a message and a link represents the reposting relation. In R-Map, the reposting tree structure can be spatialized with highlighted key players and tiled nodes. The important reposting behaviors, the following relations and the semantics relations are represented as rivers, routes and bridges, respectively, in a virtual geographical space. R-Map supports a scalable overview of a large number of information repostings with semantics. Additional interactions on the map are provided to support the investigation of temporal patterns and user behaviors in the information diffusion process. We evaluate the usability and effectiveness of our system with two use cases and a formal user study.", "keywords": "Social Media,Information Diffusion,Map-like Visual Metaphor", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934263", "refList": ["10.1109/infvis.2000.885091", "10.1109/pacificvis.2010.5429590", "10.1111/j.0020-2754.1998.00269.x", "10.1109/vast.2017.8585638", "10.1145/2700398", "10.1109/tmm.2016.2614229", "10.1109/visual.1991.175815", "10.1145/3183347", "10.1109/infvis.2001.963290", "10.1109/vast.2016.7883510", "10.1109/access.2016.2605009", "10.1109/mcg.2011.103", "10.1145/1124772.1124851", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2010.79", "10.1109/infvis.2000.885095", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1002/(sici)1097-0266(199606)17:6", "10.5670/oceanog.2016.66", "10.1559/152304003100011081", "10.1109/tit.1982.1056489", "10.1559/152304098782383034", "10.1109/tvcg.2014.2346922", "10.1007/978-3-540-85567-5\\_9", "10.1145/2488388.2488504", "10.1109/38.974518", "10.1109/bigdata.2013.6691714", "10.1109/pacificvis.2014.38", "10.1109/tvcg.2012.291", "10.1109/infvis.2005.1532128", "10.2307/2685881", "10.1007/1-4020-4179-9\\_91", "10.1109/infvis.1999.801860", "10.1109/asonam.2011.37"], "wos": 1, "children": [{"doi": "10.1111/cgf.14031", "year": "2020", "title": "The State of the Art in Map-Like Visualization", "conferenceName": "EuroVis", "authors": "Marius Hogr{\\\"{a}}fer;Magnus Heitzler;Hans{-}J{\\\"{o}}rg Schulz", "citationCount": "0", "affiliation": "Hografer, M (Corresponding Author), Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHografer, Marius; Schulz, Hans-Jorg, Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHeitzler, Magnus, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Denmark", "abstract": "Cartographic maps have been shown to provide cognitive benefits when interpreting data in relation to a geographic location. In visualization, the term map-like describes techniques that incorporate characteristics of cartographic maps in their representation of abstract data. However, the field of map-like visualization is vast and currently lacks a clear classification of the existing techniques. Moreover, choosing the right technique to support a particular visualization task is further complicated, as techniques are scattered across different domains, with each considering different characteristics as map-like. In this paper, we give an overview of the literature on map-like visualization and provide a hierarchical classification of existing techniques along two general perspectives: imitation and schematization of cartographic maps. Each perspective is further divided into four principal categories that group common map-like techniques along the visual primitives they affect. We further discuss this classification from a task-centered view and highlight open research questions.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14031", "refList": ["10.1111/j.0020-2754.1998.00269.x.21", "10.1109/iv.2005.26", "10.2307/2980460", "10.1109/tvcg.2017.2743959", "10.1016/j.jvlc.2011.11.004", "10.1145/2501988.2502046", "10.1559/1523040042742402", "10.1007/s00779-011-0500-3", "10.1111/cgf.12932", "10.1109/tvcg.2010.89", "10.1111/cgf.13200", "10.1080/15230406.2016.1160797", "10.1179/000870410x12825500202896", "10.1109/tvcg.2007.70596", "10.5167/80972.19uzh-80972", "10.1007/978-3-319-11593-1\\_2", "10.1037/aca0000175", "10.1080/17538947.2014.923942", "10.1109/tvcg.2015.2467321", "10.1109/vissof.2005.1684299", "10.1559/152304098782383034", "10.1007/978-3-642-36763-2\\_38", "10.1109/tvcg.2013.130", "10.1109/tvcg.2014.2346274", "10.1111/cgf.12648", "10.1111/0004-5608.00242", "10.1177/1473871617724212", "10.1109/iv.2004.1320123", "10.1371/journal.pcbi.1006907", "10.1109/tvcg.2016.2599030", "10.1559/1523040042742411", "10.3390/informatics5030031", "10.1111/cgf.13079", "10.1109/tvcg.2015.2467811", "10.1016/j.tics.2003.12.004", "10.1007/s00799-016-0168-4", "10.1109/infvis.2004.57", "10.1007/s10021-007-9038-7", "10.1109/pacificvis.2015.7156366", "10.1007/978-3-319-27261-0\\_1", "10.1007/978-1-4471-2804-5\\_6", "10.1109/access.2019.2939977", "10.1109/tvcg.2017.2747545", "10.1109/icdm.2003.1250978", "10.1111/cgf.13167", "10.1016/0010-0285(78)90006-3", "10.3138/nj8v-8514-871t-221k", "10.1016/j.cag.2009.06.002", "10.1111/j.1467-8659.2011.01937.x", "10.1016/b978-044451020-4/50035-9", "10.1111/1467-8659.00566", "10.5220/0006618101080119", "10.1109/pacificvis.2012.6183571", "10.1145/2038558.2038579", "10.1109/tvcg.2010.191", "10.1111/j.0033-0124.1985.00075.x", "10.1109/tvcg.2004.1260761", "10.1007/978-3-642-34848-8\\_6", "10.1145/2968220.2968239", "10.1145/3002151.3002160", "10.1109/tst.2013.6509098", "10.1109/tvcg.2013.91", "10.1177/1473871615597077", "10.1016/j.jvlc.2011.02.001", "10.1080/17445647.2014.935502", "10.1177/1687814017740710", "10.1111/1744-7917.12601", "10.1073/pnas.0400280101", "10.1111/cgf.13672", "10.1109/mcg.2006.90", "10.1002/asi.21712", "10.1007/978-3-642-33024-7\\_3", "10.1145/2801040.2801056", "10.1109/mcg.2010.101", "10.1179/003962607x165041", "10.1057/palgrave.ivs.9500039", "10.1109/vast.2016.7883510", "10.1559/152304009788988288", "10.1080/23729333.2017.1301346", "10.1109/tvcg.2013.66", "10.1111/j.1467-8659.2012.03085.x", "10.1109/tvcg.2011.288", "10.3390/ijgi9040253", "10.1109/infvis.2005.1532150", "10.1145/2254556.2254636", "10.20382//jocg.v4i1a9", "10.1016/0010-0285(81)90016-5", "10.1145/2556288.2557224", "10.1109/iv.2001.942043", "10.1021/ed1000203", "10.1016/0169-7439(87)80084-9", "10.1109/tvcg.2010.154", "10.1016/j.jvlc.2015.10.003", "10.1109/tvcg.2019.2903945", "10.1109/tvcg.2013.120", "10.1109/tvcg.2019.2934263", "10.1146/annurev-ecolsys-102209-144718", "10.1109/tvcg.2008.165", "10.3138/a477-3202-7876-n514", "10.1080/23729333.2017.1288535", "10.1111/j.0020-2754.1998.00269.x", "10.1109/mcg.2004.41", "10.1109/vast.2009.5332593", "10.1002/smr.414", "10.1007/s12650-019-00584-3", "10.1145/22949.22950", "10.1179/1743277413y.0000000036", "10.1080/15230406.2016.1262280", "10.1016/s0341-8162(01)00164-3", "10.22224/gistbok/2017.3.8", "10.1109/tvcg.2008.155", "10.1057/ivs.2008.31", "10.1016/j.cag.2004.03.012", "10.1179/1743277412y.0000000007", "10.1016/j.soncn.2011.02.001", "10.1179/caj.1987.24.1.27", "10.3138/carto.48.3.1691", "10.5220/0004267205150524", "10.1109/38.974518", "10.1145/102377.115768", "10.1057/palgrave.ivs.9500186", "10.1109/5.58325", "10.5167/80972.uzh-80972", "10.1177/030913339602000204", "10.1007/978-3-642-22300-6\\_14", "10.1109/iv.2004.1320189", "10.1111/cgf.13447", "10.1007/s11192-017-2596-3", "10.1559/1523040053722150", "10.1007/978-3-662-45803-7\\_34"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934263", "title": "R-Map: A Map Metaphor for Visualizing Information Reposting Process in Social Media", "year": "2019", "conferenceName": "VAST", "authors": "Shuai Chen;Sihang Li;Siming Chen;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Chen, Shuai; Li, Sihang, Peking Univ, Sch EECS, Minist Educ, Key Lab Machine Petrept, Beijing, Peoples R China. Yuan, Xiaoru, Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Chen, Siming, Fraunhofer Inst IAIS, St Augustin, Germany. Chen, Siming, Univ Bonn, Bonn, Germany.", "countries": "Germany;China", "abstract": "We propose R-Map (Reposting Map), a visual analytical approach with a map metaphor to support interactive exploration and analysis of the information reposting process in social media. A single original social media post can cause large cascades of repostings (i.e., retweets) on online networks, involving thousands, even millions of people with different opinions. Such reposting behaviors form the reposting tree, in which a node represents a message and a link represents the reposting relation. In R-Map, the reposting tree structure can be spatialized with highlighted key players and tiled nodes. The important reposting behaviors, the following relations and the semantics relations are represented as rivers, routes and bridges, respectively, in a virtual geographical space. R-Map supports a scalable overview of a large number of information repostings with semantics. Additional interactions on the map are provided to support the investigation of temporal patterns and user behaviors in the information diffusion process. We evaluate the usability and effectiveness of our system with two use cases and a formal user study.", "keywords": "Social Media,Information Diffusion,Map-like Visual Metaphor", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934263", "refList": ["10.1109/infvis.2000.885091", "10.1109/pacificvis.2010.5429590", "10.1111/j.0020-2754.1998.00269.x", "10.1109/vast.2017.8585638", "10.1145/2700398", "10.1109/tmm.2016.2614229", "10.1109/visual.1991.175815", "10.1145/3183347", "10.1109/infvis.2001.963290", "10.1109/vast.2016.7883510", "10.1109/access.2016.2605009", "10.1109/mcg.2011.103", "10.1145/1124772.1124851", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2010.79", "10.1109/infvis.2000.885095", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1002/(sici)1097-0266(199606)17:6", "10.5670/oceanog.2016.66", "10.1559/152304003100011081", "10.1109/tit.1982.1056489", "10.1559/152304098782383034", "10.1109/tvcg.2014.2346922", "10.1007/978-3-540-85567-5\\_9", "10.1145/2488388.2488504", "10.1109/38.974518", "10.1109/bigdata.2013.6691714", "10.1109/pacificvis.2014.38", "10.1109/tvcg.2012.291", "10.1109/infvis.2005.1532128", "10.2307/2685881", "10.1007/1-4020-4179-9\\_91", "10.1109/infvis.1999.801860", "10.1109/asonam.2011.37"], "wos": 1, "children": [{"doi": "10.1111/cgf.14031", "year": "2020", "title": "The State of the Art in Map-Like Visualization", "conferenceName": "EuroVis", "authors": "Marius Hogr{\\\"{a}}fer;Magnus Heitzler;Hans{-}J{\\\"{o}}rg Schulz", "citationCount": "0", "affiliation": "Hografer, M (Corresponding Author), Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHografer, Marius; Schulz, Hans-Jorg, Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHeitzler, Magnus, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Denmark", "abstract": "Cartographic maps have been shown to provide cognitive benefits when interpreting data in relation to a geographic location. In visualization, the term map-like describes techniques that incorporate characteristics of cartographic maps in their representation of abstract data. However, the field of map-like visualization is vast and currently lacks a clear classification of the existing techniques. Moreover, choosing the right technique to support a particular visualization task is further complicated, as techniques are scattered across different domains, with each considering different characteristics as map-like. In this paper, we give an overview of the literature on map-like visualization and provide a hierarchical classification of existing techniques along two general perspectives: imitation and schematization of cartographic maps. Each perspective is further divided into four principal categories that group common map-like techniques along the visual primitives they affect. We further discuss this classification from a task-centered view and highlight open research questions.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14031", "refList": ["10.1111/j.0020-2754.1998.00269.x.21", "10.1109/iv.2005.26", "10.2307/2980460", "10.1109/tvcg.2017.2743959", "10.1016/j.jvlc.2011.11.004", "10.1145/2501988.2502046", "10.1559/1523040042742402", "10.1007/s00779-011-0500-3", "10.1111/cgf.12932", "10.1109/tvcg.2010.89", "10.1111/cgf.13200", "10.1080/15230406.2016.1160797", "10.1179/000870410x12825500202896", "10.1109/tvcg.2007.70596", "10.5167/80972.19uzh-80972", "10.1007/978-3-319-11593-1\\_2", "10.1037/aca0000175", "10.1080/17538947.2014.923942", "10.1109/tvcg.2015.2467321", "10.1109/vissof.2005.1684299", "10.1559/152304098782383034", "10.1007/978-3-642-36763-2\\_38", "10.1109/tvcg.2013.130", "10.1109/tvcg.2014.2346274", "10.1111/cgf.12648", "10.1111/0004-5608.00242", "10.1177/1473871617724212", "10.1109/iv.2004.1320123", "10.1371/journal.pcbi.1006907", "10.1109/tvcg.2016.2599030", "10.1559/1523040042742411", "10.3390/informatics5030031", "10.1111/cgf.13079", "10.1109/tvcg.2015.2467811", "10.1016/j.tics.2003.12.004", "10.1007/s00799-016-0168-4", "10.1109/infvis.2004.57", "10.1007/s10021-007-9038-7", "10.1109/pacificvis.2015.7156366", "10.1007/978-3-319-27261-0\\_1", "10.1007/978-1-4471-2804-5\\_6", "10.1109/access.2019.2939977", "10.1109/tvcg.2017.2747545", "10.1109/icdm.2003.1250978", "10.1111/cgf.13167", "10.1016/0010-0285(78)90006-3", "10.3138/nj8v-8514-871t-221k", "10.1016/j.cag.2009.06.002", "10.1111/j.1467-8659.2011.01937.x", "10.1016/b978-044451020-4/50035-9", "10.1111/1467-8659.00566", "10.5220/0006618101080119", "10.1109/pacificvis.2012.6183571", "10.1145/2038558.2038579", "10.1109/tvcg.2010.191", "10.1111/j.0033-0124.1985.00075.x", "10.1109/tvcg.2004.1260761", "10.1007/978-3-642-34848-8\\_6", "10.1145/2968220.2968239", "10.1145/3002151.3002160", "10.1109/tst.2013.6509098", "10.1109/tvcg.2013.91", "10.1177/1473871615597077", "10.1016/j.jvlc.2011.02.001", "10.1080/17445647.2014.935502", "10.1177/1687814017740710", "10.1111/1744-7917.12601", "10.1073/pnas.0400280101", "10.1111/cgf.13672", "10.1109/mcg.2006.90", "10.1002/asi.21712", "10.1007/978-3-642-33024-7\\_3", "10.1145/2801040.2801056", "10.1109/mcg.2010.101", "10.1179/003962607x165041", "10.1057/palgrave.ivs.9500039", "10.1109/vast.2016.7883510", "10.1559/152304009788988288", "10.1080/23729333.2017.1301346", "10.1109/tvcg.2013.66", "10.1111/j.1467-8659.2012.03085.x", "10.1109/tvcg.2011.288", "10.3390/ijgi9040253", "10.1109/infvis.2005.1532150", "10.1145/2254556.2254636", "10.20382//jocg.v4i1a9", "10.1016/0010-0285(81)90016-5", "10.1145/2556288.2557224", "10.1109/iv.2001.942043", "10.1021/ed1000203", "10.1016/0169-7439(87)80084-9", "10.1109/tvcg.2010.154", "10.1016/j.jvlc.2015.10.003", "10.1109/tvcg.2019.2903945", "10.1109/tvcg.2013.120", "10.1109/tvcg.2019.2934263", "10.1146/annurev-ecolsys-102209-144718", "10.1109/tvcg.2008.165", "10.3138/a477-3202-7876-n514", "10.1080/23729333.2017.1288535", "10.1111/j.0020-2754.1998.00269.x", "10.1109/mcg.2004.41", "10.1109/vast.2009.5332593", "10.1002/smr.414", "10.1007/s12650-019-00584-3", "10.1145/22949.22950", "10.1179/1743277413y.0000000036", "10.1080/15230406.2016.1262280", "10.1016/s0341-8162(01)00164-3", "10.22224/gistbok/2017.3.8", "10.1109/tvcg.2008.155", "10.1057/ivs.2008.31", "10.1016/j.cag.2004.03.012", "10.1179/1743277412y.0000000007", "10.1016/j.soncn.2011.02.001", "10.1179/caj.1987.24.1.27", "10.3138/carto.48.3.1691", "10.5220/0004267205150524", "10.1109/38.974518", "10.1145/102377.115768", "10.1057/palgrave.ivs.9500186", "10.1109/5.58325", "10.5167/80972.uzh-80972", "10.1177/030913339602000204", "10.1007/978-3-642-22300-6\\_14", "10.1109/iv.2004.1320189", "10.1111/cgf.13447", "10.1007/s11192-017-2596-3", "10.1559/1523040053722150", "10.1007/978-3-662-45803-7\\_34"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934266", "title": "VASSL: A Visual Analytics Toolkit for Social Spambot Labeling", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;Jieqiong Zhao;David S. Ebert", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Zhao, Jieqiong; Ebert, David S., Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Purdue Univ, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Social media platforms are filled with social spambots. Detecting these malicious accounts is essential, yet challenging, as they continually evolve to evade detection techniques. In this article, we present VASSL, a visual analytics system that assists in the process of detecting and labeling spambots. Our tool enhances the performance and scalability of manual labeling by providing multiple connected views and utilizing dimensionality reduction, sentiment analysis and topic modeling, enabling insights for the identification of spambots. The system allows users to select and analyze groups of accounts in an interactive manner, which enables the detection of spambots that may not be identified when examined individually. We present a user study to objectively evaluate the performance of VASSL users, as well as capturing subjective opinions about the usefulness and the ease of use of the tool.", "keywords": "Spambot,Labeling,Detection,Visual Analytics,Social Media Annotation", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934266", "refList": ["10.7326/0003-4819-110-11-916", "10.1109/tifs.2013.2267732", "10.1111/cgf.12106", "10.1016/j.ins.2013.11.016", "10.1109/tvcg.2017.2752166", "10.1109/vast.2016.7883510", "10.1109/vl.1996.545307", "10.1145/2872518.2889302", "10.1109/tmm.2016.2614220", "10.1145/2818717", "10.1109/tdsc.2017.2681672", "10.1111/cgf.13211", "10.2307/2685478", "10.1109/tvcg.2014.2346920", "10.1109/tdsc.2016.2641441", "10.1109/tvcg.2017.2745080", "10.1109/vast.2012.6400557", "10.1109/asonam.2016.7752287", "10.1109/tvcg.2014.2346922", "10.1111/cgf.13217", "10.2307/249008", "10.1109/tvcg.2017.2711030", "10.1109/mcse.2013.70", "10.1109/tvcg.2015.2467196", "10.1016/j.comcom.2013.04.004", "10.1109/asonam.2014.6921650", "10.1109/mc.2016.183", "10.1126/science.290.5500.2323", "10.1145/3041021.3055135", "10.1162/jmlr.2003.3.4-5.993", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2013.153", "10.1109/iv.2008.89", "10.1109/mcom.2013.6588663", "10.1179/000870403235002042", "10.1145/3047010"], "wos": 1, "children": [{"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 7}, {"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13677", "year": "2019", "title": "An Ontological Framework for Supporting the Design and Evaluation of Visual Analytics Systems", "conferenceName": "EuroVis", "authors": "Min Chen;David S. Ebert", "citationCount": "5", "affiliation": "Chen, M (Corresponding Author), Univ Oxford, Oxford, England.\nChen, Min, Univ Oxford, Oxford, England.\nEbert, David S., Purdue Univ, W Lafayette, IN 47907 USA.", "countries": "USA;England", "abstract": "Designing, evaluating, and improving visual analytics (VA) systems is a primary area of activities in our discipline. In this paper, we present an ontological framework for recording and categorizing technical shortcomings to be addressed in a VA workflow, reasoning about the causes of such problems, identifying technical solutions, and anticipating secondary effects of the solutions. The methodology is built on the theoretical premise that designing a VA workflow is an optimization of the cost-benefit ratio of the processes in the workflow. It makes uses three fundamental measures to group and connect symptoms, causes, remedies, and side-effects, and guide the search for potential solutions to the problems. In terms of requirement analysis and system design, the proposed methodology can enable system designers to explore the decision space in a structured manner. In terms of evaluation, the proposed methodology is time-efficient and complementary to various forms of empirical studies, such as user surveys, controlled experiments, observational studies, focus group discussions, and so on. In general, it reduces the amount of trial-and-error in the lifecycle of VA system development.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13677", "refList": ["10.1109/tvcg.2006.178", "10.1109/infvis.2000.885092", "10.1111/cgf.12920", "10.1057/ivs.2009.26", "10.1109/mcg.2017.3271463", "10.1007/978-3-319-10578-9\\_1", "10.1109/icdmw.2008.62", "10.1109/mcg.2017.51", "10.1145/3011141.3011207", "10.1109/tvcg.2013.134", "10.1080/10618600.1996.10474696", "10.1007/978-3-540-70956-5", "10.1109/visual.1990.146375", "10.1109/tvcg.2012.219", "10.1109/vl.1996.545307", "10.1057/ivs.2009.23", "10.1002/j.1538-7305.1948.tb00917.x", "10.1109/tvcg.2010.79", "10.1109/tvcg.2013.124", "10.1111/cgf.13211", "10.1007/978-3-642-40897-7\\_9", "10.1103/physrev.108.171", "10.1109/infvis.2004.59", "10.1109/iv.2008.36", "10.1111/cgf.13210", "10.1111/j.1467-8659.2008.01230.x", "10.1001/jama.293.10.1223", "10.1177/1473871611407399", "10.1109/visual.1995.480821", "10.1117/12.539227", "10.1109/tvcg.2015.2513410", "10.1109/vast.2011.6102463", "10.7749/citiescommunitiesterritories.dec2014.029.art01", "10.1109/mcg.2005.55", "10.1109/tvcg.2012.234", "10.1109/pacificvis.2012.6183556", "10.1103/physrev.106.620", "10.1109/infvis.1997.636792", "10.2307/2104491", "10.1145/2468356.2468677", "10.1145/3173574.3173611", "10.1109/tvcg.2018.2864838", "10.1111/cgf.13092", "10.1109/visual.2004.10", "10.1109/tvcg.2017.2744319", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2603178"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934264", "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;David S. Ebert;Arif Ghafoor", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Karimzadeh, Morteza; Ebert, David S.; Ghafoor, Arif, Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.", "keywords": "Summative evaluation,usefulness,evaluation process,taxonomy,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934264", "refList": ["10.1109/tvcg.2017.2744478", "10.1109/tvcg.2018.2865025", "10.1109/tvcg.2006.85", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2017.2745181", "10.1111/cgf.13677", "10.1109/tvcg.2018.2864844", "10.1109/tvcg.2013.126", "10.1109/tvcg.2018.2864811", "10.1109/infvis.2005.1532147", "10.1177/0956797613504966", "10.1145/2669557.2669579", "10.1109/mcg.2005.102", "10.1109/visual.2003.1250426", "10.1136/bmj.39489.470347.ad", "10.1109/tvcg.2017.2744080", "10.1109/mcg.2009.53", "10.1111/j.1467-8527.2005.00307.x", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2864886", "10.1109/tvcg.2018.2864843", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2018.2865051", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2018.2865026", "10.1007/978-3-540-71080-6\\_6", "10.1109/tvcg.2018.2865020", "10.1177/1473871611407399", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2018.2864526", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2864905", "10.1049/sej.1991.0040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2711030", "10.1109/tvcg.2011.279", "10.1109/vast.2017.8585505", "10.1147/jrd.2010.2042914", "10.1016/s0378-7206(98)00044-5", "10.1145/2993901.2993913", "10.1109/tvcg.2018.2865041", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744758", "10.1145/1168149.1168158", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2012.213", "10.1109/tvcg.2017.2744738", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2018.2864826", "10.1145/1377966.1377974", "10.1109/apec.2009.4802646", "10.1145/1168149.1168152", "10.1016/j.jss.2008.03.059", "10.1109/vast.2017.8585484", "10.1109/tvcg.2017.2744818", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2865042", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030388", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "year": "2020", "conferenceName": "SciVis", "authors": "Pepe Eulzer;Sabine Bauer;Francis Kilian;Kai Lawonn", "citationCount": "0", "affiliation": "Eulzer, P (Corresponding Author), Univ Jena, Jena, Germany. Eulzer, Pepe; Lawonn, Kai, Univ Jena, Jena, Germany. Bauer, Sabine, Univ Koblenz Landau, Koblenz, Germany. Kilian, Francis, Cath Clin Koblenz Montabaur, Dept Spine Surg, Koblenz, Germany.", "countries": "Germany", "abstract": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. $\\mathrm{By}$ linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. $\\mathrm{in}$ a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.", "keywords": "Medical visualization,bioinformatics,coordinated views,focus and context,biomechanical simulation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030388", "refList": ["10.1109/tvcg.2018.2864903", "10.1177/1473871613510429", "10.1093/ehjqcco/qcz052", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2018.2865076", "10.1055/s-0039-1687862", "10.1109/visual.1990.146375", "10.1109/tvcg.2017.2744198", "10.1016/j.ijmedinf.2014.10.001", "10.1109/tvcg.2013.124", "10.1016/j.jacc", "10.1111/cgf.13167", "10.17705/1thci.00055", "10.1136/bmjqs.2009.037895", "10.1109/tvcg.2013.238", "10.1109/tvcg.2018.2865240", "10.1186/1471-2261-6-34", "10.1109/tvcg.2019.2934264", "10.1109/tvcg.2013.200", "10.1109/tvcg.2011.209", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467091", "10.1136/bmjopen-2019-033208", "10.1109/beliv.2018.8634027", "10.1109/tvcg.2012.213", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2015.2467325", "10.1145/2133806.2133821", "10.1145/1806799.1806866", "10.1108/02635570610688869", "10.1002/hbm.20701", "10.1561/1100000039", "10.1145/3025453.3025645", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 5}, {"doi": "10.1111/cgf.13987", "year": "2020", "title": "Augmenting Node-Link Diagrams with Topographic Attribute Maps", "conferenceName": "EuroVis", "authors": "Reinhold Preiner;Johanna Schmidt;Katharina Kr{\\\"{o}}sl;Tobias Schreck;Gabriel Mistelbauer", "citationCount": "0", "affiliation": "Preiner, R (Corresponding Author), Graz Univ Technol, Inst Comp Graph \\& Knowledge Visualizat, Graz, Austria.\nPreiner, R.; Schreck, T., Graz Univ Technol, Inst Comp Graph \\& Knowledge Visualizat, Graz, Austria.\nSchmidt, J.; Kroesl, K., Virtual Real \\& Visualisierung Forsch GmbH, VRVis Zentrum, Vienna, Austria.\nKroesl, K., TU Wien, Inst Visual Comp \\& Human Ctr Technol, Vienna, Austria.\nMistelbauer, G., Otto von Guericke Univ, Dept Simulat \\& Graph, Magdeburg, Germany.", "countries": "Germany;Austria", "abstract": "We propose a novel visualization technique for graphs that are attributed with scalar data. In many scenarios, these attributes (e.g., birth date in a family network) provide ambient context information for the graph structure, whose consideration is important for different visual graph analysis tasks. Graph attributes are usually conveyed using different visual representations (e.g., color, size, shape) or by reordering the graph structure according to the attribute domain (e.g., timelines). While visual encodings allow graphs to be arranged in a readable layout, assessing contextual information such as the relative similarities of attributes across the graph is often cumbersome. In contrast, attribute-based graph reordering serves the comparison task of attributes, but typically strongly impairs the readability of the structural information given by the graph's topology. In this work, we augment force-directed node-link diagrams with a continuous ambient representation of the attribute context. This way, we provide a consistent overview of the graph's topological structure as well as its attributes, supporting a wide range of graph-related analysis tasks. We resort to an intuitive height field metaphor, illustrated by a topographic map rendering using contour lines and suitable color maps. Contour lines visually connect nodes of similar attribute values, and depict their relative arrangement within the global context. Moreover, our contextual representation supports visualizing attribute value ranges associated with graph nodes (e.g., lifespans in a family network) as trajectories routed through this height field. We discuss how user interaction with both the structural and the contextual information fosters exploratory graph analysis tasks. The effectiveness and versatility of our technique is confirmed in a user study and case studies from various application domains.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13987", "refList": ["10.1109/tvcg.2013.269", "10.1109/pacificvis.2010.5429590", "10.1073/pnas.0307654100", "10.1145/2505515.2505758", "10.1559/152304082783948286", "10.1109/pacificvis.2014.47", "10.1093/bioinformatics/btp432", "10.1111/j.1467-8659.2011.01898.x", "10.1111/cgf.12931", "10.1111/cgf.12880", "10.1109/tvcg.2014.2346422", "10.1111/j.1467-8659.2009.01706.x", "10.1109/tvcg.2016.2598795", "10.1111/cgf.12800", "10.1109/tvcg.2014.2315995", "10.1111/cgf.12656", "10.1111/cgf.13728", "10.1109/tvcg.2009.122", "10.1111/cgf.13211", "10.1109/tvcg.2007.70596", "10.1109/infvis.2002.1173152", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2003.1196007", "10.1109/infvis.2005.1532150", "10.1145/3243250.3243266", "10.1080/02693799008941549", "10.1371/journal.pone.0058779", "10.1109/infvis.1995.528686", "10.1111/cgf.12872", "10.1002/spe.4380211102", "10.1109/38.974518", "10.1145/3097983.3098130", "10.1002/aris.1440370106", "10.1145/1360612.1360691", "10.1109/mc.2016.145", "10.2307/3006914", "10.1111/j.1467-8659.2009.01683.x", "10.1145/1639714.1639784"], "wos": 1, "children": [], "len": 1}], "len": 31}], "len": 119}, {"doi": "10.1109/vast.2016.7883511", "title": "How ideas flow across multiple social groups", "year": "2016", "conferenceName": "VAST", "authors": "Xiting Wang;Shixia Liu;Yang Chen;Tai-Quan Peng;Jing Su;Jing Yang;Baining Guo", "citationCount": "19", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, Sch Software, Beijing, Peoples R China. Wang, Xiting; Liu, Shixia; Chen, Yang, Tsinghua Univ, Sch Software, Beijing, Peoples R China. Peng, Tai-Quan, Michigan State Univ, E Lansing, MI 48824 USA. Su, Jing, Tsinghua Univ, Beijing, Peoples R China.", "countries": "USA;China", "abstract": "Tracking how correlated ideas flow within and across multiple social groups facilitates the understanding of the transfer of information, opinions, and thoughts on social media. In this paper, we present IdeaFlow, a visual analytics system for analyzing the lead-lag changes within and across pre-defined social groups regarding a specific set of correlated ideas, each of which is described by a set of words. To model idea flows accurately, we develop a random-walk-based correlation model and integrate it with Bayesian conditional cointegration and a tensor-based technique. To convey complex lead-lag relationships over time, IdeaFlow combines the strengths of a bubble tree, a flow map, and a timeline. In particular, we develop a Voronoi-treemap-based bubble tree to help users get an overview of a set of ideas quickly. A correlated-clustering-based layout algorithm is used to simultaneously generate multiple flow maps with less ambiguity. We also introduce a focus+context timeline to explore huge amounts of temporal data at different levels of time granularity. Quantitative evaluation and case studies demonstrate the accuracy and effectiveness of IdeaFlow.", "keywords": "", "link": "http://dx.doi.org/10.1109/VAST.2016.7883511", "refList": ["10.1109/tvcg.2015.2509990", "10.1007/s00371-013-0892-3", "10.1145/2505515.2505554", "10.1109/tvcg.2015.2467757", "10.1109/tkde.2014.2324581", "10.1109/tvcg.2015.2392771", "10.1145/129902.129906", "10.1109/tvcg.2013.221", "10.1109/infvis.2005.1532152", "10.4018/9781930708-29-7.ch002", "10.1145/2089094.2089101", "10.1109/tvcg.2012.212", "10.1109/tvcg.2013.254", "10.1007/s11390-013-1383-8", "10.1109/tvcg.2010.129", "10.1109/vast.2010.5652931", "10.1109/tvcg.2013.162", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1109/icdm.2005.77", "10.1109/tvcg.2015.2467554", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2015.2467991", "10.1109/tvcg.2010.225", "10.1109/infvis.2005.1532150", "10.1109/tvcg.2014.2388208", "10.1145/860435.860485", "10.1109/tvcg.2015.2467992", "10.1109/tvcg.2013.196", "10.1007/978-3-642-04898-2\\_110", "10.1109/vast.2012.6400485", "10.1109/infvis.2005.1532128", "10.1109/tvcg.2011.202", "10.1111/j.1533-8525.2011.01198.x", "10.1109/vast.2011.6102461", "10.1109/tkde.2015.2397432", "10.1093/bioinformatics/bts595", "10.1007/s12650-014-0246-x", "10.1109/tkde.2014.2373384", "10.1109/tvcg.2009.111", "10.1111/j.1467-8659.2009.01450.x", "10.1109/tvcg.2014.2346919", "10.1109/tvcg.2014.2346433"], "wos": 1, "children": [{"doi": "10.1109/vast.2017.8585638", "title": "E-Map: A Visual Analytics Approach for Exploring Significant Event Evolutions in Social Media", "year": "2017", "conferenceName": "VAST", "authors": "Siming Chen;Shuai Chen;Lijing Lin;Xiaoru Yuan;Jie Liang 0004;Xiaolong Zhang", "citationCount": "10", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing, Peoples R China. Yuan, XR (Corresponding Author), Peking Univ, Sch EECS, Beijing, Peoples R China. Chen, Siming; Chen, Shuai; Lin, Lijing; Yuan, Xiaoru, Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing, Peoples R China. Chen, Siming; Chen, Shuai; Lin, Lijing; Yuan, Xiaoru, Peking Univ, Sch EECS, Beijing, Peoples R China. Liang, Jie, Univ Technol, Fac Engn \\& Informat Technol, Sydney, NSW, Australia. Zhang, Xiaolong, Penn State Univ, Coll Informat Sci \\& Technol, University Pk, PA 16802 USA.", "countries": "USA;China;Australia", "abstract": "Significant events are often discussed and spread through social media, involving many people. Reposting activities and opinions expressed in social media offer good opportunities to understand the evolution of events. However, the dynamics of reposting activities and the diversity of user comments pose challenges to understand event-related social media data. We propose E-Map, a visual analytics approach that uses map-like visualization tools to help multi-faceted analysis of social media data on a significant event and in-depth understanding of the development of the event. E-Map transforms extracted keywords, messages, and reposting behaviors into map features such as cities, towns, and rivers to build a structured and semantic space for users to explore. It also visualizes complex posting and reposting behaviors as simple trajectories and connections that can be easily followed. By supporting multi-level spatial temporal exploration, E-Map helps to reveal the patterns of event development and key players in an event, disclosing the ways they shape and affect the development of the event. Two cases analysing real-world events confirm the capacities of E-Map in facilitating the analysis of event evolution with social media data.", "keywords": "Social Media,Event Analysis,Map-like Visual Metaphor,Spatial Temporal Visual Analytics", "link": "http://dx.doi.org/10.1109/VAST.2017.8585638", "refList": ["10.1109/tvcg.2007.70582", "10.1109/tvcg.2016.2598919", "10.1109/pacificvis.2010.5429590", "10.1057/ivs.2008.23", "10.1109/vast.2014.7042496", "10.1016/j.cag.2013.11.003", "10.1109/vast.2011.6102456", "10.1109/tvcg.2013.221", "10.1109/tvcg.2011.185", "10.1109/vast.2016.7883510", "10.1111/j.1467-8659.2012.03120.x", "10.1109/tvcg.2013.186", "10.1109/tmm.2016.2614220", "10.7155/jgaa.00302", "10.1109/mc.2012.430", "10.1109/tvcg.2015.2467619", "10.1109/tvcg.2010.129", "10.1016/s0341-8162(01)00164-3", "10.1109/tvcg.2016.2598590", "10.1145/2065023.2065041", "10.1111/cgf.13211", "10.1109/tvcg.2013.162", "10.1109/tvcg.2011.288", "10.1145/1963405.1963504", "10.5670/oceanog.2016.66", "10.1109/tvcg.2015.2467554", "10.1109/tvcg.2015.2467691", "10.1016/j.cag.2013.10.008", "10.1109/mcg.2015.73", "10.1109/vast.2012.6400557", "10.1109/tvcg.2016.2539960", "10.1109/tit.1982.1056489", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2014.2346922", "10.1109/tmm.2014.2384912", "10.1109/vast.2016.7883511", "10.1002/spe.4380211102", "10.1145/2488388.2488504", "10.2312/eurovisstar.20141176", "10.1109/bigdata.2013.6691714", "10.1109/tvcg.2009.171", "10.1109/tvcg.2013.196", "10.1109/vast.2008.4677356", "10.1145/2207676.2208672", "10.1111/j.1467-8659.2011.01955.x", "10.1109/pacificvis.2014.38", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2015.7156376", "10.1145/1348549.1348556", "10.1109/vast.2015.7347632", "10.1109/tvcg.2014.2346919", "10.1109/tvcg.2014.2346433"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934263", "title": "R-Map: A Map Metaphor for Visualizing Information Reposting Process in Social Media", "year": "2019", "conferenceName": "VAST", "authors": "Shuai Chen;Sihang Li;Siming Chen;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Chen, Shuai; Li, Sihang, Peking Univ, Sch EECS, Minist Educ, Key Lab Machine Petrept, Beijing, Peoples R China. Yuan, Xiaoru, Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Chen, Siming, Fraunhofer Inst IAIS, St Augustin, Germany. Chen, Siming, Univ Bonn, Bonn, Germany.", "countries": "Germany;China", "abstract": "We propose R-Map (Reposting Map), a visual analytical approach with a map metaphor to support interactive exploration and analysis of the information reposting process in social media. A single original social media post can cause large cascades of repostings (i.e., retweets) on online networks, involving thousands, even millions of people with different opinions. Such reposting behaviors form the reposting tree, in which a node represents a message and a link represents the reposting relation. In R-Map, the reposting tree structure can be spatialized with highlighted key players and tiled nodes. The important reposting behaviors, the following relations and the semantics relations are represented as rivers, routes and bridges, respectively, in a virtual geographical space. R-Map supports a scalable overview of a large number of information repostings with semantics. Additional interactions on the map are provided to support the investigation of temporal patterns and user behaviors in the information diffusion process. We evaluate the usability and effectiveness of our system with two use cases and a formal user study.", "keywords": "Social Media,Information Diffusion,Map-like Visual Metaphor", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934263", "refList": ["10.1109/infvis.2000.885091", "10.1109/pacificvis.2010.5429590", "10.1111/j.0020-2754.1998.00269.x", "10.1109/vast.2017.8585638", "10.1145/2700398", "10.1109/tmm.2016.2614229", "10.1109/visual.1991.175815", "10.1145/3183347", "10.1109/infvis.2001.963290", "10.1109/vast.2016.7883510", "10.1109/access.2016.2605009", "10.1109/mcg.2011.103", "10.1145/1124772.1124851", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2010.79", "10.1109/infvis.2000.885095", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1002/(sici)1097-0266(199606)17:6", "10.5670/oceanog.2016.66", "10.1559/152304003100011081", "10.1109/tit.1982.1056489", "10.1559/152304098782383034", "10.1109/tvcg.2014.2346922", "10.1007/978-3-540-85567-5\\_9", "10.1145/2488388.2488504", "10.1109/38.974518", "10.1109/bigdata.2013.6691714", "10.1109/pacificvis.2014.38", "10.1109/tvcg.2012.291", "10.1109/infvis.2005.1532128", "10.2307/2685881", "10.1007/1-4020-4179-9\\_91", "10.1109/infvis.1999.801860", "10.1109/asonam.2011.37"], "wos": 1, "children": [{"doi": "10.1111/cgf.14031", "year": "2020", "title": "The State of the Art in Map-Like Visualization", "conferenceName": "EuroVis", "authors": "Marius Hogr{\\\"{a}}fer;Magnus Heitzler;Hans{-}J{\\\"{o}}rg Schulz", "citationCount": "0", "affiliation": "Hografer, M (Corresponding Author), Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHografer, Marius; Schulz, Hans-Jorg, Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHeitzler, Magnus, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Denmark", "abstract": "Cartographic maps have been shown to provide cognitive benefits when interpreting data in relation to a geographic location. In visualization, the term map-like describes techniques that incorporate characteristics of cartographic maps in their representation of abstract data. However, the field of map-like visualization is vast and currently lacks a clear classification of the existing techniques. Moreover, choosing the right technique to support a particular visualization task is further complicated, as techniques are scattered across different domains, with each considering different characteristics as map-like. In this paper, we give an overview of the literature on map-like visualization and provide a hierarchical classification of existing techniques along two general perspectives: imitation and schematization of cartographic maps. Each perspective is further divided into four principal categories that group common map-like techniques along the visual primitives they affect. We further discuss this classification from a task-centered view and highlight open research questions.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14031", "refList": ["10.1111/j.0020-2754.1998.00269.x.21", "10.1109/iv.2005.26", "10.2307/2980460", "10.1109/tvcg.2017.2743959", "10.1016/j.jvlc.2011.11.004", "10.1145/2501988.2502046", "10.1559/1523040042742402", "10.1007/s00779-011-0500-3", "10.1111/cgf.12932", "10.1109/tvcg.2010.89", "10.1111/cgf.13200", "10.1080/15230406.2016.1160797", "10.1179/000870410x12825500202896", "10.1109/tvcg.2007.70596", "10.5167/80972.19uzh-80972", "10.1007/978-3-319-11593-1\\_2", "10.1037/aca0000175", "10.1080/17538947.2014.923942", "10.1109/tvcg.2015.2467321", "10.1109/vissof.2005.1684299", "10.1559/152304098782383034", "10.1007/978-3-642-36763-2\\_38", "10.1109/tvcg.2013.130", "10.1109/tvcg.2014.2346274", "10.1111/cgf.12648", "10.1111/0004-5608.00242", "10.1177/1473871617724212", "10.1109/iv.2004.1320123", "10.1371/journal.pcbi.1006907", "10.1109/tvcg.2016.2599030", "10.1559/1523040042742411", "10.3390/informatics5030031", "10.1111/cgf.13079", "10.1109/tvcg.2015.2467811", "10.1016/j.tics.2003.12.004", "10.1007/s00799-016-0168-4", "10.1109/infvis.2004.57", "10.1007/s10021-007-9038-7", "10.1109/pacificvis.2015.7156366", "10.1007/978-3-319-27261-0\\_1", "10.1007/978-1-4471-2804-5\\_6", "10.1109/access.2019.2939977", "10.1109/tvcg.2017.2747545", "10.1109/icdm.2003.1250978", "10.1111/cgf.13167", "10.1016/0010-0285(78)90006-3", "10.3138/nj8v-8514-871t-221k", "10.1016/j.cag.2009.06.002", "10.1111/j.1467-8659.2011.01937.x", "10.1016/b978-044451020-4/50035-9", "10.1111/1467-8659.00566", "10.5220/0006618101080119", "10.1109/pacificvis.2012.6183571", "10.1145/2038558.2038579", "10.1109/tvcg.2010.191", "10.1111/j.0033-0124.1985.00075.x", "10.1109/tvcg.2004.1260761", "10.1007/978-3-642-34848-8\\_6", "10.1145/2968220.2968239", "10.1145/3002151.3002160", "10.1109/tst.2013.6509098", "10.1109/tvcg.2013.91", "10.1177/1473871615597077", "10.1016/j.jvlc.2011.02.001", "10.1080/17445647.2014.935502", "10.1177/1687814017740710", "10.1111/1744-7917.12601", "10.1073/pnas.0400280101", "10.1111/cgf.13672", "10.1109/mcg.2006.90", "10.1002/asi.21712", "10.1007/978-3-642-33024-7\\_3", "10.1145/2801040.2801056", "10.1109/mcg.2010.101", "10.1179/003962607x165041", "10.1057/palgrave.ivs.9500039", "10.1109/vast.2016.7883510", "10.1559/152304009788988288", "10.1080/23729333.2017.1301346", "10.1109/tvcg.2013.66", "10.1111/j.1467-8659.2012.03085.x", "10.1109/tvcg.2011.288", "10.3390/ijgi9040253", "10.1109/infvis.2005.1532150", "10.1145/2254556.2254636", "10.20382//jocg.v4i1a9", "10.1016/0010-0285(81)90016-5", "10.1145/2556288.2557224", "10.1109/iv.2001.942043", "10.1021/ed1000203", "10.1016/0169-7439(87)80084-9", "10.1109/tvcg.2010.154", "10.1016/j.jvlc.2015.10.003", "10.1109/tvcg.2019.2903945", "10.1109/tvcg.2013.120", "10.1109/tvcg.2019.2934263", "10.1146/annurev-ecolsys-102209-144718", "10.1109/tvcg.2008.165", "10.3138/a477-3202-7876-n514", "10.1080/23729333.2017.1288535", "10.1111/j.0020-2754.1998.00269.x", "10.1109/mcg.2004.41", "10.1109/vast.2009.5332593", "10.1002/smr.414", "10.1007/s12650-019-00584-3", "10.1145/22949.22950", "10.1179/1743277413y.0000000036", "10.1080/15230406.2016.1262280", "10.1016/s0341-8162(01)00164-3", "10.22224/gistbok/2017.3.8", "10.1109/tvcg.2008.155", "10.1057/ivs.2008.31", "10.1016/j.cag.2004.03.012", "10.1179/1743277412y.0000000007", "10.1016/j.soncn.2011.02.001", "10.1179/caj.1987.24.1.27", "10.3138/carto.48.3.1691", "10.5220/0004267205150524", "10.1109/38.974518", "10.1145/102377.115768", "10.1057/palgrave.ivs.9500186", "10.1109/5.58325", "10.5167/80972.uzh-80972", "10.1177/030913339602000204", "10.1007/978-3-642-22300-6\\_14", "10.1109/iv.2004.1320189", "10.1111/cgf.13447", "10.1007/s11192-017-2596-3", "10.1559/1523040053722150", "10.1007/978-3-662-45803-7\\_34"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934289", "title": "STBins: Visual Tracking and Comparison of Multiple Data Sequences Using Temporal Binning", "year": "2019", "conferenceName": "VAST", "authors": "Ji Qi;Vincent Bloemen;Shihan Wang;Jarke J. van Wijk;Huub van de Wetering", "citationCount": "0", "affiliation": "Qi, J (Corresponding Author), Eindhoven Univ Technol, Dept Math \\& Comp Sci, Eindhoven, Netherlands. Qi, Ji; van Wijk, Jarke; van de Wetering, Huub, Eindhoven Univ Technol, Dept Math \\& Comp Sci, Eindhoven, Netherlands. Bloemen, Vincent, Univ Twente, Fac Elect Engn Math Comp Sci, Enschede, Netherlands. Wang, Shihan, Univ Amsterdam, Amsterdam, Netherlands.", "countries": "Netherlands", "abstract": "While analyzing multiple data sequences, the following questions typically arise: how does a single sequence change over time, how do multiple sequences compare within a period, and how does such comparison change over time. This paper presents a visual technique named STBins to answer these questions. STBins is designed for visual tracking of individual data sequences and also for comparison of sequences. The latter is done by showing the similarity of sequences within temporal windows. A perception study is conducted to examine the readability of alternative visual designs based on sequence tracking and comparison tasks. Also, two case studies based on real-world datasets are presented in detail to demonstrate usage of our technique.", "keywords": "Visualization,time series data,data sequence", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934289", "refList": ["10.1109/tvcg.2012.189", "10.1177/1473871611416549", "10.1145/2702123.2702419", "10.1145/2483977.2483989", "10.1109/vast.2016.7883512", "10.1109/tvcg.2016.2598797", "10.1016/j.eswa.2016.03.050", "10.1109/tvcg.2014.2346433", "10.1109/tvcg.2013.124", "10.1111/j.1469-8137.1912.tb05611.x", "10.1186/1753-6561-8-s2-s9", "10.1145/2851141.2851161", "10.1109/tvcg.2011.239", "10.1109/tvcg.2018.2864885", "10.2307/3001913", "10.1007/978-0-85729-079-3", "10.1109/tvcg.2009.117", "10.1109/tvcg.2013.200", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2016.2539960", "10.1145/2557500.2557508", "10.1109/tvcg.2012.225", "10.1186/1753-6561-8-s2-s8", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1016/j.intcom.2012.01.003", "10.1111/cgf.13264", "10.1109/tvcg.2011.232", "10.1111/cgf.12653", "10.1002/asi.21489", "10.1109/tvcg.2014.2346919", "10.1145/2254556.2254670"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2018.00032", "year": "2018", "title": "TagNet: Toward Tag-Based Sentiment Analysis of Large Social Media Data", "conferenceName": "PacificVis", "authors": "Yang Chen", "citationCount": "2", "affiliation": "", "countries": "usa;china", "abstract": "Hashtags and replies, originally introduced on Twitter, have become the most popular ways to tag short messages in social networks. While the primary uses of these human-labeled metadata are still for message retrieval and clustering, there have been increasing attempts to use them as subject or topic indicators in measuring people's continuous sentiments in large message corpora. However, conducting the analysis for large social media data is still challenging due to the message volume, heterogeneity, and temporal dependence. In this paper, we present TagNet, a novel visualization approach tailored to the tag-based sentiment analysis. TagNet combines traditional tag clouds with an improved node-link diagram to represent the time-varying heterogeneous information with reduced visual clutter. A force model is leveraged to generate layout aesthetics from which the temporal patterns of tags can be easily compared across different subsets of data. It is enhanced by visual encodings for quickly estimating the time-varying sentiment. Interaction tools are provided to improve the scalability for exploring large corpora. An example Twitter corpus illustrates the applicability and usefulness of TagNet.", "keywords": "Human-centered computing; Visualization; Visualization application domains; Visual analytics; Human-centered computing; Visualization; Visualization design and evaluation methods", "link": "https://doi.org/10.1109/PacificVis.2018.00032", "refList": ["10.1109/tvcg.2012.252", "10.1109/pacificvis.2011.5742389", "10.1198/016214506000000302", "10.1145/2063576.2063726", "10.1109/vast.2009.5333443", "10.1109/vast.2016.7883511", "10.1007/978-3-319-39931-7\\_28", "10.1109/tvcg.2017.2746018", "10.1109/tvcg.2003.1196005", "10.1109/tvcg.2010.194", "10.1126/science.220.4598.671", "10.1007/978-3-642-40669-0\\_31", "10.1007/s13278-013-0108-x", "10.1145/1520340.1520584", "10.1145/2254556.2254701", "10.1109/pacificvis.2011.5742388"], "wos": 1, "children": [], "len": 1}], "len": 15}, {"doi": "10.1109/tvcg.2017.2745083", "title": "Sequence Synopsis: Optimize Visual Summary of Temporal Event Data", "year": "2017", "conferenceName": "VAST", "authors": "Yuanzhe Chen;Panpan Xu;Ren Liu", "citationCount": "19", "affiliation": "Chen, YZ (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Hong Kong, Peoples R China. Chen, Yuanzhe, Hong Kong Univ Sci \\& Technol, Hong Kong, Hong Kong, Peoples R China. Xu, Panpan; Ren, Liu, Bosch Res North Amer, Palo Alto, CA USA.", "countries": "USA;China", "abstract": "Event sequences analysis plays an important role in many application domains such as customer behavior analysis, electronic health record analysis and vehicle fault diagnosis. Real-world event sequence data is often noisy and complex with high event cardinality, making it a challenging task to construct concise yet comprehensive overviews for such data. In this paper, we propose a novel visualization technique based on the minimum description length (MDL) principle to construct a coarse-level overview of event sequence data while balancing the information loss in it. The method addresses a fundamental trade-off in visualization design: reducing visual clutter vs. increasing the information content in a visualization. The method enables simultaneous sequence clustering and pattern extraction and is highly tolerant to noises such as missing or additional events in the data. Based on this approach we propose a visual analytics framework with multiple levels-of-detail to facilitate interactive data exploration. We demonstrate the usability and effectiveness of our approach through case studies with two real-world datasets. One dataset showcases a new application domain for event sequence visualization, i.e., fault development path analysis in vehicles for predictive maintenance. We also discuss the strengths and limitations of the proposed method based on user feedback.", "keywords": "Time Series Data,Data Transformation and Representation,Visual Knowledge Representation,Visual Analytics", "link": "http://dx.doi.org/10.1109/TVCG.2017.2745083", "refList": ["10.1109/tvcg.2015.2467622", "10.1109/infvis.2000.885091", "10.1145/2468356.2468434", "10.1145/2858036.2858107", "10.1109/ldav.2012.6378977", "10.1145/2702123.2702419", "10.1007/s00371-013-0892-3", "10.1109/vast.2009.5332595", "10.1109/tvcg.2013.214", "10.1109/icdm.2010.80", "10.1109/vast.2015.7347682", "10.1109/tvcg.2013.167", "10.1023/a:1024992613384", "10.1109/vl.1996.545307", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2207738", "10.1007/s10115-006-0027-5", "10.1145/2702123.2702262", "10.1007/978-3-642-04277-5\\_18", "10.1109/tvcg.2015.2467991", "10.1109/tvcg.2009.117", "10.1109/mcg.2015.73", "10.1109/tvcg.2013.200", "10.1109/tvcg.2016.2539960", "10.1145/2557500.2557508", "10.1109/tvcg.2012.225", "10.1109/tvcg.2014.2346682", "10.1109/mcg.2015.71", "10.1109/tvcg.2015.2502587", "10.1145/1376616.1376661", "10.3115/1075218.1075255", "10.1109/tvcg.2016.2598591", "10.1080/15230406.2016.1139467", "10.1109/vast.2012.6400494", "10.1109/tvcg.2011.179", "10.1109/pst.2008.17"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2864885", "title": "Visual Progression Analysis of Event Sequence Data", "year": "2018", "conferenceName": "VAST", "authors": "Shunan Guo;Zhuochen Jin;David Gotz;Fan Du;Hongyuan Zha;Nan Cao", "citationCount": "12", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, iDVX Lab, Shanghai, Peoples R China. Guo, Shunan; Zha, Hongyuan, East China Normal Univ, Shanghai, Peoples R China. Jin, Zhuochen; Cao, Nan, Tongji Univ, iDVX Lab, Shanghai, Peoples R China. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA. Du, Fan, Univ Maryland, College Pk, MD 20742 USA.", "countries": "USA;China", "abstract": "Event sequence data is common to a broad range of application domains, from security to health care to scholarly communication. This form of data captures information about the progression of events for an individual entity (e.g., a computer network device; a patient; an author) in the form of a series of time-stamped observations. Moreover, each event is associated with an event type (e.g., a computer login attempt, or a hospital discharge). Analyses of event sequence data have been shown to help reveal important temporal patterns, such as clinical paths resulting in improved outcomes, or an understanding of common career trajectories for scholars. Moreover, recent research has demonstrated a variety of techniques designed to overcome methodological challenges such as large volumes of data and high dimensionality. However, the effective identification and analysis of latent stages of progression, which can allow for variation within different but similarly evolving event sequences, remain a significant challenge with important real-world motivations. In this paper, we propose an unsupervised stage analysis algorithm to identify semantically meaningful progression stages as well as the critical events which help define those stages. The algorithm follows three key steps: (1) event representation estimation, (2) event sequence warping and alignment, and (3) sequence segmentation. We also present a novel visualization system, ET<sup>2</sup>, which interactively illustrates the results of the stage analysis algorithm to help reveal evolution patterns across stages. Finally, we report three forms of evaluation for ET<sup>2</sup>: (1) case studies with two real-world datasets, (2) interviews with domain expert users, and (3) a performance evaluation on the progression analysis algorithm and the visualization design.", "keywords": "Progression Analysis,Visual Analysis,Event Sequence Data", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864885", "refList": ["10.1145/2623330.2623754", "10.1007/978-3-319-06483-3\\_8", "10.1145/2702123.2702419", "10.1109/vast.2016.7883512", "10.1101/gr.75202", "10.1007/978-3-642-53914-5\\_15", "10.1145/1631162.1631169", "10.1023/a:1024992613384", "10.1016/j.jbi.2014.01.007", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1007/978-3-319-63564-4\\_3", "10.1007/s10115-004-0154-9", "10.1016/s1474-4422(09)70299-6", "10.1016/j.jbi.2012.10.001", "10.1109/tvcg.2016.2539960", "10.1145/2557500.2557508", "10.1007/s10044-013-0325-y", "10.1145/775047.775109", "10.1109/tvcg.2014.2346682", "10.3233/ida-2007-11508", "10.1109/tvcg.2017.2745320", "10.1109/tvcg.2011.188", "10.1038/sdata.2016.35", "10.1016/b978-155860915-0/50038-x", "10.1109/tvcg.2017.2745083", "10.1145/2566486.2568044", "10.1073/pnas.052587399", "10.1109/tvcg.2014.2346574", "10.1093/cercor/11.1.1", "10.1109/icde.2017.25", "10.1109/tvcg.2014.2346913", "10.1145/3025453.3025777"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934267", "title": "ProtoSteer: Steering Deep Sequence Model with Prototypes", "year": "2019", "conferenceName": "VAST", "authors": "Yao Ming;Panpan Xu;Furui Cheng;Huamin Qu;Ren Liu", "citationCount": "2", "affiliation": "Ming, Y (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Ming, Yao; Cheng, Furui; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xu, Panpan; Ren, Liu, Bosch Res North Amer, Palo Alto, CA USA.", "countries": "USA;China", "abstract": "Recently we have witnessed growing adoption of deep sequence models (e.g. LSTMs) in many application domains, including predictive health care, natural language processing, and log analysis. However, the intricate working mechanism of these models confines their accessibility to the domain experts. Their black-box nature also makes it a challenging task to incorporate domain-specific knowledge of the experts into the model. In ProtoSteer (Prototype Steering), we tackle the challenge of directly involving the domain experts to steer a deep sequence model without relying on model developers as intermediaries. Our approach originates in case-based reasoning, which imitates the common human problem-solving process of consulting past experiences to solve new problems. We utilize ProSeNet (Prototype Sequence Network), which learns a small set of exemplar cases (i.e., prototypes) from historical data. In ProtoSteer they serve both as an efficient visual summary of the original data and explanations of model decisions. With ProtoSteer the domain experts can inspect, critique, and revise the prototypes interactively. The system then incorporates user-specified prototypes and incrementally updates the model. We conduct extensive case studies and expert interviews in application domains including sentiment analysis on texts and predictive diagnostics based on vehicle fault logs. The results demonstrate that involvements of domain users can help obtain more interpretable models with concise prototypes while retaining similar accuracy.", "keywords": "Sequence Data,Explainable Artificial Intelligence (XAI),Recurrent Neural Networks (RNNs),Prototype Learning", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934267", "refList": ["10.1109/infvis.2000.885091", "10.1145/2468356.2468434", "10.1109/mcg.2018.2878902", "10.1145/2858036.2858107", "10.1145/2702123.2702419", "10.1109/vast.2015.7347682", "10.1109/tvcg.2016.2598797", "10.1145/2939672.2939778", "10.1109/tvcg.2018.2864885", "10.1109/tvcg.2018.2865044", "10.1073/pnas.95.25.14863", "10.1109/tvcg.2016.2539960", "10.1145/3025453.3025456", "10.1145/2557500.2557508", "10.1109/tvcg.2012.225", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2018.2865230", "10.1109/tvcg.2017.2745320", "10.1109/tvcg.2017.2744718", "10.18653/v1/n16-1082", "10.1016/j.neucom.2013.11.045", "10.1109/tvcg.2017.2745083", "10.1609/aimag.v35i4.2513", "10.1007/bf00155578", "10.1007/978-3-319-90403-0\\_17", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2018.2864838", "10.1109/tvcg.2018.2865027", "10.1145/312129.312298", "10.1145/3185517", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13972", "year": "2020", "title": "Boxer: Interactive Comparison of Classifier Results", "conferenceName": "EuroVis", "authors": "Michael Gleicher;Aditya Barve;Xinyi Yu;Florian Heimerl", "citationCount": "0", "affiliation": "Gleicher, M (Corresponding Author), Univ Wisconsin, Madison, WI 53706 USA.\nGleicher, Michael; Barve, Aditya; Yu, Xinyi; Heimerl, Florian, Univ Wisconsin, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Machine learning practitioners often compare the results of different classifiers to help select, diagnose and tune models. We present Boxer, a system to enable such comparison. Our system facilitates interactive exploration of the experimental results obtained by applying multiple classifiers to a common set of model inputs. The approach focuses on allowing the user to identify interesting subsets of training and testing instances and comparing performance of the classifiers on these subsets. The system couples standard visual designs with set algebra interactions and comparative elements. This allows the user to compose and coordinate views to specify subsets and assess classifier performance on them. The flexibility of these compositions allow the user to address a wide range of scenarios in developing and assessing classifiers. We demonstrate Boxer in use cases including model selection, tuning, fairness assessment, and data quality diagnosis.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13972", "refList": ["10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2016.2598838", "10.1007/s10618-014-0368-8", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1145/3287560.3287589", "10.1109/vast.2017.8585721", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2009.128", "10.1109/tvcg.2017.2744018", "10.1080/00994480.2000.10748487", "10.5555/3305890.3306024", "10.1109/iccv.2015.329", "10.1109/tvcg.2013.125", "10.1089/big.2016.0007", "10.1109/memsys.2019.8870817", "10.1145/2939672.2939778", "10.1007/s11104-019-04156-0", "10.1371/journal.pone.0181142", "10.1145/3301275.3302324", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2018.2865044", "10.1023/a:1010933404324", "10.1145/2487575.2487579", "10.1109/tvcg.2013.157", "10.1145/2783258.2788613", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934631", "10.1016/s0304-3800(02)00064-9", "10.1007/s10115-013-0679-x", "10.1109/tvcg.2019.2934267", "10.1007/978-3-319-10590-1\\_53", "10.1109/vast.2017.8585720", "10.1016/0004-3702(80)90021-1", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2009.84", "10.1007/s11263-016-0911-8", "10.1111/cgf.12918", "10.1111/cgf.12373", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1080/00207176808905715", "10.1002/er.3827", "10.1109/tvcg.2014.2346660", "10.1111/cgf.13417", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2017.2744158", "10.1016/b978-0-12-815849-4.00004-9", "10.1097/ede.0b013e3181c30fb2", "10.1111/cgf.13681", "10.1016/j.ejor.2006.04.051", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2016.2598468", "10.9735/2229-3981", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 11}, {"doi": "10.1109/tvcg.2019.2934289", "title": "STBins: Visual Tracking and Comparison of Multiple Data Sequences Using Temporal Binning", "year": "2019", "conferenceName": "VAST", "authors": "Ji Qi;Vincent Bloemen;Shihan Wang;Jarke J. van Wijk;Huub van de Wetering", "citationCount": "0", "affiliation": "Qi, J (Corresponding Author), Eindhoven Univ Technol, Dept Math \\& Comp Sci, Eindhoven, Netherlands. Qi, Ji; van Wijk, Jarke; van de Wetering, Huub, Eindhoven Univ Technol, Dept Math \\& Comp Sci, Eindhoven, Netherlands. Bloemen, Vincent, Univ Twente, Fac Elect Engn Math Comp Sci, Enschede, Netherlands. Wang, Shihan, Univ Amsterdam, Amsterdam, Netherlands.", "countries": "Netherlands", "abstract": "While analyzing multiple data sequences, the following questions typically arise: how does a single sequence change over time, how do multiple sequences compare within a period, and how does such comparison change over time. This paper presents a visual technique named STBins to answer these questions. STBins is designed for visual tracking of individual data sequences and also for comparison of sequences. The latter is done by showing the similarity of sequences within temporal windows. A perception study is conducted to examine the readability of alternative visual designs based on sequence tracking and comparison tasks. Also, two case studies based on real-world datasets are presented in detail to demonstrate usage of our technique.", "keywords": "Visualization,time series data,data sequence", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934289", "refList": ["10.1109/tvcg.2012.189", "10.1177/1473871611416549", "10.1145/2702123.2702419", "10.1145/2483977.2483989", "10.1109/vast.2016.7883512", "10.1109/tvcg.2016.2598797", "10.1016/j.eswa.2016.03.050", "10.1109/tvcg.2014.2346433", "10.1109/tvcg.2013.124", "10.1111/j.1469-8137.1912.tb05611.x", "10.1186/1753-6561-8-s2-s9", "10.1145/2851141.2851161", "10.1109/tvcg.2011.239", "10.1109/tvcg.2018.2864885", "10.2307/3001913", "10.1007/978-0-85729-079-3", "10.1109/tvcg.2009.117", "10.1109/tvcg.2013.200", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2016.2539960", "10.1145/2557500.2557508", "10.1109/tvcg.2012.225", "10.1186/1753-6561-8-s2-s8", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1016/j.intcom.2012.01.003", "10.1111/cgf.13264", "10.1109/tvcg.2011.232", "10.1111/cgf.12653", "10.1002/asi.21489", "10.1109/tvcg.2014.2346919", "10.1145/2254556.2254670"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13965", "year": "2020", "title": "Bombalytics: Visualization of Competition and Collaboration Strategies of Players in a Bomb Laying Game", "conferenceName": "EuroVis", "authors": "Shivam Agarwal;G{\\\"{u}}nter Wallner;Fabian Beck", "citationCount": "0", "affiliation": "Agarwal, S (Corresponding Author), Univ Duisburg Essen, Duisburg, Germany.\nAgarwal, Shivam; Beck, Fabian, Univ Duisburg Essen, Duisburg, Germany.\nWallner, Gunter, Eindhoven Univ Technol, Eindhoven, Netherlands.\nWallner, Gunter, Univ Appl Arts Vienna, Vienna, Austria.", "countries": "Germany;Austria;Netherlands", "abstract": "Competition and collaboration form complex interaction patterns between the agents and objects involved. Only by understanding these interaction patterns, we can reveal the strategies the participating parties applied. In this paper, we study such competition and collaboration behavior for a computer game. Serving as a testbed for artificial intelligence, the multiplayer bomb laying game Pommerman provides a rich source of advanced behavior of computer agents. We propose a visualization approach that shows an overview of multiple games, with a detailed timeline-based visualization for exploring the specifics of each game. Since an analyst can only fully understand the data when considering the direct and indirect interactions between agents, we suggest various visual encodings of these interactions. Based on feedback from expert users and an application example, we demonstrate that the approach helps identify central competition strategies and provides insights on collaboration.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13965", "refList": ["10.1145/1822348.1822349", "10.1016/j.cag.2013.11.010", "10.1145/2793107.2793112", "10.1109/tvcg.2019.2934630", "10.1145/3343055.3360747", "10.1016/j.intcom.2010.04.004", "10.1109/cig.2019.8848091", "10.1007/s11554-013-0347-0", "10.1007/s12650-019-00566-5", "10.1109/bigdata.2018.8622571", "10.1007/978-3-319-63519-4", "10.1109/tciaig.2014.2365470", "10.1016/j.entcom.2013.02.002", "10.1609/aimag.v34i3.2492", "10.1145/3311350.3357716", "10.1109/tvcg.2018.2864885", "10.1177/1473871617718377", "10.1109/tvcg.2018.2864504", "10.1111/cgf.12919", "10.1109/tvcg.2019.2934243", "10.1109/tvcg.2018.2859969", "10.1145/3235765.3235812", "10.1109/tvcg.2017.2745320", "10.1109/tvcg.2016.2598415", "10.1109/tciaig.2012.2188528", "10.1109/iscas.2019.8702471", "10.2307/350902", "10.1109/mc.2018.2890217", "10.1145/3027063.3053146", "10.1111/cgf.13436", "10.1145/3130859.3131439", "10.1016/j.compedu.2011.11.015"], "wos": 1, "children": [], "len": 1}], "len": 17}, {"doi": "10.1109/tvcg.2018.2864886", "title": "MAQUI: Interweaving Queries and Pattern Mining for Recursive Event Sequence Exploration", "year": "2018", "conferenceName": "VAST", "authors": "Po-Ming Law;Zhicheng Liu;Sana Malik;Rahul C. Basole", "citationCount": "4", "affiliation": "Law, PM (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA. Law, Po-Ming; Basole, Rahul C., Georgia Inst Technol, Atlanta, GA 30332 USA. Liu, Zhicheng; Malik, Sana, Adobe Res, San Jose, CA USA.", "countries": "USA", "abstract": "Exploring event sequences by defining queries alone or by using mining algorithms alone is often not sufficient to support analysis. Analysts often interweave querying and mining in a recursive manner during event sequence analysis: sequences extracted as query results are used for mining patterns, patterns generated are incorporated into a new query for segmenting the sequences, and the resulting segments are mined or queried again. To support flexible analysis, we propose a framework that describes the process of interwoven querying and mining. Based on this framework, we developed MAQUI, a Mining And Querying User Interface that enables recursive event sequence exploration. To understand the efficacy of MAQUI, we conducted two case studies with domain experts. The findings suggest that the capability of interweaving querying and mining helps the participants articulate their questions and gain novel insights from their data.", "keywords": "Sequential pattern mining,temporal query,event sequence exploration", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864886", "refList": ["10.1109/tvcg.2015.2467622", "10.1145/2470654.2481325", "10.1007/978-3-319-06483-3\\_8", "10.1177/1473871611416549", "10.1109/vast.2009.5332595", "10.1145/846183.846188", "10.1109/tvcg.2014.2346452", "10.1109/vast.2016.7883512", "10.1109/vast.2015.7347682", "10.1177/1473871614526077", "10.1016/j.jbi.2014.01.007", "10.1145/2856767.2856779", "10.1109/tvcg.2016.2598797", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2009.117", "10.1109/tvcg.2013.200", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2016.2539960", "10.1145/2557500.2557508", "10.1145/319950.320010", "10.1145/775047.775109", "10.1109/tvcg.2014.2346682", "10.1016/j.intcom.2012.01.003", "10.1016/j.jbi.2014.09.003", "10.1109/vlhcc.2009.5295262", "10.1109/tvcg.2017.2745083", "10.1007/s40273-015-0333-4", "10.1109/tvcg.2014.2346574", "10.1111/cgf.13208", "10.1145/3025453.3025777"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934264", "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;David S. Ebert;Arif Ghafoor", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Karimzadeh, Morteza; Ebert, David S.; Ghafoor, Arif, Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.", "keywords": "Summative evaluation,usefulness,evaluation process,taxonomy,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934264", "refList": ["10.1109/tvcg.2017.2744478", "10.1109/tvcg.2018.2865025", "10.1109/tvcg.2006.85", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2017.2745181", "10.1111/cgf.13677", "10.1109/tvcg.2018.2864844", "10.1109/tvcg.2013.126", "10.1109/tvcg.2018.2864811", "10.1109/infvis.2005.1532147", "10.1177/0956797613504966", "10.1145/2669557.2669579", "10.1109/mcg.2005.102", "10.1109/visual.2003.1250426", "10.1136/bmj.39489.470347.ad", "10.1109/tvcg.2017.2744080", "10.1109/mcg.2009.53", "10.1111/j.1467-8527.2005.00307.x", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2864886", "10.1109/tvcg.2018.2864843", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2018.2865051", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2018.2865026", "10.1007/978-3-540-71080-6\\_6", "10.1109/tvcg.2018.2865020", "10.1177/1473871611407399", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2018.2864526", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2864905", "10.1049/sej.1991.0040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2711030", "10.1109/tvcg.2011.279", "10.1109/vast.2017.8585505", "10.1147/jrd.2010.2042914", "10.1016/s0378-7206(98)00044-5", "10.1145/2993901.2993913", "10.1109/tvcg.2018.2865041", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744758", "10.1145/1168149.1168158", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2012.213", "10.1109/tvcg.2017.2744738", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2018.2864826", "10.1145/1377966.1377974", "10.1109/apec.2009.4802646", "10.1145/1168149.1168152", "10.1016/j.jss.2008.03.059", "10.1109/vast.2017.8585484", "10.1109/tvcg.2017.2744818", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2865042", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030388", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "year": "2020", "conferenceName": "SciVis", "authors": "Pepe Eulzer;Sabine Bauer;Francis Kilian;Kai Lawonn", "citationCount": "0", "affiliation": "Eulzer, P (Corresponding Author), Univ Jena, Jena, Germany. Eulzer, Pepe; Lawonn, Kai, Univ Jena, Jena, Germany. Bauer, Sabine, Univ Koblenz Landau, Koblenz, Germany. Kilian, Francis, Cath Clin Koblenz Montabaur, Dept Spine Surg, Koblenz, Germany.", "countries": "Germany", "abstract": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. $\\mathrm{By}$ linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. $\\mathrm{in}$ a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.", "keywords": "Medical visualization,bioinformatics,coordinated views,focus and context,biomechanical simulation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030388", "refList": ["10.1109/tvcg.2018.2864903", "10.1177/1473871613510429", "10.1093/ehjqcco/qcz052", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2018.2865076", "10.1055/s-0039-1687862", "10.1109/visual.1990.146375", "10.1109/tvcg.2017.2744198", "10.1016/j.ijmedinf.2014.10.001", "10.1109/tvcg.2013.124", "10.1016/j.jacc", "10.1111/cgf.13167", "10.17705/1thci.00055", "10.1136/bmjqs.2009.037895", "10.1109/tvcg.2013.238", "10.1109/tvcg.2018.2865240", "10.1186/1471-2261-6-34", "10.1109/tvcg.2019.2934264", "10.1109/tvcg.2013.200", "10.1109/tvcg.2011.209", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467091", "10.1136/bmjopen-2019-033208", "10.1109/beliv.2018.8634027", "10.1109/tvcg.2012.213", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2015.2467325", "10.1145/2133806.2133821", "10.1145/1806799.1806866", "10.1108/02635570610688869", "10.1002/hbm.20701", "10.1561/1100000039", "10.1145/3025453.3025645", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934661", "title": "Visual Analysis of High-Dimensional Event Sequence Data via Dynamic Hierarchical Aggregation", "year": "2019", "conferenceName": "VAST", "authors": "David Gotz;Jonathan Zhang;Wenyuan Wang;Joshua Shrestha;David Borland", "citationCount": "3", "affiliation": "Gotz, D (Corresponding Author), Univ N Carolina, Sch Informat \\& Lib Sci, Chapel Hill, NC 27515 USA. Gotz, David; Wang, Wenyuan, Univ N Carolina, Sch Informat \\& Lib Sci, Chapel Hill, NC 27515 USA. Zhang, Jonathan, Univ N Carolina, Dept Biostat, Chapel Hill, NC 27515 USA. Shrestha, Joshua, Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27515 USA. Borland, David, Univ N Carolina, RENCI, Chapel Hill, NC 27515 USA.", "countries": "USA", "abstract": "Temporal event data are collected across a broad range of domains, and a variety of visual analytics techniques have been developed to empower analysts working with this form of data. These techniques generally display aggregate statistics computed over sets of event sequences that share common patterns. Such techniques are often hindered, however, by the high-dimensionality of many real-world event sequence datasets which can prevent effective aggregation. A common coping strategy for this challenge is to group event types together prior to visualization, as a pre-process, so that each group can be represented within an analysis as a single event type. However, computing these event groupings as a pre-process also places significant constraints on the analysis. This paper presents a new visual analytics approach for dynamic hierarchical dimension aggregation. The approach leverages a predefined hierarchy of dimensions to computationally quantify the informativeness, with respect to a measure of interest, of alternative levels of grouping within the hierarchy at runtime. This information is then interactively visualized, enabling users to dynamically explore the hierarchy to select the most appropriate level of grouping to use at any individual step within an analysis. Key contributions include an algorithm for interactively determining the most informative set of event groupings for a specific analysis context, and a scented scatter-plus-focus visualization design with an optimization-based layout algorithm that supports interactive hierarchical exploration of alternative event type groupings. We apply these techniques to high-dimensional event sequence data from the medical domain and report findings from domain expert interviews.", "keywords": "Temporal event sequence visualization,visual analytics,hierarchical aggregation,medical informatics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934661", "refList": ["10.1145/937549.937550", "10.1109/infvis.2000.885091", "10.1145/2468356.2468434", "10.1016/j.jbi.2012.01.009", "10.1136/amiajnl-2014-002747", "10.1109/tvcg.2009.108", "10.1145/2702123.2702419", "10.3233/978-1-61499-289-9-1224", "10.13063/2327-9214", "10.1109/vast.2016.7883512", "10.1111/j.1467-8659.2011.01898.x", "10.1109/infvis.2005.1532152", "10.1177/1473871614526077", "10.1016/j.jbi.2014.01.007", "10.1145/2856767.2856779", "10.1038/nrg3208", "10.1001/jama.2014.4228", "10.1109/tvcg.2012.238", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2018.2864886", "10.2307/2983604", "10.1145/3009973", "10.1109/tvcg.2013.200", "10.1109/tvcg.2016.2539960", "10.1200/jco.2010.28.5478", "10.1145/2557500.2557508", "10.1109/tvcg.2014.2346682", "10.1111/cgf.12883", "10.1145/2678025.2701407", "10.1109/vast.2011.6102443", "10.1109/tvcg.2009.84", "10.1109/tvcg.2007.70589", "10.1109/tvcg.2017.2745320", "10.3233/978-1-61499-830-3-1327", "10.1136/jamia.2009.000893", "10.1186/2047-2501-2-3", "10.1145/2890478", "10.2307/2685881", "10.1109/vast.2014.7042487", "10.1561/1100000039", "10.1109/mcg.2016.59", "10.1109/tvcg.2017.2744686", "10.1109/vds.2017.8573439", "10.1016/j.otohns.2010.05.007", "10.1109/tvcg.2014.2346433"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934670", "title": "AirVis: Visual Analytics of Air Pollution Propagation", "year": "2019", "conferenceName": "VAST", "authors": "Zikun Deng;Di Weng;Jiahui Chen;Ren Liu;Zhibin Wang;Jie Bao 0003;Yu Zheng 0004;Yingcai Wu", "citationCount": "6", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Deng, Zikun; Weng, Di; Chen, Jiahui; Liu, Ren; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Wang, Zhibin, Zhejiang Univ, Res Ctr Air Pollut \\& Hlth, Hangzhou, Peoples R China. Bao, Jie; Zheng, Yu, JD Intelligent City Res, Beijing, Peoples R China.", "countries": "China", "abstract": "Air pollution has become a serious public health problem for many cities around the world. To find the causes of air pollution, the propagation processes of air pollutants must be studied at a large spatial scale. However, the complex and dynamic wind fields lead to highly uncertain pollutant transportation. The state-of-the-art data mining approaches cannot fully support the extensive analysis of such uncertain spatiotemporal propagation processes across multiple districts without the integration of domain knowledge. The limitation of these automated approaches motivates us to design and develop AirVis, a novel visual analytics system that assists domain experts in efficiently capturing and interpreting the uncertain propagation patterns of air pollution based on graph visualizations. Designing such a system poses three challenges: a) the extraction of propagation patterns; b) the scalability of pattern presentations; and c) the analysis of propagation processes. To address these challenges, we develop a novel pattern mining framework to model pollutant transportation and extract frequent propagation patterns efficiently from large-scale atmospheric data. Furthermore, we organize the extracted patterns hierarchically based on the minimum description length (MDL) principle and empower expert users to explore and analyze these patterns effectively on the basis of pattern topologies. We demonstrated the effectiveness of our approach through two case studies conducted with a real-world dataset and positive feedback from domain experts.", "keywords": "Air pollution propagation,pattern mining,graph visualization", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934670", "refList": ["10.1109/tvcg.2016.2598919", "10.1093/bib/bbr069", "10.1145/2487575.2488188", "10.1109/tvcg.2013.193", "10.1016/j.atmosenv.2014.12.011", "10.1109/tvcg.2013.226", "10.1109/tvcg.2018.2864503", "10.1109/tvcg.2015.2468111", "10.1109/icicta.2015.183", "10.1016/j.atmosenv.2014.05.039", "10.1111/cgf.12791", "10.1111/j.1467-8659.2009.01451.x", "10.1111/j.1467-8659.2011.01898.x", "10.5194/acp-12-5031-2012", "10.1016/j.atmosenv.2008.05.053", "10.1109/tvcg.2016.2535234", "10.1016/j.atmosres.2014.12.003", "10.1109/tvcg.2015.2467194", "10.1109/tvcg.2013.263", "10.1109/icdm.2002.1184038", "10.1145/2783258.2788573", "10.1109/tvcg.2018.2865149", "10.1109/tbdata.2017.2723899", "10.1109/tvcg.2012.311", "10.1109/vl.1996.545307", "10.1007/s12650-018-0481-7", "10.1016/j.envpol.2007.06.012", "10.3155/1047-3289.61.6.660", "10.1080/13658810701349037", "10.1145/3097983.3098090", "10.1109/tvcg.2007.70523", "10.1115/1.2128636", "10.1109/tvcg.2015.2467619", "10.3978/j.issn.2072-1439.2016.01.19", "10.1109/tkde.2005.127", "10.1017/s0269888912000331", "10.2312/conf/eg2013/stars/039-063", "10.1109/tvcg.2014.2346271", "10.1175/bams-d-14-00110.1", "10.1016/0005-1098(78)90005-5", "10.1007/s10618-006-0044-8", "10.1109/tvcg.2018.2865126", "10.2307/1912791", "10.3390/su6085322", "10.1007/s12650-018-0489-z", "10.2312/eurovisstar.20151109", "10.1109/tvcg.2011.181", "10.1126/science.298.5594.824", "10.1162/jmlr.2003.3.4-5.951", "10.1109/asonam.2014.6921638", "10.1111/j.1467-8659.2008.01213.x", "10.1038/s41598-017-18107-1", "10.1109/tvcg.2018.2865041", "10.1109/tits.2019.2901117", "10.1038/srep20668", "10.1109/tvcg.2012.265", "10.1109/tpami.2016.2608884", "10.1109/tvcg.2012.213", "10.1145/1376616.1376661", "10.1007/s00521-019-04567-1", "10.1109/tvcg.2017.2745083", "10.1126/science.243.4892.745", "10.1109/tvcg.2018.2864826", "10.1109/tnn.2003.820440", "10.1109/tvcg.2016.2598885", "10.1145/3219819.3219822", "10.1073/pnas.1502596112", "10.1016/j.envsoft.2009.01.004", "10.1002/pmic.200700095", "10.1145/2254556.2254651", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2011.202"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028958", "title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Tiankai Xie;Yuxin Ma;Hanghang Tong;My T. Thai;Ross Maciejewski", "citationCount": "0", "affiliation": "Xie, TK (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Xie, Tiankai; Ma, Yuxin; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. Tong, Hanghang, Univ Illinois, Urbana, IL USA. Thai, My T., Univ Florida, Gainesville, FL 32611 USA.", "countries": "USA", "abstract": "Graph mining plays a pivotal role across a number of disciplines, and a variety of algorithms have been developed to answer who/what type questions. For example, what items shall we recommend to a given user on an e-commerce platform? The answers to such questions are typically returned in the form of a ranked list, and graph-based ranking methods are widely used in industrial information retrieval settings. However, these ranking algorithms have a variety of sensitivities, and even small changes in rank can lead to vast reductions in product sales and page hits. As such, there is a need for tools and methods that can help model developers and analysts explore the sensitivities of graph ranking algorithms with respect to perturbations within the graph structure. In this paper, we present a visual analytics framework for explaining and exploring the sensitivity of any graph-based ranking algorithm by performing perturbation-based what-if analysis. We demonstrate our framework through three case studies inspecting the sensitivity of two classic graph-based ranking algorithms (PageRank and HITS) as applied to rankings in political news media and social networks.", "keywords": "Graph-based ranking,sensitivity analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028958", "refList": ["10.1109/wsc.2017.8247800", "10.1023/a:1022649401552", "10.1515/1559-0410.11416", "10.1109/tvcg.2016.2598919", "10.1177/1473871611416549", "10.1109/tvcg.2019.2934630", "10.1140/epjds29", "10.1109/tvcg.2019.2934670", "10.1016/j.eswa.2015.09.004", "10.1145/2702123.2702509", "10.1016/j.visinf.2018.12.001", "10.2307/3002000", "10.1109/tvcg.2019.2934399", "10.1007/s41060-016-0032-z", "10.1111/cgf.13198", "10.14778/2350229.2350254", "10.1145/2939672.2939764", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/vast.2017.8585647", "10.1007/bf01187020", "10.1109/icdm.2015.26", "10.1145/2362383.2362387", "10.1177/0049124104268644", "10.1109/vast.2011.6102442", "10.1109/infvis.2003.1249025", "10.1109/tvcg.2018.2864475", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2015.2467691", "10.1111/cgf.13210", "10.1214/aos/1176344136", "10.1109/tvcg.2015.2424872", "10.1016/j.visinf.2018.09.001", "10.1177/089443939100900106", "10.1109/tvcg.2015.2467931", "10.1162/neco.1997.9.8.1735", "10.1007/s11162-011-9241-4", "10.1111/cgf.13680", "10.1145/3065386", "10.1109/tvcg.2018.2864889", "10.1177/003804070808100402", "10.1109/icdm.2010.62", "10.1038/s41598-020-59669-x", "10.1162/153244303321897717", "10.1109/tvcg.2019.2934619", "10.1007/bf00356088", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030410", "title": "Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Wei Zeng 0002;Chengqiao Lin;Juncong Lin;Jincheng Jiang;Jiazhi Xia;Cagatay Turkay;Wei Chen", "citationCount": "0", "affiliation": "Lin, JC (Corresponding Author), Xiamen Univ, Xiamen, Peoples R China. Zeng, Wei; Jiang, Jincheng, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Lin, Chengqiao; Lin, Juncong, Xiamen Univ, Xiamen, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Turkay, Cagatay, Univ Warwick, Coventry, W Midlands, England. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China.", "countries": "China;England", "abstract": "Deep learning methods are being increasingly used for urban traffic prediction where spatiotemporal traffic data is aggregated into sequentially organized matrices that are then fed into convolution-based residual neural networks. However, the widely known modifiable areal unit problem within such aggregation processes can lead to perturbations in the network inputs. This issue can significantly destabilize the feature embeddings and the predictions - rendering deep networks much less useful for the experts. This paper approaches this challenge by leveraging unit visualization techniques that enable the investigation of many-to-many relationships between dynamically varied multi-scalar aggregations of urban traffic data and neural network predictions. Through regular exchanges with a domain expert, we design and develop a visual analytics solution that integrates 1) a Bivariate Map equipped with an advanced bivariate colormap to simultaneously depict input traffic and prediction errors across space, 2) a Moran's I Scatterplot that provides local indicators of spatial association analysis, and 3) a Multi-scale Attribution View that arranges non-linear dot plots in a tree layout to promote model analysis and comparison across scales. We evaluate our approach through a series of case studies involving a real-world dataset of Shenzhen taxi trips, and through interviews with domain experts. We observe that geographical scale variations have important impact on prediction performances, and interactive visual exploration of dynamically varying inputs and outputs benefit experts in the development of deep traffic prediction models.", "keywords": "MAUP,traffic prediction,deep learning,model diagnostic,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030410", "refList": ["10.1038/srep26377", "10.1109/mcg.2011.88", "10.1080/13658816.2015.1119279", "10.1109/tvcg.2013.226", "10.1109/pacificvis.2011.5742387", "10.1038/s41467-017-01882-w", "10.1109/tvcg.2019.2934670", "10.1111/j.1467-8659.2009.01440.x", "10.1111/cgf.13712", "10.1016/j.compenvurbsys.2008.09.006", "10.1109/pacificvis.2014.50", "10.1109/tvcg.2018.2816219", "10.1109/tvcg.2016.2535234", "10.1109/tvcg.2014.2346893", "10.3390/ijgi8080344", "10.1109/tvcg.2013.246", "10.1007/s10940-005-9003-6", "10.1016/j.compenvurbsys.2008.05.001", "10.1007/s10661-019-7831-3", "10.1111/j.1538-4632.2007.00699.x", "10.1016/j.aap.2016.08.015", "10.1080/13658816.2018.1541177", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2011.181", "10.1137/090759069", "10.1109/pacificvis.2011.5742390", "10.1214/10-aos799", "10.1109/tits.2017.2683539", "10.1109/tits.2015.2436897", "10.3390/ijerph16071150", "10.1109/tvcg.2009.145", "10.1109/tvcg.2012.265", "10.1080/10106049.2017.1404140", "10.3390/ijgi8020063", "10.3390/info6020134", "10.1080/13658816.2014.955027", "10.1109/tits.2016.2639320", "10.2307/143141", "10.1109/tvcg.2016.2598432"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030370", "title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups", "year": "2020", "conferenceName": "VAST", "authors": "Yating Lin;Kamkwai Wong;Yong Wang;Rong Zhang;Bo Dong;Huamin Qu;Qinghua Zheng", "citationCount": "0", "affiliation": "Lin, YT (Corresponding Author), Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian, Shaanxi, Peoples R China. Lin, Yating; Zheng, Qinghua, Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian, Shaanxi, Peoples R China. Wong, Kamkwai; Wang, Yong; Zhang, Rong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Dong, Bo, Xi An Jiao Tong Univ, Natl Engn Lab Big Data Analyt, Xian, Shaanxi, Peoples R China.", "countries": "China", "abstract": "Tax evasion is a serious economic problem for many countries, as it can undermine the government's tax system and lead to an unfair business competition environment. Recent research has applied data analytics techniques to analyze and detect tax evasion behaviors of individual taxpayers. However, they have failed to support the analysis and exploration of the related party transaction tax evasion (RPTTE) behaviors (e.g., transfer pricing), where a group of taxpayers is involved. In this paper, we present TaxThemis, an interactive visual analytics system to help tax officers mine and explore suspicious tax evasion groups through analyzing heterogeneous tax-related data. A taxpayer network is constructed and fused with the respective trade network to detect suspicious RPTTE groups. Rich visualizations are designed to facilitate the exploration and investigation of suspicious transactions between related taxpayers with profit and topological data analysis. Specifically, we propose a calendar heatmap with a carefully-designed encoding scheme to intuitively show the evidence of transferring revenue through related party transactions. We demonstrate the usefulness and effectiveness of TaxThemis through two case studies on real-world tax-related data and interviews with domain experts.", "keywords": "Visual Analytics,Tax Network,Tax Evasion Detection,Anomaly detection,Multidimensional data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030370", "refList": ["10.1111/cgf.12886", "10.2307/2277827", "10.1109/tvcg.2010.44", "10.1109/tits.2014.2315794", "10.1109/tvcg.2019.2934670", "10.1038/s41467-019-08987-4", "10.1111/cgf.12920", "10.1109/vast.2017.8585721", "10.1080/15230406.2015.1093431", "10.1109/tvcg.2018.2843369", "10.1038/srep01001", "10.1109/tvcg.2017.2744018", "10.1109/tvcg.2017.2744159", "10.1068/b130199p", "10.1109/tvcg.2009.143", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2019.2892483", "10.1109/pacificvis.2017.8031583", "10.1109/pacificvis48177.2020.2785", "10.2307/2686111", "10.1109/tvcg.2015.2467199", "10.1111/cgf.12114", "10.1109/tvcg.2018.2865126", "10.1111/j.1538-4632.1996.tb00936.x", "10.1109/tvcg.2019.2934619", "10.2307/2332142", "10.1007/978-3-319-10590-1\\_53", "10.1109/cvpr.2016.485", "10.1109/cvpr.2017.17", "10.2307/2986645", "10.1109/tvcg.2014.2346321", "10.1017/s0140525x16001837", "10.1109/tvcg.2016.2598541", "10.1371/journal.pone.0207377", "10.1109/tvcg.2014.2346265", "10.1007/s4095-020-0191-7", "10.3141/1644-14", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2016.90", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2785807", "10.1109/tvcg.2017.2744358", "10.1111/j.1538-4632.1995.tb00338.x", "10.1080/03081068808717359", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030469", "title": "Topology Density Map for Urban Data Visualization and Analysis", "year": "2020", "conferenceName": "VAST", "authors": "Zezheng Feng;Haotian Li;Wei Zeng 0004;Shuang-Hua Yang;Huamin Qu", "citationCount": "0", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Feng, Zezheng; Li, Haotian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yang, Shuang-Hua, Southern Univ Sci \\& Technol, Shenzhen, Peoples R China.", "countries": "China", "abstract": "Density map is an effective visualization technique for depicting the scalar field distribution in 2D space. Conventional methods for constructing density maps are mainly based on Euclidean distance, limiting their applicability in urban analysis that shall consider road network and urban traffic. In this work, we propose a new method named Topology Density Map, targeting for accurate and intuitive density maps in the context of urban environment. Based on the various constraints of road connections and traffic conditions, the method first constructs a directed acyclic graph (DAG) that propagates nonlinear scalar fields along 1D road networks. Next, the method extends the scalar fields to a 2D space by identifying key intersecting points in the DAG and calculating the scalar fields for every point, yielding a weighted Voronoi diagram like effect of space division. Two case studies demonstrate that the Topology Density Map supplies accurate information to users and provides an intuitive visualization for decision making. An interview with domain experts demonstrates the feasibility, usability, and effectiveness of our method.", "keywords": "Density map,network topology,urban data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030469", "refList": ["10.1109/vast.2009.5332584", "10.1109/tvcg.2013.193", "10.1080/03081060.2013.844903", "10.1109/tvcg.2018.2864503", "10.1145/2702123.2702419", "10.1109/tvcg.2019.2934670", "10.1109/tits.2015.2496783", "10.1177/1473871615581216", "10.3141/1617-02", "10.1145/2024156.2024169", "10.1111/cgf.13712", "10.1016/j.ejor.2007.02.005", "10.1109/tvcg.2014.2346893", "10.1007/11871842\\_29", "10.1109/vast.2010.5652478", "10.1016/j.visinf.2019.10.002", "10.1109/tvcg.2016.2616404", "10.1109/vl.1996.545307", "10.1145/2629592", "10.1155/2018/2696037", "10.1061/(asce)0733-947x(1998)124:4(368", "10.3141/1899-21", "10.1023/a:1026123329433", "10.1109/mcg.2010.79", "10.1057/palgrave.ivs.9500174", "10.1109/tcyb.2019.2963681", "10.1109/tvcg.2015.2467554", "10.1111/cgf.12114", "10.1145/2814575", "10.1016/j.jcps.2014.08.002", "10.1109/2945.981847", "10.1080/03052150210909", "10.1109/tciaig.2012.2186810", "10.1109/tits.2017.2683539", "10.1109/iv.2004.1320137", "10.1016/0377-2217(80)90126-5", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2015.2467196", "10.1145/3097983.3098056", "10.1007/s11432-018-9801-4", "10.1109/vast.2014.7042490", "10.1061/(asce)0733-947x(2006)132:2(122", "10.1016/j.tra.2008.03.011", "10.1109/tits.2014.2298892", "10.1016/j.trb.2005.12.003", "10.1007/bf01840357", "10.1109/vast.2011.6102454", "10.1109/tvcg.2013.145", "10.1007/bf02289588", "10.1109/pacificvis.2014.56", "10.1109/mcg.2018.053491730", "10.1109/tvcg.2009.111", "10.1057/palgrave.ivs.9500184", "10.1109/tvcg.2013.173", "10.1109/tvcg.2016.2598432", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934275", "title": "PlanningVis: A Visual Analytics Approach to Production Planning in Smart Factories", "year": "2019", "conferenceName": "VAST", "authors": "Dong Sun;Renfei Huang;Yuanzhe Chen;Yong Wang;Jia Zeng;Mingxuan Yuan;Ting-Chuen Pong;Huamin Qu", "citationCount": "0", "affiliation": "Wang, Y (Corresponding Author), Hong Kong Univ Sccience \\& Technol, Hong Kong, Peoples R China. Sun, Dong; Huang, Renfei; Wang, Yong; Pong, Ting-Chuen; Qu, Huamin, Hong Kong Univ Sccience \\& Technol, Hong Kong, Peoples R China. Chen, Yuanzhe; Zeng, Jia; Yuan, Mingxuan, Huawei Technol Co Ltd, Noahs Ark Lab, Shenzhen, Peoples R China.", "countries": "China", "abstract": "Production planning in the manufacturing industry is crucial for fully utilizing factory resources (e.g., machines, raw materials and workers) and reducing costs. With the advent of industry 4.0, plenty of data recording the status of factory resources have been collected and further involved in production planning, which brings an unprecedented opportunity to understand, evaluate and adjust complex production plans through a data-driven approach. However, developing a systematic analytics approach for production planning is challenging due to the large volume of production data, the complex dependency between products, and unexpected changes in the market and the plant. Previous studies only provide summarized results and fail to show details for comparative analysis of production plans. Besides, the rapid adjustment to the plan in the case of an unanticipated incident is also not supported. In this paper, we propose PlanningVis, a visual analytics system to support the exploration and comparison of production plans with three levels of details: a plan overview presenting the overall difference between plans, a product view visualizing various properties of individual products, and a production detail view displaying the product dependency and the daily production details in related factories. By integrating an automatic planning algorithm with interactive visual explorations, PlanningVis can facilitate the efficient optimization of daily production planning as well as support a quick response to unanticipated incidents in manufacturing. Two case studies with real-world data and carefully designed interviews with domain experts demonstrate the effectiveness and usability of PlanningVis.", "keywords": "Production Planning,Time Series Data,Comparative Analysis,Visual Analytics,Smart Factory,Industry 4.0", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934275", "refList": ["10.1109/infvis.2002.1173149", "10.1109/tvcg.2016.2614803", "10.1177/1473871611416549", "10.1109/wsc.2010.5679027", "10.1016/j.jvlc.2017.11.004", "10.1559/152304009788188808", "10.1109/tvcg.2018.2796591", "10.1137/0201010", "10.1016/j.cor.2008.10.009", "10.1109/tvcg.2011.185", "10.1109/tvcg.2018.2865076", "10.1016/s0377-2217(00)00196-x", "10.1016/j.compchemeng.2009.06.007", "10.1016/s0924-0136(03)00520-x", "10.1109/hicss.2013.482", "10.1109/38.536268", "10.1109/mcg.2015.45", "10.1109/iv.2001.942082", "10.1287/mnsc.18.1.12", "10.1109/infvis.2005.1532144", "10.1115/1.4034872", "10.1109/tvcg.2012.225", "10.1016/j.ijpe.2004.11.017", "10.1109/tvcg.2016.2598664", "10.1109/tvcg.2014.2346454", "10.1109/pacificvis.2010.5429613", "10.1109/tvcg.2008.166", "10.1109/tvcg.2015.2467751", "10.1109/infvis.2001.963273", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2010.162", "10.1007/bf02289565", "10.1109/infvis.1999.801851", "10.1109/pacificvis.2018.00026", "10.1115/1.4037479", "10.1016/j.ijpe.2005.09.001", "10.1007/978-0-85729-079-3", "10.1145/1133265.1133348"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934267", "title": "ProtoSteer: Steering Deep Sequence Model with Prototypes", "year": "2019", "conferenceName": "VAST", "authors": "Yao Ming;Panpan Xu;Furui Cheng;Huamin Qu;Ren Liu", "citationCount": "2", "affiliation": "Ming, Y (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Ming, Yao; Cheng, Furui; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xu, Panpan; Ren, Liu, Bosch Res North Amer, Palo Alto, CA USA.", "countries": "USA;China", "abstract": "Recently we have witnessed growing adoption of deep sequence models (e.g. LSTMs) in many application domains, including predictive health care, natural language processing, and log analysis. However, the intricate working mechanism of these models confines their accessibility to the domain experts. Their black-box nature also makes it a challenging task to incorporate domain-specific knowledge of the experts into the model. In ProtoSteer (Prototype Steering), we tackle the challenge of directly involving the domain experts to steer a deep sequence model without relying on model developers as intermediaries. Our approach originates in case-based reasoning, which imitates the common human problem-solving process of consulting past experiences to solve new problems. We utilize ProSeNet (Prototype Sequence Network), which learns a small set of exemplar cases (i.e., prototypes) from historical data. In ProtoSteer they serve both as an efficient visual summary of the original data and explanations of model decisions. With ProtoSteer the domain experts can inspect, critique, and revise the prototypes interactively. The system then incorporates user-specified prototypes and incrementally updates the model. We conduct extensive case studies and expert interviews in application domains including sentiment analysis on texts and predictive diagnostics based on vehicle fault logs. The results demonstrate that involvements of domain users can help obtain more interpretable models with concise prototypes while retaining similar accuracy.", "keywords": "Sequence Data,Explainable Artificial Intelligence (XAI),Recurrent Neural Networks (RNNs),Prototype Learning", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934267", "refList": ["10.1109/infvis.2000.885091", "10.1145/2468356.2468434", "10.1109/mcg.2018.2878902", "10.1145/2858036.2858107", "10.1145/2702123.2702419", "10.1109/vast.2015.7347682", "10.1109/tvcg.2016.2598797", "10.1145/2939672.2939778", "10.1109/tvcg.2018.2864885", "10.1109/tvcg.2018.2865044", "10.1073/pnas.95.25.14863", "10.1109/tvcg.2016.2539960", "10.1145/3025453.3025456", "10.1145/2557500.2557508", "10.1109/tvcg.2012.225", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2018.2865230", "10.1109/tvcg.2017.2745320", "10.1109/tvcg.2017.2744718", "10.18653/v1/n16-1082", "10.1016/j.neucom.2013.11.045", "10.1109/tvcg.2017.2745083", "10.1609/aimag.v35i4.2513", "10.1007/bf00155578", "10.1007/978-3-319-90403-0\\_17", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2018.2864838", "10.1109/tvcg.2018.2865027", "10.1145/312129.312298", "10.1145/3185517", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13972", "year": "2020", "title": "Boxer: Interactive Comparison of Classifier Results", "conferenceName": "EuroVis", "authors": "Michael Gleicher;Aditya Barve;Xinyi Yu;Florian Heimerl", "citationCount": "0", "affiliation": "Gleicher, M (Corresponding Author), Univ Wisconsin, Madison, WI 53706 USA.\nGleicher, Michael; Barve, Aditya; Yu, Xinyi; Heimerl, Florian, Univ Wisconsin, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Machine learning practitioners often compare the results of different classifiers to help select, diagnose and tune models. We present Boxer, a system to enable such comparison. Our system facilitates interactive exploration of the experimental results obtained by applying multiple classifiers to a common set of model inputs. The approach focuses on allowing the user to identify interesting subsets of training and testing instances and comparing performance of the classifiers on these subsets. The system couples standard visual designs with set algebra interactions and comparative elements. This allows the user to compose and coordinate views to specify subsets and assess classifier performance on them. The flexibility of these compositions allow the user to address a wide range of scenarios in developing and assessing classifiers. We demonstrate Boxer in use cases including model selection, tuning, fairness assessment, and data quality diagnosis.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13972", "refList": ["10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2016.2598838", "10.1007/s10618-014-0368-8", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1145/3287560.3287589", "10.1109/vast.2017.8585721", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2009.128", "10.1109/tvcg.2017.2744018", "10.1080/00994480.2000.10748487", "10.5555/3305890.3306024", "10.1109/iccv.2015.329", "10.1109/tvcg.2013.125", "10.1089/big.2016.0007", "10.1109/memsys.2019.8870817", "10.1145/2939672.2939778", "10.1007/s11104-019-04156-0", "10.1371/journal.pone.0181142", "10.1145/3301275.3302324", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2018.2865044", "10.1023/a:1010933404324", "10.1145/2487575.2487579", "10.1109/tvcg.2013.157", "10.1145/2783258.2788613", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934631", "10.1016/s0304-3800(02)00064-9", "10.1007/s10115-013-0679-x", "10.1109/tvcg.2019.2934267", "10.1007/978-3-319-10590-1\\_53", "10.1109/vast.2017.8585720", "10.1016/0004-3702(80)90021-1", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2009.84", "10.1007/s11263-016-0911-8", "10.1111/cgf.12918", "10.1111/cgf.12373", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1080/00207176808905715", "10.1002/er.3827", "10.1109/tvcg.2014.2346660", "10.1111/cgf.13417", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2017.2744158", "10.1016/b978-0-12-815849-4.00004-9", "10.1097/ede.0b013e3181c30fb2", "10.1111/cgf.13681", "10.1016/j.ejor.2006.04.051", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2016.2598468", "10.9735/2229-3981", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 11}, {"doi": "10.1109/tvcg.2019.2934264", "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;David S. Ebert;Arif Ghafoor", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Karimzadeh, Morteza; Ebert, David S.; Ghafoor, Arif, Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.", "keywords": "Summative evaluation,usefulness,evaluation process,taxonomy,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934264", "refList": ["10.1109/tvcg.2017.2744478", "10.1109/tvcg.2018.2865025", "10.1109/tvcg.2006.85", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2017.2745181", "10.1111/cgf.13677", "10.1109/tvcg.2018.2864844", "10.1109/tvcg.2013.126", "10.1109/tvcg.2018.2864811", "10.1109/infvis.2005.1532147", "10.1177/0956797613504966", "10.1145/2669557.2669579", "10.1109/mcg.2005.102", "10.1109/visual.2003.1250426", "10.1136/bmj.39489.470347.ad", "10.1109/tvcg.2017.2744080", "10.1109/mcg.2009.53", "10.1111/j.1467-8527.2005.00307.x", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2864886", "10.1109/tvcg.2018.2864843", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2018.2865051", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2018.2865026", "10.1007/978-3-540-71080-6\\_6", "10.1109/tvcg.2018.2865020", "10.1177/1473871611407399", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2018.2864526", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2864905", "10.1049/sej.1991.0040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2711030", "10.1109/tvcg.2011.279", "10.1109/vast.2017.8585505", "10.1147/jrd.2010.2042914", "10.1016/s0378-7206(98)00044-5", "10.1145/2993901.2993913", "10.1109/tvcg.2018.2865041", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744758", "10.1145/1168149.1168158", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2012.213", "10.1109/tvcg.2017.2744738", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2018.2864826", "10.1145/1377966.1377974", "10.1109/apec.2009.4802646", "10.1145/1168149.1168152", "10.1016/j.jss.2008.03.059", "10.1109/vast.2017.8585484", "10.1109/tvcg.2017.2744818", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2865042", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030388", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "year": "2020", "conferenceName": "SciVis", "authors": "Pepe Eulzer;Sabine Bauer;Francis Kilian;Kai Lawonn", "citationCount": "0", "affiliation": "Eulzer, P (Corresponding Author), Univ Jena, Jena, Germany. Eulzer, Pepe; Lawonn, Kai, Univ Jena, Jena, Germany. Bauer, Sabine, Univ Koblenz Landau, Koblenz, Germany. Kilian, Francis, Cath Clin Koblenz Montabaur, Dept Spine Surg, Koblenz, Germany.", "countries": "Germany", "abstract": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. $\\mathrm{By}$ linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. $\\mathrm{in}$ a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.", "keywords": "Medical visualization,bioinformatics,coordinated views,focus and context,biomechanical simulation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030388", "refList": ["10.1109/tvcg.2018.2864903", "10.1177/1473871613510429", "10.1093/ehjqcco/qcz052", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2018.2865076", "10.1055/s-0039-1687862", "10.1109/visual.1990.146375", "10.1109/tvcg.2017.2744198", "10.1016/j.ijmedinf.2014.10.001", "10.1109/tvcg.2013.124", "10.1016/j.jacc", "10.1111/cgf.13167", "10.17705/1thci.00055", "10.1136/bmjqs.2009.037895", "10.1109/tvcg.2013.238", "10.1109/tvcg.2018.2865240", "10.1186/1471-2261-6-34", "10.1109/tvcg.2019.2934264", "10.1109/tvcg.2013.200", "10.1109/tvcg.2011.209", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467091", "10.1136/bmjopen-2019-033208", "10.1109/beliv.2018.8634027", "10.1109/tvcg.2012.213", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2015.2467325", "10.1145/2133806.2133821", "10.1145/1806799.1806866", "10.1108/02635570610688869", "10.1002/hbm.20701", "10.1561/1100000039", "10.1145/3025453.3025645", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934609", "title": "VASABI: Hierarchical User Profiles for Interactive Visual User Behaviour Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Phong H. Nguyen;Rafael Henkin;Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Olivier Thonnard;Cagatay Turkay", "citationCount": "1", "affiliation": "Nguyen, PH (Corresponding Author), City Univ London, London, England. Nguyen, Phong H.; Henkin, Rafael; Turkay, Cagatay, City Univ London, London, England. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer IAIS, St Augustin, Germany. Thonnard, Olivier, Amadeus, Issy Les Moulineaux, France.", "countries": "Germany;England;France", "abstract": "User behaviour analytics (UBA) systems offer sophisticated models that capture users' behaviour over time with an aim to identify fraudulent activities that do not match their profiles. Motivated by the challenges in the interpretation of UBA models, this paper presents a visual analytics approach to help analysts gain a comprehensive understanding of user behaviour at multiple levels, namely individual and group level. We take a user-centred approach to design a visual analytics framework supporting the analysis of collections of users and the numerous sessions of activities they conduct within digital applications. The framework is centred around the concept of hierarchical user profiles that are built based on features derived from sessions, as well as on user tasks extracted using a topic modelling approach to summarise and stratify user behaviour. We externalise a series of analysis goals and tasks, and evaluate our methods through use cases conducted with experts. We observe that with the aid of interactive visual hierarchical user profiles, analysts are able to conduct exploratory and investigative analysis effectively, and able to understand the characteristics of user behaviour to make informed decisions whilst evaluating suspicious users and activities.", "keywords": "hierarchical user profiles,user behaviour analytics,visual analytics,cybersecurity", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934609", "refList": ["10.1145/2858036.2858107", "10.1109/vizsec.2018", "10.1136/bcr-2012-007875", "10.1198/10618600152628068", "10.1109/comst.2018.2800740", "10.1109/69.250074", "10.1016/j.jfda.2018.01.012", "10.1145/1502650.1502695", "10.1109/tvcg.2016.2598797", "10.1145/1242572.1242595", "10.1145/3025171.3025184", "10.1109/mcg.2009.49", "10.1057/ivs.2008.31", "10.1109/tvcg.2011.144", "10.1145/2557500.2557508", "10.1145/2671491.2671493", "10.1007/s11665-016-2173-6", "10.1109/jsyst.2014.2358997", "10.1162/jmlr.2003.3.4-5.993", "10.1109/tvcg.2017.2745083", "10.1109/3468.935043", "10.1561/1100000039", "10.1109/mcg.2015.50", "10.1179/000870403235002042"], "wos": 1, "children": [{"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934266", "title": "VASSL: A Visual Analytics Toolkit for Social Spambot Labeling", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;Jieqiong Zhao;David S. Ebert", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Zhao, Jieqiong; Ebert, David S., Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Purdue Univ, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Social media platforms are filled with social spambots. Detecting these malicious accounts is essential, yet challenging, as they continually evolve to evade detection techniques. In this article, we present VASSL, a visual analytics system that assists in the process of detecting and labeling spambots. Our tool enhances the performance and scalability of manual labeling by providing multiple connected views and utilizing dimensionality reduction, sentiment analysis and topic modeling, enabling insights for the identification of spambots. The system allows users to select and analyze groups of accounts in an interactive manner, which enables the detection of spambots that may not be identified when examined individually. We present a user study to objectively evaluate the performance of VASSL users, as well as capturing subjective opinions about the usefulness and the ease of use of the tool.", "keywords": "Spambot,Labeling,Detection,Visual Analytics,Social Media Annotation", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934266", "refList": ["10.7326/0003-4819-110-11-916", "10.1109/tifs.2013.2267732", "10.1111/cgf.12106", "10.1016/j.ins.2013.11.016", "10.1109/tvcg.2017.2752166", "10.1109/vast.2016.7883510", "10.1109/vl.1996.545307", "10.1145/2872518.2889302", "10.1109/tmm.2016.2614220", "10.1145/2818717", "10.1109/tdsc.2017.2681672", "10.1111/cgf.13211", "10.2307/2685478", "10.1109/tvcg.2014.2346920", "10.1109/tdsc.2016.2641441", "10.1109/tvcg.2017.2745080", "10.1109/vast.2012.6400557", "10.1109/asonam.2016.7752287", "10.1109/tvcg.2014.2346922", "10.1111/cgf.13217", "10.2307/249008", "10.1109/tvcg.2017.2711030", "10.1109/mcse.2013.70", "10.1109/tvcg.2015.2467196", "10.1016/j.comcom.2013.04.004", "10.1109/asonam.2014.6921650", "10.1109/mc.2016.183", "10.1126/science.290.5500.2323", "10.1145/3041021.3055135", "10.1162/jmlr.2003.3.4-5.993", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2013.153", "10.1109/iv.2008.89", "10.1109/mcom.2013.6588663", "10.1179/000870403235002042", "10.1145/3047010"], "wos": 1, "children": [{"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 7}, {"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030356", "title": "Visilant: Visual Support for the Exploration and Analytical Process Tracking in Criminal Investigations", "year": "2020", "conferenceName": "VAST", "authors": "Krist\u00edna Z\u00e1kopcanov\u00e1;Marko Reh\u00e1cek;Jozef B\u00e1trna;Daniel Plakinger;Sergej Stoppel;Barbora Kozl\u00edkov\u00e1", "citationCount": "0", "affiliation": "Zakopcanova, K (Corresponding Author), Masaryk Univ, Brno, Czech Republic. Zakopcanova, Kristina; Rehacek, Marko; Batrna, Jozef; Plakinger, Daniel; Kozlikova, Barbora, Masaryk Univ, Brno, Czech Republic. Stoppel, Sergej, Univ Bergen, Bergen, Norway. Stoppel, Sergej, Rainfall AS, Bergen, Norway.", "countries": "Republic;Norway", "abstract": "The daily routine of criminal investigators consists of a thorough analysis of highly complex and heterogeneous data of crime cases. Such data can consist of case descriptions, testimonies, criminal networks, spatial and temporal information, and virtually any other data that is relevant for the case. Criminal investigators work under heavy time pressure to analyze the data for relationships, propose and verify several hypotheses, and derive conclusions, while the data can be incomplete or inconsistent and is changed and updated throughout the investigation, as new findings are added to the case. Based on a four-year intense collaboration with criminalists, we present a conceptual design for a visual tool supporting the investigation workflow and Visilant, a web-based tool for the exploration and analysis of criminal data guided by the proposed design. Visilant aims to support namely the exploratory part of the investigation pipeline, from case overview, through exploration and hypothesis generation, to the case presentation. Visilant tracks the reasoning process and as the data is changing, it informs investigators which hypotheses are affected by the data change and should be revised. The tool was evaluated by senior criminology experts within two sessions and their feedback is summarized in the paper. Additional supplementary material contains the technical details and exemplary case study.", "keywords": "Criminal investigation,visualization,network,exploration,interaction,tracking,diagram", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030356", "refList": ["10.1145/1822348.1822349", "10.1109/vast.2009.5332595", "10.1145/3025453.3025819", "10.1109/vast.2016.7883512", "10.1109/infvis.2000.885094", "10.1080/10508406.2013.836655", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2468151", "10.1186/1753-6561-8-s2-s9", "10.1109/tvcg.2013.200", "10.1109/tvcg.2015.2465151", "10.1186/1753-6561-8-s2-s8", "10.1016/j.intcom.2012.01.003", "10.1145/2678025.2701407", "10.1109/tvcg.2017.2745320", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1007/s40593-015-0056-x", "10.1145/2818048.2820011", "10.1007/978-3-319-93843-1\\_25", "10.1109/tvcg.2011.179", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13697", "year": "2019", "title": "ChronoCorrelator: Enriching Events with Time Series", "conferenceName": "EuroVis", "authors": "M. A. M. M. van Dortmont;Stef van den Elzen;Jarke J. van Wijk", "citationCount": "0", "affiliation": "van Dortmont, MAMM (Corresponding Author), Eindhoven Univ Technol, Eindhoven, Netherlands.\nvan Dortmont, MAMM (Corresponding Author), SynerScope BV, Helvoirt, Netherlands.\nvan Dortmont, M. A. M. M.; van Wijk, J. J., Eindhoven Univ Technol, Eindhoven, Netherlands.\nvan Dortmont, M. A. M. M.; van den Elzen, S., SynerScope BV, Helvoirt, Netherlands.", "countries": "Netherlands", "abstract": "Event sequences and time series are widely recorded in many application domains; examples are stock market prices, electronic health records, server operation and performance logs. Common goals for recording are monitoring, root cause analysis and predictive analytics. Current analysis methods generally focus on the exploration of either event sequences or time series. However, deeper insights are gained by combining both. We present a visual analytics approach where users can explore both time series and event data simultaneously, combining visualization, automated methods and human interaction. We enable users to iteratively refine the visualization. Correlations between event sequences and time series can be found by means of an interactive algorithm, which also computes the presence of monotonic effects. We illustrate the effectiveness of our method by applying it to real world and synthetic data sets.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13697", "refList": ["10.1109/wise.2000.882407", "10.1136/amiajnl-2011-000463", "10.2307/2289012", "10.1109/tvcg.2013.126", "10.1145/2598153.2598172", "10.1109/vl.1996.545307", "10.1109/infvis.1999.801851", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2011.195", "10.1117/12.2039639", "10.1109/tvcg.2013.124", "10.1109/infvis.2005.1532144", "10.1109/pacificvis.2011.5742371", "10.1109/tvcg.2013.200", "10.1109/tvcg.2016.2539960", "10.1007/978-3-642-25364-5\\_22", "10.1016/j.artmed.2005.03.001", "10.1161/01.cir.101.23.e215", "10.1016/j.intcom.2012.01.003", "10.1109/tvcg.2007.70589", "10.1145/1882992.1883001", "10.1057/palgrave.ivs.9500061", "10.1109/tvcg.2008.166", "10.1038/sdata.2016.35", "10.2307/2682899", "10.1109/tvcg.2012.213", "10.1109/infvis.2001.963273", "10.1109/wsc.2003.1261490", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2015.2467325", "10.1561/1100000039", "10.1111/cgf.12653", "10.1109/tvcg.2011.179", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 73}, {"doi": "10.1109/tvcg.2018.2864844", "title": "A Visual Analytics Framework for Spatiotemporal Trade Network Analysis", "year": "2018", "conferenceName": "VAST", "authors": "Hong Wang;Yafeng Lu;Shade T. Shutters;Michael Steptoe;Feng Wang 0012;Steven Landis;Ross Maciejewski", "citationCount": "2", "affiliation": "Wang, H (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Wang, Hong; Lu, Yafeng; Shutters, Shade T.; Steptoe, Michael; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. Landis, Steven, Univ Nevada, Reno, NV 89557 USA. Wang, Feng, GE Global Res, Niskayuna, NY USA.", "countries": "USA", "abstract": "Economic globalization is increasing connectedness among regions of the world, creating complex interdependencies within various supply chains. Recent studies have indicated that changes and disruptions within such networks can serve as indicators for increased risks of violence and armed conflicts. This is especially true of countries that may not be able to compete for scarce commodities during supply shocks. Thus, network-induced vulnerability to supply disruption is typically exported from wealthier populations to disadvantaged populations. As such, researchers and stakeholders concerned with supply chains, political science, environmental studies, etc. need tools to explore the complex dynamics within global trade networks and how the structure of these networks relates to regional instability. However, the multivariate, spatiotemporal nature of the network structure creates a bottleneck in the extraction and analysis of correlations and anomalies for exploratory data analysis and hypothesis generation. Working closely with experts in political science and sustainability, we have developed a highly coordinated, multi-view framework that utilizes anomaly detection, network analytics, and spatiotemporal visualization methods for exploring the relationship between global trade networks and regional instability. Requirements for analysis and initial research questions to be investigated are elicited from domain experts, and a variety of visual encoding techniques for rapid assessment of analysis and correlations between trade goods, network patterns, and time series signatures are explored. We demonstrate the application of our framework through case studies focusing on armed conflicts in Africa, regional instability measures, and their relationship to international global trade.", "keywords": "Global trade network,anomaly detection,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864844", "refList": ["10.1109/tvcg.2008.135", "10.1111/j.1467-8659.2011.01946.x", "10.1145/1809400.1809403", "10.1109/mcg.2011.88", "10.1016/j.socnet.2010.06.001", "10.1029/2011gl046837", "10.1017/s1876404511200046", "10.1111/cgf.12791", "10.1080/14697688.2014.968356", "10.1080/13658816.2013.868466", "10.1103/physrevlett.93.188701", "10.1037/h0046049", "10.1016/j.ssresearch.2013.06.003", "10.1371/journal.pone.0098247", "10.1016/j.compenvurbsys.2009.01.007", "10.1029/2010wr010307", "10.1371/journal.pone.0097331", "10.1016/j.physa.2011.02.004", "10.1111/j.1467-9701.2011.01360.x", "10.1109/tvcg.2014.2346271", "10.1038/nature10311", "10.1371/journal.pone.0039756", "10.1007/bf01063907", "10.1109/tvcg.2015.2467991", "10.1007/978-3-642-25591-5\\_27", "10.2307/270703", "10.1109/pacificvis.2012.6183572", "10.1109/infvis.2005.1532150", "10.1140/epjb/e2007-00131-6", "10.1007/978-3-319-59096-7\\_9", "10.1109/tvcg.2014.2346682", "10.1126/science.1216556", "10.1017/cbo9780511815478", "10.1126/science.298.5594.824", "10.1371/journal.pone.0040337", "10.1109/vast.2012.6400491", "10.1159/000493316", "10.1109/infvis.1996.559226", "10.1007/978-3-319-03841-4\\_34", "10.1371/journal.pone.0088666", "10.1109/vast.2008.4677356", "10.1177/0022343310378914", "10.1073/pnas.1203176109", "10.1179/000870410x12658023467367", "10.1109/vast.2012.6400485", "10.1073/pnas.1109521108", "10.1002/(sici)1099-131x(199705)16:3", "10.1007/s00191-009-0160-x", "10.1559/152304087783875273", "10.1145/1541880.1541882", "10.1109/tvcg.2016.2598885", "10.1016/j.physa.2005.02.075", "10.1111/j.1467-8659.2009.01450.x", "10.1111/j.1467-9531.2007.00179.x", "10.1126/science.1089167", "10.1109/tvcg.2011.202"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934264", "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;David S. Ebert;Arif Ghafoor", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Karimzadeh, Morteza; Ebert, David S.; Ghafoor, Arif, Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.", "keywords": "Summative evaluation,usefulness,evaluation process,taxonomy,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934264", "refList": ["10.1109/tvcg.2017.2744478", "10.1109/tvcg.2018.2865025", "10.1109/tvcg.2006.85", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2017.2745181", "10.1111/cgf.13677", "10.1109/tvcg.2018.2864844", "10.1109/tvcg.2013.126", "10.1109/tvcg.2018.2864811", "10.1109/infvis.2005.1532147", "10.1177/0956797613504966", "10.1145/2669557.2669579", "10.1109/mcg.2005.102", "10.1109/visual.2003.1250426", "10.1136/bmj.39489.470347.ad", "10.1109/tvcg.2017.2744080", "10.1109/mcg.2009.53", "10.1111/j.1467-8527.2005.00307.x", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2864886", "10.1109/tvcg.2018.2864843", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2018.2865051", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2018.2865026", "10.1007/978-3-540-71080-6\\_6", "10.1109/tvcg.2018.2865020", "10.1177/1473871611407399", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2018.2864526", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2864905", "10.1049/sej.1991.0040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2711030", "10.1109/tvcg.2011.279", "10.1109/vast.2017.8585505", "10.1147/jrd.2010.2042914", "10.1016/s0378-7206(98)00044-5", "10.1145/2993901.2993913", "10.1109/tvcg.2018.2865041", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744758", "10.1145/1168149.1168158", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2012.213", "10.1109/tvcg.2017.2744738", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2018.2864826", "10.1145/1377966.1377974", "10.1109/apec.2009.4802646", "10.1145/1168149.1168152", "10.1016/j.jss.2008.03.059", "10.1109/vast.2017.8585484", "10.1109/tvcg.2017.2744818", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2865042", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030388", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "year": "2020", "conferenceName": "SciVis", "authors": "Pepe Eulzer;Sabine Bauer;Francis Kilian;Kai Lawonn", "citationCount": "0", "affiliation": "Eulzer, P (Corresponding Author), Univ Jena, Jena, Germany. Eulzer, Pepe; Lawonn, Kai, Univ Jena, Jena, Germany. Bauer, Sabine, Univ Koblenz Landau, Koblenz, Germany. Kilian, Francis, Cath Clin Koblenz Montabaur, Dept Spine Surg, Koblenz, Germany.", "countries": "Germany", "abstract": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. $\\mathrm{By}$ linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. $\\mathrm{in}$ a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.", "keywords": "Medical visualization,bioinformatics,coordinated views,focus and context,biomechanical simulation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030388", "refList": ["10.1109/tvcg.2018.2864903", "10.1177/1473871613510429", "10.1093/ehjqcco/qcz052", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2018.2865076", "10.1055/s-0039-1687862", "10.1109/visual.1990.146375", "10.1109/tvcg.2017.2744198", "10.1016/j.ijmedinf.2014.10.001", "10.1109/tvcg.2013.124", "10.1016/j.jacc", "10.1111/cgf.13167", "10.17705/1thci.00055", "10.1136/bmjqs.2009.037895", "10.1109/tvcg.2013.238", "10.1109/tvcg.2018.2865240", "10.1186/1471-2261-6-34", "10.1109/tvcg.2019.2934264", "10.1109/tvcg.2013.200", "10.1109/tvcg.2011.209", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467091", "10.1136/bmjopen-2019-033208", "10.1109/beliv.2018.8634027", "10.1109/tvcg.2012.213", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2015.2467325", "10.1145/2133806.2133821", "10.1145/1806799.1806866", "10.1108/02635570610688869", "10.1002/hbm.20701", "10.1561/1100000039", "10.1145/3025453.3025645", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 5}, {"doi": "10.1109/pacificvis.2017.8031600", "year": "2017", "title": "Exploring controversy via sentiment divergences of aspects in reviews", "conferenceName": "PacificVis", "authors": "Jin Xu;Yubo Tao;Hai Lin;Rongjie Zhu;Yuyu Yan", "citationCount": "6", "affiliation": "Tao, YB (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Zhejiang, Peoples R China.\nXu, Jin; Tao, Yubo; Lin, Hai; Zhu, Rongjie; Yan, Yuyu, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Zhejiang, Peoples R China.", "countries": "China", "abstract": "A visual summary of the controversial aspects of an item enables both customers and marketers to identify and address complaints and concerns about the item effectively. In this paper, we propose a novel visual analytics system, to visually explore when a controversy occurs and the causes behind the controversy via user-generated reviews with text and ratings in various domains, such as restaurants, home goods, and cultural products. Quantitative analysis of the ratings of an item is first applied to characterize the evolution of controversy over time. A novel aspect-extraction method based on hierarchical clustering is proposed to identify aspect-level reasons garnered from review texts that explain why a controversy occurs. Our system allows the user to interactively explore the time-evolving controversy trend, major aspects of reviews, and sentiment divergences of aspects to understand in depth the controversy in reviews. We evaluate the effectiveness of the proposed aspect-extraction method by means of accuracy of aspect identification, the usefulness of our system using three case studies in different domains, and a user study.", "keywords": "I.3.6 {[}Computer Graphics]: Methodology and Techniques-Interaction techniques", "link": "https://doi.org/10.1109/PACIFICVIS.2017.8031600", "refList": ["10.1021/ja202932e", "10.1162/tacl\\_a\\_00134", "10.1109/pacificvis.2016.7465278", "10.1109/tvcg.2013.221", "10.1057/palgrave.ivs.9500171", "10.1057/palcomms.2015.38", "10.1016/j.elerap.2010.04.001", "10.1063/1.4913758", "10.1109/vast.2010.5652931", "10.1109/tvcg.2014.2346920", "10.1509/jmkg.70.3.74", "10.1145/2806416.2806537", "10.1109/tvcg.2010.183", "10.1109/tvcg.2015.2467991", "10.1109/vast.2009.5333919", "10.3115/v1/d14-1110", "10.2307/2284239", "10.1109/icassp.2014.6854511", "10.1162/jmlr.2003.3.4-5.951", "10.1145/1367497.1367627", "10.1145/1134271.1134277", "10.1145/2207676.2208672", "10.18653/v1/n16-1093", "10.1145/2736277.2741099", "10.1007/s12650-014-0246-x", "10.1145/1935826.1935884"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13995", "year": "2020", "title": "GTMapLens: Interactive Lens for Geo-Text Data Browsing on Map", "conferenceName": "EuroVis", "authors": "Chao Ma;Ye Zhao;Shamal Al{-}Dohuki;Jing Yang;Xinyue Ye;Farah Kamw;Md. Amiruzzaman", "citationCount": "0", "affiliation": "Zhao, Y (Corresponding Author), Kent State Univ, Kent, OH 44240 USA.\nMa, Chao; Zhao, Ye; Amiruzzaman, Md, Kent State Univ, Kent, OH 44240 USA.\nAl-Dohuki, Shamal, Univ Duhok, Duhok, Iraq.\nYang, Jing, Univ N Carolina, Charlotte, NC USA.\nYe, Xinyue, New Jersey Inst Technol, Newark, NJ 07102 USA.\nKamw, Farah, Concordia Univ, Ann Arbor, MI USA.", "countries": "USA;Iraq", "abstract": "Data containing geospatial semantics, such as geotagged tweets, travel blogs, and crime reports, associates natural language texts with geographical locations. This paper presents a lens-based visual interaction technique, GTMapLens, to flexibly browse the geo-text data on a map. It allows users to perform dynamic focus+context exploration by using movable lenses to browse geographical regions, find locations of interest, and perform comparative and drill-down studies. Geo-text data is visualized in a way that users can easily perceive the underlying geospatial semantics along with lens moving. Based on a requirement analysis with a cohort of multidisciplinary domain experts, a set of lens interaction techniques are developed including keywords control, path management, context visualization, and snapshot anchors. They allow users to achieve a guided and controllable exploration of geo-text data. A hierarchical data model enables the interactive lens operations by accelerated data retrieval from a geo-text database. Evaluation with real-world datasets is presented to show the usability and effectiveness of GTMapLens.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13995", "refList": ["10.1109/tvcg.2018.2865235", "10.1145/1936652.1936673", "10.1145/2598153.2598200", "10.1145/1456650.1456652", "10.1109/vast.2011.6102456", "10.1145/3290605.3300864", "10.1177/1473871611413180", "10.1109/iv.2011.43", "10.1109/tvcg.2015.2467971", "10.1109/iv.2016.62", "10.1080/13658816.2017.1325488", "10.1111/cgf.12871", "10.1111/cgf.12132", "10.1109/mc.2012.430", "10.1109/tvcg.2016.2598585", "10.1109/tvcg.2011.195", "10.1109/tvcg.2015.2467619", "10.1109/pacificvis.2013.6596122", "10.13140/rg.2.2.36636.59521", "10.1145/3170427.3188506", "10.1016/b978-155860915-0/50040-8", "10.1109/infvis.2003.1249008", "10.1109/tvcg.2018.2850781", "10.1016/j.datak.2006.01.013", "10.1109/tvcg.2015.2467991", "10.1111/gec3.12404", "10.1109/tvcg.2008.149", "10.1109/visual.1998.745317", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2011.176", "10.1016/j.visinf.2018.04.006.2", "10.1111/cgf.13264", "10.1080/13658816.2010.508043", "10.1007/s41651-017-0002-6", "10.1109/tvcg.2009.65", "10.1109/vast.2011.6102498", "10.1145/3025453.3025777", "10.1109/tvcg.2006.138"], "wos": 1, "children": [], "len": 1}], "len": 221}, {"doi": "10.1109/tvcg.2018.2864814", "title": "BitExTract: Interactive Visualization for Extracting Bitcoin Exchange Intelligence", "year": "2018", "conferenceName": "VAST", "authors": "Xuanwu Yue;Xinhuan Shu;Xinyu Zhu;Xinnan Du;Zheqing Yu;Dimitrios Papadopoulos;Siyuan Liu", "citationCount": "7", "affiliation": "Yue, XW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Yue, Xuanwu; Shu, Xinhuan; Zhu, Xinyu; Du, Xinnan; Yu, Zheqing; Papadopoulos, Dimitrios, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Siyuan, Penn State Univ, University Pk, PA 16802 USA.", "countries": "USA;China", "abstract": "The emerging prosperity of cryptocurrencies, such as Bitcoin, has come into the spotlight during the past few years. Cryptocurrency exchanges, which act as the gateway to this world, now play a dominant role in the circulation of Bitcoin. Thus, delving into the analysis of the transaction patterns of exchanges can shed light on the evolution and trends in the Bitcoin market, and participants can gain hints for identifying credible exchanges as well. Not only Bitcoin practitioners but also researchers in the financial domains are interested in the business intelligence behind the curtain. However, the task of multiple exchanges exploration and comparisons has been limited owing to the lack of efficient tools. Previous methods of visualizing Bitcoin data have mainly concentrated on tracking suspicious transaction logs, but it is cumbersome to analyze exchanges and their relationships with existing tools and methods. In this paper, we present BitExTract, an interactive visual analytics system, which, to the best of our knowledge, is the first attempt to explore the evolutionary transaction patterns of Bitcoin exchanges from two perspectives, namely, exchange versus exchange and exchange versus client. In particular, BitExTract summarizes the evolution of the Bitcoin market by observing the transactions between exchanges over time via a massive sequence view. A node-link diagram with ego-centered views depicts the trading network of exchanges and their temporal transaction distribution. Moreover, BitExTract embeds multiple parallel bars on a timeline to examine and compare the evolution patterns of transactions between different exchanges. Three case studies with novel insights demonstrate the effectiveness and usability of our system.", "keywords": "Bitcoin exchange,transaction data,comparative analysis,visual analytics,FinTech", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864814", "refList": ["10.1109/tvcg.2008.117", "10.1109/mcg.2004.28", "10.1111/cgf.12791", "10.1371/journal.pone.0161197", "10.1109/tvcg.2009.180", "10.1089/big.2015.0056", "10.1007/978-3-319-70278-0\\_16", "10.1111/cgf.12931", "10.1371/journal.pone.0163477", "10.1109/vizsec.2015.7312773", "10.1109/passat/socialcom.2011.79", "10.1080/10447318.2010.516722", "10.1109/comst.2016.2535718", "10.1007/978-3-319-06793-3\\_8", "10.1109/tvcg.2011.169", "10.1007/978-3-319-70278-0\\_9", "10.1109/sp.2015.14", "10.1109/vast.2007.4389009", "10.1093/rfs/hhs091", "10.1145/2504730.2504747", "10.1109/tvcg.2014.2346913", "10.1177/1473871613487087"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934660", "title": "sPortfolio: Stratified Visual Analysis of Stock Portfolios", "year": "2019", "conferenceName": "VAST", "authors": "Xuanwu Yue;Jiaxin Bai;Qinhan Liu;Yiyang Tang;Abishek Puri;Ke Li;Huamin Qu", "citationCount": "0", "affiliation": "Yue, XW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Yue, XW (Corresponding Author), Sinovat Ventures AI Inst, Beijing, Peoples R China. Yue, Xuanwu; Bai, Jiaxin; Liu, Qinhan; Tang, Yiyang; Puri, Abishek; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Yue, Xuanwu, Sinovat Ventures AI Inst, Beijing, Peoples R China. Li, Ke, RiceQuant Co Ltd, Shenzhen, Guangdong, Peoples R China.", "countries": "China", "abstract": "Quantitative Investment, built on the solid foundation of robust financial theories, is at the center stage in investment industry today. The essence of quantitative investment is the multi-factor model, which explains the relationship between the risk and return of equities. However, the multi-factor model generates enormous quantities of factor data, through which even experienced portfolio managers find it difficult to navigate. This has led to portfolio analysis and factor research being limited by a lack of intuitive visual analytics tools. Previous portfolio visualization systems have mainly focused on the relationship between the portfolio return and stock holdings, which is insufficient for making actionable insights or understanding market trends. In this paper, we present s Portfolio, which, to the best of our knowledge, is the first visualization that attempts to explore the factor investment area. In particular, sPortfolio provides a holistic overview of the factor data and aims to facilitate the analysis at three different levels: a Risk-Factor level, for a general market situation analysis; a Multiple-Portfolio level, for understanding the portfolio strategies; and a Single-Portfolio level, for investigating detailed operations. The system's effectiveness and usability are demonstrated through three case studies. The system has passed its pilot study and is soon to be deployed in industry.", "keywords": "Stock portfolio,visual analytics,factor investment,financial data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934660", "refList": ["10.1145/3038912.3052569", "10.1109/tvcg.2008.117", "10.1109/iv.2013.31", "10.1111/cgf.12931", "10.1002/asi.20317", "10.1145/312624.312682", "10.1057/palgrave.ivs.9500081", "10.1109/iv.2009.29", "10.1109/iv.2008.80", "10.2312/vissym/eurovis06/195-202", "10.1109/vast.2009.5333920", "10.1016/j.artmed.2005.03.001", "10.1109/iv.2002.1028770", "10.1109/infvis.2003.1249027", "10.1057/palgrave.ivs.9500061", "10.2307/2329112", "10.1016/b978-155860915-0/50046-9", "10.1007/bf03182429", "10.1016/j.cag.2007.01.030", "10.1109/tvcg.2018.2864814", "10.1109/vast.2010.5652530", "10.1126/science.1127647", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2018.2864885", "title": "Visual Progression Analysis of Event Sequence Data", "year": "2018", "conferenceName": "VAST", "authors": "Shunan Guo;Zhuochen Jin;David Gotz;Fan Du;Hongyuan Zha;Nan Cao", "citationCount": "12", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, iDVX Lab, Shanghai, Peoples R China. Guo, Shunan; Zha, Hongyuan, East China Normal Univ, Shanghai, Peoples R China. Jin, Zhuochen; Cao, Nan, Tongji Univ, iDVX Lab, Shanghai, Peoples R China. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA. Du, Fan, Univ Maryland, College Pk, MD 20742 USA.", "countries": "USA;China", "abstract": "Event sequence data is common to a broad range of application domains, from security to health care to scholarly communication. This form of data captures information about the progression of events for an individual entity (e.g., a computer network device; a patient; an author) in the form of a series of time-stamped observations. Moreover, each event is associated with an event type (e.g., a computer login attempt, or a hospital discharge). Analyses of event sequence data have been shown to help reveal important temporal patterns, such as clinical paths resulting in improved outcomes, or an understanding of common career trajectories for scholars. Moreover, recent research has demonstrated a variety of techniques designed to overcome methodological challenges such as large volumes of data and high dimensionality. However, the effective identification and analysis of latent stages of progression, which can allow for variation within different but similarly evolving event sequences, remain a significant challenge with important real-world motivations. In this paper, we propose an unsupervised stage analysis algorithm to identify semantically meaningful progression stages as well as the critical events which help define those stages. The algorithm follows three key steps: (1) event representation estimation, (2) event sequence warping and alignment, and (3) sequence segmentation. We also present a novel visualization system, ET<sup>2</sup>, which interactively illustrates the results of the stage analysis algorithm to help reveal evolution patterns across stages. Finally, we report three forms of evaluation for ET<sup>2</sup>: (1) case studies with two real-world datasets, (2) interviews with domain expert users, and (3) a performance evaluation on the progression analysis algorithm and the visualization design.", "keywords": "Progression Analysis,Visual Analysis,Event Sequence Data", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864885", "refList": ["10.1145/2623330.2623754", "10.1007/978-3-319-06483-3\\_8", "10.1145/2702123.2702419", "10.1109/vast.2016.7883512", "10.1101/gr.75202", "10.1007/978-3-642-53914-5\\_15", "10.1145/1631162.1631169", "10.1023/a:1024992613384", "10.1016/j.jbi.2014.01.007", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1007/978-3-319-63564-4\\_3", "10.1007/s10115-004-0154-9", "10.1016/s1474-4422(09)70299-6", "10.1016/j.jbi.2012.10.001", "10.1109/tvcg.2016.2539960", "10.1145/2557500.2557508", "10.1007/s10044-013-0325-y", "10.1145/775047.775109", "10.1109/tvcg.2014.2346682", "10.3233/ida-2007-11508", "10.1109/tvcg.2017.2745320", "10.1109/tvcg.2011.188", "10.1038/sdata.2016.35", "10.1016/b978-155860915-0/50038-x", "10.1109/tvcg.2017.2745083", "10.1145/2566486.2568044", "10.1073/pnas.052587399", "10.1109/tvcg.2014.2346574", "10.1093/cercor/11.1.1", "10.1109/icde.2017.25", "10.1109/tvcg.2014.2346913", "10.1145/3025453.3025777"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934267", "title": "ProtoSteer: Steering Deep Sequence Model with Prototypes", "year": "2019", "conferenceName": "VAST", "authors": "Yao Ming;Panpan Xu;Furui Cheng;Huamin Qu;Ren Liu", "citationCount": "2", "affiliation": "Ming, Y (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Ming, Yao; Cheng, Furui; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xu, Panpan; Ren, Liu, Bosch Res North Amer, Palo Alto, CA USA.", "countries": "USA;China", "abstract": "Recently we have witnessed growing adoption of deep sequence models (e.g. LSTMs) in many application domains, including predictive health care, natural language processing, and log analysis. However, the intricate working mechanism of these models confines their accessibility to the domain experts. Their black-box nature also makes it a challenging task to incorporate domain-specific knowledge of the experts into the model. In ProtoSteer (Prototype Steering), we tackle the challenge of directly involving the domain experts to steer a deep sequence model without relying on model developers as intermediaries. Our approach originates in case-based reasoning, which imitates the common human problem-solving process of consulting past experiences to solve new problems. We utilize ProSeNet (Prototype Sequence Network), which learns a small set of exemplar cases (i.e., prototypes) from historical data. In ProtoSteer they serve both as an efficient visual summary of the original data and explanations of model decisions. With ProtoSteer the domain experts can inspect, critique, and revise the prototypes interactively. The system then incorporates user-specified prototypes and incrementally updates the model. We conduct extensive case studies and expert interviews in application domains including sentiment analysis on texts and predictive diagnostics based on vehicle fault logs. The results demonstrate that involvements of domain users can help obtain more interpretable models with concise prototypes while retaining similar accuracy.", "keywords": "Sequence Data,Explainable Artificial Intelligence (XAI),Recurrent Neural Networks (RNNs),Prototype Learning", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934267", "refList": ["10.1109/infvis.2000.885091", "10.1145/2468356.2468434", "10.1109/mcg.2018.2878902", "10.1145/2858036.2858107", "10.1145/2702123.2702419", "10.1109/vast.2015.7347682", "10.1109/tvcg.2016.2598797", "10.1145/2939672.2939778", "10.1109/tvcg.2018.2864885", "10.1109/tvcg.2018.2865044", "10.1073/pnas.95.25.14863", "10.1109/tvcg.2016.2539960", "10.1145/3025453.3025456", "10.1145/2557500.2557508", "10.1109/tvcg.2012.225", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2018.2865230", "10.1109/tvcg.2017.2745320", "10.1109/tvcg.2017.2744718", "10.18653/v1/n16-1082", "10.1016/j.neucom.2013.11.045", "10.1109/tvcg.2017.2745083", "10.1609/aimag.v35i4.2513", "10.1007/bf00155578", "10.1007/978-3-319-90403-0\\_17", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2018.2864838", "10.1109/tvcg.2018.2865027", "10.1145/312129.312298", "10.1145/3185517", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13972", "year": "2020", "title": "Boxer: Interactive Comparison of Classifier Results", "conferenceName": "EuroVis", "authors": "Michael Gleicher;Aditya Barve;Xinyi Yu;Florian Heimerl", "citationCount": "0", "affiliation": "Gleicher, M (Corresponding Author), Univ Wisconsin, Madison, WI 53706 USA.\nGleicher, Michael; Barve, Aditya; Yu, Xinyi; Heimerl, Florian, Univ Wisconsin, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Machine learning practitioners often compare the results of different classifiers to help select, diagnose and tune models. We present Boxer, a system to enable such comparison. Our system facilitates interactive exploration of the experimental results obtained by applying multiple classifiers to a common set of model inputs. The approach focuses on allowing the user to identify interesting subsets of training and testing instances and comparing performance of the classifiers on these subsets. The system couples standard visual designs with set algebra interactions and comparative elements. This allows the user to compose and coordinate views to specify subsets and assess classifier performance on them. The flexibility of these compositions allow the user to address a wide range of scenarios in developing and assessing classifiers. We demonstrate Boxer in use cases including model selection, tuning, fairness assessment, and data quality diagnosis.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13972", "refList": ["10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2016.2598838", "10.1007/s10618-014-0368-8", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1145/3287560.3287589", "10.1109/vast.2017.8585721", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2009.128", "10.1109/tvcg.2017.2744018", "10.1080/00994480.2000.10748487", "10.5555/3305890.3306024", "10.1109/iccv.2015.329", "10.1109/tvcg.2013.125", "10.1089/big.2016.0007", "10.1109/memsys.2019.8870817", "10.1145/2939672.2939778", "10.1007/s11104-019-04156-0", "10.1371/journal.pone.0181142", "10.1145/3301275.3302324", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2018.2865044", "10.1023/a:1010933404324", "10.1145/2487575.2487579", "10.1109/tvcg.2013.157", "10.1145/2783258.2788613", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934631", "10.1016/s0304-3800(02)00064-9", "10.1007/s10115-013-0679-x", "10.1109/tvcg.2019.2934267", "10.1007/978-3-319-10590-1\\_53", "10.1109/vast.2017.8585720", "10.1016/0004-3702(80)90021-1", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2009.84", "10.1007/s11263-016-0911-8", "10.1111/cgf.12918", "10.1111/cgf.12373", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1080/00207176808905715", "10.1002/er.3827", "10.1109/tvcg.2014.2346660", "10.1111/cgf.13417", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2017.2744158", "10.1016/b978-0-12-815849-4.00004-9", "10.1097/ede.0b013e3181c30fb2", "10.1111/cgf.13681", "10.1016/j.ejor.2006.04.051", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2016.2598468", "10.9735/2229-3981", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 11}, {"doi": "10.1109/tvcg.2019.2934289", "title": "STBins: Visual Tracking and Comparison of Multiple Data Sequences Using Temporal Binning", "year": "2019", "conferenceName": "VAST", "authors": "Ji Qi;Vincent Bloemen;Shihan Wang;Jarke J. van Wijk;Huub van de Wetering", "citationCount": "0", "affiliation": "Qi, J (Corresponding Author), Eindhoven Univ Technol, Dept Math \\& Comp Sci, Eindhoven, Netherlands. Qi, Ji; van Wijk, Jarke; van de Wetering, Huub, Eindhoven Univ Technol, Dept Math \\& Comp Sci, Eindhoven, Netherlands. Bloemen, Vincent, Univ Twente, Fac Elect Engn Math Comp Sci, Enschede, Netherlands. Wang, Shihan, Univ Amsterdam, Amsterdam, Netherlands.", "countries": "Netherlands", "abstract": "While analyzing multiple data sequences, the following questions typically arise: how does a single sequence change over time, how do multiple sequences compare within a period, and how does such comparison change over time. This paper presents a visual technique named STBins to answer these questions. STBins is designed for visual tracking of individual data sequences and also for comparison of sequences. The latter is done by showing the similarity of sequences within temporal windows. A perception study is conducted to examine the readability of alternative visual designs based on sequence tracking and comparison tasks. Also, two case studies based on real-world datasets are presented in detail to demonstrate usage of our technique.", "keywords": "Visualization,time series data,data sequence", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934289", "refList": ["10.1109/tvcg.2012.189", "10.1177/1473871611416549", "10.1145/2702123.2702419", "10.1145/2483977.2483989", "10.1109/vast.2016.7883512", "10.1109/tvcg.2016.2598797", "10.1016/j.eswa.2016.03.050", "10.1109/tvcg.2014.2346433", "10.1109/tvcg.2013.124", "10.1111/j.1469-8137.1912.tb05611.x", "10.1186/1753-6561-8-s2-s9", "10.1145/2851141.2851161", "10.1109/tvcg.2011.239", "10.1109/tvcg.2018.2864885", "10.2307/3001913", "10.1007/978-0-85729-079-3", "10.1109/tvcg.2009.117", "10.1109/tvcg.2013.200", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2016.2539960", "10.1145/2557500.2557508", "10.1109/tvcg.2012.225", "10.1186/1753-6561-8-s2-s8", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1016/j.intcom.2012.01.003", "10.1111/cgf.13264", "10.1109/tvcg.2011.232", "10.1111/cgf.12653", "10.1002/asi.21489", "10.1109/tvcg.2014.2346919", "10.1145/2254556.2254670"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13965", "year": "2020", "title": "Bombalytics: Visualization of Competition and Collaboration Strategies of Players in a Bomb Laying Game", "conferenceName": "EuroVis", "authors": "Shivam Agarwal;G{\\\"{u}}nter Wallner;Fabian Beck", "citationCount": "0", "affiliation": "Agarwal, S (Corresponding Author), Univ Duisburg Essen, Duisburg, Germany.\nAgarwal, Shivam; Beck, Fabian, Univ Duisburg Essen, Duisburg, Germany.\nWallner, Gunter, Eindhoven Univ Technol, Eindhoven, Netherlands.\nWallner, Gunter, Univ Appl Arts Vienna, Vienna, Austria.", "countries": "Germany;Austria;Netherlands", "abstract": "Competition and collaboration form complex interaction patterns between the agents and objects involved. Only by understanding these interaction patterns, we can reveal the strategies the participating parties applied. In this paper, we study such competition and collaboration behavior for a computer game. Serving as a testbed for artificial intelligence, the multiplayer bomb laying game Pommerman provides a rich source of advanced behavior of computer agents. We propose a visualization approach that shows an overview of multiple games, with a detailed timeline-based visualization for exploring the specifics of each game. Since an analyst can only fully understand the data when considering the direct and indirect interactions between agents, we suggest various visual encodings of these interactions. Based on feedback from expert users and an application example, we demonstrate that the approach helps identify central competition strategies and provides insights on collaboration.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13965", "refList": ["10.1145/1822348.1822349", "10.1016/j.cag.2013.11.010", "10.1145/2793107.2793112", "10.1109/tvcg.2019.2934630", "10.1145/3343055.3360747", "10.1016/j.intcom.2010.04.004", "10.1109/cig.2019.8848091", "10.1007/s11554-013-0347-0", "10.1007/s12650-019-00566-5", "10.1109/bigdata.2018.8622571", "10.1007/978-3-319-63519-4", "10.1109/tciaig.2014.2365470", "10.1016/j.entcom.2013.02.002", "10.1609/aimag.v34i3.2492", "10.1145/3311350.3357716", "10.1109/tvcg.2018.2864885", "10.1177/1473871617718377", "10.1109/tvcg.2018.2864504", "10.1111/cgf.12919", "10.1109/tvcg.2019.2934243", "10.1109/tvcg.2018.2859969", "10.1145/3235765.3235812", "10.1109/tvcg.2017.2745320", "10.1109/tvcg.2016.2598415", "10.1109/tciaig.2012.2188528", "10.1109/iscas.2019.8702471", "10.2307/350902", "10.1109/mc.2018.2890217", "10.1145/3027063.3053146", "10.1111/cgf.13436", "10.1145/3130859.3131439", "10.1016/j.compedu.2011.11.015"], "wos": 1, "children": [], "len": 1}], "len": 17}, {"doi": "10.1109/tvcg.2018.2864899", "title": "iStoryline: Effective Convergence to Hand-drawn Storylines", "year": "2018", "conferenceName": "InfoVis", "authors": "Tan Tang;Sadia Rubab;Jiewen Lai;Weiwei Cui;Lingyun Yu;Yingcai Wu", "citationCount": "4", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ \\& Alibaba Zhejiang Univ Joint Inst, State Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Tang, Tan; Rubab, Sadia; Lai, Jiewen; Wu, Yingcai, Zhejiang Univ \\& Alibaba Zhejiang Univ Joint Inst, State Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Cui, Weiwei, Microsoft Res, Redmond, WA USA. Yu, Lingyun, Univ Groningen, Bernoulli Inst, Groningen, Netherlands.", "countries": "USA;China;Netherlands", "abstract": "Storyline visualization techniques have progressed significantly to generate illustrations of complex stories automatically. However, the visual layouts of storylines are not enhanced accordingly despite the improvement in the performance and extension of its application area. Existing methods attempt to achieve several shared optimization goals, such as reducing empty space and minimizing line crossings and wiggles. However, these goals do not always produce optimal results when compared to hand-drawn storylines. We conducted a preliminary study to learn how users translate a narrative into a hand-drawn storyline and check whether the visual elements in hand-drawn illustrations can be mapped back to appropriate narrative contexts. We also compared the hand-drawn storylines with storylines generated by the state-of-the-art methods and found they have significant differences. Our findings led to a design space that summarizes (1) how artists utilize narrative elements and (2) the sequence of actions artists follow to portray expressive and attractive storylines. We developed iStoryline, an authoring tool for integrating high-level user interactions into optimization algorithms and achieving a balance between hand-drawn storylines and automatic layouts. iStoryline allows users to create novel storyline visualizations easily according to their preferences by modifying the automatically generated layouts. The effectiveness and usability of iStoryline are studied with qualitative evaluations.", "keywords": "Hand-drawn illustrations,automatic layout,design space,interactions,optimization", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864899", "refList": ["10.1109/tvcg.2015.2509990", "10.1109/tvcg.2016.2598647", "10.1109/mcg.2016.7", "10.1109/tvcg.2017.2745878", "10.1145/3132272.3132299", "10.1109/tvcg.2015.2392771", "10.1177/1529100614525555", "10.1109/tvcg.2012.212", "10.1109/tvcg.2015.2467451", "10.3102/0013189x09353787", "10.1109/cscwd.2014.6846815", "10.1109/tmm.2016.2614221", "10.1109/tvcg.2009.109", "10.1109/icse.2012.6227086", "10.1145/2598784.2598806", "10.1109/tvcg.2017.2743990", "10.1145/2598510.2598566", "10.1109/tvcg.2013.196", "10.1109/tvcg.2016.2598620", "10.1111/j.1467-8659.2011.01955.x", "10.1109/tvcg.2015.2467531", "10.1109/tvcg.2014.2346291", "10.1109/tvcg.2013.191", "10.1145/1842993.1843035", "10.1145/568522.568523", "10.1109/tvcg.2014.2346913", "10.1109/tvcg.2016.2598831", "10.1109/tvcg.2016.2614803"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030461", "title": "CNNPruner: Pruning Convolutional Neural Networks with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Guan Li;Junpeng Wang;Han-Wei Shen;Kaixin Chen;Guihua Shan;Zhonghua Lu", "citationCount": "0", "affiliation": "Shan, GH (Corresponding Author), Chinese Acad Sci, Comp Network Informat Ctr, Beijing, Peoples R China. Li, Guan; Chen, Kaixin; Shan, Guihua; Lu, Zhonghua, Chinese Acad Sci, Comp Network Informat Ctr, Beijing, Peoples R China. Li, Guan; Chen, Kaixin; Shan, Guihua; Lu, Zhonghua, Univ Chinese Acad Sci, Beijing, Peoples R China. Wang, Junpeng, Visa Res, Beijing, Peoples R China. Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA;China", "abstract": "Convolutional neural networks (CNNs) have demonstrated extraordinarily good performance in many computer vision tasks. The increasing size of CNN models, however, prevents them from being widely deployed to devices with limited computational resources, e.g., mobile/embedded devices. The emerging topic of model pruning strives to address this problem by removing less important neurons and fine-tuning the pruned networks to minimize the accuracy loss. Nevertheless, existing automated pruning solutions often rely on a numerical threshold of the pruning criteria, lacking the flexibility to optimally balance the trade-off between efficiency and accuracy. Moreover, the complicated interplay between the stages of neuron pruning and model fine-tuning makes this process opaque, and therefore becomes difficult to optimize. In this paper, we address these challenges through a visual analytics approach, named CNNPruner. It considers the importance of convolutional filters through both instability and sensitivity, and allows users to interactively create pruning plans according to a desired goal on model size or accuracy. Also, CNNPruner integrates state-of-the-art filter visualization techniques to help users understand the roles that different filters played and refine their pruning plans. Through comprehensive case studies on CNNs with real-world sizes, we validate the effectiveness of CNNPruner.", "keywords": "visualization,model pruning,convolutional neural network,explainable artificial intelligence", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030461", "refList": ["10.1109/tvcg.2015.2509990", "10.1145/2468356.2468434", "10.1007/978-3-642-18469-7\\_22", "10.1007/bf02603120", "10.1145/2702123.2702419", "10.1109/pacificvis.2018.00025", "10.1145/345513.345271", "10.1109/32.221135", "10.1007/s10654-016-0149-3", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.89", "10.1109/infvis.2005.1532152", "10.1007/3-540-49381-6\\_9", "10.1109/gl0bec0m38437.2019.9014197", "10.2337/dc18-s006", "10.11234/gi1990.5.90", "10.1007/bf01187020", "10.1016/j.jbi.2015.06.020", "10.1109/pacificvis.2013.6596125", "10.1109/tvcg.2015.2468151", "10.1109/tvcg.2013.124", "10.1111/cgf.13988", "10.1109/pacificvis.2011.5742371", "10.1109/tvcg.2016.2539960", "10.1109/tvcg.2013.200", "10.1109/tsmc.1977.4309760", "10.1109/tvcg.2012.225", "10.1109/tvcg.2015.2465151", "10.1186/1753-6561-8-s2-s8", "10.1109/tvcg.2018.2864899", "10.1109/tits.2015.2436897", "10.1109/tvcg.2013.196", "10.1214/ss/1177013815", "10.1111/j.2517-6161.1995.tb02031.x", "10.2144/00286ir01", "10.1109/tsmc.1981.4308636", "10.1145/3025453.3026024", "10.2337/dc14-2919", "10.2307/2683905", "10.3238/arztebl.2009.0335", "10.1109/vast.2011.6102453"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030367", "title": "Lyra 2: Designing Interactive Visualizations by Demonstration", "year": "2020", "conferenceName": "InfoVis", "authors": "Jonathan Zong;Dhiraj Barnwal;Rupayan Neogy;Arvind Satyanarayan", "citationCount": "0", "affiliation": "Zong, J (Corresponding Author), MIT, Cambridge, MA 02139 USA. Zong, Jonathan; Neogy, Rupayan; Satyanarayan, Arvind, MIT, Cambridge, MA 02139 USA. Barnwal, Dhiraj, Indian Inst Technol Kharagpur, Kharagpur, W Bengal, India.", "countries": "India;USA", "abstract": "Recent graphical interfaces offer direct manipulation mechanisms for authoring visualizations, but are largely restricted to static output. To author interactive visualizations, users must instead turn to textual specification, but such approaches impose a higher technical burden. To bridge this gap, we introduce Lyra 2, a system that extends a prior visualization design environment with novel methods for authoring interaction techniques by demonstration. Users perform an interaction (e.g., button clicks, drags, or key presses) directly on the visualization they are editing. The system interprets this performance using a set of heuristics and enumerates suggestions of possible interaction designs. These heuristics account for the properties of the interaction (e.g., target and event type) as well as the visualization (e.g., mark and scale types, and multiple views). Interaction design suggestions are displayed as thumbnails; users can preview and test these suggestions, iteratively refine them through additional demonstrations, and finally apply and customize them via property inspectors. We evaluate our approach through a gallery of diverse examples, and evaluate its usability through a first-use study and via an analysis of its cognitive dimensions. We find that, in Lyra 2, interaction design by demonstration enables users to rapidly express a wide range of interactive visualizations.", "keywords": "Direct manipulation,interactive visualization,interaction design by demonstration", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030367", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030467", "title": "PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch;Lingyun Yu;Peiran Ren;Thomas Ertl;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Knittel, Johannes; Koch, Steffen; Ertl, Thomas, Univ Stuttgart, VIS VISUS, Stuttgart, Germany. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Dept Comp Sci \\& Software Engn, Suzhou, Peoples R China. Ren, Peiran, Alibaba Grp, Hangzhou, Peoples R China.", "countries": "Germany;China", "abstract": "Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.", "keywords": "Storyline visualization,reinforcement learning,mixed-initiative design", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030467", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030396", "title": "What Makes a Data-GIF Understandable?", "year": "2020", "conferenceName": "InfoVis", "authors": "Xinhuan Shu;Aoyu Wu;Junxiu Tang;Benjamin Bach;Yingcai Wu;Huamin Qu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Shu, Xinhuan; Tang, Junxiu; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Shu, Xinhuan; Wu, Aoyu; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Shu, Xinhuan, Zhejiang Univ, Hangzhou, Peoples R China. Bach, Benjamin, Univ Edinburgh, Edinburgh, Midlothian, Scotland.", "countries": "Scotland;China", "abstract": "GIFs are enjoying increasing popularity on social media as a format for data-driven storytelling with visualization; simple visual messages are embedded in short animations that usually last less than 15 seconds and are played in automatic repetition. In this paper, we ask the question, \u201cWhat makes a data-GIF understandable?\u201d While other storytelling formats such as data videos, infographics, or data comics are relatively well studied, we have little knowledge about the design factors and principles for \u201cdata-GIFs\u201d. To close this gap, we provide results from semi-structured interviews and an online study with a total of 118 participants investigating the impact of design decisions on the understandability of data-GIFs. The study and our consequent analysis are informed by a systematic review and structured design space of 108 data-GIFs that we found online. Our results show the impact of design dimensions from our design space such as animation encoding, context preservation, or repetition on viewers understanding of the GIF's core message. The paper concludes with a list of suggestions for creating more effective Data-GIFs.", "keywords": "Data-GIFs,Data-driven Storytelling,Evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030396", "refList": ["10.1109/tvcg.2016.2598647", "10.1016/j.visinf.2020.07.001", "10.1177/1473871615594652", "10.1109/tvcg.2014.2346424", "10.1111/cgf.13195", "10.1109/vlsicircuits18222.2020.9162811", "10.1109/tvcg.2018.2864909", "10.1109/tvcg.2018.2864903", "10.1145/2702123.2702431", "10.1016/j.visinf.2019.12.002", "10.1145/3274349", "10.1111/cgf.13444", "10.1145/3206505.3206552", "10.1145/3290605.3300280", "10.1109/jstqe.2020.3021589", "10.1145/2647868.2656408", "10.1006/ijhc.1017", "10.1145/3290605.3300335", "10.1007/s12650-020-00689-0", "10.1145/2818048.2819936", "10.1111/cgf.13325", "10.1145/2858036.2858387", "10.1145/3027063.3053139", "10.1145/3290605.3300474", "10.1109/tvcg.2016.2598920", "10.1109/tvcg.2018.2864899", "10.1145/3290605.3300483", "10.1145/3173574.3173612", "10.1145/2909132.2909255", "10.1109/tvcg.2016.2598620", "10.1016/j.learninstruc.2007.09.013", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2008.125", "10.1145/3173574.3173909", "10.1109/tvcg.2019.2934397", "10.1111/cgf.13709", "10.1109/tvcg.2013.234", "10.1109/tvcg.2019.2934401", "10.1145/2858036.2858532", "10.1016/j.visinf.2020.08.001", "10.1109/tvcg.2010.179", "10.1109/cicc48029.2020.9075900"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2018.2865126", "title": "SRVis: Towards Better Spatial Integration in Ranking Visualization", "year": "2018", "conferenceName": "InfoVis", "authors": "Di Weng;Ran Chen;Zikun Deng;Feiran Wu;Jingmin Chen;Yingcai Wu", "citationCount": "6", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ \\& Alibaba Zhejiang Univ Joint Inst, State Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Weng, Di; Chen, Ran; Deng, Zikun; Wu, Yingcai, Zhejiang Univ \\& Alibaba Zhejiang Univ Joint Inst, State Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Wu, Feiran; Chen, Jingmin, Alibaba Grp, Hangzhou, Zhejiang, Peoples R China.", "countries": "China", "abstract": "Interactive ranking techniques have substantially promoted analysts' ability in making judicious and informed decisions effectively based on multiple criteria. However, the existing techniques cannot satisfactorily support the analysis tasks involved in ranking large-scale spatial alternatives, such as selecting optimal locations for chain stores, where the complex spatial contexts involved are essential to the decision-making process. Limitations observed in the prior attempts of integrating rankings with spatial contexts motivate us to develop a context-integrated visual ranking technique. Based on a set of generic design requirements we summarized by collaborating with domain experts, we propose SRVis, a novel spatial ranking visualization technique that supports efficient spatial multi-criteria decision-making processes by addressing three major challenges in the aforementioned context integration, namely, a) the presentation of spatial rankings and contexts, b) the scalability of rankings' visual representations, and c) the analysis of context-integrated spatial rankings. Specifically, we encode massive rankings and their cause with scalable matrix-based visualizations and stacked bar charts based on a novel two-phase optimization framework that minimizes the information loss, and the flexible spatial filtering and intuitive comparative analysis are adopted to enable the in-depth evaluation of the rankings and assist users in selecting the best spatial alternative. The effectiveness of the proposed technique has been evaluated and demonstrated with an empirical study of optimization methods, two case studies, and expert interviews.", "keywords": "Spatial ranking,visualization", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865126", "refList": ["10.2312/pe.eurovast.eurova13.007-011.3", "10.1109/tvcg.2013.193", "10.1109/iv.2009.59", "10.1080/13658810010005525", "10.3166/jds.12.193-208", "10.1016/j.jcss.2013.01.017", "10.1145/2702123.2702237", "10.1109/tvcg.2011.185", "10.1145/1133265.1133308", "10.1111/cgf.12132", "10.1109/mcg.2017.21", "10.1109/tvcg.2016.2598585", "10.1109/tvcg.2015.2467771", "10.1561/1500000016", "10.1109/tvcg.2014.2346913", "10.1109/mcg.2016.100", "10.1109/uidis.2001.929933", "10.1109/tvcg.2016.2642109", "10.1109/tvcg.2013.173", "10.1145/3020165.3020174", "10.1109/mcg.2015.25", "10.1109/tvcg.2015.2467717", "10.1109/pacificvis.2015.7156392", "10.1109/tbdata.2016.2586447", "10.1109/tvcg.2014.2346594", "10.1007/978-3-319-12586-2", "10.1109/smdcm.2011.5949270", "10.1080/136588199241247", "10.1109/tvcg.2008.166", "10.1007/bf02289694", "10.1109/vast.2011.6102455", "10.1111/j.0033-0124.1985.00075.x", "10.1109/tvcg.2012.253", "10.1109/tits.2013.2263225", "10.1111/cgf.12910", "10.1109/tvcg.2008.181", "10.1109/tvcg.2015.2467112", "10.1109/tvcg.2009.111", "10.1038/nature05302", "10.1145/989863.989885", "10.1109/tvcg.2016.2598416", "10.1287/moor.1.2.117", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934670", "title": "AirVis: Visual Analytics of Air Pollution Propagation", "year": "2019", "conferenceName": "VAST", "authors": "Zikun Deng;Di Weng;Jiahui Chen;Ren Liu;Zhibin Wang;Jie Bao 0003;Yu Zheng 0004;Yingcai Wu", "citationCount": "6", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Deng, Zikun; Weng, Di; Chen, Jiahui; Liu, Ren; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Wang, Zhibin, Zhejiang Univ, Res Ctr Air Pollut \\& Hlth, Hangzhou, Peoples R China. Bao, Jie; Zheng, Yu, JD Intelligent City Res, Beijing, Peoples R China.", "countries": "China", "abstract": "Air pollution has become a serious public health problem for many cities around the world. To find the causes of air pollution, the propagation processes of air pollutants must be studied at a large spatial scale. However, the complex and dynamic wind fields lead to highly uncertain pollutant transportation. The state-of-the-art data mining approaches cannot fully support the extensive analysis of such uncertain spatiotemporal propagation processes across multiple districts without the integration of domain knowledge. The limitation of these automated approaches motivates us to design and develop AirVis, a novel visual analytics system that assists domain experts in efficiently capturing and interpreting the uncertain propagation patterns of air pollution based on graph visualizations. Designing such a system poses three challenges: a) the extraction of propagation patterns; b) the scalability of pattern presentations; and c) the analysis of propagation processes. To address these challenges, we develop a novel pattern mining framework to model pollutant transportation and extract frequent propagation patterns efficiently from large-scale atmospheric data. Furthermore, we organize the extracted patterns hierarchically based on the minimum description length (MDL) principle and empower expert users to explore and analyze these patterns effectively on the basis of pattern topologies. We demonstrated the effectiveness of our approach through two case studies conducted with a real-world dataset and positive feedback from domain experts.", "keywords": "Air pollution propagation,pattern mining,graph visualization", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934670", "refList": ["10.1109/tvcg.2016.2598919", "10.1093/bib/bbr069", "10.1145/2487575.2488188", "10.1109/tvcg.2013.193", "10.1016/j.atmosenv.2014.12.011", "10.1109/tvcg.2013.226", "10.1109/tvcg.2018.2864503", "10.1109/tvcg.2015.2468111", "10.1109/icicta.2015.183", "10.1016/j.atmosenv.2014.05.039", "10.1111/cgf.12791", "10.1111/j.1467-8659.2009.01451.x", "10.1111/j.1467-8659.2011.01898.x", "10.5194/acp-12-5031-2012", "10.1016/j.atmosenv.2008.05.053", "10.1109/tvcg.2016.2535234", "10.1016/j.atmosres.2014.12.003", "10.1109/tvcg.2015.2467194", "10.1109/tvcg.2013.263", "10.1109/icdm.2002.1184038", "10.1145/2783258.2788573", "10.1109/tvcg.2018.2865149", "10.1109/tbdata.2017.2723899", "10.1109/tvcg.2012.311", "10.1109/vl.1996.545307", "10.1007/s12650-018-0481-7", "10.1016/j.envpol.2007.06.012", "10.3155/1047-3289.61.6.660", "10.1080/13658810701349037", "10.1145/3097983.3098090", "10.1109/tvcg.2007.70523", "10.1115/1.2128636", "10.1109/tvcg.2015.2467619", "10.3978/j.issn.2072-1439.2016.01.19", "10.1109/tkde.2005.127", "10.1017/s0269888912000331", "10.2312/conf/eg2013/stars/039-063", "10.1109/tvcg.2014.2346271", "10.1175/bams-d-14-00110.1", "10.1016/0005-1098(78)90005-5", "10.1007/s10618-006-0044-8", "10.1109/tvcg.2018.2865126", "10.2307/1912791", "10.3390/su6085322", "10.1007/s12650-018-0489-z", "10.2312/eurovisstar.20151109", "10.1109/tvcg.2011.181", "10.1126/science.298.5594.824", "10.1162/jmlr.2003.3.4-5.951", "10.1109/asonam.2014.6921638", "10.1111/j.1467-8659.2008.01213.x", "10.1038/s41598-017-18107-1", "10.1109/tvcg.2018.2865041", "10.1109/tits.2019.2901117", "10.1038/srep20668", "10.1109/tvcg.2012.265", "10.1109/tpami.2016.2608884", "10.1109/tvcg.2012.213", "10.1145/1376616.1376661", "10.1007/s00521-019-04567-1", "10.1109/tvcg.2017.2745083", "10.1126/science.243.4892.745", "10.1109/tvcg.2018.2864826", "10.1109/tnn.2003.820440", "10.1109/tvcg.2016.2598885", "10.1145/3219819.3219822", "10.1073/pnas.1502596112", "10.1016/j.envsoft.2009.01.004", "10.1002/pmic.200700095", "10.1145/2254556.2254651", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2011.202"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028958", "title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Tiankai Xie;Yuxin Ma;Hanghang Tong;My T. Thai;Ross Maciejewski", "citationCount": "0", "affiliation": "Xie, TK (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Xie, Tiankai; Ma, Yuxin; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. Tong, Hanghang, Univ Illinois, Urbana, IL USA. Thai, My T., Univ Florida, Gainesville, FL 32611 USA.", "countries": "USA", "abstract": "Graph mining plays a pivotal role across a number of disciplines, and a variety of algorithms have been developed to answer who/what type questions. For example, what items shall we recommend to a given user on an e-commerce platform? The answers to such questions are typically returned in the form of a ranked list, and graph-based ranking methods are widely used in industrial information retrieval settings. However, these ranking algorithms have a variety of sensitivities, and even small changes in rank can lead to vast reductions in product sales and page hits. As such, there is a need for tools and methods that can help model developers and analysts explore the sensitivities of graph ranking algorithms with respect to perturbations within the graph structure. In this paper, we present a visual analytics framework for explaining and exploring the sensitivity of any graph-based ranking algorithm by performing perturbation-based what-if analysis. We demonstrate our framework through three case studies inspecting the sensitivity of two classic graph-based ranking algorithms (PageRank and HITS) as applied to rankings in political news media and social networks.", "keywords": "Graph-based ranking,sensitivity analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028958", "refList": ["10.1109/wsc.2017.8247800", "10.1023/a:1022649401552", "10.1515/1559-0410.11416", "10.1109/tvcg.2016.2598919", "10.1177/1473871611416549", "10.1109/tvcg.2019.2934630", "10.1140/epjds29", "10.1109/tvcg.2019.2934670", "10.1016/j.eswa.2015.09.004", "10.1145/2702123.2702509", "10.1016/j.visinf.2018.12.001", "10.2307/3002000", "10.1109/tvcg.2019.2934399", "10.1007/s41060-016-0032-z", "10.1111/cgf.13198", "10.14778/2350229.2350254", "10.1145/2939672.2939764", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/vast.2017.8585647", "10.1007/bf01187020", "10.1109/icdm.2015.26", "10.1145/2362383.2362387", "10.1177/0049124104268644", "10.1109/vast.2011.6102442", "10.1109/infvis.2003.1249025", "10.1109/tvcg.2018.2864475", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2015.2467691", "10.1111/cgf.13210", "10.1214/aos/1176344136", "10.1109/tvcg.2015.2424872", "10.1016/j.visinf.2018.09.001", "10.1177/089443939100900106", "10.1109/tvcg.2015.2467931", "10.1162/neco.1997.9.8.1735", "10.1007/s11162-011-9241-4", "10.1111/cgf.13680", "10.1145/3065386", "10.1109/tvcg.2018.2864889", "10.1177/003804070808100402", "10.1109/icdm.2010.62", "10.1038/s41598-020-59669-x", "10.1162/153244303321897717", "10.1109/tvcg.2019.2934619", "10.1007/bf00356088", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030410", "title": "Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Wei Zeng 0002;Chengqiao Lin;Juncong Lin;Jincheng Jiang;Jiazhi Xia;Cagatay Turkay;Wei Chen", "citationCount": "0", "affiliation": "Lin, JC (Corresponding Author), Xiamen Univ, Xiamen, Peoples R China. Zeng, Wei; Jiang, Jincheng, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Lin, Chengqiao; Lin, Juncong, Xiamen Univ, Xiamen, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Turkay, Cagatay, Univ Warwick, Coventry, W Midlands, England. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China.", "countries": "China;England", "abstract": "Deep learning methods are being increasingly used for urban traffic prediction where spatiotemporal traffic data is aggregated into sequentially organized matrices that are then fed into convolution-based residual neural networks. However, the widely known modifiable areal unit problem within such aggregation processes can lead to perturbations in the network inputs. This issue can significantly destabilize the feature embeddings and the predictions - rendering deep networks much less useful for the experts. This paper approaches this challenge by leveraging unit visualization techniques that enable the investigation of many-to-many relationships between dynamically varied multi-scalar aggregations of urban traffic data and neural network predictions. Through regular exchanges with a domain expert, we design and develop a visual analytics solution that integrates 1) a Bivariate Map equipped with an advanced bivariate colormap to simultaneously depict input traffic and prediction errors across space, 2) a Moran's I Scatterplot that provides local indicators of spatial association analysis, and 3) a Multi-scale Attribution View that arranges non-linear dot plots in a tree layout to promote model analysis and comparison across scales. We evaluate our approach through a series of case studies involving a real-world dataset of Shenzhen taxi trips, and through interviews with domain experts. We observe that geographical scale variations have important impact on prediction performances, and interactive visual exploration of dynamically varying inputs and outputs benefit experts in the development of deep traffic prediction models.", "keywords": "MAUP,traffic prediction,deep learning,model diagnostic,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030410", "refList": ["10.1038/srep26377", "10.1109/mcg.2011.88", "10.1080/13658816.2015.1119279", "10.1109/tvcg.2013.226", "10.1109/pacificvis.2011.5742387", "10.1038/s41467-017-01882-w", "10.1109/tvcg.2019.2934670", "10.1111/j.1467-8659.2009.01440.x", "10.1111/cgf.13712", "10.1016/j.compenvurbsys.2008.09.006", "10.1109/pacificvis.2014.50", "10.1109/tvcg.2018.2816219", "10.1109/tvcg.2016.2535234", "10.1109/tvcg.2014.2346893", "10.3390/ijgi8080344", "10.1109/tvcg.2013.246", "10.1007/s10940-005-9003-6", "10.1016/j.compenvurbsys.2008.05.001", "10.1007/s10661-019-7831-3", "10.1111/j.1538-4632.2007.00699.x", "10.1016/j.aap.2016.08.015", "10.1080/13658816.2018.1541177", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2011.181", "10.1137/090759069", "10.1109/pacificvis.2011.5742390", "10.1214/10-aos799", "10.1109/tits.2017.2683539", "10.1109/tits.2015.2436897", "10.3390/ijerph16071150", "10.1109/tvcg.2009.145", "10.1109/tvcg.2012.265", "10.1080/10106049.2017.1404140", "10.3390/ijgi8020063", "10.3390/info6020134", "10.1080/13658816.2014.955027", "10.1109/tits.2016.2639320", "10.2307/143141", "10.1109/tvcg.2016.2598432"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030370", "title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups", "year": "2020", "conferenceName": "VAST", "authors": "Yating Lin;Kamkwai Wong;Yong Wang;Rong Zhang;Bo Dong;Huamin Qu;Qinghua Zheng", "citationCount": "0", "affiliation": "Lin, YT (Corresponding Author), Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian, Shaanxi, Peoples R China. Lin, Yating; Zheng, Qinghua, Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian, Shaanxi, Peoples R China. Wong, Kamkwai; Wang, Yong; Zhang, Rong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Dong, Bo, Xi An Jiao Tong Univ, Natl Engn Lab Big Data Analyt, Xian, Shaanxi, Peoples R China.", "countries": "China", "abstract": "Tax evasion is a serious economic problem for many countries, as it can undermine the government's tax system and lead to an unfair business competition environment. Recent research has applied data analytics techniques to analyze and detect tax evasion behaviors of individual taxpayers. However, they have failed to support the analysis and exploration of the related party transaction tax evasion (RPTTE) behaviors (e.g., transfer pricing), where a group of taxpayers is involved. In this paper, we present TaxThemis, an interactive visual analytics system to help tax officers mine and explore suspicious tax evasion groups through analyzing heterogeneous tax-related data. A taxpayer network is constructed and fused with the respective trade network to detect suspicious RPTTE groups. Rich visualizations are designed to facilitate the exploration and investigation of suspicious transactions between related taxpayers with profit and topological data analysis. Specifically, we propose a calendar heatmap with a carefully-designed encoding scheme to intuitively show the evidence of transferring revenue through related party transactions. We demonstrate the usefulness and effectiveness of TaxThemis through two case studies on real-world tax-related data and interviews with domain experts.", "keywords": "Visual Analytics,Tax Network,Tax Evasion Detection,Anomaly detection,Multidimensional data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030370", "refList": ["10.1111/cgf.12886", "10.2307/2277827", "10.1109/tvcg.2010.44", "10.1109/tits.2014.2315794", "10.1109/tvcg.2019.2934670", "10.1038/s41467-019-08987-4", "10.1111/cgf.12920", "10.1109/vast.2017.8585721", "10.1080/15230406.2015.1093431", "10.1109/tvcg.2018.2843369", "10.1038/srep01001", "10.1109/tvcg.2017.2744018", "10.1109/tvcg.2017.2744159", "10.1068/b130199p", "10.1109/tvcg.2009.143", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2019.2892483", "10.1109/pacificvis.2017.8031583", "10.1109/pacificvis48177.2020.2785", "10.2307/2686111", "10.1109/tvcg.2015.2467199", "10.1111/cgf.12114", "10.1109/tvcg.2018.2865126", "10.1111/j.1538-4632.1996.tb00936.x", "10.1109/tvcg.2019.2934619", "10.2307/2332142", "10.1007/978-3-319-10590-1\\_53", "10.1109/cvpr.2016.485", "10.1109/cvpr.2017.17", "10.2307/2986645", "10.1109/tvcg.2014.2346321", "10.1017/s0140525x16001837", "10.1109/tvcg.2016.2598541", "10.1371/journal.pone.0207377", "10.1109/tvcg.2014.2346265", "10.1007/s4095-020-0191-7", "10.3141/1644-14", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2016.90", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2785807", "10.1109/tvcg.2017.2744358", "10.1111/j.1538-4632.1995.tb00338.x", "10.1080/03081068808717359", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030469", "title": "Topology Density Map for Urban Data Visualization and Analysis", "year": "2020", "conferenceName": "VAST", "authors": "Zezheng Feng;Haotian Li;Wei Zeng 0004;Shuang-Hua Yang;Huamin Qu", "citationCount": "0", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Feng, Zezheng; Li, Haotian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yang, Shuang-Hua, Southern Univ Sci \\& Technol, Shenzhen, Peoples R China.", "countries": "China", "abstract": "Density map is an effective visualization technique for depicting the scalar field distribution in 2D space. Conventional methods for constructing density maps are mainly based on Euclidean distance, limiting their applicability in urban analysis that shall consider road network and urban traffic. In this work, we propose a new method named Topology Density Map, targeting for accurate and intuitive density maps in the context of urban environment. Based on the various constraints of road connections and traffic conditions, the method first constructs a directed acyclic graph (DAG) that propagates nonlinear scalar fields along 1D road networks. Next, the method extends the scalar fields to a 2D space by identifying key intersecting points in the DAG and calculating the scalar fields for every point, yielding a weighted Voronoi diagram like effect of space division. Two case studies demonstrate that the Topology Density Map supplies accurate information to users and provides an intuitive visualization for decision making. An interview with domain experts demonstrates the feasibility, usability, and effectiveness of our method.", "keywords": "Density map,network topology,urban data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030469", "refList": ["10.1109/vast.2009.5332584", "10.1109/tvcg.2013.193", "10.1080/03081060.2013.844903", "10.1109/tvcg.2018.2864503", "10.1145/2702123.2702419", "10.1109/tvcg.2019.2934670", "10.1109/tits.2015.2496783", "10.1177/1473871615581216", "10.3141/1617-02", "10.1145/2024156.2024169", "10.1111/cgf.13712", "10.1016/j.ejor.2007.02.005", "10.1109/tvcg.2014.2346893", "10.1007/11871842\\_29", "10.1109/vast.2010.5652478", "10.1016/j.visinf.2019.10.002", "10.1109/tvcg.2016.2616404", "10.1109/vl.1996.545307", "10.1145/2629592", "10.1155/2018/2696037", "10.1061/(asce)0733-947x(1998)124:4(368", "10.3141/1899-21", "10.1023/a:1026123329433", "10.1109/mcg.2010.79", "10.1057/palgrave.ivs.9500174", "10.1109/tcyb.2019.2963681", "10.1109/tvcg.2015.2467554", "10.1111/cgf.12114", "10.1145/2814575", "10.1016/j.jcps.2014.08.002", "10.1109/2945.981847", "10.1080/03052150210909", "10.1109/tciaig.2012.2186810", "10.1109/tits.2017.2683539", "10.1109/iv.2004.1320137", "10.1016/0377-2217(80)90126-5", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2015.2467196", "10.1145/3097983.3098056", "10.1007/s11432-018-9801-4", "10.1109/vast.2014.7042490", "10.1061/(asce)0733-947x(2006)132:2(122", "10.1016/j.tra.2008.03.011", "10.1109/tits.2014.2298892", "10.1016/j.trb.2005.12.003", "10.1007/bf01840357", "10.1109/vast.2011.6102454", "10.1109/tvcg.2013.145", "10.1007/bf02289588", "10.1109/pacificvis.2014.56", "10.1109/mcg.2018.053491730", "10.1109/tvcg.2009.111", "10.1057/palgrave.ivs.9500184", "10.1109/tvcg.2013.173", "10.1109/tvcg.2016.2598432", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2020.3030370", "title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups", "year": "2020", "conferenceName": "VAST", "authors": "Yating Lin;Kamkwai Wong;Yong Wang;Rong Zhang;Bo Dong;Huamin Qu;Qinghua Zheng", "citationCount": "0", "affiliation": "Lin, YT (Corresponding Author), Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian, Shaanxi, Peoples R China. Lin, Yating; Zheng, Qinghua, Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian, Shaanxi, Peoples R China. Wong, Kamkwai; Wang, Yong; Zhang, Rong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Dong, Bo, Xi An Jiao Tong Univ, Natl Engn Lab Big Data Analyt, Xian, Shaanxi, Peoples R China.", "countries": "China", "abstract": "Tax evasion is a serious economic problem for many countries, as it can undermine the government's tax system and lead to an unfair business competition environment. Recent research has applied data analytics techniques to analyze and detect tax evasion behaviors of individual taxpayers. However, they have failed to support the analysis and exploration of the related party transaction tax evasion (RPTTE) behaviors (e.g., transfer pricing), where a group of taxpayers is involved. In this paper, we present TaxThemis, an interactive visual analytics system to help tax officers mine and explore suspicious tax evasion groups through analyzing heterogeneous tax-related data. A taxpayer network is constructed and fused with the respective trade network to detect suspicious RPTTE groups. Rich visualizations are designed to facilitate the exploration and investigation of suspicious transactions between related taxpayers with profit and topological data analysis. Specifically, we propose a calendar heatmap with a carefully-designed encoding scheme to intuitively show the evidence of transferring revenue through related party transactions. We demonstrate the usefulness and effectiveness of TaxThemis through two case studies on real-world tax-related data and interviews with domain experts.", "keywords": "Visual Analytics,Tax Network,Tax Evasion Detection,Anomaly detection,Multidimensional data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030370", "refList": ["10.1111/cgf.12886", "10.2307/2277827", "10.1109/tvcg.2010.44", "10.1109/tits.2014.2315794", "10.1109/tvcg.2019.2934670", "10.1038/s41467-019-08987-4", "10.1111/cgf.12920", "10.1109/vast.2017.8585721", "10.1080/15230406.2015.1093431", "10.1109/tvcg.2018.2843369", "10.1038/srep01001", "10.1109/tvcg.2017.2744018", "10.1109/tvcg.2017.2744159", "10.1068/b130199p", "10.1109/tvcg.2009.143", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2019.2892483", "10.1109/pacificvis.2017.8031583", "10.1109/pacificvis48177.2020.2785", "10.2307/2686111", "10.1109/tvcg.2015.2467199", "10.1111/cgf.12114", "10.1109/tvcg.2018.2865126", "10.1111/j.1538-4632.1996.tb00936.x", "10.1109/tvcg.2019.2934619", "10.2307/2332142", "10.1007/978-3-319-10590-1\\_53", "10.1109/cvpr.2016.485", "10.1109/cvpr.2017.17", "10.2307/2986645", "10.1109/tvcg.2014.2346321", "10.1017/s0140525x16001837", "10.1109/tvcg.2016.2598541", "10.1371/journal.pone.0207377", "10.1109/tvcg.2014.2346265", "10.1007/s4095-020-0191-7", "10.3141/1644-14", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2016.90", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2785807", "10.1109/tvcg.2017.2744358", "10.1111/j.1538-4632.1995.tb00338.x", "10.1080/03081068808717359", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}], "len": 15}, {"doi": "10.1109/tvcg.2019.2934655", "title": "Visual Analytics for Electromagnetic Situation Awareness in Radio Monitoring and Management", "year": "2019", "conferenceName": "VAST", "authors": "Ying Zhao;Xiaobo Luo;Xiaoru Lin;Hairong Wang;Xiaoyan Kui;Fangfang Zhou;Jinsong Wang;Yi Chen 0007;Wei Chen", "citationCount": "3", "affiliation": "Kui, XY; Zhou, FF (Corresponding Author), Cent S Univ, Sch Comp Sci \\& Engn, Changsha, Hunan, Peoples R China. Zhao, Ying; Luo, Xiaobo; Lin, Xiaoru; Kui, Xiaoyan; Zhou, Fangfang, Cent S Univ, Sch Comp Sci \\& Engn, Changsha, Hunan, Peoples R China. Wang, Hairong, Cent S Univ, Sch Automat, Changsha, Hunan, Peoples R China. Wang, Jinsong, Southwest Elect \\& Telecom Engn Inst, Shanghai, Peoples R China. Chen, Yi, Beijing Technol \\& Business Univ, Beijing Key Lab Big Data Technol Food Safety, Beijing, Peoples R China. Chen, Wei, Zhejiang Univ, Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China.", "countries": "China", "abstract": "Traditional radio monitoring and management largely depend on radio spectrum data analysis, which requires considerable domain experience and heavy cognition effort and frequently results in incorrect signal judgment and incomprehensive situation awareness. Faced with increasingly complicated electromagnetic environments, radio supervisors urgently need additional data sources and advanced analytical technologies to enhance their situation awareness ability. This paper introduces a visual analytics approach for electromagnetic situation awareness. Guided by a detailed scenario and requirement analysis, we first propose a signal clustering method to process radio signal data and a situation assessment model to obtain qualitative and quantitative descriptions of the electromagnetic situations. We then design a two-module interface with a set of visualization views and interactions to help radio supervisors perceive and understand the electromagnetic situations by a joint analysis of radio signal data and radio spectrum data. Evaluations on real-world data sets and an interview with actual users demonstrate the effectiveness of our prototype system. Finally, we discuss the limitations of the proposed approach and provide future work directions.", "keywords": "Radio monitoring and management,radio signal data,radio spectrum data,situation awareness,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934655", "refList": ["10.1016/j.newast.2010.07.009", "10.1109/tvcg.2017.2744459", "10.1109/tvcg.2018.2864503", "10.1145/1029208.1029219", "10.1109/pacificvis.2014.54", "10.1109/tvcg.2018.2829750", "10.1016/j.jvlc.2017.11.004", "10.1109/tcyb.2015.2448236", "10.1016/j.patrec.2017.11.011", "10.1109/tvcg.2015.2505305", "10.1109/vast.2014.7042528", "10.1007/s12650-018-0530-2", "10.1109/tvcg.2018.2816203", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2017.2745180", "10.1109/mcg.2018.2879067", "10.1109/tvcg.2018.2859973", "10.1109/tvcg.2016.2598460", "10.1109/infvis.2005.1532134", "10.1109/tvcg.2018.2851227", "10.1155/2012/920671", "10.1109/vast.2014.7042479", "10.1109/tvcg.2013.228", "10.1109/tvcg.2018.2865077", "10.1016/j.csda.2005.10.001", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2011.239", "10.1145/3173574.3174237", "10.1007/s13042-016-0603-2", "10.1109/tvcg.2018.2865020", "10.1017/s1041610219000024", "10.1109/2945.981848", "10.1109/tvcg.2010.193", "10.1109/tvcg.2018.2802520", "10.1145/3200766", "10.1109/wcncw.2015.7122557", "10.1145/3006299.3006312", "10.1109/icsssm.2007.4280175", "10.1126/science.1242072", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2015.2467196", "10.1109/tvcg.2016.2598619", "10.1109/tvcg.2016.2598664", "10.1109/tvcg.2013.196", "10.1109/comst.2016.2631080", "10.1109/tvcg.2018.2865029", "10.1007/s10816-016-9307-x", "10.1109/tvcg.2008.166", "10.1109/tvcg.2017.2758362", "10.1109/vizsec.2005.1532072", "10.1518/001872095779049543", "10.1109/tvcg.2007.70415", "10.1016/j.ins.2018.01.013", "10.1109/tvcg.2014.2346911", "10.1109/mim.2013.6616284", "10.1109/jsyst.2014.2358997", "10.1111/cgf.12910", "10.1109/isi.2009.5137305", "10.1109/tvcg.2011.179", "10.1007/s11277-015-2631-8", "10.1111/cgf.12396", "10.1109/tvcg.2013.2297933", "10.1109/tvcg.2014.2346926", "10.1109/tvcg.2014.2346913", "10.1109/pacificvis.2018.00030", "10.1109/tvcg.2014.2346433", "10.1109/tvcg.2016.2614803"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 277}, "index": 409, "embedding": [-0.007524282671511173, 2.6719484329223633, -1.0741424560546875, -2.5161209106445312, -0.6870212554931641, 0.16650904715061188, -0.6826844215393066, 0.956805408000946, 1.8313664197921753, 3.550564765930176, -0.3824901282787323, 1.8011616468429565, 0.5813019871711731, -0.7879454493522644, 4.403634548187256, 3.8584065437316895, 3.6944308280944824, 1.6840720176696777, 0.6718355417251587, 0.3605295717716217, -0.06630208343267441, 0.20933188498020172, 2.813870668411255, 6.232932090759277, -2.565751552581787, 5.1028947830200195, -0.5670700073242188, 3.8256938457489014, 1.3264553546905518, 2.2138171195983887, 1.3509434461593628, 3.362999200820923], "projection": [0.6470395922660828, 10.73762035369873], "size": 139, "height": 7, "width": 45}