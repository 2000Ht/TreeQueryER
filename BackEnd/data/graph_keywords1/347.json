{"data": {"doi": "10.1109/tvcg.2014.2346411", "title": "Visualization of Brain Microstructure Through Spherical Harmonics Illumination of High Fidelity Spatio-Angular Fields", "year": "2014", "conferenceName": "SciVis", "authors": "Sujal Bista;Jiachen Zhuo;Rao P. Gullapalli;Amitabh Varshney", "citationCount": "10", "affiliation": "Bista, S (Corresponding Author), Univ Maryland, College Pk, MD 20742 USA. Bista, Sujal; Varshney, Amitabh, Univ Maryland, College Pk, MD 20742 USA. Zhuo, Jiachen; Gullapalli, Rao P., Univ Maryland, Sch Med, Baltimore, MD 21201 USA.", "countries": "USA", "abstract": "Diffusion kurtosis imaging (DKI) is gaining rapid adoption in the medical imaging community due to its ability to measure the non-Gaussian property of water diffusion in biological tissues. Compared to traditional diffusion tensor imaging (DTI), DKI can provide additional details about the underlying microstructural characteristics of the neural tissues. It has shown promising results in studies on changes in gray matter and mild traumatic brain injury where DTI is often found to be inadequate. The DKI dataset, which has high-fidelity spatio-angular fields, is difficult to visualize. Glyph-based visualization techniques are commonly used for visualization of DTI datasets; however, due to the rapid changes in orientation, lighting, and occlusion, visually analyzing the much more higher fidelity DKI data is a challenge. In this paper, we provide a systematic way to manage, analyze, and visualize high-fidelity spatio-angular fields from DKI datasets, by using spherical harmonics lighting functions to facilitate insights into the brain microstructure.", "keywords": "Diffusion Kurtosis Imaging, Diffusion Tensor Imaging, Spatio-Angular Fields, Spherical Harmonics Fields, Tensor Fields", "link": "http://dx.doi.org/10.1109/TVCG.2014.2346411", "refList": ["10.1016/j.neuroimage.2012.02.054", "10.1109/tvcg.2008.162", "10.1111/j.1467-8659.2011.01933.x", "10.1109/tvcg.2010.61", "10.1016/j.mri.2012.10.027", "10.1007/978-3-642-41914-0\\_27", "10.1109/tvcg.2012.231", "10.1002/nbm.1518", "10.1089/neu.2011.1763", "10.1145/566654.566612", "10.1587/transinf.e92.d.369", "10.1002/nbm.1020", "10.1016/j.neuroimage.2005.09.027", "10.1002/mrm.21725", "10.1109/tvcg.2007.70602", "10.1002/mrm.20503", "10.1109/svv.1998.729588", "10.1161/strokeaha.112.661926", "10.1007/978-3-642-23629-7\\_22", "10.1109/visual.2004.5", "10.2312/vissym/vissym04/147-154", "10.1109/tvcg.2010.244", "10.1109/visual.1999.809886", "10.1109/tvcg.2002.1021579", "10.1111/j.1467-8659.2009.01675.x", "10.1109/visual.2004.64", "10.1109/visual.2004.62", "10.1109/pacificvis.2009.4906856", "10.1145/1670671.1670676", "10.1109/tvcg.2008.148", "10.1002/mrm.20318", "10.1109/tvcg.2010.199", "10.1109/tvcg.2013.172", "10.1109/38.511", "10.1109/2945.856994", "10.1109/tvcg.2011.198", "10.1016/j.neuroimage.2011.07.050", "10.1111/cgf.12099", "10.1111/j.1467-8659.2009.01690.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2015.2467963", "title": "Anisotropic Ambient Volume Shading", "year": "2015", "conferenceName": "SciVis", "authors": "Marco Ament;Carsten Dachsbacher", "citationCount": "10", "affiliation": "Ament, M (Corresponding Author), Karlsruhe Inst Technol, D-76021 Karlsruhe, Germany. Ament, Marco; Dachsbacher, Carsten, Karlsruhe Inst Technol, D-76021 Karlsruhe, Germany.", "countries": "Germany", "abstract": "We present a novel method to compute anisotropic shading for direct volume rendering to improve the perception of the orientation and shape of surface-like structures. We determine the scale-aware anisotropy of a shading point by analyzing its ambient region. We sample adjacent points with similar scalar values to perform a principal component analysis by computing the eigenvectors and eigenvalues of the covariance matrix. In particular, we estimate the tangent directions, which serve as the tangent frame for anisotropic bidirectional reflectance distribution functions. Moreover, we exploit the ratio of the eigenvalues to measure the magnitude of the anisotropy at each shading point. Altogether, this allows us to model a data-driven, smooth transition from isotropic to strongly anisotropic volume shading. In this way, the shape of volumetric features can be enhanced significantly by aligning specular highlights along the principal direction of anisotropy. Our algorithm is independent of the transfer function, which allows us to compute all shading parameters once and store them with the data set. We integrated our method in a GPU-based volume renderer, which offers interactive control of the transfer function, light source positions, and viewpoint. Our results demonstrate the benefit of anisotropic shading for visualization to achieve data-driven local illumination for improved perception compared to isotropic shading.", "keywords": "Direct volume rendering, volume illumination, anisotropic shading", "link": "http://dx.doi.org/10.1109/TVCG.2015.2467963", "refList": ["10.1109/visual.1996.567777", "10.2312/conf/eg2012/stars/053-074", "10.1109/tvcg.2014.2346411", "10.1109/tvcg.2010.37", "10.1109/tvcg.2013.129", "10.1145/2492684", "10.1145/965141.563893", "10.1111/j.1467-8659.2009.01695.x", "10.1109/tvcg.2012.232", "10.1109/tvcg.2011.35", "10.1109/tvcg.2009.45", "10.1109/visual.2004.5", "10.1371/journal.pone.0038586", "10.1109/tvcg.2003.1196000", "10.1109/visual.1999.809886", "10.1145/2448196.2448205", "10.1145/340916.340922", "10.1068/p3060", "10.1109/visual.1994.346331", "10.1109/visual.2003.1250414", "10.1086/144246", "10.1109/tip.2003.819861", "10.1109/svv.1998.729583", "10.1145/1283900.1283908", "10.1109/tvcg.2011.161", "10.1111/cgf.12300", "10.1167/3.5.3", "10.1109/38.511", "10.1109/tvcg.2011.198", "10.1109/tvcg.2012.267", "10.1111/j.1467-8659.2009.01477.x", "10.1117/12.429489", "10.1109/tvcg.2014.2346333", "10.1109/2945.468400", "10.1109/pacificvis.2010.5429594"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2016.2599041", "title": "OSPRay - A CPU Ray Tracing Framework for Scientific Visualization", "year": "2016", "conferenceName": "SciVis", "authors": "Ingo Wald;Gregory P. Johnson;Jefferson Amstutz;Carson Brownlee;Aaron Knoll;Jim Jeffers;Johannes G\u00fcnther;Paul A. Navr\u00e1til", "citationCount": "55", "affiliation": "Wald, I (Corresponding Author), Intel Corp, Santa Clara, CA 95051 USA. Wald, I.; Johnson, G. P.; Amstutz, J.; Brownlee, C.; Jeffers, J.; Guenther, J., Intel Corp, Santa Clara, CA 95051 USA. Brownlee, C.; Navratil, P., Texas Adv Comp Ctr, Austin, TX USA. Knoll, A., Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA. Knoll, A., Argonne Natl Lab, 9700 S Cass Ave, Argonne, IL 60439 USA.", "countries": "USA", "abstract": "Scientific data is continually increasing in complexity, variety and size, making efficient visualization and specifically rendering an ongoing challenge. Traditional rasterization-based visualization approaches encounter performance and quality limitations, particularly in HPC environments without dedicated rendering hardware. In this paper, we present OSPRay, a turn-key CPU ray tracing framework oriented towards production-use scientific visualization which can utilize varying SIMD widths and multiple device backends found across diverse HPC resources. This framework provides a high-quality, efficient CPU-based solution for typical visualization workloads, which has already been integrated into several prevalent visualization packages. We show that this system delivers the performance, high-level API simplicity, and modular device support needed to provide a compelling new rendering framework for implementing efficient scientific visualization workflows.", "keywords": "", "link": "http://dx.doi.org/10.1109/TVCG.2016.2599041", "refList": ["10.1111/j.1467-8659.2005.00855.x", "10.1109/tvcg.2015.2467963", "10.1016/0263-7855(96)00018-5", "10.1145/1778765.1778803", "10.1145/2019627.2019634", "10.1109/rt.2007.4342603", "10.1109/mcg.2009.130", "10.1109/pacificvis.2011.5742355", "10.1145/300523.300537", "10.1371/journal.pone.0038586", "10.1109/2945.795215", "10.1145/2601097.2601199", "10.1109/pvgs.2003.1249046", "10.1109/tvcg.2010.173"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2017.2743979", "title": "Screen-Space Normal Distribution Function Caching for Consistent Multi-Resolution Rendering of Large Particle Data", "year": "2017", "conferenceName": "SciVis", "authors": "Mohamed Ibrahim;Patrick Wickenhauser;Peter Rautek;Guido Reina;Markus Hadwiger", "citationCount": "1", "affiliation": "Ibrahim, M (Corresponding Author), KAUST, Thuwal 239556900, Saudi Arabia. Ibrahim, Mohamed; Rautek, Peter; Hadwiger, Markus, KAUST, Thuwal 239556900, Saudi Arabia. Wickenhaeuser, Patrick; Reina, Guido, Univ Stuttgart, Visualizat Res Ctr VISUS, Stuttgart, Germany.", "countries": "Germany;Arabia", "abstract": "Molecular dynamics (MD) simulations are crucial to investigating important processes in physics and thermodynamics. The simulated atoms are usually visualized as hard spheres with Phong shading, where individual particles and their local density can be perceived well in close-up views. However, for large-scale simulations with 10 million particles or more, the visualization of large fields-of-view usually suffers from strong aliasing artifacts, because the mismatch between data size and output resolution leads to severe under-sampling of the geometry. Excessive super-sampling can alleviate this problem, but is prohibitively expensive. This paper presents a novel visualization method for large-scale particle data that addresses aliasing while enabling interactive high-quality rendering. We introduce the novel concept of screen-space normal distribution functions (S-NDFs) for particle data. S-NDFs represent the distribution of surface normals that map to a given pixel in screen space, which enables high-quality re-lighting without re-rendering particles. In order to facilitate interactive zooming, we cache S-NDFs in a screen-space mipmap (S-MIP). Together, these two concepts enable interactive, scale-consistent re-lighting and shading changes, as well as zooming, without having to re-sample the particle data. We show how our method facilitates the interactive exploration of real-world large-scale MD simulation data in different scenarios.", "keywords": "Multiresolution Techniques,Point-Based Data,Glyph-based Techniques,Scalability Issues,Molecular Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2017.2743979", "refList": ["10.1109/tvcg.2011.81", "10.1109/tvcg.2010.215", "10.2312/spbg/spbg06/059-065", "10.1111/j.1467-8659.2009.01698.x", "10.1109/tvcg.2014.2350479", "10.1145/2508363.2508422", "10.1145/965141.563893", "10.1111/j.1467-8659.2012.03128.x", "10.1111/j.1467-8659.2011.01964.x", "10.1111/cgf.12363", "10.2312/vcbm.20151209", "10.1145/2366145.2366152", "10.1109/visual.2003.1250404", "10.1007/978-3-642-38750-0\\_1", "10.1109/tvcg.2007.70439", "10.1109/scivis.2015.7429492", "10.1145/1239451.1239479", "10.2312/egwr/egsr07/195-206", "10.1109/tvcg.2014.2346324", "10.1111/cgf.12197", "10.2312/egpgv/egpgv13/009-016", "10.1109/tvcg.2009.142", "10.1109/tvcg.2016.2599041", "10.1109/sc.2014.40", "10.1109/visual.2004.103", "10.1021/ct500169q", "10.1145/1730804.1730834"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2017.2745105", "title": "CasCADe: A Novel 4D Visualization System for Virtual Construction Planning", "year": "2017", "conferenceName": "InfoVis", "authors": "Paulo Ivson;Daniel Nascimento;Waldemar Celes Filho;Simone D. J. Barbosa", "citationCount": "3", "affiliation": "Ivson, P (Corresponding Author), Pontificia Univ Catolica Rio de Janeiro, Tecgraf Inst, Rio de Janeiro, Brazil. Ivson, Paulo; Nascimento, Daniel; Celes, Waldemar, Pontificia Univ Catolica Rio de Janeiro, Tecgraf Inst, Rio de Janeiro, Brazil. Barbosa, Simone D. J., Pontificia Univ Catolica Rio de Janeiro, Informat Dept, Rio de Janeiro, Brazil.", "countries": "Brazil", "abstract": "Building Information Modeling (BIM) provides an integrated 3D environment to manage large-scale engineering projects. The Architecture, Engineering and Construction (AEC) industry explores 4D visualizations over these datasets for virtual construction planning. However, existing solutions lack adequate visual mechanisms to inspect the underlying schedule and make inconsistencies readily apparent. The goal of this paper is to apply best practices of information visualization to improve 4D analysis of construction plans. We first present a review of previous work that identifies common use cases and limitations. We then consulted with AEC professionals to specify the main design requirements for such applications. These guided the development of CasCADe, a novel 4D visualization system where task sequencing and spatio-temporal simultaneity are immediately apparent. This unique framework enables the combination of diverse analytical features to create an information-rich analysis environment. We also describe how engineering collaborators used CasCADe to review the real-world construction plans of an Oil &amp;amp; Gas process plant. The system made evident schedule uncertainties, identified work-space conflicts and helped analyze other constructability issues. The results and contributions of this paper suggest new avenues for future research in information visualization for the AEC industry.", "keywords": "Visualization in physical sciences and engineering,design studies,integrating spatial and non-spatial data visualization,task and requirements analysis", "link": "http://dx.doi.org/10.1109/TVCG.2017.2745105", "refList": ["10.1061/(asce)0733-9364(2000)126:4(251", "10.1109/tvcg.2013.126", "10.1109/tvcg.2011.127", "10.1111/j.1467-8659.2010.01725.x", "10.1145/1053427.1053439", "10.1061/(asce)0733-9364(2002)128:4(287", "10.1016/s0926-5805(98)00053-3", "10.1109/2945.981847", "10.1109/tvcg.2009.102", "10.1016/j.autcon.2009.11.004", "10.1109/iv.1999.781617", "10.1016/j.autcon.2014.03.009", "10.1109/tvcg.2007.70539", "10.1111/1467-8659.00689", "10.1109/mcg.2003.1210862", "10.1016/s0097-8493(00)00130-8", "10.1016/j.autcon.2012.08.004", "10.1061/(asce)0887-3801(2009)23:6(391", "10.1109/tii.2012.2188901", "10.1109/mcg.2003.1242376", "10.1109/tvcg.2007.70570", "10.1145/345513.345271", "10.1016/j.autcon.2010.09.016", "10.1006/ijhc.2000.0418", "10.1111/j.1467-8659.2012.03081.x", "10.1109/tvcg.2009.152", "10.1061/(asce)0733-9364(2004)130:4(598", "10.1145/199404.199409", "10.1109/tvcg.2011.144", "10.1109/iv.2004.1320137", "10.1016/j.autcon.2011.07.005", "10.1109/tvcg.2008.194", "10.1109/tvcg.2012.110", "10.1109/iv.2008.18", "10.1109/tvcg.2012.213", "10.1109/tvcg.2016.2599041", "10.1109/tvcg.2007.70515", "10.1145/2699276.2699280", "10.1179/000870403235002042", "10.1111/j.1467-9671.2010.01194.x", "10.1109/tvcg.2008.59", "10.1109/visual.1997.663876", "10.1109/tvcg.2007.70574", "10.1109/2945.466717", "10.1016/j.autcon.2006.04.001", "10.1016/j.cad.2014.02.001", "10.1109/tvcg.2004.1260759", "10.1145/1279640.1279642", "10.1016/j.autcon.2011.10.003", "10.1109/visual.1996.568118", "10.1016/j.autcon.2014.04.009", "10.1061/(asce)0733-9364(1996)122:4(337", "10.1109/vl.1996.545307", "10.1108/09699980910970860", "10.1109/visual.1995.480803", "10.1109/tvcg.2006.115", "10.1016/j.autcon.2004.06.002", "10.1016/j.autcon.2013.09.003", "10.1061/(asce)co.1943-7862.0000102", "10.1016/j.autcon.2009.11.015", "10.1109/tvcg.2009.84", "10.1145/1090122.1090154", "10.1061/9780784413616.040", "10.1145/1360612.1360700", "10.3758/bf03206757", "10.1016/j.aei.2015.01.011", "10.1016/j.autcon.2015.02.007", "10.1111/j.1538-4632.2005.00575.x", "10.1061/(asce)0887-3801(2002)16:2(124", "10.1145/882262.882299", "10.1016/j.autcon.2004.08.012", "10.1016/j.autcon.2011.12.011", "10.1016/j.compenvurbsys.2009.07.003", "10.1061/(asce)lm.1943-5630.0000127", "10.1080/0144619042000201376", "10.1109/tvcg.2005.62", "10.1109/cmv.2007.20", "10.1109/70.56661", "10.1109/2945.841119", "10.1109/tvcg.2007.70535", "10.1109/iv.2011.15", "10.1016/j.cag.2010.11.015", "10.1016/j.aei.2009.05.002", "10.1109/iv.2003.1218054", "10.1109/sibgrapi.2007.36", "10.1109/tvcg.2012.265", "10.1145/2601097.2601199", "10.1109/2945.675649", "10.1109/iv.1997.626539", "10.1016/j.autcon.2008.10.003", "10.1109/2945.468391"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2018.2864841", "title": "A Declarative Grammar of Flexible Volume Visualization Pipelines", "year": "2018", "conferenceName": "SciVis", "authors": "Min Shih;Charles Rozhon;Kwan-Liu Ma", "citationCount": "1", "affiliation": "Shih, M (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Shih, Min; Rozhon, Charles; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "This paper presents a declarative grammar for conveniently and effectively specifying advanced volume visualizations. Existing methods for creating volume visualizations either lack the flexibility to specify sophisticated visualizations or are difficult to use for those unfamiliar with volume rendering implementation and parameterization. Our design provides the ability to quickly create expressive visualizations without knowledge of the volume rendering implementation. It attempts to capture aspects of those difficult but powerful methods while remaining flexible and easy to use. As a proof of concept, our current implementation of the grammar allows users to combine multiple data variables in various ways and define transfer functions for diverse input data. The grammar also has the ability to describe advanced shading effects and create animations. We demonstrate the power and flexibility of our approach using multiple practical volume visualizations.", "keywords": "Volume visualization,direct volume rendering,declarative specification,multivariate/multimodal volume data,animation", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864841", "refList": ["10.1109/38.31462", "10.1007/978-0-387-98141-3\\_1", "10.1109/visual.1992.235219", "10.1109/visual.2004.95", "10.1080/14685240802376389", "10.1109/tvcg.2015.2467449", "10.1109/visual.2005.1532788", "10.1109/tvcg.2011.185", "10.1145/2345156.2254079", "10.1109/scivis.2015.7429514", "10.1109/tvcg.2014.2346318", "10.1111/j.1467-8659.2011.01952.x", "10.2312/vissym/eurovis07/115-122", "10.1109/tvcg.2002.1021579", "10.1109/mcg.2009.130", "10.1109/tvcg.2015.2467091", "10.1109/tvcg.2007.70555", "10.1111/j.1467-8659.2007.01095.x", "10.1016/j.parco.2007.09.001", "10.1109/tvcg.2009.174", "10.1109/tvcg.2007.70534", "10.1109/tvcg.2016.2599041", "10.1109/tvcg.2014.2346322", "10.1109/mcg.2008.96", "10.1109/tvcg.2009.189", "10.1109/ipdps.2011.385", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13958", "year": "2020", "title": "CPU Ray Tracing of Tree-Based Adaptive Mesh Refinement Data", "conferenceName": "EuroVis", "authors": "Feng Wang;Nathan Marshak;William Usher;Carsten Burstedde;Aaron Knoll;Timo Heister;Chris R. Johnson", "citationCount": "0", "affiliation": "Wang, F (Corresponding Author), Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA.\nWang, Feng; Marshak, Nathan; Usher, Will; Johnson, Chris R., Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA.\nUsher, Will; Knoll, Aaron, Intel Corp, Santa Clara, CA 95051 USA.\nBurstedde, Carsten, Univ Bonn, Inst Numer Simulat, Bonn, Germany.\nHeister, Timo, Clemson Univ, Sch Math \\& Stat Sci, Clemson, SC 29631 USA.", "countries": "Germany;USA", "abstract": "Adaptive mesh refinement (AMR) techniques allow for representing a simulation's computation domain in an adaptive fashion. Although these techniques have found widespread adoption in high-performance computing simulations, visualizing their data output interactively and without cracks or artifacts remains challenging. In this paper, we present an efficient solution for direct volume rendering and hybrid implicit isosurface ray tracing of tree-based AMR (TB-AMR) data. We propose a novel reconstruction strategy, Generalized Trilinear Interpolation (GTI), to interpolate across AMR level boundaries without cracks or discontinuities in the surface normal. We employ a general sparse octree structure supporting a wide range of AMR data, and use it to accelerate volume rendering, hybrid implicit isosurface rendering and value queries. We demonstrate that our approach achieves artifact-free isosurface and volume rendering and provides higher quality output images compared to existing methods at interactive rendering rates.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13958", "refList": ["10.1109/tvcg.2010.240", "10.1016/s0010-4655(99)00501-9", "10.1109/5992.774839", "10.1145/3139295.3139305", "10.2312/vissym/eurovis06/259-266", "10.1145/195826.195828", "10.1109/ldav48142.2019.8944267", "10.2312/evs.20191167", "10.1109/tvcg.2018.2864850", "10.1109/tvcg.2005.79", "10.21105/joss.01370", "10.1016/0021-9991(84)90073-1", "10.1109/visual.1998.745713", "10.1145/2601097.2601199", "10.1109/ldav.2013.6675156", "10.1109/tvcg.2009.149", "10.1007/s00371-008-0215-2", "10.1109/tvcg.2016.2599041", "10.1137/100791634", "10.1109/inpar.2012.6339601", "10.2312/vg/vg-pbg08/163-170", "10.1109/ldav.2012.6378973", "10.2312/pgv.20181091"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1111/cgf.12914", "year": "2016", "title": "MCFTLE: Monte Carlo Rendering of Finite-Time Lyapunov Exponent Fields", "conferenceName": "EuroVis", "authors": "Tobias G{\\\"{u}}nther;Alexander Kuhn;Holger Theisel", "citationCount": "7", "affiliation": "Gunther, T (Corresponding Author), Univ Magdeburg, Visual Comp Grp, D-39106 Magdeburg, Germany.\nGuenther, Tobias; Theisel, Holger, Univ Magdeburg, Visual Comp Grp, D-39106 Magdeburg, Germany.\nKuhn, Alexander, Zuse Inst Berlin, Berlin, Germany.", "countries": "Germany", "abstract": "Traditionally, Lagrangian fields such as finite-time Lyapunov exponents (FTLE) are precomputed on a discrete grid and are ray casted afterwards. This, however, introduces both grid discretization errors and sampling errors during ray marching. In this work, we apply a progressive, view-dependent Monte Carlo-based approach for the visualization of such Lagrangian fields in time-dependent flows. Our approach avoids grid discretization and ray marching errors completely, is consistent, and has a low memory consumption. The system provides noisy previews that converge over time to an accurate high-quality visualization. Compared to traditional approaches, the proposed system avoids explicitly predefined fieldline seeding structures, and uses a Monte Carlo sampling strategy named Woodcock tracking to distribute samples along the view ray. An acceleration of this sampling strategy requires local upper bounds for the FTLE values, which we progressively acquire during the rendering. Our approach is tailored for high-quality visualizations of complex FTLE fields and is guaranteed to faithfully represent detailed ridge surface structures as indicators for Lagrangian coherent structures (LCS). We demonstrate the effectiveness of our approach by using a set of analytic test cases and real-world numerical simulations.", "keywords": "", "link": "https://doi.org/10.1111/cgf.12914", "refList": ["10.13182/nse72-1", "10.1142/s0218127404011430", "10.1109/pacificvis.2012.6183582", "10.13182/nse68-1", "10.1109/tvcg.2014.2346319", "10.1016/j.physd.2005.10.007", "10.1016/s0167-2789(00)00142-1", "10.1111/j.1467-8659.2010.01831.x", "10.1007/s00371-005-0287-1", "10.1145/2601097.2601219", "10.1111/j.1467-8659.2012.03148.x", "10.1117/12.2083253", "10.1109/tvcg.2007.70551", "10.1109/tvcg.2007.70554", "10.1371/journal.pone.0038586", "10.1145/1409060.1409083", "10.1109/tvcg.2012.131", "10.1109/tvcg.2010.227", "10.1109/tvcg.2015.2467963", "10.1145/2661229.2661292", "10.1109/tvcg.2014.2325043", "10.1109/tvcg.2011.265", "10.1016/s0167-2789(00)00199-8", "10.1016/j.jqsrt.2013.04.001", "10.1109/tvcg.2013.128", "10.1007/978-3-540-74496-2\\_35", "10.1016/j.jocs.2014.12.002", "10.1111/cgf.12280", "10.1111/cgf.12269", "10.1109/tvcg.2012.33", "10.1145/1866158.1866199", "10.1111/cgf.12592", "10.1146/annurev-fluid-010313-141322", "10.1109/2945.468400", "10.1111/j.1467-8659.2011.01901.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934313", "title": "Accelerated Monte Carlo Rendering of Finite-Time Lyapunov Exponents", "year": "2019", "conferenceName": "SciVis", "authors": "Irene Baeza Rojo;Markus H. Gross;Tobias G\u00fcnther", "citationCount": "0", "affiliation": "Rojo, IB (Corresponding Author), Swiss Fed Inst Technol, Comp Graph Lab, Zurich, Switzerland. Rojo, Irene Baeza; Gross, Markus; Guenther, Tobias, Swiss Fed Inst Technol, Comp Graph Lab, Zurich, Switzerland.", "countries": "Switzerland", "abstract": "Time-dependent fluid flows often contain numerous hyperbolic Lagrangian coherent structures, which act as transport barriers that guide the advection. The finite-time Lyapunov exponent is a commonly-used approximation to locate these repelling or attracting structures. Especially on large numerical simulations, the FTLE ridges can become arbitrarily sharp and very complex. Thus, the discrete sampling onto a grid for a subsequent direct volume rendering is likely to miss sharp ridges in the visualization. For this reason, an unbiased Monte Carlo-based rendering approach was recently proposed that treats the FTLE field as participating medium with single scattering. This method constructs a ground truth rendering without discretization, but it is prohibitively slow with render times in the order of days or weeks for a single image. In this paper, we accelerate the rendering process significantly, which allows us to compute video sequence of high-resolution FTLE animations in a much more reasonable time frame. For this, we follow two orthogonal approaches to improve on the rendering process: the volumetric light path integration in gradient domain and an acceleration of the transmittance estimation. We analyze the convergence and performance of the proposed method and demonstrate the approach by rendering complex FTLE fields in several 3D vector fields.", "keywords": "Scientific visualization,Monte Carlo,feature extraction,finite-time Lyapunov exponents,gradient domain,Fourier", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934313", "refList": ["10.1088/1749-4699/1/1/015005", "10.1145/2661229.2661291", "10.1145/63039.63042", "10.1111/cgf.13319", "10.1145/2766997", "10.1109/pacificvis.2012.6183582", "10.1103/physrevfluids.2.090502", "10.1111/cgf.13104", "10.1145/3197517.3201363", "10.1016/j.physd.2005.10.007", "10.1145/1882261.1866199", "10.1016/s0167-2789(00)00142-1", "10.1111/j.1467-8659.2010.01831.x", "10.1007/s00371-005-0287-1", "10.1111/j.1467-8659.2012.03148.x", "10.1109/tvcg.2007.70551", "10.1109/tvcg.2007.70554", "10.1371/journal.pone.0038586", "10.1109/tvcg.2010.227", "10.1145/2661229.2661292", "10.1109/tvcg.2014.2325043", "10.1088/0022-3727/1/11/423", "10.1109/tvcg.2011.265", "10.1016/s0167-2789(00)00199-8", "10.1111/cgf.12914", "10.1016/j.jqsrt.2013.04.001", "10.1109/tvcg.2013.128", "10.1007/978-3-540-74496-2\\_35", "10.1063/1.3270044", "10.1016/j.jocs.2014.12.002", "10.1111/j.1467-8659.2008.01165.x", "10.1111/cgf.13342", "10.1111/cgf.12269", "10.1109/tvcg.2012.33", "10.1145/3072959.3073665", "10.1111/cgf.13102", "10.1145/3072959.3073601", "10.1111/cgf.12592", "10.1111/j.1467-8659.2003.00723.x", "10.1146/annurev-fluid-010313-141322", "10.1002/qj.828", "10.1111/j.1467-8659.2011.01901.x"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.12916", "year": "2016", "title": "Decoupled Shading for Real-time Heterogeneous Volume Illumination", "conferenceName": "EuroVis", "authors": "Y. Zhang;K.{-}L. Ma", "citationCount": "4", "affiliation": "Zhang, Y (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA.\nZhang, Y.; Ma, K. -L., Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "Existing real-time volume rendering techniques which support global illumination are limited in modeling distinct realistic appearances for classified volume data, which is a desired capability in many fields of study for illustration and education. Directly extending the emission-absorption volume integral with heterogeneous material shading becomes unaffordable for real-time applications because the high-frequency view-dependent global lighting needs to be evaluated per sample along the volume integral. In this paper, we present a decoupled shading algorithm for multi-material volume rendering that separates global incident lighting evaluation from per-sample material shading under multiple light sources. We show how the incident lighting calculation can be optimized through a sparse volume integration method. The quality, performance and usefulness of our new multi-material volume rendering method is demonstrated through several examples.", "keywords": "Global illumination; high performance; multi-material; multiple scattering; soft shadow; volume rendering", "link": "https://doi.org/10.1111/cgf.12916", "refList": ["10.1109/tvcg.2013.17", "10.1111/j.1467-8659.2009.01464.x", "10.1145/197938.197971", "10.1109/tvcg.2013.129", "10.1145/360349.360353", "10.1145/965141.563893", "10.1111/j.1467-8659.2008.01154.x", "10.1109/tvcg.2011.35", "10.1371/journal.pone.0038586", "10.1109/tvcg.2015.2467963", "10.1111/cgf.12252", "10.1016/j.cag.2010.03.005", "10.1111/j.1467-8659.2007.01095.x", "10.1109/tvcg.2013.172", "10.1109/tvcg.2011.198", "10.1109/tvcg.2014.2346333", "10.1145/360825.360839", "10.1109/2945.468400", "10.1109/tvcg.2007.70573", "10.1109/pacificvis.2010.5429594"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2017.2744438", "title": "Interactive Dynamic Volume Illumination with Refraction and Caustics", "year": "2017", "conferenceName": "SciVis", "authors": "Jens G. Magnus;Stefan Bruckner", "citationCount": "4", "affiliation": "Magnus, JG (Corresponding Author), Univ Bergen, Bergen, Norway. Magnus, Jens G.; Bruckner, Stefan, Univ Bergen, Bergen, Norway.", "countries": "Norway", "abstract": "In recent years, significant progress has been made in developing high-quality interactive methods for realistic volume illumination. However, refraction - despite being an important aspect of light propagation in participating media - has so far only received little attention. In this paper, we present a novel approach for refractive volume illumination including caustics capable of interactive frame rates. By interleaving light and viewing ray propagation, our technique avoids memory-intensive storage of illumination information and does not require any precomputation. It is fully dynamic and all parameters such as light position and transfer function can be modified interactively without a performance penalty.", "keywords": "Interactive volume rendering,illumination,refraction,shadows,caustics", "link": "http://dx.doi.org/10.1109/TVCG.2017.2744438", "refList": ["10.1111/j.1467-8659.2009.01464.x", "10.1073/pnas.1500913112", "10.1145/1268517.1268548", "10.1109/tvcg.2011.211", "10.1109/tvcg.2003.1196003", "10.1109/tvcg.2013.129", "10.1109/tvcg.2010.255", "10.1145/2492684", "10.1111/j.1467-8659.2009.01695.x", "10.1111/j.1467-8659.2008.01154.x", "10.1109/pacificvis.2015.7156382", "10.1109/mcg.2010.98", "10.1145/1360612.1360634", "10.1111/1467-8659.1020121", "10.1016/j.gmod.2006.07.003", "10.1145/1073204.1073310", "10.1109/tvcg.2009.45", "10.1177/0956797611408734", "10.2312/vissym/eurovis05/215-222", "10.1111/j.1467-8659.2004.00794.x", "10.1145/1111411.1111439", "10.1038/srep44274", "10.1145/2448196.2448205", "10.2312/egwr/egsr07/195-206", "10.1145/1944745.1944764", "10.1111/j.1467-8659.2010.01733.x", "10.1109/tvcg.2002.1021575", "10.1111/cgf.12252", "10.1109/38.55151", "10.1111/cgf.12916", "10.1109/tvcg.2011.161", "10.1145/2557605", "10.1109/tvcg.2011.198", "10.2312/egwr/egsr05/291-300", "10.1145/357290.357293", "10.1109/tvcg.2014.2346333", "10.1109/2945.468400", "10.1145/1230100.1230116"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 19}, {"doi": "10.1111/cgf.12934", "year": "2016", "title": "State of the Art in Transfer Functions for Direct Volume Rendering", "conferenceName": "EuroVis", "authors": "Patric Ljung;Jens H. Kr{\\\"{u}}ger;M. Eduard Gr{\\\"{o}}ller;Markus Hadwiger;Charles D. Hansen;Anders Ynnerman", "citationCount": "33", "affiliation": "Ljung, P (Corresponding Author), Linkoping Univ, S-58183 Linkoping, Sweden.\nLjung, Patric; Ynnerman, Anders, Linkoping Univ, S-58183 Linkoping, Sweden.\nKrueger, Jens, Univ Duisburg Essen, CoViDAG, Essen, Germany.\nKrueger, Jens; Hansen, Charles D., Univ Utah, Sci Comp \\& Imaging Inst, Salt Lake City, UT 84112 USA.\nGroeller, Eduard, TU Wien, Vienna, Austria.\nGroeller, Eduard, Univ Bergen, N-5020 Bergen, Norway.\nHadwiger, Markus, King Abdullah Univ Sci \\& Technol, Thuwal, Saudi Arabia.", "countries": "USA;Arabia;Germany;Austria;Sweden;Norway", "abstract": "A central topic in scientific visualization is the transfer function (TF) for volume rendering. The TF serves a fundamental role in translating scalar and multivariate data into color and opacity to express and reveal the relevant features present in the data studied. Beyond this core functionality, TFs also serve as a tool for encoding and utilizing domain knowledge and as an expression for visual design of material appearances. TFs also enable interactive volumetric exploration of complex data. The purpose of this state-of-the-art report (STAR) is to provide an overview of research into the various aspects of TFs, which lead to interpretation of the underlying data through the use of meaningful visual representations. The STAR classifies TF research into the following aspects: dimensionality, derived attributes, aggregated attributes, rendering aspects, automation, and user interfaces. The STAR concludes with some interesting research challenges that form the basis of an agenda for the development of next generation TF tools and methodologies.", "keywords": "", "link": "https://doi.org/10.1111/cgf.12934", "refList": ["10.1109/tvcg.2008.198", "10.2312/vissym/vissym04/017-024.8", "10.2312/vissym/vissym02/115-124", "10.1109/tvcg.2011.97", "10.1109/visual.2003.1250413", "10.1109/tvcg.2015.2467031", "10.1109/tvcg.2009.120", "10.1109/pccga.2004.1348348", "10.1111/j.1467-8659.2011.01944.x", "10.1109/tvcg.2008.170", "10.1109/svv.1998.729588", "10.1111/cgf.12365", "10.1109/tvcg.2009.115", "10.1371/journal.pone.0038586", "10.1109/tvcg.2010.195", "10.1109/tvcg.2006.148", "10.1111/cgf.12623", "10.1109/visual.2003.1250386", "10.1109/sccg.2001.945360", "10.1109/ldav.2014.7013202", "10.1109/pacificvis.2014.24", "10.1109/visual.1999.809932", "10.1109/38.865879", "10.1109/tvcg.2011.261", "10.1109/pacificvis.2009.4906854", "10.1109/tvcg.2008.162", "10.2312/vg/vg-pbg08/041-048", "10.2312/vg/vg06/001-008", "10.1109/tvcg.2014.2346411", "10.1109/2945.998670", "10.2312/vissym/eurovis05/263-270", "10.2312/conf/eg2012/stars/075-094", "10.1016/j.cag.2012.02.007", "10.1109/tvcg.2015.2467294", "10.1109/tvcg.2014.2346351", "10.1109/tvcg.2008.25", "10.2312/vissym/eurovis07/115-122", "10.1109/visual.1999.809886", "10.1111/cgf.12624", "10.1109/tvcg.2008.169", "10.1109/tvcg.2002.1021579", "10.1109/tvcg.2007.1051", "10.1109/tvcg.2010.239", "10.1111/cgf.12371", "10.1109/tvcg.2007.70518", "10.1109/tvcg.2014.2346324", "10.1109/visual.1998.745319", "10.1109/jbhi.2013.2263227", "10.1109/tvcg.2012.80", "10.1109/tvcg.2006.100", "10.1007/s10915-011-9501-7", "10.1109/pacificvis.2013.6596129", "10.1109/pacificvis.2010.5429615", "10.1109/tvcg.2006.124", "10.1145/1375714.1375729", "10.1109/visual.2004.48", "10.1109/ldav.2011.6092313", "10.1109/38.920623", "10.2312/vissym/eurovis05/069-076", "10.1109/2945.646238", "10.1016/j.gmod.2003.08.002", "10.1109/tvcg.2010.35", "10.1109/svv.1998.729580", "10.1109/visual.1995.480803", "10.2312/vissym/eurovis06/251-258", "10.2312/vissym/eurovis06/227-234", "10.1109/tvcg.2015.2467431", "10.2312/vissym/vissym04/017-024", "10.1109/tvcg.2006.39", "10.1016/j.cmpb.2007.03.008", "10.1109/pacificvis.2011.5742368", "10.1109/tvcg.2007.47", "10.1111/j.1467-8659.2007.01095.x", "10.1109/tvcg.2010.170", "10.2312/vcbm/vcbm08/101-108", "10.2312/vissym/eurovis06/243-250", "10.1109/visual.2000.885678", "10.2312/vg/vg07/001-008", "10.1109/tvcg.2009.185", "10.1016/j.cag.2008.08.006", "10.1109/icma.2007.4303986", "10.1109/tvcg.2005.38", "10.1109/tvcg.2006.96", "10.1111/j.1467-8659.2009.01474.x", "10.1109/tvcg.2009.189", "10.2312/vissym/eurovis07/131-138", "10.1109/pccga.2002.1167880", "10.1111/j.1467-8659.2012.03123.x", "10.2312/vissym/eurovis05/271-278", "10.1109/tvcg.2009.25", "10.1111/j.1467-8659.2008.01216.x", "10.1109/tvcg.2012.231", "10.1109/tvcg.2011.258", "10.1111/j.1467-8659.2005.00855.x", "10.1109/visual.1996.568113", "10.1109/tvcg.2005.62", "10.1109/tvcg.2011.23", "10.1109/tvcg.2014.2359462", "10.1109/icics.2009.5397587", "10.1109/2945.942694", "10.1109/pacificvis.2009.4906857", "10.1109/tvcg.2007.70591", "10.1109/pacificvis.2010.5429624", "10.1109/visual.2003.1250414", "10.2312/vg/vg05/137-145", "10.1109/tvcg.2012.105", "10.1111/j.1467-8659.2012.03122.x", "10.1109/38.511", "10.1109/2945.856994", "10.1109/tvcg.2006.72", "10.1109/2945.468400", "10.2312/vg/vg10/077-083", "10.1057/ivs.2010.6"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2017.2744078", "title": "An Intelligent System Approach for Probabilistic Volume Rendering Using Hierarchical 3D Convolutional Sparse Coding", "year": "2017", "conferenceName": "SciVis", "authors": "Tran Minh Quan;Junyoung Choi;Haejin Jeong;Won-Ki Jeong", "citationCount": "4", "affiliation": "Ulsan Nat'l Inst. of Science and Technology (UNIST);Ulsan Nat'l Inst. of Science and Technology (UNIST);Ulsan Nat'l Inst. of Science and Technology (UNIST);Ulsan Nat'l Inst. of Science and Technology (UNIST)", "countries": "Ulsan Nat'l Inst. of Science and Technology (UNIST)", "abstract": "In this paper, we propose a novel machine learning-based voxel classification method for highly-accurate volume rendering. Unlike conventional voxel classification methods that incorporate intensity-based features, the proposed method employs dictionary based features learned directly from the input data using hierarchical multi-scale 3D convolutional sparse coding, a novel extension of the state-of-the-art learning-based sparse feature representation method. The proposed approach automatically generates high-dimensional feature vectors in up to 75 dimensions, which are then fed into an intelligent system built on a random forest classifier for accurately classifying voxels from only a handful of selection scribbles made directly on the input data by the user. We apply the probabilistic transfer function to further customize and refine the rendered result. The proposed method is more intuitive to use and more robust to noise in comparison with conventional intensity-based classification methods. We evaluate the proposed method using several synthetic and real-world volume datasets, and demonstrate the methods usability through a user study.", "keywords": "Volume Rendering,Machine Learning,Hierarchically Convolutional Sparse Coding", "link": "http://dx.doi.org/10.1109/TVCG.2017.2744078", "refList": ["10.1207/s15327051hci0701\\_3", "10.1109/tvcg.2008.162", "10.1109/cvpr.2013.57", "10.1109/cvpr.2015.7299149", "10.1109/isbi.2015.7164109", "10.1109/38.920623", "10.1109/tvcg.2009.25", "10.1109/tvcg.2012.231", "10.1109/cvpr.2010.5539957", "10.1007/978-3-319-46726-9\\_56", "10.1109/tsp.2006.881199", "10.1109/cvpr.2006.142", "10.1109/svv.1998.729588", "10.1109/cvpr.2014.394", "10.1111/cgf.12623", "10.1109/icassp.2014.6854992", "10.1111/cgf.12624", "10.1109/tvcg.2002.1021579", "10.1145/54852.378484", "10.1111/cgf.12934", "10.4135/9781412961288.n154", "10.1109/tip.2015.2495260", "10.1109/tvcg.2012.105", "10.1145/3065386", "10.1109/38.511", "10.1109/tvcg.2005.38", "10.1007/978-3-319-12643-2\\_31", "10.1145/360825.360839", "10.2307/1932409", "10.1109/pacificvis.2013.6596129", "10.1109/tvcg.2011.261", "10.1023/a:1010933404324"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00041", "year": "2019", "title": "DNN-VolVis: Interactive Volume Visualization Supported by Deep Neural Network", "conferenceName": "PacificVis", "authors": "Fan Hong;Can Liu;Xiaoru Yuan", "citationCount": "2", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.\nYuan, XR (Corresponding Author), Peking Univ, Sch EECS, Beijing, Peoples R China.\nYuan, XR (Corresponding Author), Peking Univ, Beijing Engn Technol Res Ctr Virtual Simulat \\& Vi, Beijing, Peoples R China.\nHong, Fan; Liu, Can; Yuan, Xiaoru, Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.\nHong, Fan; Liu, Can; Yuan, Xiaoru, Peking Univ, Sch EECS, Beijing, Peoples R China.\nYuan, Xiaoru, Peking Univ, Beijing Engn Technol Res Ctr Virtual Simulat \\& Vi, Beijing, Peoples R China.", "countries": "China", "abstract": "In this work, we propose a novel approach of volume visualization without explicit traditional rendering pipeline. In our proposed method, volumetric images can be interactively `reversed' given the volumetric data and a static volume rendered image under the desired rendering effect. Our pipeline enables 3D-navigation on it for exploring the given volumetric data without explicit transfer function. In our approach, deep neural networks, combined usage of Generative Adversarial Networks (GANs) and Convolutional Neural Networks (CNN) are employed to synthesize high-resolution and perceptually authentic images directly, inheriting the desired transfer function and viewing parameter implicitly given by the input images respectively.", "keywords": "Deep learning; volume rendering; transfer function; generative adversarial network; machine learning", "link": "https://doi.org/10.1109/PacificVis.2019.00041", "refList": ["10.1109/iccv.2017.629", "10.1109/tvcg.2008.162", "10.2312/vissym/eurovis05/271-278", "10.1109/38.920623", "10.1109/tvcg.2009.25", "10.1109/tvcg.2017.2744078", "10.1109/visual.2003.1250413", "10.1109/cvpr.2017.19", "10.1109/tvcg.2010.35", "10.1109/cvpr.2015.7298594", "10.1109/visual.1996.568113", "10.1109/5.726791", "10.1109/svv.1998.729588", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/83.535842", "10.1016/0893-6080(91)90009-t", "10.1109/tvcg.2006.148", "10.2312/vissym/eurovis07/115-122", "10.1038/323533a0", "10.1109/tvcg.2002.1021579", "10.1109/tvcg.2007.1051", "10.1145/1830483.1830503", "10.1109/cvpr.2016.90", "10.1109/tvcg.2012.80", "10.1007/978-3-319-24574-4\\_28", "10.1109/tvcg.2005.38", "10.1109/tvcg.2009.189", "10.1109/pacificvis.2013.6596129", "10.1109/visual.1999.809932", "10.1007/978-3-319-46493-0\\_47", "10.1109/tvcg.2011.261", "10.1109/pccga.2002.1167880"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030374", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data", "year": "2020", "conferenceName": "SciVis", "authors": "Yifan Wang;Guoli Yan;Haikuan Zhu;Sagar Buch;Ying Wang;E. Mark Haacke;Jing Hua;Zichun Zhong", "citationCount": "0", "affiliation": "Wang, YF (Corresponding Author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Wang, Yifan; Yan, Guoli; Zhu, Haikuan; Hua, Jing; Zhong, Zichun, Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Buch, Sagar; Wang, Ying; Haacke, Ewart Mark, Wayne State Univ, Dept Radiol, Detroit, MI 48201 USA.", "countries": "USA", "abstract": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.", "keywords": "Deep neural network,3D cerebrovascular segmentation and visualization,maximum intensity projection (MIP),joint embedding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030374", "refList": ["10.1109/cluster.2018.00036", "10.1109/iccv.2015.123", "10.1109/tvcg.2013.133", "10.1109/cvpr.2017.19", "10.1109/tvcg.2019.2934312", "10.1007/978-3-319-46487-9\\_40", "10.1109/tvcg.2006.175", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2018.2880207", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/tvcg.2018.2816059", "10.1109/pacificvis.2009.4906852", "10.1109/tvcg.2018.2864808", "10.1109/mcg.2018.2881523", "10.1109/tvcg.2015.2467431", "10.1109/tpami.2015.2439281", "10.1109/pacificvis48177.2020.8737", "10.1109/cvpr.2019.00244", "10.1109/tvcg.2006.165", "10.1109/bigdata.2018.8622520", "10.1007/978-3-319-46466-4\\_29", "10.1109/visual.2019.8933759", "10.1145/3197517.3201304", "10.1109/pacificvis.2011.5742378", "10.1109/cvpr.2006.91", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1007/978-3-319-24574-4\\_28", "10.1109/pacificvis.2019.00041", "10.1109/tvcg.2019.2934332", "10.1109/cvpr.2018.00916", "10.1007/978-3-319-46475-6\\_43"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.8737", "year": "2020", "title": "SSR-VFD: Spatial Super-Resolution for Vector Field Data Analysis and Visualization", "conferenceName": "PacificVis", "authors": "Li Guo;Shaojie Ye;Jun Han;Hao Zheng;Han Gao;Danny Z. Chen;Jian{-}Xun Wang;Chaoli Wang", "citationCount": "1", "affiliation": "Guo, L (Corresponding Author), Nankai Univ, Tianjin, Peoples R China.\nGuo, Li, Nankai Univ, Tianjin, Peoples R China.\nYe, Shaojie, Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.\nHan, Jun; Zheng, Hao; Gao, Han; Chen, Danny Z.; Wang, Jian-Xun; Wang, Chaoli, Univ Notre Dame, Notre Dame, IN 46556 USA.", "countries": "USA;China", "abstract": "We present SSR-VFD, a novel deep learning framework that produces coherent spatial super-resolution (SSR) of three-dimensional vector field data (VFD). SSR-VFD is the first work that advocates a machine learning approach to generate high-resolution vector fields from low-resolution ones. The core of SSR-VFD lies in the use of three separate neural nets that take the three components of a low-resolution vector field as input and jointly output a synthesized high-resolution vector field. To capture spatial coherence, we take into account magnitude and angle losses in network optimization. Our method can work in the in situ scenario where VFD are down-sampled at simulation time for storage saving and these reduced VFD are upsampled back to their original resolution during postprocessing. To demonstrate the effectiveness of SSR-VFD, we show quantitative and qualitative results with several vector field data sets of different characteristics and compare our method against volume upscaling using bicubic interpolation, and two solutions based on CNN and GAN, respectively.", "keywords": "Spatial super-resolution; vector field data; convolutional neural network; deep learning", "link": "https://doi.org/10.1109/PacificVis48177.2020.8737", "refList": ["10.1016/j.ijvsm.2017.05.001", "10.1016/j.jvs.2005.01.020", "10.1109/iccv.2015.123", "10.1111/cgf.13620", "10.1109/cvpr.2019.00831", "10.1109/cvpr.2019.00817", "10.1109/cvpr.2019.00399", "10.1109/tvcg.2019.2934312", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2018.2816059", "10.1109/mcg.2018.2881523", "10.1109/tpami.2015.2439281", "10.1109/bigdata.2018.8622520", "10.1145/3197517.3201304", "10.1109/tvcg.2018.2796085", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1109/pacificvis.2019.00041", "10.1111/cgf.13689"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030374", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data", "year": "2020", "conferenceName": "SciVis", "authors": "Yifan Wang;Guoli Yan;Haikuan Zhu;Sagar Buch;Ying Wang;E. Mark Haacke;Jing Hua;Zichun Zhong", "citationCount": "0", "affiliation": "Wang, YF (Corresponding Author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Wang, Yifan; Yan, Guoli; Zhu, Haikuan; Hua, Jing; Zhong, Zichun, Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Buch, Sagar; Wang, Ying; Haacke, Ewart Mark, Wayne State Univ, Dept Radiol, Detroit, MI 48201 USA.", "countries": "USA", "abstract": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.", "keywords": "Deep neural network,3D cerebrovascular segmentation and visualization,maximum intensity projection (MIP),joint embedding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030374", "refList": ["10.1109/cluster.2018.00036", "10.1109/iccv.2015.123", "10.1109/tvcg.2013.133", "10.1109/cvpr.2017.19", "10.1109/tvcg.2019.2934312", "10.1007/978-3-319-46487-9\\_40", "10.1109/tvcg.2006.175", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2018.2880207", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/tvcg.2018.2816059", "10.1109/pacificvis.2009.4906852", "10.1109/tvcg.2018.2864808", "10.1109/mcg.2018.2881523", "10.1109/tvcg.2015.2467431", "10.1109/tpami.2015.2439281", "10.1109/pacificvis48177.2020.8737", "10.1109/cvpr.2019.00244", "10.1109/tvcg.2006.165", "10.1109/bigdata.2018.8622520", "10.1007/978-3-319-46466-4\\_29", "10.1109/visual.2019.8933759", "10.1145/3197517.3201304", "10.1109/pacificvis.2011.5742378", "10.1109/cvpr.2006.91", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1007/978-3-319-24574-4\\_28", "10.1109/pacificvis.2019.00041", "10.1109/tvcg.2019.2934332", "10.1109/cvpr.2018.00916", "10.1007/978-3-319-46475-6\\_43"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.14037", "year": "2020", "title": "State of the Art in Time-Dependent Flow Topology: Interpreting Physical Meaningfulness Through Mathematical Properties", "conferenceName": "EuroVis", "authors": "Roxana Bujack;Lin Yan;Ingrid Hotz;Christoph Garth;Bei Wang", "citationCount": "0", "affiliation": "Bujack, R (Corresponding Author), Los Alamos Natl Lab, Los Alamos, NM 87545 USA.\nBujack, Roxana, Los Alamos Natl Lab, Los Alamos, NM 87545 USA.\nYan, Lin; Wang, Bei, Univ Utah, Sci Comp \\& Imaging Inst, Salt Lake City, UT 84112 USA.\nHotz, Ingrid, Linkopings Univ, Sci Visualizat Grp, Linkoping, Sweden.\nGarth, Christoph, Univ Kaiserslautern, Kaiserslautern, Germany.", "countries": "Sweden;Germany;USA", "abstract": "We present a state-of-the-art report on time-dependent flow topology. We survey representative papers in visualization and provide a taxonomy of existing approaches that generalize flow topology from time-independent to time-dependent settings. The approaches are classified based upon four categories: tracking of steady topology, reference frame adaption, pathline classification or clustering, and generalization of critical points. Our unique contributions include introducing a set of desirable mathematical properties to interpret physical meaningfulness for time-dependent flow visualization, inferring mathematical properties associated with selective research papers, and utilizing such properties for classification. The five most important properties identified in the existing literature include coincidence with the steady case, induction of a partition within the domain, Lagrangian invariance, objectivity, and Galilean invariance.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14037", "refList": ["10.2514/6.1995-1715", "10.1111/cgf.12100", "10.1007/978-3-540-70823-0\\_1", "10.1111/cgf.12885", "10.1063/1.857730", "10.1109/tvcg.2019.2934312", "10.1109/tvcg.2013.92", "10.1063/1.166399", "10.1109/tvcg.2005.68.3", "10.1063/1.4971788", "10.1111/cgf.12933", "10.1145/3072959.3073684", "10.1063/1.4982720", "10.1109/tvcg.2018.2864432", "10.1017/s0022112004002526", "10.1109/tvcg.2010.93", "10.1109/pacificvis.2016.7465253", "10.1109/tvcg.2018.2864505", "10.1063/1.858828", "10.1109/tvcg.2019.2934255", "10.5194/npg-9-237-2002", "10.1063/1.4800210", "10.1007/978-3-540-88606-8\\_12", "10.1111/j.1467-8659.2011.01942.x", "10.1109/2945.928168", "10.1007/978-3-662-10388-31", "10.1016/j.physd.2013.01.013", "10.1111/cgf.12358", "10.1017/s002211209900720x", "10.1017/s0022112097008057", "10.1111/j.1467-8659.2009.01546.x", "10.1109/2.35197", "10.1111/j.1467-8659.2012.03089.x", "10.1111/cgf.13319", "10.1109/tvcg.2019.2934375.3", "10.1016/j.crme.2015.08.002.4", "10.1016/s0167-2789(00)00142-1", "10.1111/cgf.12359", "10.1103/physreve.93.063107", "10.1016/j.physd.2009.05.005", "10.1063/1.1477449", "10.1111/j.1467-8659.2009.01686.x", "10.1109/pacificvis.2011.5742374", "10.1017/jfm.2013.391", "10.1111/cgf.12121", "10.1186/1743-422x-3-15", "10.1063/1.3502450", "10.1063/1.868323", "10.1016/j.cnsns.2013.05.002", "10.1109/visual.2004.99", "10.1109/visual.2004.107", "10.1111/j.1467-8659.2009.01604.x", "10.1146/annurev-fluid-010313-141322", "10.1063/1.3690153", "10.1109/tvcg.2019.2934375", "10.1109/tvcg.2013.143", "10.1137/130940633", "10.1016/j.cag.2014.01.007", "10.1016/s0097-8493(02)00056-0", "10.1109/vl.1996.545307", "10.1109/tvcg.2011.284", "10.1109/tvcg.2011.265", "10.1017/jfm.2016.792", "10.1109/tvcg.2013.208", "10.1007/s12650-016-0348-8", "10.1023/b:elas.0000005548.36767.e7", "10.1007/978-1-4939-0419-8\\_\\_9", "10.1007/bf00849110", "10.1615/int.j.uncertaintyquantification.2012003956", "10.1017/s0022112096001802", "10.1017/s0962492902000065", "10.1109/pacificvis.2019.00041", "10.1111/j.1467-8659.2011.01901.x", "10.1109/tvcg.2008.33", "10.1016/j.physd.2005.10.007", "10.1016/s0894-1777(96)00090-8", "10.1145/2517327.2442526", "10.1109/tvcg.2017.2743938", "10.1109/tvcg.2018.2816059", "10.1109/tvcg.2019.2934242", "10.2514/6.1995-1715.4", "10.1111/cgf.12109", "10.1109/tvcg.2011.269", "10.1109/visual.1990.146359", "10.1111/j.1467-8659.2003.00723.x", "10.5194/npg-18-977-2011", "10.1109/visual.1998.745296", "10.1109/tvcg.2007.70557", "10.1109/tvcg.2014.2312012"], "wos": 1, "children": [], "len": 1}], "len": 9}], "len": 11}, {"doi": "10.1109/tvcg.2018.2864816", "title": "Interactive Visualization of 3D Histopathology in Native Resolution", "year": "2018", "conferenceName": "SciVis", "authors": "Martin Falk;Anders Ynnerman;Darren Treanor;Claes Lundstr\u00f6m", "citationCount": "1", "affiliation": "Falk, M (Corresponding Author), Linkoping Univ, Dept Sci \\& Technol, Linkoping, Sweden. Falk, Martin; Ynnerman, Anders, Linkoping Univ, Dept Sci \\& Technol, Linkoping, Sweden. Treanor, Darren, Leeds Teaching Hosp NHS Trust, Leeds, W Yorkshire, England. Treanor, Darren; Lundstrom, Claes, Linkoping Univ, Ctr Med Image Sci \\& Visualizat CMIV, Linkoping, Sweden. Lundstrom, Claes, Sectra AB, Linkoping, Sweden.", "countries": "Sweden;England", "abstract": "We present a visualization application that enables effective interactive visual analysis of large-scale 3D histopathology, that is, high-resolution 3D microscopy data of human tissue. Clinical work flows and research based on pathology have, until now, largely been dominated by 2D imaging. As we will show in the paper, studying volumetric histology data will open up novel and useful opportunities for both research and clinical practice. Our starting point is the current lack of appropriate visualization tools in histopathology, which has been a limiting factor in the uptake of digital pathology. Visualization of 3D histology data does pose difficult challenges in several aspects. The full-color datasets are dense and large in scale, on the order of 100,000 \u00d7 100,000 \u00d7 100 voxels. This entails serious demands on both rendering performance and user experience design. Despite this, our developed application supports interactive study of 3D histology datasets at native resolution. Our application is based on tailoring and tuning of existing methods, system integration work, as well as a careful study of domain specific demands emanating from a close participatory design process with domain experts as team members. Results from a user evaluation employing the tool demonstrate a strong agreement among the 14 participating pathologists that 3D histopathology will be a valuable and enabling tool for their work.", "keywords": "Histology,Pathology,Volume Rendering,Expert Evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864816", "refList": ["10.4103/2153-3539.129452", "10.2312/vissym/vissym02/115-124", "10.1371/journal.pone.0126817", "10.1093/ajcp/138.suppl1.288", "10.1109/tvcg.2009.150", "10.4103/2153-3539.151890", "10.1109/mcg.2010.26", "10.1109/tbme.2014.2303294", "10.1109/tvcg.2012.240", "10.1111/cgf.12605", "10.1007/s00371-008-0261-9", "10.1109/tvcg.2002.1021579", "10.1016/j.mri.2012.05.001", "10.1109/pacificvis.2017.8031591", "10.1111/his.12629", "10.2312/vissym/vissym00/137-146", "10.4103/2153-3539.151894", "10.1186/s12859-017-1934-z", "10.1117/12.813756", "10.1145/2834117", "10.1111/his.13452", "10.1111/cgf.12934", "10.1001/jama.2017.14585", "10.1016/j.ajpath.2012.01.033", "10.1109/visual.2002.1183757", "10.4103/2153-3539.114206", "10.17629/www.diagnosticpathology.eu-2016-2:232", "10.1109/mcg.2013.55", "10.1073/pnas.1710742114", "10.1117/12.529137", "10.2312/vg/vg-pbg08/163-170", "10.4103/2153-3539.119005"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13183", "year": "2017", "title": "Visual Analysis of Confocal Raman Spectroscopy Data using Cascaded Transfer Function Design", "conferenceName": "EuroVis", "authors": "Christoph M. Schikora;Markus Plack;Rainer Bornemann;Peter Haring Bol{\\'{\\i}}var;Andreas Kolb", "citationCount": "0", "affiliation": "Schikora, CM (Corresponding Author), Univ Siegen, Comp Graph \\& Multimedia Syst, Siegen, Germany.\nSchikora, Christoph M.; Plack, Markus; Kolb, Andreas, Univ Siegen, Comp Graph \\& Multimedia Syst, Siegen, Germany.\nBornemann, Rainer; Bolivar, Peter Haring, Univ Siegen, High Frequency \\& Quantum Elect, Siegen, Germany.", "countries": "Germany", "abstract": "2D Confocal Raman Microscopy (CRM) data consist of high dimensional per-pixel spectral data of 1000 bands and allows for complex spectral and spatial-spectral analysis tasks, i.e., in material discrimination, material thickness, and spatial material distributions. Currently, simple integral methods are commonly applied as visual analysis solutions to CRM data which exhibit restricted discrimination power in various regards. In this paper we present a novel approach for the visual analysis of 2D multispectral CRM data using multi-variate visualization techniques. Due to the large amount of data and the demand of an explorative approach without a-priori restriction, our system allows for arbitrary interactive (de)selection of varaibles w/o limitation and an unrestricted online definition/construction of new, combined properties. Our approach integrates CRM specific quantitative measures and handles material-related features for mixed materials in a quantitative manner. Technically, we realize the online definition/construction of new, combined properties as semi-automatic, cascaded, 1D and 2D multidimensional transfer functions (MD-TFs). By interactively incorporating new (raw or derived) properties, the dimensionality of the MD-TF space grows during the exploration procedure and is virtually unlimited. The final visualization is achieved by an enhanced color mixing step which improves saturation and contrast.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13183", "refList": ["10.1016/j.sab.2006.12.002", "10.1021/nl061702a", "10.1007/978-3-642-12522-5\\_4", "10.1016/j.carbon.2016.01.001", "10.1137/040616024", "10.1016/j.cag.2012.02.007", "10.1111/cgf.12365", "10.2312/vissym/eur0vis05/117-123", "10.1007/s11664-009-0803-6", "10.1109/mcse.2012.27", "10.1109/tvcg.2002.1021579", "10.1007/978-3-540-85567-5\\_50", "10.1109/pacificvis.2010.5429612", "10.1109/tgrs.2010.2051553", "10.1109/tvcg.2007.70591", "10.1021/ac034173t", "10.1366/000370210792434350", "10.1111/cgf.12934", "10.1109/tvcg.2012.110", "10.1109/tvcg.2012.105", "10.1109/tvcg.2006.164", "10.1109/tvcg.2009.199", "10.1109/visual.2003.1250412", "10.1109/igarss.2011.6049397", "10.1007/978-3-642-12522-5", "10.1109/pacificvis.2013.6596129", "10.1039/c4an01061b", "10.1109/tvcg.2011.261"], "wos": 1, "children": [], "len": 1}], "len": 17}], "len": 39}, "index": 347, "embedding": [-1.005408525466919, 1.037095069885254, -1.0271400213241577, -1.7564774751663208, -0.6604432463645935, 0.16650904715061188, -0.7167134881019592, 1.0217056274414062, 0.5587377548217773, 0.8444823622703552, 0.1932263970375061, -0.13081619143486023, 2.073491334915161, 1.976759910583496, 0.853571891784668, 0.9270975589752197, -0.213409423828125, 0.0788058489561081, 0.18228264153003693, 3.4574742317199707, -0.06630208343267441, 2.0393450260162354, 0.4409768581390381, 1.7885390520095825, -2.4801409244537354, 3.3189516067504883, -0.5328229665756226, 0.3427847623825073, 0.9411208033561707, -2.4156227111816406, 0.5421043038368225, 0.99918532371521], "projection": [0.6874610185623169, 8.800553321838379], "size": 20, "height": 6, "width": 7}