{"data": {"doi": "10.1109/tvcg.2019.2934536", "title": "Estimating Color-Concept Associations from Image Statistics", "year": "2019", "conferenceName": "InfoVis", "authors": "Ragini Rathore;Zachary Leggon;Laurent Lessard;Karen B. Schloss", "citationCount": "1", "affiliation": "Rathore, R (Corresponding Author), Univ Wisconsin, Comp Sci, Madison, WI 53706 USA. Rathore, R (Corresponding Author), Univ Wisconsin, WID, Madison, WI 53706 USA. Rathore, Ragini, Univ Wisconsin, Comp Sci, Madison, WI 53706 USA. Rathore, Ragini; Leggon, Zachary; Lessard, Laurent; Schloss, Karen B., Univ Wisconsin, WID, Madison, WI 53706 USA. Leggon, Zachary, Univ Wisconsin, Biol, Madison, WI 53706 USA. Lessard, Laurent, Univ Wisconsin, Elect \\& Comp Engn, Madison, WI 53706 USA. Schloss, Karen B., Univ Wisconsin, Psychol, Madison, WI 53706 USA.", "countries": "USA", "abstract": "To interpret the meanings of colors in visualizations of categorical information, people must determine how distinct colors correspond to different concepts. This process is easier when assignments between colors and concepts in visualizations match people's expectations, making color palettes semantically interpretable. Efforts have been underway to optimize color palette design for semantic interpretablity, but this requires having good estimates of human color-concept associations. Obtaining these data from humans is costly, which motivates the need for automated methods. We developed and evaluated a new method for automatically estimating color-concept associations in a way that strongly correlates with human ratings. Building on prior studies using Google Images, our approach operates directly on Google Image search results without the need for humans in the loop. Specifically, we evaluated several methods for extracting raw pixel content of the images in order to best estimate color-concept associations obtained from human ratings. The most effective method extracted colors using a combination of cylindrical sectors and color categories in color space. We demonstrate that our approach can accurately estimate average human color-concept associations for different fruits using only a small set of images. The approach also generalizes moderately well to more complicated recycling-related concepts of objects that can appear in any color.", "keywords": "Visual Reasoning,Visual Communication,Visual Encoding,Color Perception,Color Cognition,Color Categories", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934536", "refList": ["10.1145/3009924", "10.1023/b:bttj.0000047600.45421.6d", "10.1525/ae.1974.1.1.02a00030", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2016.2598918", "10.1146/annurev-vision-091517-034231", "10.1109/tvcg.2015.2467471", "10.1037/xge0000076", "10.1016/b978-0-08-042415-6.50014-4", "10.1146/annurev-psych-120710-100504", "10.1073/pnas.0906172107", "10.1109/visual.1996.568118", "10.1163/156856808784532662", "10.1073/pnas.1532837100", "10.1016/j.cogpsych.2004.10.001", "10.7717/peerj.453", "10.1037/met0000159", "10.1073/pnas.1619666114", "10.1080/00221309.1962.9711531", "10.1002/col.20010", "10.1037/xlm0000357", "10.1073/pnas.1513298113", "10.1016/0010-0285(73)90017-0", "10.1111/cgf.12127", "10.1007/bf00133570", "10.1145/3025453.3026041", "10.1037/xge0000560", "10.3758/bf03207619", "10.1371/journal.pone.0149538", "10.1186/s41235-018-0090-y", "10.1525/aa.1984.86.1.02a00050", "10.1016/j.apergo.2018.08.010", "10.1073/pnas.0701644104", "10.1109/cvpr.2015.7298965", "10.1002/col.21756", "10.1145/2207676.2208547", "10.1023/a:1008036829907", "10.1109/tvcg.2018.2865147"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030439", "title": "Rainbows Revisited: Modeling Effective Colormap Design for Graphical Inference", "year": "2020", "conferenceName": "InfoVis", "authors": "Khairi Reda;Danielle Albers Szafir", "citationCount": "0", "affiliation": "Reda, K (Corresponding Author), Indiana Univ Purdue Univ, Indianapolis, IN 46202 USA. Reda, Khairi, Indiana Univ Purdue Univ, Indianapolis, IN 46202 USA. Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Color mapping is a foundational technique for visualizing scalar data. Prior literature offers guidelines for effective colormap design, such as emphasizing luminance variation while limiting changes in hue. However, empirical studies of color are largely focused on perceptual tasks. This narrow focus inhibits our understanding of how generalizable these guidelines are, particularly to tasks like visual inference that require synthesis and judgement across multiple percepts. Furthermore, the emphasis on traditional ramp designs (e.g., sequential or diverging) may sideline other key metrics or design strategies. We study how a cognitive metric-color name variation-impacts people's ability to make model-based judgments. In two graphical inference experiments, participants saw a series of color-coded scalar fields sampled from different models and assessed the relationships between these models. Contrary to conventional guidelines, participants were more accurate when viewing colormaps that cross a variety of uniquely nameable colors. We modeled participants' performance using this metric and found that it provides a better fit to the experimental data than do existing design principles. Our findings indicate cognitive advantages for colorful maps like rainbow, which exhibit high color categorization, despite their traditionally undesirable perceptual properties. We also found no evidence that color categorization would lead observers to infer false data features. Our results provide empirically grounded metrics for predicting a colormap's performance and suggest alternative guidelines for designing new quantitative colormaps to support inference. The data and materials for this paper are available at: https://osf.io/tck2r/", "keywords": "Color,perception,graphical inference,scalar data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030439", "refList": ["10.1109/tvcg.2012.233", "10.1109/tvcg.2017.2744359", "10.1111/j.1756-8765.2010.01113.x", "10.1109/tvcg.2016.2598918", "10.1109/tvcg.2015.2467471", "10.1016/b978-0-08-042415-6.50014-4", "10.1146/annurev-psych-120710-100504", "10.1080/13875868.2015.1137577", "10.1109/visual.1996.568118", "10.1006/ijhc.1017", "10.1109/tvcg.2014.2346983", "10.3758/bf03201236", "10.2307/2981473", "10.1037/0033-2909.114.3.510", "10.1111/cgf.12127", "10.1002/nav.3800020109", "10.1145/3025453.3026041", "10.1111/j.1756-8765.2011.01150.x", "10.1016/0010-0285(80)90005-5", "10.1207/s15516709cog1003\\_2", "10.1109/tvcg.2019.2934536", "10.1186/s41235-018-0090-y", "10.1023/a:1013180410169", "10.1038/s41562-017-0058", "10.20982/tqmp.01.1.p042", "10.1137/0105003", "10.1559/152304089783813918", "10.1109/tvcg.2018.2865147", "10.1179/000870403235002042", "10.1179/caj.1968.5.1.54"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030434", "title": "Semantic Discriminability for Visual Communication", "year": "2020", "conferenceName": "InfoVis", "authors": "Karen B. Schloss;Zachary Leggon;Laurent Lessard", "citationCount": "0", "affiliation": "Schloss, KB (Corresponding Author), Univ Wisconsin, Psychol, Madison, WI 53706 USA. Schloss, KB (Corresponding Author), Univ Wisconsin, Wisconsin Inst Discovery, Madison, WI 53706 USA. Schloss, Karen B., Univ Wisconsin, Psychol, Madison, WI 53706 USA. Schloss, Karen B.; Leggon, Zachary, Univ Wisconsin, Wisconsin Inst Discovery, Madison, WI 53706 USA. Leggon, Zachary, Univ Wisconsin, Biol, Madison, WI USA. Lessard, Laurent, Northeastern Univ, Mech \\& Ind Engn, Boston, MA 02115 USA.", "countries": "USA", "abstract": "To interpret information visualizations, observers must determine how visual features map onto concepts. First and foremost, this ability depends on perceptual discriminability; observers must be able to see the difference between different colors for those colors to communicate different meanings. However, the ability to interpret visualizations also depends on semantic discriminability, the degree to which observers can infer a unique mapping between visual features and concepts, based on the visual features and concepts alone (i.e., without help from verbal cues such as legends or labels). Previous evidence suggested that observers were better at interpreting encoding systems that maximized semantic discriminability (maximizing association strength between assigned colors and concepts while minimizing association strength between unassigned colors and concepts), compared to a system that only maximized color-concept association strength. However, increasing semantic discriminability also resulted in increased perceptual distance, so it is unclear which factor was responsible for improved performance. In the present study, we conducted two experiments that tested for independent effects of semantic distance and perceptual distance on semantic discriminability of bar graph data visualizations. Perceptual distance was large enough to ensure colors were more than just noticeably different. We found that increasing semantic distance improved performance, independent of variation in perceptual distance, and when these two factors were uncorrelated, responses were dominated by semantic distance. These results have implications for navigating trade-offs in color palette design optimization for visual communication.", "keywords": "Visual Reasoning,Information Visualization,Visual Communication,Visual Encoding,Color Perception,Color Cognition", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030434", "refList": ["10.1109/tvcg.2012.233", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2016.2598918", "10.1109/tvcg.2015.2467471", "10.1016/b978-0-08-042415-6.50014-4", "10.1146/annurev-psych-120710-100504", "10.1080/13875868.2015.1137577", "10.1109/visual.1996.568118", "10.1006/ijhc.1017", "10.1109/tvcg.2014.2346983", "10.1145/2254556.2254572", "10.3758/bf03201236", "10.2307/2981473", "10.1037/0033-2909.114.3.510", "10.1111/cgf.12127", "10.1002/nav.3800020109", "10.1145/3025453.3026041", "10.1111/j.1756-8765.2011.01150.x", "10.1016/0010-0285(80)90005-5", "10.1207/s15516709cog1003\\_2", "10.1109/tvcg.2019.2934536", "10.1186/s41235-018-0090-y", "10.1023/a:1013180410169", "10.1038/s41562-017-0058", "10.20982/tqmp.01.1.p042", "10.1137/0105003", "10.1559/152304089783813918", "10.1109/tvcg.2018.2865147", "10.1179/000870403235002042", "10.1179/caj.1968.5.1.54"], "wos": 1, "children": [], "len": 1}], "len": 5}, "index": 911, "embedding": [1.3202568292617798, 0.07176069170236588, 1.9032762050628662, 0.5786672830581665, -0.6539726853370667, 0.16650904715061188, 0.4205814301967621, 1.3898718357086182, -0.4441397190093994, -0.8419216275215149, 0.7951267957687378, -0.2370772659778595, -0.14601923525333405, 0.35906848311424255, 1.1155683994293213, 0.1938219517469406, 0.16825935244560242, 0.07072390615940094, -0.7047576308250427, 0.2866613566875458, -0.06630208343267441, 1.247479796409607, -0.2864200174808502, -0.1390654742717743, -0.11232183873653412, 0.08711085468530655, -0.5452967882156372, -0.1959298998117447, 0.016983531415462494, 2.993595600128174, 0.24740248918533325, 0.49538370966911316], "projection": [-3.7616400718688965, 5.5790252685546875], "size": 3, "height": 2, "width": 2}