{"data": {"doi": "10.1109/scivis.2015.7429485", "title": "A Classification of User Tasks in Visual Analysis of Volume Data", "year": "2015", "conferenceName": "SciVis", "authors": "Bireswar Laha;Doug A. Bowman;David H. Laidlaw;John J. Socha", "citationCount": "6", "affiliation": "Laha, B (Corresponding Author), Stanford Univ, Stanford, CA 94305 USA. Laha, Bireswar, Stanford Univ, Stanford, CA 94305 USA. Bowman, Doug A.; Socha, John J., Virginia Tech, Blacksburg, VA USA. Laidlaw, David H., Brown Univ, Providence, RI 02912 USA.", "countries": "USA", "abstract": "Empirical findings from studies in one scientific domain have very limited applicability to other domains, unless we formally establish deeper insights on the generalizability of task types. We present a domain-independent classification of visual analysis tasks with volume visualizations. This taxonomy will help researchers design experiments, ensure coverage, and generate hypotheses in empirical studies with volume datasets. To develop our taxonomy, we first interviewed scientists working with spatial data in disparate domains. We then ran a survey to evaluate the design participants in which were scientists and professionals from around the world, working with volume data in various scientific domains. Respondents agreed substantially with our taxonomy design, but also suggested important refinements. We report the results in the form of a goal-based generic categorization of visual analysis tasks with volume visualizations. Our taxonomy covers tasks performed with a wide variety of volume datasets.", "keywords": "Task Taxonomy, Empirical Evaluation, Volume Visualization, Scientific Visualization, Virtual Reality, 3D Interaction", "link": "http://dx.doi.org/10.1109/SciVis.2015.7429485", "refList": ["10.1109/tvcg.2007.70433", "10.1145/234313.234383", "10.1109/mc.2013.178", "10.1109/tvcg.2012.42", "10.1006/ijhc.2000.0412", "10.1109/vl.1996.545307", "10.1117/12.536578", "10.1109/tvcg.2013.124", "10.1109/infvis.2004.59", "10.1109/tvcg.2005.2", "10.1109/tvcg.2004.46", "10.1109/tvcg.2012.216", "10.1007/978-3-319-06793-3\\_5", "10.1109/infvis.2004.10", "10.1109/tvcg.2013.130", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2014.20", "10.1145/1168149.1168169", "10.1016/0953-5438(93)90014-k", "10.1109/tvcg.2009.126", "10.1109/tvcg.2013.120", "10.1109/mcg.2004.1274065"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2016.2598602", "title": "Comparing Cross-Sections and 3D Renderings for Surface Matching Tasks Using Physical Ground Truths", "year": "2016", "conferenceName": "SciVis", "authors": "Andreas J. Lind;Stefan Bruckner", "citationCount": "1", "affiliation": "Lind, AJ (Corresponding Author), Univ Bergen, Bergen, Norway. Lind, Andreas J.; Bruckner, Stefan, Univ Bergen, Bergen, Norway.", "countries": "Norway", "abstract": "Within the visualization community there are some well-known techniques for visualizing 3D spatial data and some general assumptions about how perception affects the performance of these techniques in practice. However, there is a lack of empirical research backing up the possible performance differences among the basic techniques for general tasks. One such assumption is that 3D renderings are better for obtaining an overview, whereas cross sectional visualizations such as the commonly used Multi-Planar Reformation (MPR) are better for supporting detailed analysis tasks. In the present study we investigated this common assumption by examining the difference in performance between MPR and 3D rendering for correctly identifying a known surface. We also examined whether prior experience working with image data affects the participant's performance, and whether there was any difference between interactive or static versions of the visualizations. Answering this question is important because it can be used as part of a scientific and empirical basis for determining when to use which of the two techniques. An advantage of the present study compared to other studies is that several factors were taken into account to compare the two techniques. The problem was examined through an experiment with 45 participants, where physical objects were used as the known surface (ground truth). Our findings showed that: 1. The 3D renderings largely outperformed the cross sections; 2. Interactive visualizations were partially more effective than static visualizations; and 3. The high experience group did not generally outperform the low experience group.", "keywords": "Human-Computer Interaction;Quantitative Evaluation and Volume Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2016.2598602", "refList": ["10.1111/opo.12131", "10.1145/263407.263408", "10.1057/ivs.2009.22", "10.1006/ijhc.2000.0413", "10.2312/vmv.20141273", "10.1016/j.visres.2004.11.015", "10.1162/jocn\\_a\\_00072", "10.1145/312624.312634", "10.1145/1462048.1462054", "10.1097/00000637-200044050-00015", "10.1109/tvcg.2008.108", "10.1109/isbi.2008.4541122", "10.1109/scivis.2015.7429485", "10.1016/j.tripleo.2004.09.012", "10.1109/tvcg.2013.240", "10.1016/j.tics.2007.10.001", "10.1111/j.1467-8659.2011.01930.x", "10.1016/0030-4220(89)90173-4", "10.1016/j.neuropsychologia.2004.04.025", "10.1109/pacificvis.2012.6183579", "10.1089/lap.2012.0220", "10.1109/tvcg.2007.70569", "10.1109/tvcg.2011.161", "10.1016/0895-6111(95)00022-4", "10.1109/tvcg.2013.121", "10.1109/tvcg.2007.70542", "10.1016/0278-2391(95)90707-6", "10.1145/2816795.2818091", "10.1146/annurev.ps.38.020187.000333", "10.1098/rstb.1997.0128"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2018.00035", "year": "2018", "title": "An Automatic Deformation Approach for Occlusion Free Egocentric Data Exploration", "conferenceName": "PacificVis", "authors": "Cheng Li;Joachim Moortgat;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Moortgat, Joachim; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Occlusion management is an important task for three dimension data exploration. For egocentric data exploration, the occlusion problems, caused by the camera being too close to opaque data elements, have not been well addressed by previous studies. In this paper, we propose an automatic approach to resolve these problems and provide an occlusion free egocentric data exploration. Our system utilizes a state transition model to monitor both the camera and the data, and manages the initiation, duration, and termination of deformation with animation. Our method can be applied to multiple types of scientific datasets, including volumetric data, polygon mesh data, and particle data. We demonstrate our method with different exploration tasks, including camera navigation, isovalue adjustment, transfer function adjustment, and time varying exploration. We have collaborated with a domain expert and received positive feedback.", "keywords": "Data Deformation; Occlusion Management; Data Exploration; Egocentric Visualization", "link": "https://doi.org/10.1109/PacificVis.2018.00035", "refList": ["10.1145/989863.989871", "10.1109/tvcg.2008.59", "10.1111/j.1467-8659.2008.01332.x", "10.1109/tvcg.2016.2599049", "10.1109/pacificvis.2014.14", "10.1109/tvcg.2007.70433", "10.1007/s10596-015-9501-z", "10.1109/38.610209", "10.1109/tvcg.2003.1207447", "10.1111/j.1467-8659.2008.01181.x", "10.1109/tvcg.2012.42", "10.1109/tvcg.2016.2599217", "10.1002/fld.3764", "10.1109/pacificvis.2013.6596123", "10.1109/tvcg.2006.140", "10.1109/tvcg.2012.143", "10.1109/tvcg.2007.70565", "10.1109/scivis.2015.7429485", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.167", "10.1145/505008.505039", "10.1109/tvcg.2006.144", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/38.595268", "10.1109/tvcg.2016.2518338", "10.1109/tvcg.2014.20", "10.1145/1980462.1980487", "10.1111/cgf.12466", "10.2312/compaesth/compaesth05/209-216", "10.1109/tvcg.2010.127", "10.1016/j.advwatres.2016.01.002", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2009.144", "10.1145/1462055.1462056"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 7}, "index": 279, "embedding": [0.02456575259566307, 0.5558885335922241, -1.060706377029419, -0.14095023274421692, -0.6745967864990234, 0.16650904715061188, 2.3916807174682617, 3.003784656524658, -0.43282294273376465, 0.29692259430885315, -0.239134281873703, -0.0693778470158577, -0.13324885070323944, 0.48942169547080994, 0.4049369990825653, 1.699219822883606, -0.17821377515792847, 0.08175134658813477, -0.650787889957428, 0.43877536058425903, -0.06630208343267441, 2.808889389038086, -0.9128071665763855, -0.061188362538814545, -1.2758228778839111, 0.09129026532173157, -0.5500518083572388, -0.15232810378074646, 1.4467897415161133, 1.1826177835464478, -0.48055699467658997, 0.6463647484779358], "projection": [-3.7964444160461426, -2.2990317344665527], "size": 4, "height": 3, "width": 2}