{"data": {"doi": "10.1109/tvcg.2018.2865049", "title": "MotionRugs: Visualizing Collective Trends in Space and Time", "year": "2018", "conferenceName": "VAST", "authors": "Juri Buchm\u00fcller;Dominik J\u00e4ckle;Eren Cakmak;Ulrik Brandes;Daniel A. Keim", "citationCount": "4", "affiliation": "Buchmuller, J (Corresponding Author), Univ Konstanz, Constance, Germany. Buchmueller, Jun; Jaeckie, Dominik; Cakmak, Eren; Keim, Daniel A., Univ Konstanz, Constance, Germany. Brandes, Ulrik, Swiss Fed Inst Technol, Zurich, Switzerland.", "countries": "Germany;Switzerland", "abstract": "Understanding the movement patterns of collectives, such as flocks of birds or fish swarms, is an interesting open research question. The collectives are driven by mutual objectives or react to individual direction changes and external influence factors and stimuli. The challenge in visualizing collective movement data is to show space and time of hundreds of movements at the same time to enable the detection of spatiotemporal patterns. In this paper, we propose MotionRugs, a novel space efficient technique for visualizing moving groups of entities. Building upon established space-partitioning strategies, our approach reduces the spatial dimensions in each time step to a one-dimensional ordered representation of the individual entities. By design, MotionRugs provides an overlap-free, compact overview of the development of group movements over time and thus, enables analysts to visually identify and explore group-specific temporal patterns. We demonstrate the usefulness of our approach in the field of fish swarm analysis and report on initial feedback of domain experts from the field of collective behavior.", "keywords": "Spatio-Temporal Visualization,Spatial Abstraction,Spatial Index Structures,Collective Movement", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865049", "refList": ["10.1007/978-0-387-39940-9\\_1131", "10.1109/tvcg.2013.193", "10.1109/pacificvis.2014.57", "10.1109/tvcg.2015.2417572", "10.1007/bf01199431", "10.1111/j.1467-8659.2009.01440.x", "10.1109/tvcg.2011.226", "10.1145/1376616.1376618", "10.1177/1473871612457601", "10.1109/tvcg.2010.75", "10.1109/tvcg.2013.263", "10.1109/vast.2009.5332593", "10.1145/130881.130882", "10.1080/13658816.2016.1199806", "10.1109/tvcg.2013.192", "10.1109/tvcg.2006.193", "10.5220/0005716900480059", "10.1007/bf00288933", "10.1111/cgf.12654", "10.1080/13658816.2015.1081205", "10.1109/icdm.2003.1250978", "10.1145/361002.361007", "10.1109/pacificvis.2014.48", "10.1109/iv.2008.24", "10.1016/s1045-926x(03)00046-6", "10.1145/1520340.1520516", "10.3138/carto.42.4.349", "10.1007/s10844-006-9952-8", "10.1109/tvcg.2008.125", "10.1109/vast.2014.7042477", "10.1109/tvcg.2009.145", "10.1109/tvcg.2012.265", "10.1179/000870410x12658023467367", "10.2307/2332226", "10.1109/tvcg.2010.78", "10.1016/0146-664x(82)90128-9", "10.1177/1473871613477851", "10.1145/93597.98742", "10.1111/cgf.12653", "10.1145/93597.98741", "10.1179/000870403235002042"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934671", "title": "A Natural-language-based Visual Query Approach of Uncertain Human Trajectories", "year": "2019", "conferenceName": "VAST", "authors": "Zhaosong Huang;Ye Zhao;Wei Chen;Shengjie Gao;Kejie Yu;Weixia Xu;MingJie Tang;Minfeng Zhu;Mingliang Xu", "citationCount": "0", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Xu, ML (Corresponding Author), Zhengzhou Univ, Sch Informat Engn, Zhengzhou 450000, Peoples R China. Huang, Zhaosong; Chen, Wei; Gao, Shengjie; Yu, Kejie; Xu, Weixia; Zhu, Minfeng, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Zhao, Ye, Kent State Univ, Dept Comp Sci, Kent, OH 44242 USA. Tang, Mingjie, Ant Financial, San Mateo, CA USA. Xu, Mingliang, Zhengzhou Univ, Sch Informat Engn, Zhengzhou 450000, Peoples R China.", "countries": "USA;China", "abstract": "Visual querying is essential for interactively exploring massive trajectory data. However, the data uncertainty imposes profound challenges to fulfill advanced analytics requirements. On the one hand, many underlying data does not contain accurate geographic coordinates, e.g., positions of a mobile phone only refer to the regions (i.e., mobile cell stations) in which it resides, instead of accurate GPS coordinates. On the other hand, domain experts and general users prefer a natural way, such as using a natural language sentence, to access and analyze massive movement data. In this paper, we propose a visual analytics approach that can extract spatial-temporal constraints from a textual sentence and support an effective query method over uncertain mobile trajectory data. It is built up on encoding massive, spatially uncertain trajectories by the semantic information of the POls and regions covered by them, and then storing the trajectory documents in text database with an effective indexing scheme. The visual interface facilitates query condition specification, situation-aware visualization, and semantic exploration of large trajectory data. Usage scenarios on real-world human mobility datasets demonstrate the effectiveness of our approach.", "keywords": "Natural-language-based Visual Query,Spatial Uncertaity,Trajectory Exploration", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934671", "refList": ["10.1017/s1351324909005129", "10.1109/tvcg.2013.226", "10.1109/tvcg.2018.2864503", "10.1561/1500000019", "10.1109/icde.2010.5447829", "10.1109/tvcg.2008.172", "10.1177/1473871612457601", "10.1145/2442810.2442828", "10.1109/pacificvis.2014.50", "10.1109/vast.2014.7042486", "10.1201/9781420008609.ch3", "10.1177/1473871613480062", "10.1109/tvcg.2017.2744159", "10.1145/2629592", "10.1109/tvcg.2016.2616404", "10.1109/vl.1996.545307", "10.5311/josis.2012.4.62", "10.5220/0005716900480059", "10.1109/tvcg.2018.2865049", "10.1109/tvcg.2015.2467619", "10.1109/tvcg.2015.2467771", "10.1109/tvcg.2011.233", "10.1186/s12859-015-0564-6", "10.1111/cgf.12114", "10.1108/eb026562", "10.1002/inc3.362", "10.1109/vast.2014.7042495", "10.1145/299917.299920", "10.1162/coli.2009.35.4.35403", "10.1016/j.datak.2007.10.008", "10.13140/2.1.2393.1847", "10.1109/tits.2017.2683539", "10.1109/tbdata.2017.2667700", "10.1177/1473871617692841", "10.1109/tits.2015.2436897", "10.1109/tvcg.2013.179", "10.1145/2339530.2339561", "10.1109/vast.2008.4677356", "10.1109/access.2016.2553681", "10.1109/tvcg.2017.2758362", "10.14778/2732232.2732235", "10.1109/tits.2017.2711644", "10.1162/jmlr.2003.3.4-5.993", "10.1145/330560.330692", "10.1109/tvcg.2014.2371856", "10.1111/cgf.12778", "10.1145/2743025", "10.1109/tnn.2003.820440", "10.1109/tvcg.2016.2598885", "10.1109/tits.2016.2639320", "10.1145/1341012.1341041", "10.1109/tvcg.2016.2598416", "10.1145/2501654.2501656", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2018.2865042"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934258", "title": "Scale-Space Splatting: Reforming Spacetime for Cross-Scale Exploration of Integral Measures in Molecular Dynamics", "year": "2019", "conferenceName": "SciVis", "authors": "Juraj P\u00e1lenik;Jan Byska;Stefan Bruckner;Helwig Hauser", "citationCount": "0", "affiliation": "Palenik, J (Corresponding Author), Univ Bergen, Bergen, Norway. Palenik, Juraj; Byska, Jan; Bruckner, Stefan; Hauser, Helwig, Univ Bergen, Bergen, Norway. Byska, Jan, Masaryk Univ, Brno, Czech Republic.", "countries": "Republic;Norway", "abstract": "Understanding large amounts of spatiotemporal data from particle-based simulations, such as molecular dynamics, often relies on the computation and analysis of aggregate measures. These, however, by virtue of aggregation, hide structural information about the space/time localization of the studied phenomena. This leads to degenerate cases where the measures fail to capture distinct behaviour. In order to drill into these aggregate values, we propose a multi-scale visual exploration technique. Our novel representation, based on partial domain aggregation, enables the construction of a continuous scale-space for discrete datasets and the simultaneous exploration of scales in both space and time. We link these two scale-spaces in a scale-space space-time cube and model linked views as orthogonal slices through this cube, thus enabling the rapid identification of spatio-temporal patterns at multiple scales. To demonstrate the effectiveness of our approach, we showcase an advanced exploration of a protein-ligand simulation.", "keywords": "Scale space,time-series,scientific simulation,multi-scale analysis,space-time cube,molecular dynamics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934258", "refList": ["10.1111/cgf.13445", "10.1109/tvcg.2012.191", "10.1201/b14876", "10.1145/827051.827056", "10.1080/07391102.2005.10531229", "10.1021/acs.jcim.7b00070", "10.1007/978-3-8348-9190-729", "10.1111/cgf.13427", "10.1109/tvcg.2018.2864851", "10.1111/cgf.12612", "10.1109/tvcg.2015.2467434", "10.1016/0021-9991(82)90025-0", "10.1111/cgf.13177", "10.1007/978-1-4757-6465-9", "10.1145/1044588.1044615", "10.2307/2669996", "10.1109/tvcg.2018.2865049", "10.1007/978-3-540-70823-0\\_3", "10.1111/cgf.13640", "10.1109/tvcg.2010.253", "10.1109/tvcg.2009.177", "10.1016/j.csda.2018.03.014", "10.1109/tvcg.2009.200", "10.1109/visual.2003.1250353", "10.2312/eurovisshort.20171146", "10.1109/tvcg.2010.148", "10.1002/9780470316849", "10.1109/visual.1993.398859", "10.1007/978-1-4612-4026-6", "10.1016/j.jmb.2018.09.004", "10.1111/cgf.13415", "10.2312/molva.20181102", "10.1371/journal.pcbi.1002708", "10.1007/s10851-010-0242-2", "10.1016/0263-7855(96)00018-5", "10.1109/visual.2003.1250402", "10.1007/bf01936872", "10.1002/wics.79", "10.1111/j.1467-8659.2009.01684.x", "10.1109/tvcg.2018.2864510", "10.1111/cgf.12804", "10.1117/12.904668", "10.1002/jcc.20289", "10.1088/0965-0393/18/1/015012", "10.1109/34.56205", "10.1111/j.1435-5597.1970.tb01464.x", "10.1007/978-0-85729-079-3", "10.1007/s11263-005-1838-7"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2019.00045", "year": "2019", "title": "Space-Time Slicing: Visualizing Object Detector Performance in Driving Video Sequences", "conferenceName": "PacificVis", "authors": "Teng{-}Yok Lee;Kent Wittenburg", "citationCount": "0", "affiliation": "Lee, TY (Corresponding Author), Mitsubishi Elect Res Labs, Cambridge, MA 02139 USA.\nLee, Teng-Yok; Wittenburg, Kent, Mitsubishi Elect Res Labs, Cambridge, MA 02139 USA.", "countries": "USA", "abstract": "Development of object detectors for video in applications such as autonomous driving requires an iterative training process with data that initially requires human labeling. Later stages of development require tuning a large set of parameters that may not have labeled data available. For each training iteration and parameter selection decision, insight is needed into object detector performance. This work presents a visualization method called Space-Time Slicing to assist a human developer in the development of object detectors for driving applications without requiring labeled data. Space-Time Slicing reveals patterns in the detection data that can suggest the presence of false positives and false negatives. It may be used to set such parameters as image pixel size in data preprocessing and confidence thresholds for object classifiers by comparing performance across different conditions.", "keywords": "", "link": "https://doi.org/10.1109/PacificVis.2019.00045", "refList": ["10.1109/tvcg.2011.208", "10.1145/1778765.1778825", "10.1145/1141911.1141967", "10.1145/1276377.1276504", "10.1109/tvcg.2018.2865049", "10.1145/354384.354535", "10.1111/j.1467-8659.2012.03118.x", "10.1109/tvcg.2016.2598828", "10.1111/j.1467-8659.2012.03158.x", "10.1109/tvcg.2008.40", "10.1007/s11263-009-0275-4", "10.1109/visual.2003.1250401", "10.1109/pacificvis.2010.5429602"], "wos": 1, "children": [], "len": 1}], "len": 7}, "index": 813, "embedding": [1.5213817358016968, 0.7035951018333435, 0.26353728771209717, 2.7629802227020264, -0.6415178775787354, 0.16650904715061188, -0.7643030881881714, 0.478503555059433, -0.4787178039550781, -0.49221011996269226, 0.629734992980957, 0.2599482536315918, -0.15792734920978546, 1.1871862411499023, -0.8806813955307007, 1.618037462234497, -0.21153569221496582, 0.9681631922721863, -0.7424624562263489, 2.8876099586486816, -0.06630208343267441, 1.4449288845062256, 0.6462073922157288, 0.3915119469165802, -0.046037666499614716, 0.6293695569038391, -0.5536867380142212, -0.1945725977420807, 1.7990106344223022, -0.3799589276313782, 0.02594102919101715, 0.70134437084198], "projection": [-2.2674977779388428, 7.40624475479126], "size": 4, "height": 2, "width": 3}