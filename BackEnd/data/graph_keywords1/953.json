{"data": {"doi": "10.1109/tvcg.2019.2934786", "title": "The Perceptual Proxies of Visual Comparison", "year": "2019", "conferenceName": "InfoVis", "authors": "Nicole Jardine;Brian D. Ondov;Niklas Elmqvist;Steven Franconeri", "citationCount": "4", "affiliation": "Jardine, N (Corresponding Author), Northwestern Univ, Evanston, IL 60208 USA. Jardine, Nicole; Franconeri, Steven, Northwestern Univ, Evanston, IL 60208 USA. Jardine, Nicole, Cook Cty Assessors Off, Chicago, IL USA. Ondov, Brian D., NIH, Bldg 10, Bethesda, MD 20892 USA. Ondov, Brian D.; Elmqvist, Niklas, Univ Maryland, College Pk, MD 20742 USA.", "countries": "USA", "abstract": "Perceptual tasks in visualizations often involve comparisons. Of two sets of values depicted in two charts, which set had values that were the highest overall? Which had the widest range? Prior empirical work found that the performance on different visual comparison tasks (e.g., \u201cbiggest delta\u201d, \u201cbiggest correlation\u201d) varied widely across different combinations of marks and spatial arrangements. In this paper, we expand upon these combinations in an empirical evaluation of two new comparison tasks: the \u201cbiggest mean\u201d and \u201cbiggest range\u201d between two sets of values. We used a staircase procedure to titrate the difficulty of the data comparison to assess which arrangements produced the most precise comparisons for each task. We find visual comparisons of biggest mean and biggest range are supported by some chart arrangements more than others, and that this pattern is substantially different from the pattern for other tasks. To synthesize these dissonant findings, we argue that we must understand which features of a visualization are actually used by the human visual system to solve a given task. We call these perceptual proxies. For example, when comparing the means of two bar charts, the visual system might use a \u201cMean length\u201d proxy that isolates the actual lengths of the bars and then constructs a true average across these lengths. Alternatively, it might use a \u201cHull Area\u201d proxy that perceives an implied hull bounded by the bars of each chart and then compares the areas of these hulls. We propose a series of potential proxies across different tasks, marks, and spatial arrangements. Simple models of these proxies can be empirically evaluated for their explanatory power by matching their performance to human performance across these marks, arrangements, and tasks. We use this process to highlight candidates for perceptual proxies that might scale more broadly to explain performance in visual comparison.", "keywords": "Graphical perception,visual perception,visual comparison,crowdsourced evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934786", "refList": ["10.1177/1473871611416549", "10.1109/tvcg.2014.2346979", "10.1016/0010-0285(77)90012-3", "10.1037/a0026284", "10.1016/s1364-6613(97)01105-4", "10.3758/bf03201236", "10.1093/acprof:osobl/9780199734337.003.0030", "10.1109/infvis.2005.1532136", "10.3758/bf03204219", "10.1109/tvcg.2017.2744199", "10.1016/s0042-6989(99)00029-2", "10.1167/16.5.11", "10.1109/34.730558", "10.1145/2046684.2046699", "10.3758/s13414-012-0322-z", "10.1177/0956797615585002", "10.1109/tvcg.2018.2810918", "10.1109/tvcg.2018.2864884", "10.1016/0042-6989(85)90208-1", "10.1016/j.tics.2011.01.003", "10.1146/annurev-psych-010416-044232", "10.1109/tvcg.2010.162", "10.1109/tvcg.2015.2466971", "10.1109/tvcg.2007.70515", "10.1111/j.1467-8659.2009.01694.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030454", "title": "Objective Observer-Relative Flow Visualization in Curved Spaces for Unsteady 2D Geophysical Flows", "year": "2020", "conferenceName": "SciVis", "authors": "Peter Rautek;Matej Mlejnek;Johanna Beyer;Jakob Troidl;Hanspeter Pfister;Thomas Theu\u00dfl;Markus Hadwiger", "citationCount": "0", "affiliation": "Rautek, P (Corresponding Author), King Abdullah Univ Sci \\& Technol KAUST, Visual Comp Ctr, Thuwal 239556900, Saudi Arabia. Rautek, Peter; Mlejnek, Matej; Troidl, Jakob; Hadwiger, Markus, King Abdullah Univ Sci \\& Technol KAUST, Visual Comp Ctr, Thuwal 239556900, Saudi Arabia. Beyer, Johanna; Pfister, Hanspeter, Harvard Univ, Cambridge, MA 02138 USA. Troidl, Jakob, TU Wien, Vienna, Austria. Theussl, Thomas, King Abdullah Univ Sci \\& Technol KAUST, Core Labs, Thuwal 239556000, Saudi Arabia.", "countries": "USA;Arabia;Austria", "abstract": "Computing and visualizing features in fluid flow often depends on the observer, or reference frame, relative to which the input velocity field is given. A desired property of feature detectors is therefore that they are objective, meaning independent of the input reference frame. However, the standard definition of objectivity is only given for Euclidean domains and cannot be applied in curved spaces. We build on methods from mathematical physics and Riemannian geometry to generalize objectivity to curved spaces, using the powerful notion of symmetry groups as the basis for definition. From this, we develop a general mathematical framework for the objective computation of observer fields for curved spaces, relative to which other computed measures become objective. An important property of our framework is that it works intrinsically in 2D, instead of in the 3D ambient space. This enables a direct generalization of the 2D computation via optimization of observer fields in flat space to curved domains, without having to perform optimization in 3D. We specifically develop the case of unsteady 2D geophysical flows given on spheres, such as the Earth. Our observer fields in curved spaces then enable objective feature computation as well as the visualization of the time evolution of scalar and vector fields, such that the automatically computed reference frames follow moving structures like vortices in a way that makes them appear to be steady.", "keywords": "Flow visualization,observer fields,frames of reference,objectivity,symmetry groups,intrinsic covariant derivatives", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030454", "refList": ["10.1001/archpediatrics.2011.97", "10.1037/0033-2909.115.2.228", "10.1057/s41267-019-00289-7", "10.1097/00001888-199805000-00024", "10.1145/3173574.3173718", "10.1126/science.7455683", "10.1109/tvcg.2012.199", "10.2307/1914185", "10.1371/journal.pone.0142444", "10.1177/0956797613504966", "10.1198/0003130032369", "10.1037/0033-2909.111.2.361", "10.1037/a0014474", "10.1186/s41235-019-0182-3", "10.1016/0041-5553(76)90154-3", "10.1037/a0025185", "10.3758/s13423-013-0572-3", "10.1111/1467-985x.00120", "10.1098/rsta.1895.0010", "10.1109/tvcg.2019.2934786", "10.1037/h0046162", "10.1177/2059799116672879", "10.1167/16.5.11", "10.1111/j.1740-9713.2013.00636.x", "10.1037/a0023265", "10.1073/pnas.1722389115", "10.2307/2288400", "10.3389/fnins.2012.00001", "10.1016/j.ijforecast.2012.02.006", "10.1186/s13639-018-0087-0", "10.1037/1082-989x.10.4.389", "10.1006/cogp.1998.0710", "10.1177/0963721413481473", "10.1037/0033-295x.102.4.684", "10.1126/science.185.4157.1124", "10.1109/tvcg.2014.2346298", "10.1145/3313831.3376454", "10.1037/0033-295x.107.3.500", "10.1037/h0042769", "10.1037/0003-066x.60.2.170", "10.3389/fpsyg.2019.00813", "10.1111/rssa.12378"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030429", "title": "Revealing Perceptual Proxies with Adversarial Examples", "year": "2020", "conferenceName": "InfoVis", "authors": "Brian D. Ondov;Fumeng Yang;Matthew Kay;Niklas Elmqvist;Steven Franconeri", "citationCount": "0", "affiliation": "Ondov, BD (Corresponding Author), NIH, Bldg 10, Bethesda, MD 20892 USA. Ondov, BD (Corresponding Author), Univ Maryland, College Pk, MD 20742 USA. Ondov, Brian D., NIH, Bldg 10, Bethesda, MD 20892 USA. Ondov, Brian D.; Elmqvist, Niklas, Univ Maryland, College Pk, MD 20742 USA. Yang, Fumeng, Brown Univ, Providence, RI 02912 USA. Kay, Matthew, Univ Michigan, Ann Arbor, MI 48109 USA. Franconeri, Steven, Northwestern Univ, Evanston, IL USA.", "countries": "USA", "abstract": "Data visualizations convert numbers into visual marks so that our visual system can extract data from an image instead of raw numbers. Clearly, the visual system does not compute these values as a computer would, as an arithmetic mean or a correlation. Instead, it extracts these patterns using perceptual proxies; heuristic shortcuts of the visual marks, such as a center of mass or a shape envelope. Understanding which proxies people use would lead to more effective visualizations. We present the results of a series of crowdsourced experiments that measure how powerfully a set of candidate proxies can explain human performance when comparing the mean and range of pairs of data series presented as bar charts. We generated datasets where the correct answer-the series with the larger arithmetic mean or range-was pitted against an \u201cadversarial\u201d series that should be seen as larger if the viewer uses a particular candidate proxy. We used both Bayesian logistic regression models and a robust Bayesian mixed-effects linear model to measure how strongly each adversarial proxy could drive viewers to answer incorrectly and whether different individuals may use different proxies. Finally, we attempt to construct adversarial datasets from scratch, using an iterative crowdsourcing procedure to perform black-box optimization.", "keywords": "Perceptual proxies,vision science,crowdsourced evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030429", "refList": ["10.3389/fpsyg.2012.00054", "10.3758/bf03209345", "10.1167/14.8.23", "10.1109/tvcg.2014.2346979", "10.1111/j.1467-8659.2009.01694.x", "10.1109/mcg.2007.323435", "10.1080/00223980.1947.9917350", "10.1016/j.tics.2009.07.001", "10.1007/bf00308884", "10.1016/s0020-7373(86)80019-0", "10.1037/h0043158", "10.1038/nn.2706", "10.1037/0278-7393.25.4.986", "10.1177/0956797617709814", "10.1109/6.736450", "10.2307/1131314", "10.1016/j.cub.2014.07.030", "10.1109/tvcg.2014.2346320", "10.1109/tvcg.2019.2934786", "10.1111/cgf.12888", "10.1109/tvcg.2018.2865240", "10.1109/tvcg.2015.2467671", "10.1037/0096-1523.16.4.683", "10.1037/h0046162", "10.1037/h0044417", "10.1037/0096-3445.106.4.341", "10.2307/2288400", "10.1109/tvcg.2018.2810918", "10.1002/wcs.26", "10.1016/j.visres.2014.02.006", "10.1016/0010-0277(92)90002-y", "10.1017/s0142716400009796", "10.1037/0033-295x.107.3.500", "10.1038/s41598-018-37610-7", "10.1016/j.actpsy.2011.09.006"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030335", "title": "Visual Reasoning Strategies for Effect Size Judgments and Decisions", "year": "2020", "conferenceName": "InfoVis", "authors": "Alex Kale;Matthew Kay;Jessica Hullman", "citationCount": "0", "affiliation": "Kale, A (Corresponding Author), Univ Washington, Seattle, WA 98195 USA. Kale, Alex, Univ Washington, Seattle, WA 98195 USA. Kay, Matthew, Univ Michigan, Ann Arbor, MI 48109 USA. Hullman, Jessica, Northwestern Univ, Evanston, IL 60208 USA.", "countries": "USA", "abstract": "Uncertainty visualizations often emphasize point estimates to support magnitude estimates or decisions through visual comparison. However, when design choices emphasize means, users may overlook uncertainty information and misinterpret visual distance as a proxy for effect size. We present findings from a mixed design experiment on Mechanical Turk which tests eight uncertainty visualization designs: 95% containment intervals, hypothetical outcome plots, densities, and quantile dotplots, each with and without means added. We find that adding means to uncertainty visualizations has small biasing effects on both magnitude estimation and decision-making, consistent with discounting uncertainty. We also see that visualization designs that support the least biased effect size estimation do not support the best decision-making, suggesting that a chart user's sense of effect size may not necessarily be identical when they use the same information for different tasks. In a qualitative analysis of users' strategy descriptions, we find that many users switch strategies and do not employ an optimal strategy when one exists. Uncertainty visualizations which are optimally designed in theory may not be the most effective in practice because of the ways that users satisfice with heuristics, suggesting opportunities to better understand visualization effectiveness by modeling sets of potential strategies.", "keywords": "Uncertainty visualization,graphical perception,data cognition", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030335", "refList": ["10.1001/archpediatrics.2011.97", "10.1037/0033-2909.115.2.228", "10.1109/tvcg.2017.2744683", "10.1057/s41267-019-00289-7", "10.1097/00001888-199805000-00024", "10.1145/3173574.3173718", "10.1155/2019/2318680", "10.1109/tvcg.2012.199", "10.3389/fpsyg", "10.2307/1914185", "10.1371/journal.pone.0142444", "10.1177/0956797613504966", "10.1198/0003130032369", "10.1037/a0014474", "10.1186/s41235-019-0182-3", "10.1016/0041-5553(76)90154-3", "10.1037/a0025185", "10.3758/s13423-013-0572-3", "10.1111/1467-985x.00120", "10.1098/rsta.1895.0010", "10.1109/tvcg.2019.2934786", "10.1037/h0046162", "10.1177/2059799116672879", "10.3389/fnins.2012.00007", "10.1167/16.5.11", "10.1111/j.1740-9713.2013.00636.x", "10.1037/a0023265", "10.1073/pnas.1722389115", "10.2307/2288400", "10.1371/journal.pone.0087357", "10.1016/j.ijforecast.2012.02.006", "10.1186/s13639-018-0087-0", "10.1037/1082-989x.10.4.389", "10.1006/cogp.1998.0710", "10.1177/0963721413481473", "10.1037/0033-295x.102.4.684", "10.1126/science.185.4157.1124", "10.1109/tvcg.2014.2346298", "10.1145/3313831.3376454", "10.1037/0033-295x.107.3.500", "10.1037/h0042769", "10.1037/0003-066x.60.2.170", "10.1126/science.7455683", "10.1111/rssa.12378"], "wos": 1, "children": [], "len": 1}], "len": 7}, "index": 953, "embedding": [1.708320140838623, 1.0965214967727661, -0.1557057648897171, 1.7331992387771606, -0.6604432463645935, 0.16650904715061188, -0.7167134881019592, 0.08176558464765549, -0.43251052498817444, -0.3158200979232788, 0.12239395081996918, 0.8664740920066833, -0.13702842593193054, 2.8823182582855225, 0.2837555706501007, 2.0332884788513184, 0.7199447154998779, 1.4714112281799316, -0.6558851003646851, 1.0972371101379395, -0.06630208343267441, 0.5078718662261963, 1.4915997982025146, 0.5870593786239624, 0.7453505396842957, 0.10712844133377075, -0.5328229665756226, 0.3497920632362366, 1.8746309280395508, 0.28924453258514404, 0.4909861087799072, 1.5223199129104614], "projection": [-2.379047393798828, 7.3237996101379395], "size": 4, "height": 2, "width": 3}