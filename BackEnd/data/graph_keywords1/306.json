{"data": {"title": "A Modular Degree-of-Interest Specification for the Visual Analysis of Large Dynamic Networks", "doi": "10.1109/tvcg.2013.109", "year": "2014", "conferenceName": "TVCG", "authors": "James Abello;Steffen Hadlak;Heidrun Schumann;Hans-Joerg Schulz", "citationCount": "22", "affiliation": "Abello, J (Corresponding Author), Rutgers State Univ, 96 Frelinghuysen Rd, Piscataway, NJ 08854 USA.\nAbello, James, Rutgers State Univ, Piscataway, NJ 08854 USA.\nHadlak, Steffen; Schumann, Heidrun; Schulz, Hans-Joerg, Univ Rostock, D-18059 Rostock, Mecklenburg Vor, Germany.", "countries": "USA;Germany", "abstract": "Large dynamic networks are targets of analysis in many fields. Tracking temporal changes at scale in these networks is challenging due in part to the fact that small changes can be missed or drowned-out by the rest of the network. For static networks, current approaches allow the identification of specific network elements within their context. However, in the case of dynamic networks, the user is left alone with finding salient local network elements and tracking them over time. In this work, we introduce a modular DoI specification to flexibly define what salient changes are and to assign them a measure of their importance in a time-varying setting. The specification takes into account neighborhood structure information, numerical attributes of nodes/edges, and their temporal evolution. A tailored visualization of the DoI specification complements our approach. Alongside a traditional node-link view of the dynamic network, it serves as an interface for the interactive definition of a DoI function. By using it to successively refine and investigate the captured details, it supports the analysis of dynamic networks from an initial view until pinpointing a user's analysis goal. We report on applying our approach to scientific coauthorship networks and give concrete results for the DBLP data set.", "keywords": "Time-varying graphs; dynamic graph visualization; degree-of-interest", "refList": ["10.1109/tvcg.2008.152", "10.1111/j.1467-8659.2011.01957.x", "10.1109/vlhcc.2009.5295284", "10.1109/iv.2001.942071", "10.1007/978-1-4419-0312-9\\_5", "10.1145/985692.985723", "10.14778/1687553.1687577", "10.1109/cgiv.2006.20", "10.1109/icdm.2010.145", "10.1145/989863.989941", "10.1002/spe.4380211102", "10.1111/j.1467-8659.2012.03091.x", "10.1057/ivs.2010.10", "10.1007/978-3-642-15464-5\\_24", "10.1109/tvcg.2006.148", "10.1109/icdm.2011.135", "10.1109/tvcg.2008.11", "10.1145/22339.22342", "10.1109/tvcg.2008.166", "10.1016/j.cag.2009.06.002", "10.1073/pnas.0307545100", "10.1109/iv.2009.24", "10.1109/tvcg.2009.108", "10.1109/tvcg.2009.84", "10.1177/0037549707078278", "10.1145/1083784.1083791", "10.1109/pacificvis.2009.4906845", "10.1145/1385569.1385639"], "wos": "", "children": [{"doi": "10.1109/tvcg.2015.2468111", "title": "MobilityGraphs: Visual Analysis of Mass Mobility Dynamics via Spatio-Temporal Graphs and Clustering", "year": "2015", "conferenceName": "VAST", "authors": "Tatiana von Landesberger;Felix Brodkorb;Philipp Roskosch;Natalia V. Andrienko;Gennady L. Andrienko;Andreas Kerren", "citationCount": "92", "affiliation": "von Landesberger, T (Corresponding Author), Tech Univ Darmstadt, Darmstadt, Germany. von Landesberger, Tatiana; Brodkorb, Felix; Roskosch, Philipp, Tech Univ Darmstadt, Darmstadt, Germany. Andrienko, Natalia; Andrienko, Gennady, Fraunhofer IAIS, Bonn, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London EC1V 0HB, England. Kerren, Andreas, Linnaeus Univ, Vaxjo, Sweden.", "countries": "Sweden;Germany;England", "abstract": "Learning more about people mobility is an important task for official decision makers and urban planners. Mobility data sets characterize the variation of the presence of people in different places over time as well as movements (or flows) of people between the places. The analysis of mobility data is challenging due to the need to analyze and compare spatial situations (i.e., presence and flows of people at certain time moments) and to gain an understanding of the spatio-temporal changes (variations of situations over time). Traditional flow visualizations usually fail due to massive clutter. Modern approaches offer limited support for investigating the complex variation of the movements over longer time periods. We propose a visual analytics methodology that solves these issues by combined spatial and temporal simplifications. We have developed a graph-based method, called MobilityGraphs, which reveals movement patterns that were occluded in flow maps. Our method enables the visual representation of the spatio-temporal variation of movements for long time series of spatial situations originally containing a large number of intersecting flows. The interactive system supports data exploration from various perspectives and at various levels of detail by interactive setting of clustering parameters. The feasibility our approach was tested on aggregated mobility data derived from a set of geolocated Twitter posts within the Greater London city area and mobile phone call data records in Abidjan, Ivory Coast. We could show that MobilityGraphs support the identification of regular daily and weekly movement patterns of resident population.", "keywords": "Visual analytics, movement data, networks, graphs, temporal aggregation, spatial aggregation, flows, clustering", "link": "http://dx.doi.org/10.1109/TVCG.2015.2468111", "refList": ["10.1111/j.1467-8659.2011.01946.x", "10.1109/infvis.2004.18", "10.1109/vast.2012.6400553", "10.1109/vast.2009.5333893", "10.1111/tgis.12100", "10.1109/tvcg.2011.226", "10.1111/tgis.12042", "10.1111/j.1467-8659.2011.01898.x", "10.1006/ijhc.2002.1017", "10.1145/1497577.1497578", "10.1109/tvcg.2013.246", "10.1080/10447318.2010.516722", "10.1109/infvis.1999.801851", "10.1109/tvcg.2013.254", "10.1109/tvcg.2009.143", "10.1016/j.compenvurbsys.2009.01.007", "10.1006/ijhc.1017", "10.1016/j.jvlc.2011.02.003", "10.1007/978-3-319-06793-3\\_8", "10.1109/tvcg.2011.233", "10.1109/tvcg.2014.2346271", "10.1007/978-0-85729-079-3", "10.1109/iv.2002.1028770", "10.1109/infvis.2005.1532150", "10.1080/13658810701674970", "10.1109/pacificvis.2011.5742390", "10.3138/carto.42.4.349", "10.1109/tvcg.2013.109", "10.1109/tvcg.2008.125", "10.1016/j.trc.2014.03.007", "10.1109/tvcg.2014.2346441", "10.1179/000870410x12658023467367", "10.3138/carto.46.4.239", "10.1109/tvcg.2010.78", "10.1559/152304087783875273", "10.1109/tvcg.2011.202", "10.1109/iv.2013.8", "10.1016/j.cosrev.2007.05.001", "10.1111/j.1467-8659.2009.01450.x", "10.1007/s13218-012-0177-4"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2016.2598585", "title": "Urban Pulse: Capturing the Rhythm of Cities", "year": "2016", "conferenceName": "SciVis", "authors": "Fabio Miranda;Harish Doraiswamy;Marcos Lage;Kai Zhao;Bruno Gon\u00e7alves;Luc Wilson;Mondrian Hsieh;Cl\u00e1udio T. Silva", "citationCount": "31", "affiliation": "Miranda, F (Corresponding Author), NYU, New York, NY 10003 USA. Miranda, Fabio; Doraiswamy, Harish; Zhao, Kai; Goncalves, Bruno; Silva, Claudio T., NYU, New York, NY 10003 USA. Lage, Marcos, Univ Fed Fluminense, BR-24220000 Niteroi, RJ, Brazil. Wilson, Luc; Hsieh, Mondrian, Kohn Pedersen Fox Associates PC, New York, NY USA.", "countries": "USA;Brazil", "abstract": "Cities are inherently dynamic. Interesting patterns of behavior typically manifest at several key areas of a city over multiple temporal resolutions. Studying these patterns can greatly help a variety of experts ranging from city planners and architects to human behavioral experts. Recent technological innovations have enabled the collection of enormous amounts of data that can help in these studies. However, techniques using these data sets typically focus on understanding the data in the context of the city, thus failing to capture the dynamic aspects of the city. The goal of this work is to instead understand the city in the context of multiple urban data sets. To do so, we define the concept of an \u201curban pulse\u201d which captures the spatio-temporal activity in a city across multiple temporal resolutions. The prominent pulses in a city are obtained using the topology of the data sets, and are characterized as a set of beats. The beats are then used to analyze and compare different pulses. We also design a visual exploration framework that allows users to explore the pulses within and across multiple cities under different conditions. Finally, we present three case studies carried out by experts from two different domains that demonstrate the utility of our framework.", "keywords": "Topology-based techniques;urban data;visual exploration", "link": "http://dx.doi.org/10.1109/TVCG.2016.2598585", "refList": ["10.1145/2487575.2488188", "10.1109/tvcg.2013.226", "10.3138/cart.50.2.2662", "10.1007/s00454-002-2885-2", "10.1109/tvcg.2015.2468111", "10.1111/j.1467-8659.2009.01440.x", "10.1109/vast.2015.7347636", "10.1145/777792.777846", "10.1177/1473871612457601", "10.1109/tvcg.2015.2467194", "10.1109/tvcg.2014.2346449", "10.1111/cgf.12383", "10.1109/tvcg.2004.3", "10.1109/tvcg.2012.311", "10.1140/epjds/s13688-016-0073-5", "10.1109/tvcg.2014.2346898", "10.1109/mis.2012.23", "10.1007/s11390-013-1383-8", "10.1109/tvcg.2015.2467619", "10.1007/978-3-642-37583-5\\_6", "10.1007/s00454-006-1265-8", "10.1109/tvcg.2015.2467592", "10.1109/tvcg.2009.100", "10.1109/tvcg.2010.253", "10.1109/tvcg.2013.228", "10.1089/big.2014.0020", "10.1109/tvcg.2009.69", "10.1109/vast.2015.7347630", "10.1109/tvcg.2011.181", "10.1145/2567948.2577020", "10.1109/icdmw.2015.176", "10.1111/cgf.12628", "10.1098/rsif.2015.0473", "10.1371/journal.pone.0037027", "10.1109/bigdata.2014.7004260", "10.1111/j.1467-8659.2010.01763.x", "10.1109/tvcg.2013.131", "10.1109/mprv.2014.31", "10.1371/journal.pone.0061981", "10.1038/nbt.1754", "10.2307/2317380"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865126", "title": "SRVis: Towards Better Spatial Integration in Ranking Visualization", "year": "2018", "conferenceName": "InfoVis", "authors": "Di Weng;Ran Chen;Zikun Deng;Feiran Wu;Jingmin Chen;Yingcai Wu", "citationCount": "6", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ \\& Alibaba Zhejiang Univ Joint Inst, State Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Weng, Di; Chen, Ran; Deng, Zikun; Wu, Yingcai, Zhejiang Univ \\& Alibaba Zhejiang Univ Joint Inst, State Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Wu, Feiran; Chen, Jingmin, Alibaba Grp, Hangzhou, Zhejiang, Peoples R China.", "countries": "China", "abstract": "Interactive ranking techniques have substantially promoted analysts' ability in making judicious and informed decisions effectively based on multiple criteria. However, the existing techniques cannot satisfactorily support the analysis tasks involved in ranking large-scale spatial alternatives, such as selecting optimal locations for chain stores, where the complex spatial contexts involved are essential to the decision-making process. Limitations observed in the prior attempts of integrating rankings with spatial contexts motivate us to develop a context-integrated visual ranking technique. Based on a set of generic design requirements we summarized by collaborating with domain experts, we propose SRVis, a novel spatial ranking visualization technique that supports efficient spatial multi-criteria decision-making processes by addressing three major challenges in the aforementioned context integration, namely, a) the presentation of spatial rankings and contexts, b) the scalability of rankings' visual representations, and c) the analysis of context-integrated spatial rankings. Specifically, we encode massive rankings and their cause with scalable matrix-based visualizations and stacked bar charts based on a novel two-phase optimization framework that minimizes the information loss, and the flexible spatial filtering and intuitive comparative analysis are adopted to enable the in-depth evaluation of the rankings and assist users in selecting the best spatial alternative. The effectiveness of the proposed technique has been evaluated and demonstrated with an empirical study of optimization methods, two case studies, and expert interviews.", "keywords": "Spatial ranking,visualization", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865126", "refList": ["10.2312/pe.eurovast.eurova13.007-011.3", "10.1109/tvcg.2013.193", "10.1109/iv.2009.59", "10.1080/13658810010005525", "10.3166/jds.12.193-208", "10.1016/j.jcss.2013.01.017", "10.1145/2702123.2702237", "10.1109/tvcg.2011.185", "10.1145/1133265.1133308", "10.1111/cgf.12132", "10.1109/mcg.2017.21", "10.1109/tvcg.2016.2598585", "10.1109/tvcg.2015.2467771", "10.1561/1500000016", "10.1109/tvcg.2014.2346913", "10.1109/mcg.2016.100", "10.1109/uidis.2001.929933", "10.1109/tvcg.2016.2642109", "10.1109/tvcg.2013.173", "10.1145/3020165.3020174", "10.1109/mcg.2015.25", "10.1109/tvcg.2015.2467717", "10.1109/pacificvis.2015.7156392", "10.1109/tbdata.2016.2586447", "10.1109/tvcg.2014.2346594", "10.1007/978-3-319-12586-2", "10.1109/smdcm.2011.5949270", "10.1080/136588199241247", "10.1109/tvcg.2008.166", "10.1007/bf02289694", "10.1109/vast.2011.6102455", "10.1111/j.0033-0124.1985.00075.x", "10.1109/tvcg.2012.253", "10.1109/tits.2013.2263225", "10.1111/cgf.12910", "10.1109/tvcg.2008.181", "10.1109/tvcg.2015.2467112", "10.1109/tvcg.2009.111", "10.1038/nature05302", "10.1145/989863.989885", "10.1109/tvcg.2016.2598416", "10.1287/moor.1.2.117", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934670", "title": "AirVis: Visual Analytics of Air Pollution Propagation", "year": "2019", "conferenceName": "VAST", "authors": "Zikun Deng;Di Weng;Jiahui Chen;Ren Liu;Zhibin Wang;Jie Bao 0003;Yu Zheng 0004;Yingcai Wu", "citationCount": "6", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Deng, Zikun; Weng, Di; Chen, Jiahui; Liu, Ren; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Wang, Zhibin, Zhejiang Univ, Res Ctr Air Pollut \\& Hlth, Hangzhou, Peoples R China. Bao, Jie; Zheng, Yu, JD Intelligent City Res, Beijing, Peoples R China.", "countries": "China", "abstract": "Air pollution has become a serious public health problem for many cities around the world. To find the causes of air pollution, the propagation processes of air pollutants must be studied at a large spatial scale. However, the complex and dynamic wind fields lead to highly uncertain pollutant transportation. The state-of-the-art data mining approaches cannot fully support the extensive analysis of such uncertain spatiotemporal propagation processes across multiple districts without the integration of domain knowledge. The limitation of these automated approaches motivates us to design and develop AirVis, a novel visual analytics system that assists domain experts in efficiently capturing and interpreting the uncertain propagation patterns of air pollution based on graph visualizations. Designing such a system poses three challenges: a) the extraction of propagation patterns; b) the scalability of pattern presentations; and c) the analysis of propagation processes. To address these challenges, we develop a novel pattern mining framework to model pollutant transportation and extract frequent propagation patterns efficiently from large-scale atmospheric data. Furthermore, we organize the extracted patterns hierarchically based on the minimum description length (MDL) principle and empower expert users to explore and analyze these patterns effectively on the basis of pattern topologies. We demonstrated the effectiveness of our approach through two case studies conducted with a real-world dataset and positive feedback from domain experts.", "keywords": "Air pollution propagation,pattern mining,graph visualization", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934670", "refList": ["10.1109/tvcg.2016.2598919", "10.1093/bib/bbr069", "10.1145/2487575.2488188", "10.1109/tvcg.2013.193", "10.1016/j.atmosenv.2014.12.011", "10.1109/tvcg.2013.226", "10.1109/tvcg.2018.2864503", "10.1109/tvcg.2015.2468111", "10.1109/icicta.2015.183", "10.1016/j.atmosenv.2014.05.039", "10.1111/cgf.12791", "10.1111/j.1467-8659.2009.01451.x", "10.1111/j.1467-8659.2011.01898.x", "10.5194/acp-12-5031-2012", "10.1016/j.atmosenv.2008.05.053", "10.1109/tvcg.2016.2535234", "10.1016/j.atmosres.2014.12.003", "10.1109/tvcg.2015.2467194", "10.1109/tvcg.2013.263", "10.1109/icdm.2002.1184038", "10.1145/2783258.2788573", "10.1109/tvcg.2018.2865149", "10.1109/tbdata.2017.2723899", "10.1109/tvcg.2012.311", "10.1109/vl.1996.545307", "10.1007/s12650-018-0481-7", "10.1016/j.envpol.2007.06.012", "10.3155/1047-3289.61.6.660", "10.1080/13658810701349037", "10.1145/3097983.3098090", "10.1109/tvcg.2007.70523", "10.1115/1.2128636", "10.1109/tvcg.2015.2467619", "10.3978/j.issn.2072-1439.2016.01.19", "10.1109/tkde.2005.127", "10.1017/s0269888912000331", "10.2312/conf/eg2013/stars/039-063", "10.1109/tvcg.2014.2346271", "10.1175/bams-d-14-00110.1", "10.1016/0005-1098(78)90005-5", "10.1007/s10618-006-0044-8", "10.1109/tvcg.2018.2865126", "10.2307/1912791", "10.3390/su6085322", "10.1007/s12650-018-0489-z", "10.2312/eurovisstar.20151109", "10.1109/tvcg.2011.181", "10.1126/science.298.5594.824", "10.1162/jmlr.2003.3.4-5.951", "10.1109/asonam.2014.6921638", "10.1111/j.1467-8659.2008.01213.x", "10.1038/s41598-017-18107-1", "10.1109/tvcg.2018.2865041", "10.1109/tits.2019.2901117", "10.1038/srep20668", "10.1109/tvcg.2012.265", "10.1109/tpami.2016.2608884", "10.1109/tvcg.2012.213", "10.1145/1376616.1376661", "10.1007/s00521-019-04567-1", "10.1109/tvcg.2017.2745083", "10.1126/science.243.4892.745", "10.1109/tvcg.2018.2864826", "10.1109/tnn.2003.820440", "10.1109/tvcg.2016.2598885", "10.1145/3219819.3219822", "10.1073/pnas.1502596112", "10.1016/j.envsoft.2009.01.004", "10.1002/pmic.200700095", "10.1145/2254556.2254651", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2011.202"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028958", "title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Tiankai Xie;Yuxin Ma;Hanghang Tong;My T. Thai;Ross Maciejewski", "citationCount": "0", "affiliation": "Xie, TK (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Xie, Tiankai; Ma, Yuxin; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. Tong, Hanghang, Univ Illinois, Urbana, IL USA. Thai, My T., Univ Florida, Gainesville, FL 32611 USA.", "countries": "USA", "abstract": "Graph mining plays a pivotal role across a number of disciplines, and a variety of algorithms have been developed to answer who/what type questions. For example, what items shall we recommend to a given user on an e-commerce platform? The answers to such questions are typically returned in the form of a ranked list, and graph-based ranking methods are widely used in industrial information retrieval settings. However, these ranking algorithms have a variety of sensitivities, and even small changes in rank can lead to vast reductions in product sales and page hits. As such, there is a need for tools and methods that can help model developers and analysts explore the sensitivities of graph ranking algorithms with respect to perturbations within the graph structure. In this paper, we present a visual analytics framework for explaining and exploring the sensitivity of any graph-based ranking algorithm by performing perturbation-based what-if analysis. We demonstrate our framework through three case studies inspecting the sensitivity of two classic graph-based ranking algorithms (PageRank and HITS) as applied to rankings in political news media and social networks.", "keywords": "Graph-based ranking,sensitivity analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028958", "refList": ["10.1109/wsc.2017.8247800", "10.1023/a:1022649401552", "10.1515/1559-0410.11416", "10.1109/tvcg.2016.2598919", "10.1177/1473871611416549", "10.1109/tvcg.2019.2934630", "10.1140/epjds29", "10.1109/tvcg.2019.2934670", "10.1016/j.eswa.2015.09.004", "10.1145/2702123.2702509", "10.1016/j.visinf.2018.12.001", "10.2307/3002000", "10.1109/tvcg.2019.2934399", "10.1007/s41060-016-0032-z", "10.1111/cgf.13198", "10.14778/2350229.2350254", "10.1145/2939672.2939764", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/vast.2017.8585647", "10.1007/bf01187020", "10.1109/icdm.2015.26", "10.1145/2362383.2362387", "10.1177/0049124104268644", "10.1109/vast.2011.6102442", "10.1109/infvis.2003.1249025", "10.1109/tvcg.2018.2864475", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2015.2467691", "10.1111/cgf.13210", "10.1214/aos/1176344136", "10.1109/tvcg.2015.2424872", "10.1016/j.visinf.2018.09.001", "10.1177/089443939100900106", "10.1109/tvcg.2015.2467931", "10.1162/neco.1997.9.8.1735", "10.1007/s11162-011-9241-4", "10.1111/cgf.13680", "10.1145/3065386", "10.1109/tvcg.2018.2864889", "10.1177/003804070808100402", "10.1109/icdm.2010.62", "10.1038/s41598-020-59669-x", "10.1162/153244303321897717", "10.1109/tvcg.2019.2934619", "10.1007/bf00356088", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030410", "title": "Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Wei Zeng 0002;Chengqiao Lin;Juncong Lin;Jincheng Jiang;Jiazhi Xia;Cagatay Turkay;Wei Chen", "citationCount": "0", "affiliation": "Lin, JC (Corresponding Author), Xiamen Univ, Xiamen, Peoples R China. Zeng, Wei; Jiang, Jincheng, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Lin, Chengqiao; Lin, Juncong, Xiamen Univ, Xiamen, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Turkay, Cagatay, Univ Warwick, Coventry, W Midlands, England. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China.", "countries": "China;England", "abstract": "Deep learning methods are being increasingly used for urban traffic prediction where spatiotemporal traffic data is aggregated into sequentially organized matrices that are then fed into convolution-based residual neural networks. However, the widely known modifiable areal unit problem within such aggregation processes can lead to perturbations in the network inputs. This issue can significantly destabilize the feature embeddings and the predictions - rendering deep networks much less useful for the experts. This paper approaches this challenge by leveraging unit visualization techniques that enable the investigation of many-to-many relationships between dynamically varied multi-scalar aggregations of urban traffic data and neural network predictions. Through regular exchanges with a domain expert, we design and develop a visual analytics solution that integrates 1) a Bivariate Map equipped with an advanced bivariate colormap to simultaneously depict input traffic and prediction errors across space, 2) a Moran's I Scatterplot that provides local indicators of spatial association analysis, and 3) a Multi-scale Attribution View that arranges non-linear dot plots in a tree layout to promote model analysis and comparison across scales. We evaluate our approach through a series of case studies involving a real-world dataset of Shenzhen taxi trips, and through interviews with domain experts. We observe that geographical scale variations have important impact on prediction performances, and interactive visual exploration of dynamically varying inputs and outputs benefit experts in the development of deep traffic prediction models.", "keywords": "MAUP,traffic prediction,deep learning,model diagnostic,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030410", "refList": ["10.1038/srep26377", "10.1109/mcg.2011.88", "10.1080/13658816.2015.1119279", "10.1109/tvcg.2013.226", "10.1109/pacificvis.2011.5742387", "10.1038/s41467-017-01882-w", "10.1109/tvcg.2019.2934670", "10.1111/j.1467-8659.2009.01440.x", "10.1111/cgf.13712", "10.1016/j.compenvurbsys.2008.09.006", "10.1109/pacificvis.2014.50", "10.1109/tvcg.2018.2816219", "10.1109/tvcg.2016.2535234", "10.1109/tvcg.2014.2346893", "10.3390/ijgi8080344", "10.1109/tvcg.2013.246", "10.1007/s10940-005-9003-6", "10.1016/j.compenvurbsys.2008.05.001", "10.1007/s10661-019-7831-3", "10.1111/j.1538-4632.2007.00699.x", "10.1016/j.aap.2016.08.015", "10.1080/13658816.2018.1541177", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2011.181", "10.1137/090759069", "10.1109/pacificvis.2011.5742390", "10.1214/10-aos799", "10.1109/tits.2017.2683539", "10.1109/tits.2015.2436897", "10.3390/ijerph16071150", "10.1109/tvcg.2009.145", "10.1109/tvcg.2012.265", "10.1080/10106049.2017.1404140", "10.3390/ijgi8020063", "10.3390/info6020134", "10.1080/13658816.2014.955027", "10.1109/tits.2016.2639320", "10.2307/143141", "10.1109/tvcg.2016.2598432"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030370", "title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups", "year": "2020", "conferenceName": "VAST", "authors": "Yating Lin;Kamkwai Wong;Yong Wang;Rong Zhang;Bo Dong;Huamin Qu;Qinghua Zheng", "citationCount": "0", "affiliation": "Lin, YT (Corresponding Author), Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian, Shaanxi, Peoples R China. Lin, Yating; Zheng, Qinghua, Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian, Shaanxi, Peoples R China. Wong, Kamkwai; Wang, Yong; Zhang, Rong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Dong, Bo, Xi An Jiao Tong Univ, Natl Engn Lab Big Data Analyt, Xian, Shaanxi, Peoples R China.", "countries": "China", "abstract": "Tax evasion is a serious economic problem for many countries, as it can undermine the government's tax system and lead to an unfair business competition environment. Recent research has applied data analytics techniques to analyze and detect tax evasion behaviors of individual taxpayers. However, they have failed to support the analysis and exploration of the related party transaction tax evasion (RPTTE) behaviors (e.g., transfer pricing), where a group of taxpayers is involved. In this paper, we present TaxThemis, an interactive visual analytics system to help tax officers mine and explore suspicious tax evasion groups through analyzing heterogeneous tax-related data. A taxpayer network is constructed and fused with the respective trade network to detect suspicious RPTTE groups. Rich visualizations are designed to facilitate the exploration and investigation of suspicious transactions between related taxpayers with profit and topological data analysis. Specifically, we propose a calendar heatmap with a carefully-designed encoding scheme to intuitively show the evidence of transferring revenue through related party transactions. We demonstrate the usefulness and effectiveness of TaxThemis through two case studies on real-world tax-related data and interviews with domain experts.", "keywords": "Visual Analytics,Tax Network,Tax Evasion Detection,Anomaly detection,Multidimensional data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030370", "refList": ["10.1111/cgf.12886", "10.2307/2277827", "10.1109/tvcg.2010.44", "10.1109/tits.2014.2315794", "10.1109/tvcg.2019.2934670", "10.1038/s41467-019-08987-4", "10.1111/cgf.12920", "10.1109/vast.2017.8585721", "10.1080/15230406.2015.1093431", "10.1109/tvcg.2018.2843369", "10.1038/srep01001", "10.1109/tvcg.2017.2744018", "10.1109/tvcg.2017.2744159", "10.1068/b130199p", "10.1109/tvcg.2009.143", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2019.2892483", "10.1109/pacificvis.2017.8031583", "10.1109/pacificvis48177.2020.2785", "10.2307/2686111", "10.1109/tvcg.2015.2467199", "10.1111/cgf.12114", "10.1109/tvcg.2018.2865126", "10.1111/j.1538-4632.1996.tb00936.x", "10.1109/tvcg.2019.2934619", "10.2307/2332142", "10.1007/978-3-319-10590-1\\_53", "10.1109/cvpr.2016.485", "10.1109/cvpr.2017.17", "10.2307/2986645", "10.1109/tvcg.2014.2346321", "10.1017/s0140525x16001837", "10.1109/tvcg.2016.2598541", "10.1371/journal.pone.0207377", "10.1109/tvcg.2014.2346265", "10.1007/s4095-020-0191-7", "10.3141/1644-14", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2016.90", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2785807", "10.1109/tvcg.2017.2744358", "10.1111/j.1538-4632.1995.tb00338.x", "10.1080/03081068808717359", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030469", "title": "Topology Density Map for Urban Data Visualization and Analysis", "year": "2020", "conferenceName": "VAST", "authors": "Zezheng Feng;Haotian Li;Wei Zeng 0004;Shuang-Hua Yang;Huamin Qu", "citationCount": "0", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Feng, Zezheng; Li, Haotian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yang, Shuang-Hua, Southern Univ Sci \\& Technol, Shenzhen, Peoples R China.", "countries": "China", "abstract": "Density map is an effective visualization technique for depicting the scalar field distribution in 2D space. Conventional methods for constructing density maps are mainly based on Euclidean distance, limiting their applicability in urban analysis that shall consider road network and urban traffic. In this work, we propose a new method named Topology Density Map, targeting for accurate and intuitive density maps in the context of urban environment. Based on the various constraints of road connections and traffic conditions, the method first constructs a directed acyclic graph (DAG) that propagates nonlinear scalar fields along 1D road networks. Next, the method extends the scalar fields to a 2D space by identifying key intersecting points in the DAG and calculating the scalar fields for every point, yielding a weighted Voronoi diagram like effect of space division. Two case studies demonstrate that the Topology Density Map supplies accurate information to users and provides an intuitive visualization for decision making. An interview with domain experts demonstrates the feasibility, usability, and effectiveness of our method.", "keywords": "Density map,network topology,urban data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030469", "refList": ["10.1109/vast.2009.5332584", "10.1109/tvcg.2013.193", "10.1080/03081060.2013.844903", "10.1109/tvcg.2018.2864503", "10.1145/2702123.2702419", "10.1109/tvcg.2019.2934670", "10.1109/tits.2015.2496783", "10.1177/1473871615581216", "10.3141/1617-02", "10.1145/2024156.2024169", "10.1111/cgf.13712", "10.1016/j.ejor.2007.02.005", "10.1109/tvcg.2014.2346893", "10.1007/11871842\\_29", "10.1109/vast.2010.5652478", "10.1016/j.visinf.2019.10.002", "10.1109/tvcg.2016.2616404", "10.1109/vl.1996.545307", "10.1145/2629592", "10.1155/2018/2696037", "10.1061/(asce)0733-947x(1998)124:4(368", "10.3141/1899-21", "10.1023/a:1026123329433", "10.1109/mcg.2010.79", "10.1057/palgrave.ivs.9500174", "10.1109/tcyb.2019.2963681", "10.1109/tvcg.2015.2467554", "10.1111/cgf.12114", "10.1145/2814575", "10.1016/j.jcps.2014.08.002", "10.1109/2945.981847", "10.1080/03052150210909", "10.1109/tciaig.2012.2186810", "10.1109/tits.2017.2683539", "10.1109/iv.2004.1320137", "10.1016/0377-2217(80)90126-5", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2015.2467196", "10.1145/3097983.3098056", "10.1007/s11432-018-9801-4", "10.1109/vast.2014.7042490", "10.1061/(asce)0733-947x(2006)132:2(122", "10.1016/j.tra.2008.03.011", "10.1109/tits.2014.2298892", "10.1016/j.trb.2005.12.003", "10.1007/bf01840357", "10.1109/vast.2011.6102454", "10.1109/tvcg.2013.145", "10.1007/bf02289588", "10.1109/pacificvis.2014.56", "10.1109/mcg.2018.053491730", "10.1109/tvcg.2009.111", "10.1057/palgrave.ivs.9500184", "10.1109/tvcg.2013.173", "10.1109/tvcg.2016.2598432", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2020.3030370", "title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups", "year": "2020", "conferenceName": "VAST", "authors": "Yating Lin;Kamkwai Wong;Yong Wang;Rong Zhang;Bo Dong;Huamin Qu;Qinghua Zheng", "citationCount": "0", "affiliation": "Lin, YT (Corresponding Author), Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian, Shaanxi, Peoples R China. Lin, Yating; Zheng, Qinghua, Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian, Shaanxi, Peoples R China. Wong, Kamkwai; Wang, Yong; Zhang, Rong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Dong, Bo, Xi An Jiao Tong Univ, Natl Engn Lab Big Data Analyt, Xian, Shaanxi, Peoples R China.", "countries": "China", "abstract": "Tax evasion is a serious economic problem for many countries, as it can undermine the government's tax system and lead to an unfair business competition environment. Recent research has applied data analytics techniques to analyze and detect tax evasion behaviors of individual taxpayers. However, they have failed to support the analysis and exploration of the related party transaction tax evasion (RPTTE) behaviors (e.g., transfer pricing), where a group of taxpayers is involved. In this paper, we present TaxThemis, an interactive visual analytics system to help tax officers mine and explore suspicious tax evasion groups through analyzing heterogeneous tax-related data. A taxpayer network is constructed and fused with the respective trade network to detect suspicious RPTTE groups. Rich visualizations are designed to facilitate the exploration and investigation of suspicious transactions between related taxpayers with profit and topological data analysis. Specifically, we propose a calendar heatmap with a carefully-designed encoding scheme to intuitively show the evidence of transferring revenue through related party transactions. We demonstrate the usefulness and effectiveness of TaxThemis through two case studies on real-world tax-related data and interviews with domain experts.", "keywords": "Visual Analytics,Tax Network,Tax Evasion Detection,Anomaly detection,Multidimensional data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030370", "refList": ["10.1111/cgf.12886", "10.2307/2277827", "10.1109/tvcg.2010.44", "10.1109/tits.2014.2315794", "10.1109/tvcg.2019.2934670", "10.1038/s41467-019-08987-4", "10.1111/cgf.12920", "10.1109/vast.2017.8585721", "10.1080/15230406.2015.1093431", "10.1109/tvcg.2018.2843369", "10.1038/srep01001", "10.1109/tvcg.2017.2744018", "10.1109/tvcg.2017.2744159", "10.1068/b130199p", "10.1109/tvcg.2009.143", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2019.2892483", "10.1109/pacificvis.2017.8031583", "10.1109/pacificvis48177.2020.2785", "10.2307/2686111", "10.1109/tvcg.2015.2467199", "10.1111/cgf.12114", "10.1109/tvcg.2018.2865126", "10.1111/j.1538-4632.1996.tb00936.x", "10.1109/tvcg.2019.2934619", "10.2307/2332142", "10.1007/978-3-319-10590-1\\_53", "10.1109/cvpr.2016.485", "10.1109/cvpr.2017.17", "10.2307/2986645", "10.1109/tvcg.2014.2346321", "10.1017/s0140525x16001837", "10.1109/tvcg.2016.2598541", "10.1371/journal.pone.0207377", "10.1109/tvcg.2014.2346265", "10.1007/s4095-020-0191-7", "10.3141/1644-14", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2016.90", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2785807", "10.1109/tvcg.2017.2744358", "10.1111/j.1538-4632.1995.tb00338.x", "10.1080/03081068808717359", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}], "len": 15}, {"doi": "10.1111/cgf.13713", "year": "2019", "title": "Bird's-Eye - Large-Scale Visual Analytics of City Dynamics using Social Location Data", "conferenceName": "EuroVis", "authors": "Robert Kr{\\\"{u}}ger;Qi Han;Nikolay Ivanov;Sanae Mahtal;Dennis Thom;Hanspeter Pfister;Thomas Ertl", "citationCount": "2", "affiliation": "Krueger, R (Corresponding Author), Harvard Univ, Visual Comp Grp, Cambridge, MA 02138 USA.\nKrueger, Robert; Pfister, Hanspeter, Harvard Univ, Visual Comp Grp, Cambridge, MA 02138 USA.\nHang, Qi; Ivanov, Nikolay; Mahtal, Sanae; Thom, Dennis; Ertl, Thomas, Univ Stuttgart, Inst Visualizat \\& Interact Syst, Stuttgart, Germany.", "countries": "Germany;USA", "abstract": "The analysis of behavioral city dynamics, such as temporal patterns of visited places and citizens' mobility routines, is an essential task for urban and transportation planning. Social media applications such as Foursquare and Twitter provide access to large-scale and up-to-date dynamic movement data that not only help to understand the social life and pulse of a city but also to maintain and improve urban infrastructure. However, the fast growth rate of this data poses challenges for conventional methods to provide up-to-date, flexible analysis. Therefore, planning authorities barely consider it. We present a system and design study to leverage social media data that assist urban and transportation planners to achieve better monitoring and analysis of city dynamics such as visited places and mobility patterns in large metropolitan areas. We conducted a goal-and-task analysis with urban planning experts. To address these goals, we designed a system with a scalable data monitoring back-end and an interactive visual analytics interface. The monitoring component uses intelligent pre-aggregation to allow dynamic queries in near real-time. The visual analytics interface leverages unsupervised learning to reveal clusters, routines, and unusual behavior in massive data, allowing to understand patterns in time and space. We evaluated our approach based on a qualitative user study with urban planning experts which demonstrates that intuitive integration of advanced analytical tools with visual interfaces is pivotal in making behavioral city dynamics accessible to practitioners. Our interviews also revealed areas for future research.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13713", "refList": ["10.1016/j.jvlc.2014.10.028", "10.1109/tvcg.2015.2468111", "10.1111/cgf.12129", "10.1109/tvcg.2011.127", "10.1145/2516604.2516616", "10.1126/science.1200970", "10.1145/2629592", "10.4000/netcom.2725", "10.1109/tvcg.2016.2598585", "10.1145/2365952.2366028", "10.1109/tvcg.2015.2467619", "10.1109/tvcg.2009.100", "10.1109/iv.2010.94", "10.1145/2424321.2424395", "10.1109/pacificvis.2012.6183572", "10.1109/tits.2017.2727281", "10.1177/1473871617692841", "10.1109/tvcg.2013.179", "10.1080/09669582.2010.502576", "10.1145/2378023.2378027", "10.1109/ictai.2016.0063", "10.1109/tvcg.2017.2758362", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2012.213", "10.1057/udi.2010.20", "10.1109/tvcg.2014.2371856", "10.1109/ictai.2016.60", "10.1109/tits.2016.2639320", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2008.152", "10.1109/tvcg.2015.2511733"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13995", "year": "2020", "title": "GTMapLens: Interactive Lens for Geo-Text Data Browsing on Map", "conferenceName": "EuroVis", "authors": "Chao Ma;Ye Zhao;Shamal Al{-}Dohuki;Jing Yang;Xinyue Ye;Farah Kamw;Md. Amiruzzaman", "citationCount": "0", "affiliation": "Zhao, Y (Corresponding Author), Kent State Univ, Kent, OH 44240 USA.\nMa, Chao; Zhao, Ye; Amiruzzaman, Md, Kent State Univ, Kent, OH 44240 USA.\nAl-Dohuki, Shamal, Univ Duhok, Duhok, Iraq.\nYang, Jing, Univ N Carolina, Charlotte, NC USA.\nYe, Xinyue, New Jersey Inst Technol, Newark, NJ 07102 USA.\nKamw, Farah, Concordia Univ, Ann Arbor, MI USA.", "countries": "USA;Iraq", "abstract": "Data containing geospatial semantics, such as geotagged tweets, travel blogs, and crime reports, associates natural language texts with geographical locations. This paper presents a lens-based visual interaction technique, GTMapLens, to flexibly browse the geo-text data on a map. It allows users to perform dynamic focus+context exploration by using movable lenses to browse geographical regions, find locations of interest, and perform comparative and drill-down studies. Geo-text data is visualized in a way that users can easily perceive the underlying geospatial semantics along with lens moving. Based on a requirement analysis with a cohort of multidisciplinary domain experts, a set of lens interaction techniques are developed including keywords control, path management, context visualization, and snapshot anchors. They allow users to achieve a guided and controllable exploration of geo-text data. A hierarchical data model enables the interactive lens operations by accelerated data retrieval from a geo-text database. Evaluation with real-world datasets is presented to show the usability and effectiveness of GTMapLens.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13995", "refList": ["10.1109/tvcg.2018.2865235", "10.1145/1936652.1936673", "10.1145/2598153.2598200", "10.1145/1456650.1456652", "10.1109/vast.2011.6102456", "10.1145/3290605.3300864", "10.1177/1473871611413180", "10.1109/iv.2011.43", "10.1109/tvcg.2015.2467971", "10.1109/iv.2016.62", "10.1080/13658816.2017.1325488", "10.1111/cgf.12871", "10.1111/cgf.12132", "10.1109/mc.2012.430", "10.1109/tvcg.2016.2598585", "10.1109/tvcg.2011.195", "10.1109/tvcg.2015.2467619", "10.1109/pacificvis.2013.6596122", "10.13140/rg.2.2.36636.59521", "10.1145/3170427.3188506", "10.1016/b978-155860915-0/50040-8", "10.1109/infvis.2003.1249008", "10.1109/tvcg.2018.2850781", "10.1016/j.datak.2006.01.013", "10.1109/tvcg.2015.2467991", "10.1111/gec3.12404", "10.1109/tvcg.2008.149", "10.1109/visual.1998.745317", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2011.176", "10.1016/j.visinf.2018.04.006.2", "10.1111/cgf.13264", "10.1080/13658816.2010.508043", "10.1007/s41651-017-0002-6", "10.1109/tvcg.2009.65", "10.1109/vast.2011.6102498", "10.1145/3025453.3025777", "10.1109/tvcg.2006.138"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13968", "year": "2020", "title": "Ocupado: Visualizing Location-Based Counts Over Time Across Buildings", "conferenceName": "EuroVis", "authors": "Michael Oppermann;Tamara Munzner", "citationCount": "0", "affiliation": "Oppermann, M (Corresponding Author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada.\nOppermann, Michael; Munzner, Tamara, Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada.", "countries": "Canada", "abstract": "Understanding how spaces in buildings are being used is vital for optimizing space utilization, for improving resource allocation, and for the design of new facilities. We present a multi-year design study that resulted in Ocupado, a set of visual decision-support tools centered around occupancy data for stakeholders in facilities management and planning. Ocupado uses WiFi devices as a proxy for human presence, capturing location-based counts that preserve privacy without trajectories. We contribute data and task abstractions for studying space utilization for combinations of data granularities in both space and time. In addition, we contribute generalizable design choices for visualizing location-based counts relating to indoor environments. We provide evidence of Ocupado's utility through multiple analysis scenarios with real-world data refined through extensive stakeholder feedback, and discussion of its take-up by our industry partner.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13968", "refList": ["10.1111/j.1467-8659.2011.01946.x", "10.1109/vast.2012.6400553", "10.1007/978-3-642-37583-5", "10.14257/ijt.2018.6.1.01", "10.1145/882082.882086", "10.1109/bdva.2018.8534026", "10.1109/tvcg.2014.2346312", "10.1016/j.pmcj.2016.08.011", "10.1057/palgrave.ivs.9500165", "10.1109/tvcg.2014.2346323", "10.1109/infvis.1999.801851", "10.1109/igcc.2011.6008560", "10.1016/j.energy.2015.09.002", "10.1080/13658816.2010.511223", "10.1109/tvcg.2007.70621", "10.1109/tvcg.2016.2598585", "10.1109/tvcg.2011.195", "10.1109/tvcg.2009.100", "10.1109/rev.2014.6784260", "10.1145/642611.642616", "10.1007/3-540-31190-4", "10.1109/vast.2015.7347672", "10.1109/vast.2012.6400557", "10.1109/infvis.2005.1532150", "10.1111/j.1467-8659.2009.01710.x", "10.1109/tvcg.2011.209", "10.1109/tvcg.2007.70583", "10.1111/cgf.12883", "10.1007/3-4.540-31190-4", "10.1145/2517351.2517370", "10.1007/s12273-017-0355-2", "10.1145/985692.985706", "10.3138/carto.46.4.239", "10.1016/j.autcon.2014.03.023", "10.1109/tvcg.2012.213", "10.1051/sbuild/2017005", "10.1109/tvcg.2010.162", "10.1145/1360612.1360700", "10.1038/srep01376", "10.1109/tvcg.2018.2811488", "10.1145/3025453.3026055", "10.1109/tvcg.2009.111", "10.1109/tvcg.2015.2466971", "10.1109/tvcg.2013.173", "10.1109/tvcg.2017.2666146", "10.1109/tvcg.2018.2864914", "10.1007/978-0-85729-079-3", "10.1109/tvcg.2016.2614803"], "wos": 1, "children": [], "len": 1}], "len": 23}, {"doi": "10.1109/tvcg.2018.2865018", "title": "TPFlow: Progressive Partition and Multidimensional Pattern Extraction for Large-Scale Spatio-Temporal Data Analysis", "year": "2018", "conferenceName": "VAST", "authors": "Dongyu Liu;Panpan Xu;Ren Liu", "citationCount": "6", "affiliation": "Liu, DY (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Dongyu, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Dongyu; Xu, Panpan; Ren, Liu, Bosch Res North Amer, Sunnyvale, CA USA.", "countries": "USA;China", "abstract": "Consider a multi-dimensional spatio-temporal (ST) dataset where each entry is a numerical measure defined by the corresponding temporal, spatial and other domain-specific dimensions. A typical approach to explore such data utilizes interactive visualizations with multiple coordinated views. Each view displays the aggregated measures along one or two dimensions. By brushing on the views, analysts can obtain detailed information. However, this approach often cannot provide sufficient guidance for analysts to identify patterns hidden within subsets of data. Without a priori hypotheses, analysts need to manually select and iterate through different slices to search for patterns, which can be a tedious and lengthy process. In this work, we model multidimensional ST data as tensors and propose a novel piecewise rank-one tensor decomposition algorithm which supports automatically slicing the data into homogeneous partitions and extracting the latent patterns in each partition for comparison and visual summarization. The algorithm optimizes a quantitative measure about how faithfully the extracted patterns visually represent the original data. Based on the algorithm we further propose a visual analytics framework that supports a top-down, progressive partitioning workflow for level-of-detail multidimensional ST data exploration. We demonstrate the general applicability and effectiveness of our technique on three datasets from different application domains: regional sales trend analysis, customer traffic analysis in department stores, and taxi trip analysis with origin-destination (OD) data. We further interview domain experts to verify the usability of the prototype.", "keywords": "Spatio-temporal data,tensor decomposition,interactive exploration,automatic pattern discoveries", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865018", "refList": ["10.1007/s13177-014-0099-7", "10.1109/tvcg.2013.226", "10.1111/j.1467-8659.2009.01664.x", "10.1007/s00371-013-0892-3", "10.1109/tvcg.2015.2468111", "10.2307/3151680", "10.1111/cgf.12129", "10.1145/2254556.2254659", "10.1109/tvcg.2017.2744419", "10.1145/2632048.2636073", "10.1109/2945.841121", "10.1109/tvcg.2014.2346449", "10.1109/tvcg.2007.70515", "10.1109/tsmc.2018.2871100", "10.1145/2629592", "10.1109/tvcg.2016.2616404", "10.1109/vl.1996.545307", "10.1109/infvis.2003.1249018", "10.1198/jcgs.2010.09051", "10.1007/978-3-319-71249-9\\_35", "10.1109/tvcg.2016.2598624", "10.1109/tvcg.2017.2744805", "10.1109/icde.2014.6816674", "10.1111/cgf.12888", "10.1109/tvcg.2006.161", "10.1109/tvcg.2016.2598862", "10.1016/s1045-926x(03)00046-6", "10.1089/cmb.2018.0139", "10.1109/tits.2017.2683539", "10.1109/tbdata.2016.2586447", "10.1137/07070111x", "10.1007/bf02310791", "10.1109/tits.2015.2436897", "10.1016/j.trc.2017.10.023", "10.1109/tvcg.2013.179", "10.1177/1473871616667632", "10.1137/s0895479899352045", "10.1109/jas.2017.7510538", "10.1111/j.1467-8659.2012.03117.x", "10.1007/bf01908075", "10.1002/widm.1", "10.2307/2669946", "10.1137/s0895479801387413", "10.1109/tvcg.2015.2467112", "10.1109/tvcg.2014.2346574", "10.1057/palgrave.ivs.9500184", "10.1109/infvis.1999.801851", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028889", "title": "A Visual Analytics Framework for Reviewing Multivariate Time-Series Data with Dimensionality Reduction", "year": "2020", "conferenceName": "VAST", "authors": "Takanori Fujiwara;Shilpika;Naohisa Sakamoto;Jorji Nonaka;Keiji Yamamoto;Kwan-Liu Ma", "citationCount": "0", "affiliation": "Fujiwara, T (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Fujiwara, Takanori; Shilpika; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA. Sakamoto, Naohisa, Kobe Univ, Kobe, Hyogo, Japan. Nonaka, Jorji; Yamamoto, Keiji, RIKEN R CCS, Kobe, Hyogo, Japan.", "countries": "Japan;USA", "abstract": "Data-driven problem solving in many real-world applications involves analysis of time-dependent multivariate data, for which dimensionality reduction (DR) methods are often used to uncover the intrinsic structure and features of the data. However, DR is usually applied to a subset of data that is either single-time-point multivariate or univariate time-series, resulting in the need to manually examine and correlate the DR results out of different data subsets. When the number of dimensions is large either in terms of the number of time points or attributes, this manual task becomes too tedious and infeasible. In this paper, we present MulTiDR, a new DR framework that enables processing of time-dependent multivariate data as a whole to provide a comprehensive overview of the data. With the framework, we employ DR in two steps. When treating the instances, time points, and attributes of the data as a 3D array, the first DR step reduces the three axes of the array to two, and the second DR step visualizes the data in a lower-dimensional space. In addition, by coupling with a contrastive learning method and interactive visualizations, our framework enhances analysts' ability to interpret DR results. We demonstrate the effectiveness of our framework with four case studies using real-world datasets.", "keywords": "Multivariate time-series,tensor,data cube,dimensionality reduction,interpretability,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028889", "refList": ["10.1016/j.neucom.2008.12.017", "10.1109/tvcg.2015.2467591", "10.1007/bf02289464", "10.1109/tvcg.2015.2468111", "10.1177/1350650119867242", "10.1109/pacificvis48177.2020.9280", "10.1109/tvcg.2017.2744419", "10.1109/tvcg.2011.185", "10.1109/bigdata.2015.7363807", "10.1145/2669557.2669559", "10.1016/j.patcog.2011.01.004", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1186/1475-925x-14-s2-s6", "10.1007/s12650-018-0530-2", "10.1109/tkde.2018.2878247", "10.1016/j.visinf.2018.04.010", "10.1002/1099-128x(200005/06)14:3", "10.1371/journal.pone.0107878", "10.1109/pacificvis.2017.8031601", "10.1016/j.jbi.2019.103291", "10.1109/daac49578.2019.00008", "10.1109/allerton.2019.8919886", "10.1007/978-3-319-13105-4\\_14", "10.4258/hir.2016.22.3.156", "10.1109/tvcg.2015.2467553", "10.1145/2245276.2245469", "10.1007/bf02294485", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2016.2598470", "10.1137/07070111x", "10.1109/tvcg.2019.2934433", "10.1109/tvcg.2016.2640960", "10.1007/bf02310791", "10.1109/tvcg.2016.2534558", "10.1109/access.2016.2529723", "10.1109/tvcg.2016.2598664", "10.2312/eurovisshort.20161164", "10.1109/tvcg.2018.2865018", "10.1111/cgf.12804", "10.1109/tvcg.2018.2846735", "10.1016/j.comnet.2017.06.013", "10.1146/annurev-statistics-041715-033624", "10.1109/pacificvis.2018.00026", "10.1109/tvcg.2015.2467851"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030425", "title": "Visual Analysis of Argumentation in Essays", "year": "2020", "conferenceName": "VAST", "authors": "Dora Kiesel;Patrick Riehmann;Henning Wachsmuth;Benno Stein;Bernd Fr\u00f6hlich", "citationCount": "0", "affiliation": "Kiesel, D (Corresponding Author), Bauhaus Univ Weimar, Weimar, Germany. Kiesel, Dora; Riehmann, Patrick; Stein, Benno; Froehlich, Bernd, Bauhaus Univ Weimar, Weimar, Germany. Wachsmuth, Henning, Paderborn Univ, Paderborn, Germany.", "countries": "Germany", "abstract": "This paper presents a visual analytics system for exploring, analyzing and comparing argument structures in essay corpora. We provide an overview of the corpus by a list of ArguLines which represent the argument units of each essay by a sequence of glyphs. Each glyph encodes the stance, the depth and the relative position of an argument unit. The overview can be ordered in various ways to reveal patterns and outliers. Subsets of essays can be selected and analyzed in detail using the Argument Unit Occurrence Tree which aggregates the argument structures using hierarchical histograms. This hierarchical view facilitates the estimation of statistics and trends concerning the progression of the argumentation in the essays. It also provides insights into the commonalities and differences between selected subsets. The text view is the necessary textual basis to verify conclusions from the other views and the annotation process. Linking the views and interaction techniques for visual filtering, studying the evolution of stance within a subset of essays and scrutinizing the order of argumentative units enable a deep analysis of essay corpora. Our expert reviews confirmed the utility of the system and revealed detailed and previously unknown information about the argumentation in our sample corpus.", "keywords": "Information Visualization,Text Analysis,User Interfaces,Visual Analytics,Argumentation Visualization,Glyph-based Techniques,Text and Document Data,Tree-based Visualization,Coordinated and Multiple Views,Close and Distant Reading", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030425", "refList": ["10.1109/tvcg.2014.2346575", "10.14778/2735479.2735485", "10.14778/3192965.3192971", "10.1111/cgf.12129", "10.1145/2206869.2206874", "10.1109/icde.2016.7498300", "10.14778/2831360.2831371", "10.1145/3035918.3056097", "10.1023/a:1009726021843", "10.1111/cgf.13678", "10.1145/3183713.3196905", "10.14778/3115404.3115418", "10.1109/tvcg.2012.180", "10.1109/icde.1999.754950", "10.14778/1453856.1453924", "10.1145/3209900.3209901", "10.1002/spe.2325", "10.1145/42201.42203", "10.1109/tvcg.2013.124", "10.1109/vast.2008.4677357", "10.1109/icde.2016.7498287", "10.1109/icde.2014.6816674", "10.14778/2732951.2732964", "10.1109/tvcg.2015.2467551", "10.1145/1084805.1084812", "10.1109/2.781635", "10.14778/2732279.2732280", "10.1109/icde.2015.7113427", "10.1109/tvcg.2015.2467091", "10.1007/s00778-017-0486-1", "10.1109/tvcg.2003.1196005", "10.1109/icde.2004.1320035", "10.1109/tvcg.2016.2607714", "10.14778/2732951.2732953", "10.1109/tvcg.2008.131", "10.1109/tvcg.2018.2865018", "10.1145/2133806.2133821", "10.1109/icde.2019.00035", "10.1109/tpds.2005.144", "10.14778/3236187.3236212", "10.1109/tvcg.2009.111", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/tvcg.2019.2934670", "title": "AirVis: Visual Analytics of Air Pollution Propagation", "year": "2019", "conferenceName": "VAST", "authors": "Zikun Deng;Di Weng;Jiahui Chen;Ren Liu;Zhibin Wang;Jie Bao 0003;Yu Zheng 0004;Yingcai Wu", "citationCount": "6", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Deng, Zikun; Weng, Di; Chen, Jiahui; Liu, Ren; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Wang, Zhibin, Zhejiang Univ, Res Ctr Air Pollut \\& Hlth, Hangzhou, Peoples R China. Bao, Jie; Zheng, Yu, JD Intelligent City Res, Beijing, Peoples R China.", "countries": "China", "abstract": "Air pollution has become a serious public health problem for many cities around the world. To find the causes of air pollution, the propagation processes of air pollutants must be studied at a large spatial scale. However, the complex and dynamic wind fields lead to highly uncertain pollutant transportation. The state-of-the-art data mining approaches cannot fully support the extensive analysis of such uncertain spatiotemporal propagation processes across multiple districts without the integration of domain knowledge. The limitation of these automated approaches motivates us to design and develop AirVis, a novel visual analytics system that assists domain experts in efficiently capturing and interpreting the uncertain propagation patterns of air pollution based on graph visualizations. Designing such a system poses three challenges: a) the extraction of propagation patterns; b) the scalability of pattern presentations; and c) the analysis of propagation processes. To address these challenges, we develop a novel pattern mining framework to model pollutant transportation and extract frequent propagation patterns efficiently from large-scale atmospheric data. Furthermore, we organize the extracted patterns hierarchically based on the minimum description length (MDL) principle and empower expert users to explore and analyze these patterns effectively on the basis of pattern topologies. We demonstrated the effectiveness of our approach through two case studies conducted with a real-world dataset and positive feedback from domain experts.", "keywords": "Air pollution propagation,pattern mining,graph visualization", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934670", "refList": ["10.1109/tvcg.2016.2598919", "10.1093/bib/bbr069", "10.1145/2487575.2488188", "10.1109/tvcg.2013.193", "10.1016/j.atmosenv.2014.12.011", "10.1109/tvcg.2013.226", "10.1109/tvcg.2018.2864503", "10.1109/tvcg.2015.2468111", "10.1109/icicta.2015.183", "10.1016/j.atmosenv.2014.05.039", "10.1111/cgf.12791", "10.1111/j.1467-8659.2009.01451.x", "10.1111/j.1467-8659.2011.01898.x", "10.5194/acp-12-5031-2012", "10.1016/j.atmosenv.2008.05.053", "10.1109/tvcg.2016.2535234", "10.1016/j.atmosres.2014.12.003", "10.1109/tvcg.2015.2467194", "10.1109/tvcg.2013.263", "10.1109/icdm.2002.1184038", "10.1145/2783258.2788573", "10.1109/tvcg.2018.2865149", "10.1109/tbdata.2017.2723899", "10.1109/tvcg.2012.311", "10.1109/vl.1996.545307", "10.1007/s12650-018-0481-7", "10.1016/j.envpol.2007.06.012", "10.3155/1047-3289.61.6.660", "10.1080/13658810701349037", "10.1145/3097983.3098090", "10.1109/tvcg.2007.70523", "10.1115/1.2128636", "10.1109/tvcg.2015.2467619", "10.3978/j.issn.2072-1439.2016.01.19", "10.1109/tkde.2005.127", "10.1017/s0269888912000331", "10.2312/conf/eg2013/stars/039-063", "10.1109/tvcg.2014.2346271", "10.1175/bams-d-14-00110.1", "10.1016/0005-1098(78)90005-5", "10.1007/s10618-006-0044-8", "10.1109/tvcg.2018.2865126", "10.2307/1912791", "10.3390/su6085322", "10.1007/s12650-018-0489-z", "10.2312/eurovisstar.20151109", "10.1109/tvcg.2011.181", "10.1126/science.298.5594.824", "10.1162/jmlr.2003.3.4-5.951", "10.1109/asonam.2014.6921638", "10.1111/j.1467-8659.2008.01213.x", "10.1038/s41598-017-18107-1", "10.1109/tvcg.2018.2865041", "10.1109/tits.2019.2901117", "10.1038/srep20668", "10.1109/tvcg.2012.265", "10.1109/tpami.2016.2608884", "10.1109/tvcg.2012.213", "10.1145/1376616.1376661", "10.1007/s00521-019-04567-1", "10.1109/tvcg.2017.2745083", "10.1126/science.243.4892.745", "10.1109/tvcg.2018.2864826", "10.1109/tnn.2003.820440", "10.1109/tvcg.2016.2598885", "10.1145/3219819.3219822", "10.1073/pnas.1502596112", "10.1016/j.envsoft.2009.01.004", "10.1002/pmic.200700095", "10.1145/2254556.2254651", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2011.202"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028958", "title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Tiankai Xie;Yuxin Ma;Hanghang Tong;My T. Thai;Ross Maciejewski", "citationCount": "0", "affiliation": "Xie, TK (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Xie, Tiankai; Ma, Yuxin; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. Tong, Hanghang, Univ Illinois, Urbana, IL USA. Thai, My T., Univ Florida, Gainesville, FL 32611 USA.", "countries": "USA", "abstract": "Graph mining plays a pivotal role across a number of disciplines, and a variety of algorithms have been developed to answer who/what type questions. For example, what items shall we recommend to a given user on an e-commerce platform? The answers to such questions are typically returned in the form of a ranked list, and graph-based ranking methods are widely used in industrial information retrieval settings. However, these ranking algorithms have a variety of sensitivities, and even small changes in rank can lead to vast reductions in product sales and page hits. As such, there is a need for tools and methods that can help model developers and analysts explore the sensitivities of graph ranking algorithms with respect to perturbations within the graph structure. In this paper, we present a visual analytics framework for explaining and exploring the sensitivity of any graph-based ranking algorithm by performing perturbation-based what-if analysis. We demonstrate our framework through three case studies inspecting the sensitivity of two classic graph-based ranking algorithms (PageRank and HITS) as applied to rankings in political news media and social networks.", "keywords": "Graph-based ranking,sensitivity analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028958", "refList": ["10.1109/wsc.2017.8247800", "10.1023/a:1022649401552", "10.1515/1559-0410.11416", "10.1109/tvcg.2016.2598919", "10.1177/1473871611416549", "10.1109/tvcg.2019.2934630", "10.1140/epjds29", "10.1109/tvcg.2019.2934670", "10.1016/j.eswa.2015.09.004", "10.1145/2702123.2702509", "10.1016/j.visinf.2018.12.001", "10.2307/3002000", "10.1109/tvcg.2019.2934399", "10.1007/s41060-016-0032-z", "10.1111/cgf.13198", "10.14778/2350229.2350254", "10.1145/2939672.2939764", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/vast.2017.8585647", "10.1007/bf01187020", "10.1109/icdm.2015.26", "10.1145/2362383.2362387", "10.1177/0049124104268644", "10.1109/vast.2011.6102442", "10.1109/infvis.2003.1249025", "10.1109/tvcg.2018.2864475", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2015.2467691", "10.1111/cgf.13210", "10.1214/aos/1176344136", "10.1109/tvcg.2015.2424872", "10.1016/j.visinf.2018.09.001", "10.1177/089443939100900106", "10.1109/tvcg.2015.2467931", "10.1162/neco.1997.9.8.1735", "10.1007/s11162-011-9241-4", "10.1111/cgf.13680", "10.1145/3065386", "10.1109/tvcg.2018.2864889", "10.1177/003804070808100402", "10.1109/icdm.2010.62", "10.1038/s41598-020-59669-x", "10.1162/153244303321897717", "10.1109/tvcg.2019.2934619", "10.1007/bf00356088", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030410", "title": "Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Wei Zeng 0002;Chengqiao Lin;Juncong Lin;Jincheng Jiang;Jiazhi Xia;Cagatay Turkay;Wei Chen", "citationCount": "0", "affiliation": "Lin, JC (Corresponding Author), Xiamen Univ, Xiamen, Peoples R China. Zeng, Wei; Jiang, Jincheng, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Lin, Chengqiao; Lin, Juncong, Xiamen Univ, Xiamen, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Turkay, Cagatay, Univ Warwick, Coventry, W Midlands, England. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China.", "countries": "China;England", "abstract": "Deep learning methods are being increasingly used for urban traffic prediction where spatiotemporal traffic data is aggregated into sequentially organized matrices that are then fed into convolution-based residual neural networks. However, the widely known modifiable areal unit problem within such aggregation processes can lead to perturbations in the network inputs. This issue can significantly destabilize the feature embeddings and the predictions - rendering deep networks much less useful for the experts. This paper approaches this challenge by leveraging unit visualization techniques that enable the investigation of many-to-many relationships between dynamically varied multi-scalar aggregations of urban traffic data and neural network predictions. Through regular exchanges with a domain expert, we design and develop a visual analytics solution that integrates 1) a Bivariate Map equipped with an advanced bivariate colormap to simultaneously depict input traffic and prediction errors across space, 2) a Moran's I Scatterplot that provides local indicators of spatial association analysis, and 3) a Multi-scale Attribution View that arranges non-linear dot plots in a tree layout to promote model analysis and comparison across scales. We evaluate our approach through a series of case studies involving a real-world dataset of Shenzhen taxi trips, and through interviews with domain experts. We observe that geographical scale variations have important impact on prediction performances, and interactive visual exploration of dynamically varying inputs and outputs benefit experts in the development of deep traffic prediction models.", "keywords": "MAUP,traffic prediction,deep learning,model diagnostic,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030410", "refList": ["10.1038/srep26377", "10.1109/mcg.2011.88", "10.1080/13658816.2015.1119279", "10.1109/tvcg.2013.226", "10.1109/pacificvis.2011.5742387", "10.1038/s41467-017-01882-w", "10.1109/tvcg.2019.2934670", "10.1111/j.1467-8659.2009.01440.x", "10.1111/cgf.13712", "10.1016/j.compenvurbsys.2008.09.006", "10.1109/pacificvis.2014.50", "10.1109/tvcg.2018.2816219", "10.1109/tvcg.2016.2535234", "10.1109/tvcg.2014.2346893", "10.3390/ijgi8080344", "10.1109/tvcg.2013.246", "10.1007/s10940-005-9003-6", "10.1016/j.compenvurbsys.2008.05.001", "10.1007/s10661-019-7831-3", "10.1111/j.1538-4632.2007.00699.x", "10.1016/j.aap.2016.08.015", "10.1080/13658816.2018.1541177", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2011.181", "10.1137/090759069", "10.1109/pacificvis.2011.5742390", "10.1214/10-aos799", "10.1109/tits.2017.2683539", "10.1109/tits.2015.2436897", "10.3390/ijerph16071150", "10.1109/tvcg.2009.145", "10.1109/tvcg.2012.265", "10.1080/10106049.2017.1404140", "10.3390/ijgi8020063", "10.3390/info6020134", "10.1080/13658816.2014.955027", "10.1109/tits.2016.2639320", "10.2307/143141", "10.1109/tvcg.2016.2598432"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030370", "title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups", "year": "2020", "conferenceName": "VAST", "authors": "Yating Lin;Kamkwai Wong;Yong Wang;Rong Zhang;Bo Dong;Huamin Qu;Qinghua Zheng", "citationCount": "0", "affiliation": "Lin, YT (Corresponding Author), Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian, Shaanxi, Peoples R China. Lin, Yating; Zheng, Qinghua, Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian, Shaanxi, Peoples R China. Wong, Kamkwai; Wang, Yong; Zhang, Rong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Dong, Bo, Xi An Jiao Tong Univ, Natl Engn Lab Big Data Analyt, Xian, Shaanxi, Peoples R China.", "countries": "China", "abstract": "Tax evasion is a serious economic problem for many countries, as it can undermine the government's tax system and lead to an unfair business competition environment. Recent research has applied data analytics techniques to analyze and detect tax evasion behaviors of individual taxpayers. However, they have failed to support the analysis and exploration of the related party transaction tax evasion (RPTTE) behaviors (e.g., transfer pricing), where a group of taxpayers is involved. In this paper, we present TaxThemis, an interactive visual analytics system to help tax officers mine and explore suspicious tax evasion groups through analyzing heterogeneous tax-related data. A taxpayer network is constructed and fused with the respective trade network to detect suspicious RPTTE groups. Rich visualizations are designed to facilitate the exploration and investigation of suspicious transactions between related taxpayers with profit and topological data analysis. Specifically, we propose a calendar heatmap with a carefully-designed encoding scheme to intuitively show the evidence of transferring revenue through related party transactions. We demonstrate the usefulness and effectiveness of TaxThemis through two case studies on real-world tax-related data and interviews with domain experts.", "keywords": "Visual Analytics,Tax Network,Tax Evasion Detection,Anomaly detection,Multidimensional data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030370", "refList": ["10.1111/cgf.12886", "10.2307/2277827", "10.1109/tvcg.2010.44", "10.1109/tits.2014.2315794", "10.1109/tvcg.2019.2934670", "10.1038/s41467-019-08987-4", "10.1111/cgf.12920", "10.1109/vast.2017.8585721", "10.1080/15230406.2015.1093431", "10.1109/tvcg.2018.2843369", "10.1038/srep01001", "10.1109/tvcg.2017.2744018", "10.1109/tvcg.2017.2744159", "10.1068/b130199p", "10.1109/tvcg.2009.143", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2019.2892483", "10.1109/pacificvis.2017.8031583", "10.1109/pacificvis48177.2020.2785", "10.2307/2686111", "10.1109/tvcg.2015.2467199", "10.1111/cgf.12114", "10.1109/tvcg.2018.2865126", "10.1111/j.1538-4632.1996.tb00936.x", "10.1109/tvcg.2019.2934619", "10.2307/2332142", "10.1007/978-3-319-10590-1\\_53", "10.1109/cvpr.2016.485", "10.1109/cvpr.2017.17", "10.2307/2986645", "10.1109/tvcg.2014.2346321", "10.1017/s0140525x16001837", "10.1109/tvcg.2016.2598541", "10.1371/journal.pone.0207377", "10.1109/tvcg.2014.2346265", "10.1007/s4095-020-0191-7", "10.3141/1644-14", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2016.90", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2785807", "10.1109/tvcg.2017.2744358", "10.1111/j.1538-4632.1995.tb00338.x", "10.1080/03081068808717359", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030469", "title": "Topology Density Map for Urban Data Visualization and Analysis", "year": "2020", "conferenceName": "VAST", "authors": "Zezheng Feng;Haotian Li;Wei Zeng 0004;Shuang-Hua Yang;Huamin Qu", "citationCount": "0", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Feng, Zezheng; Li, Haotian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yang, Shuang-Hua, Southern Univ Sci \\& Technol, Shenzhen, Peoples R China.", "countries": "China", "abstract": "Density map is an effective visualization technique for depicting the scalar field distribution in 2D space. Conventional methods for constructing density maps are mainly based on Euclidean distance, limiting their applicability in urban analysis that shall consider road network and urban traffic. In this work, we propose a new method named Topology Density Map, targeting for accurate and intuitive density maps in the context of urban environment. Based on the various constraints of road connections and traffic conditions, the method first constructs a directed acyclic graph (DAG) that propagates nonlinear scalar fields along 1D road networks. Next, the method extends the scalar fields to a 2D space by identifying key intersecting points in the DAG and calculating the scalar fields for every point, yielding a weighted Voronoi diagram like effect of space division. Two case studies demonstrate that the Topology Density Map supplies accurate information to users and provides an intuitive visualization for decision making. An interview with domain experts demonstrates the feasibility, usability, and effectiveness of our method.", "keywords": "Density map,network topology,urban data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030469", "refList": ["10.1109/vast.2009.5332584", "10.1109/tvcg.2013.193", "10.1080/03081060.2013.844903", "10.1109/tvcg.2018.2864503", "10.1145/2702123.2702419", "10.1109/tvcg.2019.2934670", "10.1109/tits.2015.2496783", "10.1177/1473871615581216", "10.3141/1617-02", "10.1145/2024156.2024169", "10.1111/cgf.13712", "10.1016/j.ejor.2007.02.005", "10.1109/tvcg.2014.2346893", "10.1007/11871842\\_29", "10.1109/vast.2010.5652478", "10.1016/j.visinf.2019.10.002", "10.1109/tvcg.2016.2616404", "10.1109/vl.1996.545307", "10.1145/2629592", "10.1155/2018/2696037", "10.1061/(asce)0733-947x(1998)124:4(368", "10.3141/1899-21", "10.1023/a:1026123329433", "10.1109/mcg.2010.79", "10.1057/palgrave.ivs.9500174", "10.1109/tcyb.2019.2963681", "10.1109/tvcg.2015.2467554", "10.1111/cgf.12114", "10.1145/2814575", "10.1016/j.jcps.2014.08.002", "10.1109/2945.981847", "10.1080/03052150210909", "10.1109/tciaig.2012.2186810", "10.1109/tits.2017.2683539", "10.1109/iv.2004.1320137", "10.1016/0377-2217(80)90126-5", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2015.2467196", "10.1145/3097983.3098056", "10.1007/s11432-018-9801-4", "10.1109/vast.2014.7042490", "10.1061/(asce)0733-947x(2006)132:2(122", "10.1016/j.tra.2008.03.011", "10.1109/tits.2014.2298892", "10.1016/j.trb.2005.12.003", "10.1007/bf01840357", "10.1109/vast.2011.6102454", "10.1109/tvcg.2013.145", "10.1007/bf02289588", "10.1109/pacificvis.2014.56", "10.1109/mcg.2018.053491730", "10.1109/tvcg.2009.111", "10.1057/palgrave.ivs.9500184", "10.1109/tvcg.2013.173", "10.1109/tvcg.2016.2598432", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934806", "title": "GenerativeMap: Visualization and Exploration of Dynamic Density Maps via Generative Learning Model", "year": "2019", "conferenceName": "InfoVis", "authors": "Chen Chen;Changbo Wang;Xue Bai;Peiying Zhang;Chenhui Li", "citationCount": "0", "affiliation": "Li, CH (Corresponding Author), East China Normal Univ, Sch Comp Sci \\& Technol, Shanghai, Peoples R China. Chen, Chen; Wang, Changbo; Bai, Xue; Zhang, Peiying; Li, Chenhui, East China Normal Univ, Sch Comp Sci \\& Technol, Shanghai, Peoples R China.", "countries": "China", "abstract": "The density map is widely used for data sampling, time-varying detection, ensemble representation, etc. The visualization of dynamic evolution is a challenging task when exploring spatiotemporal data. Many approaches have been provided to explore the variation of data patterns over time, which commonly need multiple parameters and preprocessing works. Image generation is a well-known topic in deep learning, and a variety of generating models have been promoted in recent years. In this paper, we introduce a general pipeline called GenerativeMap to extract dynamics of density maps by generating interpolation information. First, a trained generative model comprises an important part of our approach, which can generate nonlinear and natural results by implementing a few parameters. Second, a visual presentation is proposed to show the density change, which is combined with the level of detail and blue noise sampling for a better visual effect. Third, for dynamic visualization of large-scale density maps, we extend this approach to show the evolution in regions of interest, which costs less to overcome the drawback of the learning-based generative model. We demonstrate our method on different types of cases, and we evaluate and compare the approach from multiple aspects. The results help identify the effectiveness of our approach and confirm its applicability in different scenarios.", "keywords": "Density map,deep learning,spatiotemporal data,generative model", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934806", "refList": ["10.1002/rcs.128", "10.1109/tvcg.2015.2468111", "10.1109/icip.2000.899541", "10.1109/tvcg.2011.155", "10.1145/1778765.1778816", "10.1016/j.cviu.2015.02.008", "10.2312/localchapterevents/tpcg/tpcg09/149-163", "10.1111/j.1467-8659.2011.01964.x", "10.1109/visual.1998.745282", "10.1016/j.patrec.2014.01.008", "10.1109/pacificvis.2014.15", "10.1111/cgf.13201", "10.1109/tvcg.2014.2346920", "10.1016/j.softx.2019.02.007", "10.1111/cgf.12933", "10.1109/tvcg.2007.70599", "10.1109/iccv.2017.206", "10.1109/pacificvis.2014.48", "10.1109/tvcg.2016.2607204", "10.1007/s12650-018-0516-0", "10.1214/10-aos799", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2779501", "10.1145/1778765.1778785", "10.1145/1866158.1866189", "10.1007/s11390-015-1535-0", "10.1155/2016/6156513", "10.1109/tvcg.2013.131", "10.1109/iccv.2017.299", "10.1109/tvcg.2016.2598869"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3028889", "title": "A Visual Analytics Framework for Reviewing Multivariate Time-Series Data with Dimensionality Reduction", "year": "2020", "conferenceName": "VAST", "authors": "Takanori Fujiwara;Shilpika;Naohisa Sakamoto;Jorji Nonaka;Keiji Yamamoto;Kwan-Liu Ma", "citationCount": "0", "affiliation": "Fujiwara, T (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Fujiwara, Takanori; Shilpika; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA. Sakamoto, Naohisa, Kobe Univ, Kobe, Hyogo, Japan. Nonaka, Jorji; Yamamoto, Keiji, RIKEN R CCS, Kobe, Hyogo, Japan.", "countries": "Japan;USA", "abstract": "Data-driven problem solving in many real-world applications involves analysis of time-dependent multivariate data, for which dimensionality reduction (DR) methods are often used to uncover the intrinsic structure and features of the data. However, DR is usually applied to a subset of data that is either single-time-point multivariate or univariate time-series, resulting in the need to manually examine and correlate the DR results out of different data subsets. When the number of dimensions is large either in terms of the number of time points or attributes, this manual task becomes too tedious and infeasible. In this paper, we present MulTiDR, a new DR framework that enables processing of time-dependent multivariate data as a whole to provide a comprehensive overview of the data. With the framework, we employ DR in two steps. When treating the instances, time points, and attributes of the data as a 3D array, the first DR step reduces the three axes of the array to two, and the second DR step visualizes the data in a lower-dimensional space. In addition, by coupling with a contrastive learning method and interactive visualizations, our framework enhances analysts' ability to interpret DR results. We demonstrate the effectiveness of our framework with four case studies using real-world datasets.", "keywords": "Multivariate time-series,tensor,data cube,dimensionality reduction,interpretability,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028889", "refList": ["10.1016/j.neucom.2008.12.017", "10.1109/tvcg.2015.2467591", "10.1007/bf02289464", "10.1109/tvcg.2015.2468111", "10.1177/1350650119867242", "10.1109/pacificvis48177.2020.9280", "10.1109/tvcg.2017.2744419", "10.1109/tvcg.2011.185", "10.1109/bigdata.2015.7363807", "10.1145/2669557.2669559", "10.1016/j.patcog.2011.01.004", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1186/1475-925x-14-s2-s6", "10.1007/s12650-018-0530-2", "10.1109/tkde.2018.2878247", "10.1016/j.visinf.2018.04.010", "10.1002/1099-128x(200005/06)14:3", "10.1371/journal.pone.0107878", "10.1109/pacificvis.2017.8031601", "10.1016/j.jbi.2019.103291", "10.1109/daac49578.2019.00008", "10.1109/allerton.2019.8919886", "10.1007/978-3-319-13105-4\\_14", "10.4258/hir.2016.22.3.156", "10.1109/tvcg.2015.2467553", "10.1145/2245276.2245469", "10.1007/bf02294485", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2016.2598470", "10.1137/07070111x", "10.1109/tvcg.2019.2934433", "10.1109/tvcg.2016.2640960", "10.1007/bf02310791", "10.1109/tvcg.2016.2534558", "10.1109/access.2016.2529723", "10.1109/tvcg.2016.2598664", "10.2312/eurovisshort.20161164", "10.1109/tvcg.2018.2865018", "10.1111/cgf.12804", "10.1109/tvcg.2018.2846735", "10.1016/j.comnet.2017.06.013", "10.1146/annurev-statistics-041715-033624", "10.1109/pacificvis.2018.00026", "10.1109/tvcg.2015.2467851"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2017.8031572", "year": "2017", "title": "HoNVis: Visualizing and exploring higher-order networks", "conferenceName": "PacificVis", "authors": "Jun Tao;Jian Xu;Chaoli Wang;Nitesh V. Chawla", "citationCount": "16", "affiliation": "Tao, J (Corresponding Author), Univ Notre Dame, Notre Dame, IN 46556 USA.\nTao, Jun; Xu, Jian; Wang, Chaoli; Chawla, Nitesh V., Univ Notre Dame, Notre Dame, IN 46556 USA.", "countries": "USA", "abstract": "Unlike the conventional first-order network (FoN), the higher-order network (HoN) provides a more accurate description of transitions by creating additional nodes to encode higher-order dependencies. However, there exists no visualization and exploration tool for the HoN. For applications such as the development of strategies to control species invasion through global shipping which is known to exhibit higher-order dependencies, the existing FoN visualization tools are limited. In this paper, we present HoNVis, a novel visual analytics framework for exploring higher-order dependencies of the global ocean shipping network. Our framework leverages coordinated multiple views to reveal the network structure at three levels of detail (i.e., the global, local, and individual port levels). Users can quickly identify ports of interest at the global level and specify a port to investigate its higher-order nodes at the individual port level. Investigating a larger-scale impact is enabled through the exploration of HoN at the local level. Using the global ocean shipping network data, we demonstrate the effectiveness of our approach with a real-world use case conducted by domain experts specializing in species invasion. Finally, we discuss the generalizability of this framework to other real-world applications such as information diffusion in social networks and epidemic spreading through air transportation.", "keywords": "", "link": "https://doi.org/10.1109/PACIFICVIS.2017.8031572", "refList": ["10.1016/j.ecolecon.2004.10.002", "10.2312/eurovisstar.20141170", "10.1109/tvcg.2015.2468111", "10.1111/j.1472-4642.2010.00696.x", "10.1111/j.1467-8659.2011.01898.x", "10.1126/sciadv.1600028", "10.1109/tvcg.2009.181", "10.1109/tvcg.2009.143", "10.1214/aoms/1177729694", "10.1098/rsif.2009.0495", "10.1890/070064", "10.1109/iv.2009.108", "10.1038/ncomms5630", "10.1641/b580507", "10.1111/cgf.12883", "10.1088/1742-5468/2008/10/p10008", "10.1371/journal.pone.0098679", "10.1641/b570707", "10.1109/pacificvis.2016.7465254", "10.1111/j.1467-8659.2009.01450.x", "10.1098/rspb.2003.2629"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2017.8031583", "year": "2017", "title": "MobiSeg: Interactive region segmentation using heterogeneous mobility data", "conferenceName": "PacificVis", "authors": "Wenchao Wu;Yixian Zheng;Nan Cao;Haipeng Zeng;Bing Ni;Huamin Qu;Lionel M. Ni", "citationCount": "13", "affiliation": "Wu, WC (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Hong Kong, Peoples R China.\nWu, Wenchao; Zheng, Yixian; Zeng, Haipeng; Ni, Bing; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Hong Kong, Peoples R China.\nCao, Nan, Tongji Univ, Shanghai, Peoples R China.\nNi, Lionel M., Univ Macau, Taipa, Macao, Peoples R China.", "countries": "China", "abstract": "With the acceleration of urbanization and modern civilization, more and more complex regions are formed in urban area. Although understanding these regions could provide huge insights to facilitate valuable applications for urban planning and business intelligence, few methods have been developed to effectively capture the rapid transformation of urban regions. In recent years, the widely applied location-acquisition technologies offer a more effective way to capture the dynamics of a city through analyzing people's movement activities based on mobility data. However, several challenges exist, including data sparsity and difficulties in result understanding and validation. To tackle these challenges, in this paper, we propose MobiSeg, an interactive visual analytics system, which supports the exploration of people's movement activities to segment the urban area into regions sharing similar activity patterns. A joint analysis is conducted on three types of heterogeneous mobility data (i.e., taxi trajectories, metro passenger RFID card data, and telco data), which can complement each other and provide a full picture of people's activities in a region. In addition, advanced analytical algorithms (e.g., non-negative matrix factorization (NMF) based method to capture latent activity patterns, as well as metric learning to calibrate and supervise the underlying analysis) and novel visualization designs are integrated into our system to provide a comprehensive solution to region segmentation in urban areas. We demonstrate the effectiveness of our system via case studies with real-world datasets and qualitative interviews with domain experts.", "keywords": "", "link": "https://doi.org/10.1109/PACIFICVIS.2017.8031583", "refList": ["10.1109/tvcg.2013.226", "10.1109/tvcg.2010.44", "10.1561/2200000019", "10.1109/tvcg.2015.2468111", "10.1109/tits.2012.2209201", "10.2307/621647", "10.1109/tkde.2014.2345405", "10.1109/tvcg.2015.2467194", "10.1109/pacificvis.2011.5742386", "10.1080/01431160512331316838", "10.1007/s10980-005-5238-8", "10.1145/2629592", "10.1109/tvcg.2015.2467619", "10.1109/tvcg.2015.2467592", "10.1109/tvcg.2013.228", "10.1111/cgf.12654", "10.1109/tvcg.2014.2346271", "10.1109/percomw.2011.5766912", "10.1111/cgf.12114", "10.1007/978-3-662-44848-9\\_32", "10.1109/infvis.2005.1532144", "10.1109/vast.2015.7347630", "10.1017/s0026749x10000314", "10.1137/s0036144599352836", "10.1145/860435.860485", "10.1109/tvcg.2013.168", "10.1109/tbdata.2016.2586447", "10.1109/vast.2014.7042490", "10.1109/tvcg.2012.265", "10.1109/vast.2011.6102455", "10.1162/jmlr.2003.3.4-5.993", "10.1016/j.landurbplan.2009.05.001", "10.1109/tvcg.2015.2440259", "10.1111/cgf.12107"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030370", "title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups", "year": "2020", "conferenceName": "VAST", "authors": "Yating Lin;Kamkwai Wong;Yong Wang;Rong Zhang;Bo Dong;Huamin Qu;Qinghua Zheng", "citationCount": "0", "affiliation": "Lin, YT (Corresponding Author), Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian, Shaanxi, Peoples R China. Lin, Yating; Zheng, Qinghua, Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian, Shaanxi, Peoples R China. Wong, Kamkwai; Wang, Yong; Zhang, Rong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Dong, Bo, Xi An Jiao Tong Univ, Natl Engn Lab Big Data Analyt, Xian, Shaanxi, Peoples R China.", "countries": "China", "abstract": "Tax evasion is a serious economic problem for many countries, as it can undermine the government's tax system and lead to an unfair business competition environment. Recent research has applied data analytics techniques to analyze and detect tax evasion behaviors of individual taxpayers. However, they have failed to support the analysis and exploration of the related party transaction tax evasion (RPTTE) behaviors (e.g., transfer pricing), where a group of taxpayers is involved. In this paper, we present TaxThemis, an interactive visual analytics system to help tax officers mine and explore suspicious tax evasion groups through analyzing heterogeneous tax-related data. A taxpayer network is constructed and fused with the respective trade network to detect suspicious RPTTE groups. Rich visualizations are designed to facilitate the exploration and investigation of suspicious transactions between related taxpayers with profit and topological data analysis. Specifically, we propose a calendar heatmap with a carefully-designed encoding scheme to intuitively show the evidence of transferring revenue through related party transactions. We demonstrate the usefulness and effectiveness of TaxThemis through two case studies on real-world tax-related data and interviews with domain experts.", "keywords": "Visual Analytics,Tax Network,Tax Evasion Detection,Anomaly detection,Multidimensional data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030370", "refList": ["10.1111/cgf.12886", "10.2307/2277827", "10.1109/tvcg.2010.44", "10.1109/tits.2014.2315794", "10.1109/tvcg.2019.2934670", "10.1038/s41467-019-08987-4", "10.1111/cgf.12920", "10.1109/vast.2017.8585721", "10.1080/15230406.2015.1093431", "10.1109/tvcg.2018.2843369", "10.1038/srep01001", "10.1109/tvcg.2017.2744018", "10.1109/tvcg.2017.2744159", "10.1068/b130199p", "10.1109/tvcg.2009.143", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2019.2892483", "10.1109/pacificvis.2017.8031583", "10.1109/pacificvis48177.2020.2785", "10.2307/2686111", "10.1109/tvcg.2015.2467199", "10.1111/cgf.12114", "10.1109/tvcg.2018.2865126", "10.1111/j.1538-4632.1996.tb00936.x", "10.1109/tvcg.2019.2934619", "10.2307/2332142", "10.1007/978-3-319-10590-1\\_53", "10.1109/cvpr.2016.485", "10.1109/cvpr.2017.17", "10.2307/2986645", "10.1109/tvcg.2014.2346321", "10.1017/s0140525x16001837", "10.1109/tvcg.2016.2598541", "10.1371/journal.pone.0207377", "10.1109/tvcg.2014.2346265", "10.1007/s4095-020-0191-7", "10.3141/1644-14", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2016.90", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2785807", "10.1109/tvcg.2017.2744358", "10.1111/j.1538-4632.1995.tb00338.x", "10.1080/03081068808717359", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2019.00027", "year": "2019", "title": "Visual Analytics of Taxi Trajectory Data via Topical Sub-trajectories", "conferenceName": "PacificVis", "authors": "Sichen Jin;Yubo Tao;Yuyu;Yuyu Yan;Jin Xu", "citationCount": "1", "affiliation": "Tao, YB (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Zhejiang, Peoples R China.\nJin, Sichen; Tao, Yubo; Yan, Yuyu; Xu, Jin; Lin, Hai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Zhejiang, Peoples R China.", "countries": "China", "abstract": "GPS-based taxi trajectories contain valuable knowledge about movement behaviors for transportation and urban planning. Topic modeling is an effective tool to extract semantic information from taxi trajectories. However, previous methods generally ignore the direction of trajectories. In this paper, we employ the bigram topic model instead of traditional topic models to analyze textualized trajectories to take into account the direction information of trajectories. We further propose a modified Apriori algorithm to extract frequent sub-trajectories and use them to represent each topic as topical sub-trajectories. Finally, we design a visual analytics system with several linked views to facilitate users to interactively explore topics, sub-trajectories, and trips. We demonstrate the effectiveness of our system via case studies with Chengdu taxi trajectory data.", "keywords": "Human-centered computing; Visualization; Visualization application domains; Visual analytics", "link": "https://doi.org/10.1109/PacificVis.2019.00027", "refList": ["10.1145/2629592", "10.1007/s12650-018-0481-7", "10.1109/tvcg.2010.44", "10.1145/1143844.1143967", "10.3109/15569527.2010.547541", "10.1109/pacificvis.2017.8031583", "10.18653/v1/p17-2084", "10.1109/tits.2015.2436897", "10.1177/1473871612457601", "10.1109/69.250074", "10.1109/pacificvis.2014.50", "10.1109/tvcg.2016.2598416", "10.1109/mdm.2013.23", "10.1109/vast.2011.6102455"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/pacificvis.2016.7465266", "year": "2016", "title": "TravelDiff: Visual comparison analytics for massive movement patterns derived from Twitter", "conferenceName": "PacificVis", "authors": "Robert Kr{\\\"{u}}ger;Guodao Sun;Fabian Beck;Ronghua Liang;Thomas Ertl", "citationCount": "12", "affiliation": "Krueger, R (Corresponding Author), Univ Stuttgart, VIS, Stuttgart, Germany.\nKrueger, Robert; Ertl, Thomas, Univ Stuttgart, VIS, Stuttgart, Germany.\nSun, Guodao; Liang, Ronghua, Zhejiang Univ Technol, Hangzhou, Zhejiang, Peoples R China.\nBeck, Fabian, Univ Stuttgart, VISUS, Stuttgart, Germany.", "countries": "Germany;China", "abstract": "Geo-tagged microblog data covers billions of movement patterns on a global and local scale. Understanding these patterns could guide urban and traffic planning or help coping with disaster situations. We present a visual analytics system to investigate travel trajectories of people reconstructed from microblog messages. To analyze seasonal changes and events and to validate movement patterns against other data sources, we contribute highly interactive visual comparison methods that normalize and contrast trajectories as well as density maps within a single view. We also compute an adaptive hierarchical graph from the trajectories to abstract individual movements into higher-level structures. Specific challenges that we tackle are, among others, the spatio-temporal sparsity of the data, the volume of data varying by region, and a diverse mix of means of transportation. The applicability of our approach is presented in three case studies.", "keywords": "", "link": "https://doi.org/10.1109/PACIFICVIS.2016.7465266", "refList": ["10.1145/1835449.1835522", "10.1109/tvcg.2008.117", "10.1109/tvcg.2013.226", "10.1152/japplphysiol.00767.2005", "10.1177/1473871611416549", "10.1038/nature02541", "10.1145/2534732.2534741", "10.1145/2755492.2755497", "10.1109/tvcg.2015.2468111", "10.1371/journal.pone.0129202", "10.1109/tdsc.2012.75", "10.1038/nature06958", "10.1109/tvcg.2011.226", "10.1111/cgf.12129", "10.1109/vast.2011.6102456", "10.1177/1473871612457601", "10.1109/hicss.2014.189", "10.1111/j.1467-8659.2011.01898.x", "10.1007/978-3-642-18469-7\\_5", "10.1109/tvcg.2013.186", "10.1006/ijhc.2000.0418", "10.1145/1772690.1772777", "10.1109/tvcg.2013.122", "10.1109/tvcg.2015.2467619", "10.1109/tvcg.2013.228", "10.1109/vast.2014.7042509", "10.1109/iv.2009.108", "10.1177/1473871612455983", "10.1109/iceccs.2009.15", "10.1109/infvis.2004.46", "10.1109/tvcg.2015.2467196", "10.1109/tvcg.2012.265", "10.3138/carto.46.4.239", "10.1109/tvcg.2011.190", "10.1109/pacificvis.2011.5742384", "10.1068/b32047", "10.1007/978-3-319-15168-7\\_2"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030419", "title": "Comparative Layouts Revisited: Design Space, Guidelines, and Future Directions", "year": "2020", "conferenceName": "InfoVis", "authors": "Sehi L'Yi;Jaemin Jo;Jinwook Seo", "citationCount": "0", "affiliation": "L'Yi, S (Corresponding Author), Harvard Med Sch, Boston, MA 02115 USA. L'Yi, Sehi, Harvard Med Sch, Boston, MA 02115 USA. Jo, Jaemin, Sungkyunkwan Univ, Seoul, South Korea. Seo, Jinwook, Seoul Natl Univ, Seoul, South Korea.", "countries": "USA;Korea", "abstract": "We present a systematic review on three comparative layouts-juxtaposition, superposition, and explicit-encoding-which are information visualization (InfoVis) layouts designed to support comparison tasks. For the last decade, these layouts have served as fundamental idioms in designing many visualization systems. However, we found that the layouts have been used with inconsistent terms and confusion, and the lessons from previous studies are fragmented. The goal of our research is to distill the results from previous studies into a consistent and reusable framework. We review 127 research papers, including 15 papers with quantitative user studies, which employed comparative layouts. We first alleviate the ambiguous boundaries in the design space of comparative layouts by suggesting lucid terminology (e.g., chart-wise and item-wise juxtaposition). We then identify the diverse aspects of comparative layouts, such as the advantages and concerns of using each layout in the real-world scenarios and researchers' approaches to overcome the concerns. Building our knowledge on top of the initial insights gained from the Gleicher et al.'s survey [19], we elaborate on relevant empirical evidence that we distilled from our survey (e.g., the actual effectiveness of the layouts in different study settings) and identify novel facets that the original work did not cover (e.g., the familiarity of the layouts to people). Finally, we show the consistent and contradictory results on the performance of comparative layouts and offer practical implications for using the layouts by suggesting trade-offs and seven actionable guidelines.", "keywords": "Comparative layout,visual comparison,literature review,juxtaposition,superposition,explicit-encoding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030419", "refList": ["10.1109/tvcg.2013.233", "10.1111/cgf.12380", "10.1177/1473871611416549", "10.1145/2702123.2702419", "10.1109/tvcg.2014.2322363", "10.1111/cgf.12791", "10.1145/2702123.2702130", "10.1145/2702123.2702217", "10.1109/tvcg.2012.237", "10.1177/1473871613480062", "10.1109/mcg.2017.377152546", "10.1109/tvcg.2013.213", "10.1111/cgf.12369", "10.1109/tvcg.2017.2744198", "10.1145/3139295.3139309", "10.1109/tvcg.2019.2934801", "10.1109/tvcg.2013.122", "10.1109/tvcg.2017.2747545", "10.1109/tvcg.2015.2413774", "10.1037/0096-1523.24.3.719", "10.1109/tvcg.2014.2346320", "10.1109/tvcg.2007.70535", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1145/2556288.2557141", "10.1109/tvcg.2013.149", "10.1145/1165734.1165736", "10.5220/0006127502170224", "10.1109/tvcg.2017.2745298", "10.1177/1473871617692841", "10.1190/int-2017-0083.1", "10.1190/int-2014-0283.1", "10.1109/tvcg.2016.2598796", "10.1111/cgf.13401", "10.1016/j.cag.2017.05.005", "10.1177/1473871616667632", "10.1145/3025453.3025882", "10.1145/2470654.2470724", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2010.164", "10.1145/3103010.3103013", "10.1109/pacificvis.2016.7465266", "10.1109/pacificvis.2012.6183556", "10.1109/tvcg.2015.2467751", "10.1109/tvcg.2018.2796557", "10.1111/cgf.13531", "10.1109/tvcg.2013.161", "10.1109/iv.2018.00051", "10.1109/tvcg.2010.162", "10.1109/tvcg.2018.2864510", "10.1109/iv.2017.30", "10.1109/tvcg.2007.70623"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2019.00021", "year": "2019", "title": "Visual Analytics of Dynamic Interplay Between Behaviors in MMORPGs", "conferenceName": "PacificVis", "authors": "Junhua Lu;Xiao Xie;Ji Lan;Tai{-}Quan Peng;Wei Chen;Yingcai Wu", "citationCount": "0", "affiliation": "Chen, W; Wu, YC (Corresponding Author), Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.\nLu, Junhua; Xie, Xiao; Lan, Ji; Chen, Wei; Wu, Yingcai, Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.\nPeng, Tai-Quan, Michigan State Univ, Dept Commun, E Lansing, MI 48824 USA.", "countries": "USA;China", "abstract": "The rapid development of massively multiplayer online role-playing games (MMORPGs) has led operators to record huge amounts of fine-grained data from the in-game activities of players. These data provide considerable opportunities with which to study the dynamic interplay among player behaviors and investigate the roles of various social structures that underlie such interplay. However, modeling and visualizing these behavioral data remain a challenge. In this study, we propose a novel influence-susceptible model to measure the dynamic interplay among multiple behaviors. Based on this model, we introduce a new visual analytics system called BeXplorer. BeXplorer enables analysts to interactively explore the dynamic interplay between player purchase and communication behaviors and to examine the manner in which this interplay is bound by social structures where players are embedded.", "keywords": "Human-centered computing; Visualization; Visualization application domains; Visual analytics", "link": "https://doi.org/10.1109/PacificVis.2019.00021", "refList": ["10.1109/tvcg.2015.2509990", "10.1108/s0733-558x(2014)0000040008", "10.1016/j.chb.2016.12.043", "10.1109/tciaig.2010.2072506", "10.1093/restud/rdq014", "10.1016/0304-4076(77)90008-2", "10.1109/tvcg.2013.221", "10.1007/bf02476440", "10.1109/vast.2017.8585594", "10.1086/599247", "10.1002/wics.101", "10.1109/tvcg.2015.2467621", "10.1109/tvcg.2018.2816203", "10.2307/2118364", "10.2307/2281644", "10.1177/0049124193022001006", "10.1111/cgf.12888", "10.1109/pacificvis.2017.8031576", "10.1109/tvcg.2010.225", "10.1109/tvcg.2012.225", "10.1109/pacificvis.2015.7156367", "10.1177/1461444809105346", "10.1103/physrevlett.89.208701", "10.1109/tvcg.2016.2534558", "10.1109/tvcg.2014.2346919", "10.1109/tvcg.2016.2598415", "10.1109/pacificvis.2016.7465266", "10.1109/pacificvis.2015.7156376", "10.1126/science.1215842", "10.1177/0049124111404820", "10.2307/2289447", "10.1073/pnas.1004008107", "10.1109/infvis.2000.885098", "10.1109/pacificvis.2018.00026", "10.1146/annurev-soc-071913-043145"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13713", "year": "2019", "title": "Bird's-Eye - Large-Scale Visual Analytics of City Dynamics using Social Location Data", "conferenceName": "EuroVis", "authors": "Robert Kr{\\\"{u}}ger;Qi Han;Nikolay Ivanov;Sanae Mahtal;Dennis Thom;Hanspeter Pfister;Thomas Ertl", "citationCount": "2", "affiliation": "Krueger, R (Corresponding Author), Harvard Univ, Visual Comp Grp, Cambridge, MA 02138 USA.\nKrueger, Robert; Pfister, Hanspeter, Harvard Univ, Visual Comp Grp, Cambridge, MA 02138 USA.\nHang, Qi; Ivanov, Nikolay; Mahtal, Sanae; Thom, Dennis; Ertl, Thomas, Univ Stuttgart, Inst Visualizat \\& Interact Syst, Stuttgart, Germany.", "countries": "Germany;USA", "abstract": "The analysis of behavioral city dynamics, such as temporal patterns of visited places and citizens' mobility routines, is an essential task for urban and transportation planning. Social media applications such as Foursquare and Twitter provide access to large-scale and up-to-date dynamic movement data that not only help to understand the social life and pulse of a city but also to maintain and improve urban infrastructure. However, the fast growth rate of this data poses challenges for conventional methods to provide up-to-date, flexible analysis. Therefore, planning authorities barely consider it. We present a system and design study to leverage social media data that assist urban and transportation planners to achieve better monitoring and analysis of city dynamics such as visited places and mobility patterns in large metropolitan areas. We conducted a goal-and-task analysis with urban planning experts. To address these goals, we designed a system with a scalable data monitoring back-end and an interactive visual analytics interface. The monitoring component uses intelligent pre-aggregation to allow dynamic queries in near real-time. The visual analytics interface leverages unsupervised learning to reveal clusters, routines, and unusual behavior in massive data, allowing to understand patterns in time and space. We evaluated our approach based on a qualitative user study with urban planning experts which demonstrates that intuitive integration of advanced analytical tools with visual interfaces is pivotal in making behavioral city dynamics accessible to practitioners. Our interviews also revealed areas for future research.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13713", "refList": ["10.1016/j.jvlc.2014.10.028", "10.1109/tvcg.2015.2468111", "10.1111/cgf.12129", "10.1109/tvcg.2011.127", "10.1145/2516604.2516616", "10.1126/science.1200970", "10.1145/2629592", "10.4000/netcom.2725", "10.1109/tvcg.2016.2598585", "10.1145/2365952.2366028", "10.1109/tvcg.2015.2467619", "10.1109/tvcg.2009.100", "10.1109/iv.2010.94", "10.1145/2424321.2424395", "10.1109/pacificvis.2012.6183572", "10.1109/tits.2017.2727281", "10.1177/1473871617692841", "10.1109/tvcg.2013.179", "10.1080/09669582.2010.502576", "10.1145/2378023.2378027", "10.1109/ictai.2016.0063", "10.1109/tvcg.2017.2758362", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2012.213", "10.1057/udi.2010.20", "10.1109/tvcg.2014.2371856", "10.1109/ictai.2016.60", "10.1109/tits.2016.2639320", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2008.152", "10.1109/tvcg.2015.2511733"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13170", "year": "2017", "title": "Visual Comparison of Eye Movement Patterns", "conferenceName": "EuroVis", "authors": "Tanja Blascheck;Markus Schweizer;Fabian Beck;Thomas Ertl", "citationCount": "3", "affiliation": "Blascheck, T (Corresponding Author), Univ Stuttgart, Inst Visualizat \\& Interact Syst, Stuttgart, Germany.\nBlascheck, Tanja; Schweizer, Markus; Ertl, Thomas, Univ Stuttgart, Inst Visualizat \\& Interact Syst, Stuttgart, Germany.\nBeck, Fabian, Univ Duisburg Essen, Inst Comp Sci \\& Business Informat Syst, Essen, Germany.", "countries": "Germany", "abstract": "In eye tracking research, finding eye movement patterns and similar strategies between participants' eye movements is important to understand task solving strategies and obstacles. In this application paper, we present a graph comparison method using radial graphs that show Areas of Interest (AOIs) and their transitions. An analyst investigates a single graph based on dwell times, directed transitions, and temporal AOI sequences. Two graphs can be compared directly and temporal changes may be analyzed. A list and matrix approach facilitate the analyst to contrast more than two graphs guided by visually encoded graph similarities. We evaluated our approach in case studies with three eye tracking and visualization experts. They identified temporal transition patterns of eye movements across participants, groups of participants, and outliers.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13170", "refList": ["10.1145/2857491.2857524", "10.1109/tvcg.2008.117", "10.1177/1473871611416549", "10.1109/tvcg.2015.2468111", "10.1111/cgf.12791", "10.1109/iv.2011.49", "10.1109/tvcg.2012.276", "10.1109/infvis.2001.963281", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2013.263", "10.1109/tvcg.2009.181", "10.1080/13875868.2016.1226839", "10.1109/tvcg.2015.2467871", "10.1109/vissoft.2013.6650549", "10.1145/2578153.2578175", "10.1109/iv.2009.108", "10.1177/1473871612455983", "10.1145/1743666.1743721", "10.1109/vast.2016.7883520", "10.1007/978-3-319-47024-5\\_7", "10.1016/b978-044451020-4/50035-9", "10.1109/iv.2016.28", "10.1109/tvcg.2014.2346677", "10.1145/2470654.2470724", "10.1016/s0169-8141(98)00068-7", "10.1109/tvcg.2007.70521", "10.1111/cgf.12115", "10.1145/2509315.2509326", "10.1109/pacificvis.2016.7465266", "10.1145/2669557.2669558", "10.1109/tvcg.2010.149", "10.1109/tvcg.2009.150", "10.1179/000870403235002042"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00037", "year": "2019", "title": "A Visual Approach for the Comparative Analysis of Character Networks in Narrative Texts", "conferenceName": "PacificVis", "authors": "Markus John;Martin Baumann", "citationCount": "1", "affiliation": "John, M (Corresponding Author), Univ Stuttgart, Inst Visualizat \\& Interact Syst VIS, Stuttgart, Germany.\nJohn, Markus; Baumann, Martin; Schuetz, David; Koch, Steffen; Ertl, Thomas, Univ Stuttgart, Inst Visualizat \\& Interact Syst VIS, Stuttgart, Germany.", "countries": "Germany", "abstract": "The analysis of a novel's plot and characters are challenging and time-consuming tasks in literary criticism. Typically, humanities scholars want to describe and compare characters' personality traits, their roles, their relationships, and the evolution of these aspects over the course of a novel. Nowadays, due to the digitization of literature, humanities scholars can be supported in these endeavors with computational methods. In this paper, we present an approach that offers several means to analyze the plot and characters of a novel visually. Analysts can easily switch between an adjacency matrix and a node-link representation, which provide an overview of the characters and the relationships between them. Both views enable analysts to select different text ranges of the novel for studying the commonalities and differences of the character constellations within these ranges. We offer interactive visual representations to help investigate the relationships between the characters in more detail. Additionally, we link the visual representations with the novels' texts to support the inspection and verification of previously generated ideas and hypotheses. To demonstrate the benefits and limitations of our approach, we present two usage scenarios. The first one is based on a fictitious analysis and the second one discusses applications that were carried out during joint workshops with humanities scholars. Finally, we present and discuss the insights gained by an expert study and the design decisions of our approach.", "keywords": "Visual text analysis; document analysis; close reading; distant reading; digital humanities; graph comparison", "link": "https://doi.org/10.1109/PacificVis.2019.00037", "refList": ["10.1177/1473871611416549", "10.1145/2702123.2702476", "10.1111/cgf.13181", "10.1111/cgf.12791", "10.1111/cgf.13170", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2011.185", "10.1109/vast.2009.5333248", "10.1109/tvcg.2015.2467971", "10.1080/10447318.2010.516722", "10.1136/qshc.2004.010033", "10.1109/tvcg.2011.169", "10.3115/v1/p14-5010", "10.1109/tmm.2016.2614184", "10.1109/tvcg.2009.106", "10.1111/cgf.12124", "10.1109/iv.2016.28", "10.1145/2470654.2470724", "10.1057/palgrave.ivs.9500180", "10.1109/pacificvis.2011.5742388", "10.2190/ec.44.1.a", "10.13140/2.1.1341.1520", "10.1109/tvcg.2011.232", "10.1109/vast.2007.4389004", "10.1057/palgrave.ivs.9500092", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.13211", "year": "2017", "title": "Social Media Visual Analytics", "conferenceName": "EuroVis", "authors": "Siming Chen;Lijing Lin;Xiaoru Yuan", "citationCount": "29", "affiliation": "Chen, SM (Corresponding Author), Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.\nChen, SM (Corresponding Author), Peking Univ, Sch EECS, Beijing, Peoples R China.\nChen, Siming; Lin, Lijing; Yuan, Xiaoru, Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.\nChen, Siming; Lin, Lijing; Yuan, Xiaoru, Peking Univ, Sch EECS, Beijing, Peoples R China.", "countries": "China", "abstract": "With the development of social media (e.g. Twitter, Flickr, Foursquare, Sina Weibo, etc.), a large number of people are now using them and post microblogs, messages and multi-media information. The everyday usage of social media results in big open social media data. The data offer fruitful information and reflect social behaviors of people. There is much visualization and visual analytics research on such data. We collect state-of-the-art research and put it into three main categories: social network, spatial temporal information and text analysis. We further summarize the visual analytics pipeline for the social media, combining the above categories and supporting complex tasks. With these techniques, social media analytics can apply to multiple disciplines. We summarize the applications and public tools to further investigate the challenges and trends.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13211", "refList": ["10.1016/j.cag.2013.11.003", "10.1109/vast.2010.5652922", "10.1007/s12650-015-0277-y", "10.1007/978-3-540-70956-5", "10.1007/s00146-014-0549-4", "10.1109/tmm.2014.2340133", "10.1109/tmm.2015.2510329", "10.1057/palgrave.ivs.9500116", "10.1109/tvcg.2010.129", "10.1109/tvcg.2016.2598590", "10.1145/2065023.2065041", "10.1109/tvcg.2013.162", "10.1145/1963405.1963504", "10.1177/1473871613490678", "10.1007/978-0-85729-436-4\\_9", "10.1145/2488388.2488504", "10.1109/bigdata.2015.7363826", "10.1109/tvcg.2009.171", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/tvcg.2014.2371856", "10.1145/2089094.2089102", "10.1109/tvcg.2014.2346919", "10.1109/tvcg.2014.2346433", "10.1109/tvcg.2015.2509990", "10.1109/pacificvis.2015.7156366", "10.1016/j.jocs.2010.12.007", "10.1109/tvcg.2011.169", "10.1109/tvcg.2015.2467619", "10.1016/j.cag.2013.10.008", "10.1007/978-3-319-06793-3\\_3", "10.1109/tmm.2009.2012912", "10.1109/pacificvis.2015.7156367", "10.1109/tvcg.2014.2346922", "10.1371/journal.pone.0101837", "10.1109/tmm.2015.2425143", "10.1145/2493102.2493108", "10.1109/tvcg.2015.2467196", "10.1109/tvcg.2013.196", "10.1109/mcg.2014.61", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2006.160", "10.1057/palgrave.ivs.9500092", "10.1016/j.giq.2012.06.002", "10.1016/b978-0-12-382229-1.00002-3", "10.1109/vast.2016.7883510", "10.1111/j.1467-8659.2012.03120.x", "10.1109/tmm.2016.2614220", "10.7155/jgaa.00302", "10.1109/tvcg.2014.2346920", "10.1109/vast.2015.7347631", "10.1371/journal.pone.0095043", "10.1109/vast.2014.7042495", "10.1511/2001.4.344", "10.1109/vast.2012.6400557", "10.1109/pacificvis.2012.6183572", "10.1145/2567948.2577020", "10.1016/j.joi.2014.07.006", "10.1111/j.1467-8659.2009.01687.x", "10.1145/2801040.2801054", "10.1016/j.bushor.2009.09.003", "10.1109/infvis.2000.885098", "10.1109/ldav.2013.6675163", "10.1007/s13218-012-0177-4", "10.1080/13658816.2013.825724", "10.1109/tvcg.2007.70582", "10.1109/mc.2013.152", "10.1109/vast.2014.7042496", "10.1109/vast.2011.6102456", "10.1145/2733373.2806236", "10.1109/tvcg.2013.221", "10.1177/1473871615576925", "10.1109/vast.2016.7883513", "10.1109/tvcg.2013.186", "10.1109/mc.2012.430", "10.1109/tvcg.2015.2467554", "10.1109/pacificvis.2014.48", "10.1145/2212776.2212796", "10.1109/tvcg.2006.107", "10.1109/pacificvis.2014.38", "10.1109/pacificvis.2015.7156376", "10.1109/vast.2014.7042535", "10.1109/tvcg.2014.2359887"], "wos": 1, "children": [{"doi": "10.1109/vast.2017.8585638", "title": "E-Map: A Visual Analytics Approach for Exploring Significant Event Evolutions in Social Media", "year": "2017", "conferenceName": "VAST", "authors": "Siming Chen;Shuai Chen;Lijing Lin;Xiaoru Yuan;Jie Liang 0004;Xiaolong Zhang", "citationCount": "10", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing, Peoples R China. Yuan, XR (Corresponding Author), Peking Univ, Sch EECS, Beijing, Peoples R China. Chen, Siming; Chen, Shuai; Lin, Lijing; Yuan, Xiaoru, Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing, Peoples R China. Chen, Siming; Chen, Shuai; Lin, Lijing; Yuan, Xiaoru, Peking Univ, Sch EECS, Beijing, Peoples R China. Liang, Jie, Univ Technol, Fac Engn \\& Informat Technol, Sydney, NSW, Australia. Zhang, Xiaolong, Penn State Univ, Coll Informat Sci \\& Technol, University Pk, PA 16802 USA.", "countries": "USA;China;Australia", "abstract": "Significant events are often discussed and spread through social media, involving many people. Reposting activities and opinions expressed in social media offer good opportunities to understand the evolution of events. However, the dynamics of reposting activities and the diversity of user comments pose challenges to understand event-related social media data. We propose E-Map, a visual analytics approach that uses map-like visualization tools to help multi-faceted analysis of social media data on a significant event and in-depth understanding of the development of the event. E-Map transforms extracted keywords, messages, and reposting behaviors into map features such as cities, towns, and rivers to build a structured and semantic space for users to explore. It also visualizes complex posting and reposting behaviors as simple trajectories and connections that can be easily followed. By supporting multi-level spatial temporal exploration, E-Map helps to reveal the patterns of event development and key players in an event, disclosing the ways they shape and affect the development of the event. Two cases analysing real-world events confirm the capacities of E-Map in facilitating the analysis of event evolution with social media data.", "keywords": "Social Media,Event Analysis,Map-like Visual Metaphor,Spatial Temporal Visual Analytics", "link": "http://dx.doi.org/10.1109/VAST.2017.8585638", "refList": ["10.1109/tvcg.2007.70582", "10.1109/tvcg.2016.2598919", "10.1109/pacificvis.2010.5429590", "10.1057/ivs.2008.23", "10.1109/vast.2014.7042496", "10.1016/j.cag.2013.11.003", "10.1109/vast.2011.6102456", "10.1109/tvcg.2013.221", "10.1109/tvcg.2011.185", "10.1109/vast.2016.7883510", "10.1111/j.1467-8659.2012.03120.x", "10.1109/tvcg.2013.186", "10.1109/tmm.2016.2614220", "10.7155/jgaa.00302", "10.1109/mc.2012.430", "10.1109/tvcg.2015.2467619", "10.1109/tvcg.2010.129", "10.1016/s0341-8162(01)00164-3", "10.1109/tvcg.2016.2598590", "10.1145/2065023.2065041", "10.1111/cgf.13211", "10.1109/tvcg.2013.162", "10.1109/tvcg.2011.288", "10.1145/1963405.1963504", "10.5670/oceanog.2016.66", "10.1109/tvcg.2015.2467554", "10.1109/tvcg.2015.2467691", "10.1016/j.cag.2013.10.008", "10.1109/mcg.2015.73", "10.1109/vast.2012.6400557", "10.1109/tvcg.2016.2539960", "10.1109/tit.1982.1056489", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2014.2346922", "10.1109/tmm.2014.2384912", "10.1109/vast.2016.7883511", "10.1002/spe.4380211102", "10.1145/2488388.2488504", "10.2312/eurovisstar.20141176", "10.1109/bigdata.2013.6691714", "10.1109/tvcg.2009.171", "10.1109/tvcg.2013.196", "10.1109/vast.2008.4677356", "10.1145/2207676.2208672", "10.1111/j.1467-8659.2011.01955.x", "10.1109/pacificvis.2014.38", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2015.7156376", "10.1145/1348549.1348556", "10.1109/vast.2015.7347632", "10.1109/tvcg.2014.2346919", "10.1109/tvcg.2014.2346433"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934263", "title": "R-Map: A Map Metaphor for Visualizing Information Reposting Process in Social Media", "year": "2019", "conferenceName": "VAST", "authors": "Shuai Chen;Sihang Li;Siming Chen;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Chen, Shuai; Li, Sihang, Peking Univ, Sch EECS, Minist Educ, Key Lab Machine Petrept, Beijing, Peoples R China. Yuan, Xiaoru, Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Chen, Siming, Fraunhofer Inst IAIS, St Augustin, Germany. Chen, Siming, Univ Bonn, Bonn, Germany.", "countries": "Germany;China", "abstract": "We propose R-Map (Reposting Map), a visual analytical approach with a map metaphor to support interactive exploration and analysis of the information reposting process in social media. A single original social media post can cause large cascades of repostings (i.e., retweets) on online networks, involving thousands, even millions of people with different opinions. Such reposting behaviors form the reposting tree, in which a node represents a message and a link represents the reposting relation. In R-Map, the reposting tree structure can be spatialized with highlighted key players and tiled nodes. The important reposting behaviors, the following relations and the semantics relations are represented as rivers, routes and bridges, respectively, in a virtual geographical space. R-Map supports a scalable overview of a large number of information repostings with semantics. Additional interactions on the map are provided to support the investigation of temporal patterns and user behaviors in the information diffusion process. We evaluate the usability and effectiveness of our system with two use cases and a formal user study.", "keywords": "Social Media,Information Diffusion,Map-like Visual Metaphor", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934263", "refList": ["10.1109/infvis.2000.885091", "10.1109/pacificvis.2010.5429590", "10.1111/j.0020-2754.1998.00269.x", "10.1109/vast.2017.8585638", "10.1145/2700398", "10.1109/tmm.2016.2614229", "10.1109/visual.1991.175815", "10.1145/3183347", "10.1109/infvis.2001.963290", "10.1109/vast.2016.7883510", "10.1109/access.2016.2605009", "10.1109/mcg.2011.103", "10.1145/1124772.1124851", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2010.79", "10.1109/infvis.2000.885095", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1002/(sici)1097-0266(199606)17:6", "10.5670/oceanog.2016.66", "10.1559/152304003100011081", "10.1109/tit.1982.1056489", "10.1559/152304098782383034", "10.1109/tvcg.2014.2346922", "10.1007/978-3-540-85567-5\\_9", "10.1145/2488388.2488504", "10.1109/38.974518", "10.1109/bigdata.2013.6691714", "10.1109/pacificvis.2014.38", "10.1109/tvcg.2012.291", "10.1109/infvis.2005.1532128", "10.2307/2685881", "10.1007/1-4020-4179-9\\_91", "10.1109/infvis.1999.801860", "10.1109/asonam.2011.37"], "wos": 1, "children": [{"doi": "10.1111/cgf.14031", "year": "2020", "title": "The State of the Art in Map-Like Visualization", "conferenceName": "EuroVis", "authors": "Marius Hogr{\\\"{a}}fer;Magnus Heitzler;Hans{-}J{\\\"{o}}rg Schulz", "citationCount": "0", "affiliation": "Hografer, M (Corresponding Author), Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHografer, Marius; Schulz, Hans-Jorg, Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHeitzler, Magnus, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Denmark", "abstract": "Cartographic maps have been shown to provide cognitive benefits when interpreting data in relation to a geographic location. In visualization, the term map-like describes techniques that incorporate characteristics of cartographic maps in their representation of abstract data. However, the field of map-like visualization is vast and currently lacks a clear classification of the existing techniques. Moreover, choosing the right technique to support a particular visualization task is further complicated, as techniques are scattered across different domains, with each considering different characteristics as map-like. In this paper, we give an overview of the literature on map-like visualization and provide a hierarchical classification of existing techniques along two general perspectives: imitation and schematization of cartographic maps. Each perspective is further divided into four principal categories that group common map-like techniques along the visual primitives they affect. We further discuss this classification from a task-centered view and highlight open research questions.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14031", "refList": ["10.1111/j.0020-2754.1998.00269.x.21", "10.1109/iv.2005.26", "10.2307/2980460", "10.1109/tvcg.2017.2743959", "10.1016/j.jvlc.2011.11.004", "10.1145/2501988.2502046", "10.1559/1523040042742402", "10.1007/s00779-011-0500-3", "10.1111/cgf.12932", "10.1109/tvcg.2010.89", "10.1111/cgf.13200", "10.1080/15230406.2016.1160797", "10.1179/000870410x12825500202896", "10.1109/tvcg.2007.70596", "10.5167/80972.19uzh-80972", "10.1007/978-3-319-11593-1\\_2", "10.1037/aca0000175", "10.1080/17538947.2014.923942", "10.1109/tvcg.2015.2467321", "10.1109/vissof.2005.1684299", "10.1559/152304098782383034", "10.1007/978-3-642-36763-2\\_38", "10.1109/tvcg.2013.130", "10.1109/tvcg.2014.2346274", "10.1111/cgf.12648", "10.1111/0004-5608.00242", "10.1177/1473871617724212", "10.1109/iv.2004.1320123", "10.1371/journal.pcbi.1006907", "10.1109/tvcg.2016.2599030", "10.1559/1523040042742411", "10.3390/informatics5030031", "10.1111/cgf.13079", "10.1109/tvcg.2015.2467811", "10.1016/j.tics.2003.12.004", "10.1007/s00799-016-0168-4", "10.1109/infvis.2004.57", "10.1007/s10021-007-9038-7", "10.1109/pacificvis.2015.7156366", "10.1007/978-3-319-27261-0\\_1", "10.1007/978-1-4471-2804-5\\_6", "10.1109/access.2019.2939977", "10.1109/tvcg.2017.2747545", "10.1109/icdm.2003.1250978", "10.1111/cgf.13167", "10.1016/0010-0285(78)90006-3", "10.3138/nj8v-8514-871t-221k", "10.1016/j.cag.2009.06.002", "10.1111/j.1467-8659.2011.01937.x", "10.1016/b978-044451020-4/50035-9", "10.1111/1467-8659.00566", "10.5220/0006618101080119", "10.1109/pacificvis.2012.6183571", "10.1145/2038558.2038579", "10.1109/tvcg.2010.191", "10.1111/j.0033-0124.1985.00075.x", "10.1109/tvcg.2004.1260761", "10.1007/978-3-642-34848-8\\_6", "10.1145/2968220.2968239", "10.1145/3002151.3002160", "10.1109/tst.2013.6509098", "10.1109/tvcg.2013.91", "10.1177/1473871615597077", "10.1016/j.jvlc.2011.02.001", "10.1080/17445647.2014.935502", "10.1177/1687814017740710", "10.1111/1744-7917.12601", "10.1073/pnas.0400280101", "10.1111/cgf.13672", "10.1109/mcg.2006.90", "10.1002/asi.21712", "10.1007/978-3-642-33024-7\\_3", "10.1145/2801040.2801056", "10.1109/mcg.2010.101", "10.1179/003962607x165041", "10.1057/palgrave.ivs.9500039", "10.1109/vast.2016.7883510", "10.1559/152304009788988288", "10.1080/23729333.2017.1301346", "10.1109/tvcg.2013.66", "10.1111/j.1467-8659.2012.03085.x", "10.1109/tvcg.2011.288", "10.3390/ijgi9040253", "10.1109/infvis.2005.1532150", "10.1145/2254556.2254636", "10.20382//jocg.v4i1a9", "10.1016/0010-0285(81)90016-5", "10.1145/2556288.2557224", "10.1109/iv.2001.942043", "10.1021/ed1000203", "10.1016/0169-7439(87)80084-9", "10.1109/tvcg.2010.154", "10.1016/j.jvlc.2015.10.003", "10.1109/tvcg.2019.2903945", "10.1109/tvcg.2013.120", "10.1109/tvcg.2019.2934263", "10.1146/annurev-ecolsys-102209-144718", "10.1109/tvcg.2008.165", "10.3138/a477-3202-7876-n514", "10.1080/23729333.2017.1288535", "10.1111/j.0020-2754.1998.00269.x", "10.1109/mcg.2004.41", "10.1109/vast.2009.5332593", "10.1002/smr.414", "10.1007/s12650-019-00584-3", "10.1145/22949.22950", "10.1179/1743277413y.0000000036", "10.1080/15230406.2016.1262280", "10.1016/s0341-8162(01)00164-3", "10.22224/gistbok/2017.3.8", "10.1109/tvcg.2008.155", "10.1057/ivs.2008.31", "10.1016/j.cag.2004.03.012", "10.1179/1743277412y.0000000007", "10.1016/j.soncn.2011.02.001", "10.1179/caj.1987.24.1.27", "10.3138/carto.48.3.1691", "10.5220/0004267205150524", "10.1109/38.974518", "10.1145/102377.115768", "10.1057/palgrave.ivs.9500186", "10.1109/5.58325", "10.5167/80972.uzh-80972", "10.1177/030913339602000204", "10.1007/978-3-642-22300-6\\_14", "10.1109/iv.2004.1320189", "10.1111/cgf.13447", "10.1007/s11192-017-2596-3", "10.1559/1523040053722150", "10.1007/978-3-662-45803-7\\_34"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934263", "title": "R-Map: A Map Metaphor for Visualizing Information Reposting Process in Social Media", "year": "2019", "conferenceName": "VAST", "authors": "Shuai Chen;Sihang Li;Siming Chen;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Chen, Shuai; Li, Sihang, Peking Univ, Sch EECS, Minist Educ, Key Lab Machine Petrept, Beijing, Peoples R China. Yuan, Xiaoru, Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Chen, Siming, Fraunhofer Inst IAIS, St Augustin, Germany. Chen, Siming, Univ Bonn, Bonn, Germany.", "countries": "Germany;China", "abstract": "We propose R-Map (Reposting Map), a visual analytical approach with a map metaphor to support interactive exploration and analysis of the information reposting process in social media. A single original social media post can cause large cascades of repostings (i.e., retweets) on online networks, involving thousands, even millions of people with different opinions. Such reposting behaviors form the reposting tree, in which a node represents a message and a link represents the reposting relation. In R-Map, the reposting tree structure can be spatialized with highlighted key players and tiled nodes. The important reposting behaviors, the following relations and the semantics relations are represented as rivers, routes and bridges, respectively, in a virtual geographical space. R-Map supports a scalable overview of a large number of information repostings with semantics. Additional interactions on the map are provided to support the investigation of temporal patterns and user behaviors in the information diffusion process. We evaluate the usability and effectiveness of our system with two use cases and a formal user study.", "keywords": "Social Media,Information Diffusion,Map-like Visual Metaphor", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934263", "refList": ["10.1109/infvis.2000.885091", "10.1109/pacificvis.2010.5429590", "10.1111/j.0020-2754.1998.00269.x", "10.1109/vast.2017.8585638", "10.1145/2700398", "10.1109/tmm.2016.2614229", "10.1109/visual.1991.175815", "10.1145/3183347", "10.1109/infvis.2001.963290", "10.1109/vast.2016.7883510", "10.1109/access.2016.2605009", "10.1109/mcg.2011.103", "10.1145/1124772.1124851", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2010.79", "10.1109/infvis.2000.885095", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1002/(sici)1097-0266(199606)17:6", "10.5670/oceanog.2016.66", "10.1559/152304003100011081", "10.1109/tit.1982.1056489", "10.1559/152304098782383034", "10.1109/tvcg.2014.2346922", "10.1007/978-3-540-85567-5\\_9", "10.1145/2488388.2488504", "10.1109/38.974518", "10.1109/bigdata.2013.6691714", "10.1109/pacificvis.2014.38", "10.1109/tvcg.2012.291", "10.1109/infvis.2005.1532128", "10.2307/2685881", "10.1007/1-4020-4179-9\\_91", "10.1109/infvis.1999.801860", "10.1109/asonam.2011.37"], "wos": 1, "children": [{"doi": "10.1111/cgf.14031", "year": "2020", "title": "The State of the Art in Map-Like Visualization", "conferenceName": "EuroVis", "authors": "Marius Hogr{\\\"{a}}fer;Magnus Heitzler;Hans{-}J{\\\"{o}}rg Schulz", "citationCount": "0", "affiliation": "Hografer, M (Corresponding Author), Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHografer, Marius; Schulz, Hans-Jorg, Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHeitzler, Magnus, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Denmark", "abstract": "Cartographic maps have been shown to provide cognitive benefits when interpreting data in relation to a geographic location. In visualization, the term map-like describes techniques that incorporate characteristics of cartographic maps in their representation of abstract data. However, the field of map-like visualization is vast and currently lacks a clear classification of the existing techniques. Moreover, choosing the right technique to support a particular visualization task is further complicated, as techniques are scattered across different domains, with each considering different characteristics as map-like. In this paper, we give an overview of the literature on map-like visualization and provide a hierarchical classification of existing techniques along two general perspectives: imitation and schematization of cartographic maps. Each perspective is further divided into four principal categories that group common map-like techniques along the visual primitives they affect. We further discuss this classification from a task-centered view and highlight open research questions.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14031", "refList": ["10.1111/j.0020-2754.1998.00269.x.21", "10.1109/iv.2005.26", "10.2307/2980460", "10.1109/tvcg.2017.2743959", "10.1016/j.jvlc.2011.11.004", "10.1145/2501988.2502046", "10.1559/1523040042742402", "10.1007/s00779-011-0500-3", "10.1111/cgf.12932", "10.1109/tvcg.2010.89", "10.1111/cgf.13200", "10.1080/15230406.2016.1160797", "10.1179/000870410x12825500202896", "10.1109/tvcg.2007.70596", "10.5167/80972.19uzh-80972", "10.1007/978-3-319-11593-1\\_2", "10.1037/aca0000175", "10.1080/17538947.2014.923942", "10.1109/tvcg.2015.2467321", "10.1109/vissof.2005.1684299", "10.1559/152304098782383034", "10.1007/978-3-642-36763-2\\_38", "10.1109/tvcg.2013.130", "10.1109/tvcg.2014.2346274", "10.1111/cgf.12648", "10.1111/0004-5608.00242", "10.1177/1473871617724212", "10.1109/iv.2004.1320123", "10.1371/journal.pcbi.1006907", "10.1109/tvcg.2016.2599030", "10.1559/1523040042742411", "10.3390/informatics5030031", "10.1111/cgf.13079", "10.1109/tvcg.2015.2467811", "10.1016/j.tics.2003.12.004", "10.1007/s00799-016-0168-4", "10.1109/infvis.2004.57", "10.1007/s10021-007-9038-7", "10.1109/pacificvis.2015.7156366", "10.1007/978-3-319-27261-0\\_1", "10.1007/978-1-4471-2804-5\\_6", "10.1109/access.2019.2939977", "10.1109/tvcg.2017.2747545", "10.1109/icdm.2003.1250978", "10.1111/cgf.13167", "10.1016/0010-0285(78)90006-3", "10.3138/nj8v-8514-871t-221k", "10.1016/j.cag.2009.06.002", "10.1111/j.1467-8659.2011.01937.x", "10.1016/b978-044451020-4/50035-9", "10.1111/1467-8659.00566", "10.5220/0006618101080119", "10.1109/pacificvis.2012.6183571", "10.1145/2038558.2038579", "10.1109/tvcg.2010.191", "10.1111/j.0033-0124.1985.00075.x", "10.1109/tvcg.2004.1260761", "10.1007/978-3-642-34848-8\\_6", "10.1145/2968220.2968239", "10.1145/3002151.3002160", "10.1109/tst.2013.6509098", "10.1109/tvcg.2013.91", "10.1177/1473871615597077", "10.1016/j.jvlc.2011.02.001", "10.1080/17445647.2014.935502", "10.1177/1687814017740710", "10.1111/1744-7917.12601", "10.1073/pnas.0400280101", "10.1111/cgf.13672", "10.1109/mcg.2006.90", "10.1002/asi.21712", "10.1007/978-3-642-33024-7\\_3", "10.1145/2801040.2801056", "10.1109/mcg.2010.101", "10.1179/003962607x165041", "10.1057/palgrave.ivs.9500039", "10.1109/vast.2016.7883510", "10.1559/152304009788988288", "10.1080/23729333.2017.1301346", "10.1109/tvcg.2013.66", "10.1111/j.1467-8659.2012.03085.x", "10.1109/tvcg.2011.288", "10.3390/ijgi9040253", "10.1109/infvis.2005.1532150", "10.1145/2254556.2254636", "10.20382//jocg.v4i1a9", "10.1016/0010-0285(81)90016-5", "10.1145/2556288.2557224", "10.1109/iv.2001.942043", "10.1021/ed1000203", "10.1016/0169-7439(87)80084-9", "10.1109/tvcg.2010.154", "10.1016/j.jvlc.2015.10.003", "10.1109/tvcg.2019.2903945", "10.1109/tvcg.2013.120", "10.1109/tvcg.2019.2934263", "10.1146/annurev-ecolsys-102209-144718", "10.1109/tvcg.2008.165", "10.3138/a477-3202-7876-n514", "10.1080/23729333.2017.1288535", "10.1111/j.0020-2754.1998.00269.x", "10.1109/mcg.2004.41", "10.1109/vast.2009.5332593", "10.1002/smr.414", "10.1007/s12650-019-00584-3", "10.1145/22949.22950", "10.1179/1743277413y.0000000036", "10.1080/15230406.2016.1262280", "10.1016/s0341-8162(01)00164-3", "10.22224/gistbok/2017.3.8", "10.1109/tvcg.2008.155", "10.1057/ivs.2008.31", "10.1016/j.cag.2004.03.012", "10.1179/1743277412y.0000000007", "10.1016/j.soncn.2011.02.001", "10.1179/caj.1987.24.1.27", "10.3138/carto.48.3.1691", "10.5220/0004267205150524", "10.1109/38.974518", "10.1145/102377.115768", "10.1057/palgrave.ivs.9500186", "10.1109/5.58325", "10.5167/80972.uzh-80972", "10.1177/030913339602000204", "10.1007/978-3-642-22300-6\\_14", "10.1109/iv.2004.1320189", "10.1111/cgf.13447", "10.1007/s11192-017-2596-3", "10.1559/1523040053722150", "10.1007/978-3-662-45803-7\\_34"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934266", "title": "VASSL: A Visual Analytics Toolkit for Social Spambot Labeling", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;Jieqiong Zhao;David S. Ebert", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Zhao, Jieqiong; Ebert, David S., Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Purdue Univ, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Social media platforms are filled with social spambots. Detecting these malicious accounts is essential, yet challenging, as they continually evolve to evade detection techniques. In this article, we present VASSL, a visual analytics system that assists in the process of detecting and labeling spambots. Our tool enhances the performance and scalability of manual labeling by providing multiple connected views and utilizing dimensionality reduction, sentiment analysis and topic modeling, enabling insights for the identification of spambots. The system allows users to select and analyze groups of accounts in an interactive manner, which enables the detection of spambots that may not be identified when examined individually. We present a user study to objectively evaluate the performance of VASSL users, as well as capturing subjective opinions about the usefulness and the ease of use of the tool.", "keywords": "Spambot,Labeling,Detection,Visual Analytics,Social Media Annotation", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934266", "refList": ["10.7326/0003-4819-110-11-916", "10.1109/tifs.2013.2267732", "10.1111/cgf.12106", "10.1016/j.ins.2013.11.016", "10.1109/tvcg.2017.2752166", "10.1109/vast.2016.7883510", "10.1109/vl.1996.545307", "10.1145/2872518.2889302", "10.1109/tmm.2016.2614220", "10.1145/2818717", "10.1109/tdsc.2017.2681672", "10.1111/cgf.13211", "10.2307/2685478", "10.1109/tvcg.2014.2346920", "10.1109/tdsc.2016.2641441", "10.1109/tvcg.2017.2745080", "10.1109/vast.2012.6400557", "10.1109/asonam.2016.7752287", "10.1109/tvcg.2014.2346922", "10.1111/cgf.13217", "10.2307/249008", "10.1109/tvcg.2017.2711030", "10.1109/mcse.2013.70", "10.1109/tvcg.2015.2467196", "10.1016/j.comcom.2013.04.004", "10.1109/asonam.2014.6921650", "10.1109/mc.2016.183", "10.1126/science.290.5500.2323", "10.1145/3041021.3055135", "10.1162/jmlr.2003.3.4-5.993", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2013.153", "10.1109/iv.2008.89", "10.1109/mcom.2013.6588663", "10.1179/000870403235002042", "10.1145/3047010"], "wos": 1, "children": [{"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 7}, {"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13677", "year": "2019", "title": "An Ontological Framework for Supporting the Design and Evaluation of Visual Analytics Systems", "conferenceName": "EuroVis", "authors": "Min Chen;David S. Ebert", "citationCount": "5", "affiliation": "Chen, M (Corresponding Author), Univ Oxford, Oxford, England.\nChen, Min, Univ Oxford, Oxford, England.\nEbert, David S., Purdue Univ, W Lafayette, IN 47907 USA.", "countries": "USA;England", "abstract": "Designing, evaluating, and improving visual analytics (VA) systems is a primary area of activities in our discipline. In this paper, we present an ontological framework for recording and categorizing technical shortcomings to be addressed in a VA workflow, reasoning about the causes of such problems, identifying technical solutions, and anticipating secondary effects of the solutions. The methodology is built on the theoretical premise that designing a VA workflow is an optimization of the cost-benefit ratio of the processes in the workflow. It makes uses three fundamental measures to group and connect symptoms, causes, remedies, and side-effects, and guide the search for potential solutions to the problems. In terms of requirement analysis and system design, the proposed methodology can enable system designers to explore the decision space in a structured manner. In terms of evaluation, the proposed methodology is time-efficient and complementary to various forms of empirical studies, such as user surveys, controlled experiments, observational studies, focus group discussions, and so on. In general, it reduces the amount of trial-and-error in the lifecycle of VA system development.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13677", "refList": ["10.1109/tvcg.2006.178", "10.1109/infvis.2000.885092", "10.1111/cgf.12920", "10.1057/ivs.2009.26", "10.1109/mcg.2017.3271463", "10.1007/978-3-319-10578-9\\_1", "10.1109/icdmw.2008.62", "10.1109/mcg.2017.51", "10.1145/3011141.3011207", "10.1109/tvcg.2013.134", "10.1080/10618600.1996.10474696", "10.1007/978-3-540-70956-5", "10.1109/visual.1990.146375", "10.1109/tvcg.2012.219", "10.1109/vl.1996.545307", "10.1057/ivs.2009.23", "10.1002/j.1538-7305.1948.tb00917.x", "10.1109/tvcg.2010.79", "10.1109/tvcg.2013.124", "10.1111/cgf.13211", "10.1007/978-3-642-40897-7\\_9", "10.1103/physrev.108.171", "10.1109/infvis.2004.59", "10.1109/iv.2008.36", "10.1111/cgf.13210", "10.1111/j.1467-8659.2008.01230.x", "10.1001/jama.293.10.1223", "10.1177/1473871611407399", "10.1109/visual.1995.480821", "10.1117/12.539227", "10.1109/tvcg.2015.2513410", "10.1109/vast.2011.6102463", "10.7749/citiescommunitiesterritories.dec2014.029.art01", "10.1109/mcg.2005.55", "10.1109/tvcg.2012.234", "10.1109/pacificvis.2012.6183556", "10.1103/physrev.106.620", "10.1109/infvis.1997.636792", "10.2307/2104491", "10.1145/2468356.2468677", "10.1145/3173574.3173611", "10.1109/tvcg.2018.2864838", "10.1111/cgf.13092", "10.1109/visual.2004.10", "10.1109/tvcg.2017.2744319", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2603178"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934264", "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;David S. Ebert;Arif Ghafoor", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Karimzadeh, Morteza; Ebert, David S.; Ghafoor, Arif, Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.", "keywords": "Summative evaluation,usefulness,evaluation process,taxonomy,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934264", "refList": ["10.1109/tvcg.2017.2744478", "10.1109/tvcg.2018.2865025", "10.1109/tvcg.2006.85", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2017.2745181", "10.1111/cgf.13677", "10.1109/tvcg.2018.2864844", "10.1109/tvcg.2013.126", "10.1109/tvcg.2018.2864811", "10.1109/infvis.2005.1532147", "10.1177/0956797613504966", "10.1145/2669557.2669579", "10.1109/mcg.2005.102", "10.1109/visual.2003.1250426", "10.1136/bmj.39489.470347.ad", "10.1109/tvcg.2017.2744080", "10.1109/mcg.2009.53", "10.1111/j.1467-8527.2005.00307.x", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2864886", "10.1109/tvcg.2018.2864843", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2018.2865051", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2018.2865026", "10.1007/978-3-540-71080-6\\_6", "10.1109/tvcg.2018.2865020", "10.1177/1473871611407399", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2018.2864526", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2864905", "10.1049/sej.1991.0040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2711030", "10.1109/tvcg.2011.279", "10.1109/vast.2017.8585505", "10.1147/jrd.2010.2042914", "10.1016/s0378-7206(98)00044-5", "10.1145/2993901.2993913", "10.1109/tvcg.2018.2865041", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744758", "10.1145/1168149.1168158", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2012.213", "10.1109/tvcg.2017.2744738", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2018.2864826", "10.1145/1377966.1377974", "10.1109/apec.2009.4802646", "10.1145/1168149.1168152", "10.1016/j.jss.2008.03.059", "10.1109/vast.2017.8585484", "10.1109/tvcg.2017.2744818", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2865042", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030388", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "year": "2020", "conferenceName": "SciVis", "authors": "Pepe Eulzer;Sabine Bauer;Francis Kilian;Kai Lawonn", "citationCount": "0", "affiliation": "Eulzer, P (Corresponding Author), Univ Jena, Jena, Germany. Eulzer, Pepe; Lawonn, Kai, Univ Jena, Jena, Germany. Bauer, Sabine, Univ Koblenz Landau, Koblenz, Germany. Kilian, Francis, Cath Clin Koblenz Montabaur, Dept Spine Surg, Koblenz, Germany.", "countries": "Germany", "abstract": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. $\\mathrm{By}$ linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. $\\mathrm{in}$ a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.", "keywords": "Medical visualization,bioinformatics,coordinated views,focus and context,biomechanical simulation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030388", "refList": ["10.1109/tvcg.2018.2864903", "10.1177/1473871613510429", "10.1093/ehjqcco/qcz052", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2018.2865076", "10.1055/s-0039-1687862", "10.1109/visual.1990.146375", "10.1109/tvcg.2017.2744198", "10.1016/j.ijmedinf.2014.10.001", "10.1109/tvcg.2013.124", "10.1016/j.jacc", "10.1111/cgf.13167", "10.17705/1thci.00055", "10.1136/bmjqs.2009.037895", "10.1109/tvcg.2013.238", "10.1109/tvcg.2018.2865240", "10.1186/1471-2261-6-34", "10.1109/tvcg.2019.2934264", "10.1109/tvcg.2013.200", "10.1109/tvcg.2011.209", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467091", "10.1136/bmjopen-2019-033208", "10.1109/beliv.2018.8634027", "10.1109/tvcg.2012.213", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2015.2467325", "10.1145/2133806.2133821", "10.1145/1806799.1806866", "10.1108/02635570610688869", "10.1002/hbm.20701", "10.1561/1100000039", "10.1145/3025453.3025645", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 5}, {"doi": "10.1111/cgf.13987", "year": "2020", "title": "Augmenting Node-Link Diagrams with Topographic Attribute Maps", "conferenceName": "EuroVis", "authors": "Reinhold Preiner;Johanna Schmidt;Katharina Kr{\\\"{o}}sl;Tobias Schreck;Gabriel Mistelbauer", "citationCount": "0", "affiliation": "Preiner, R (Corresponding Author), Graz Univ Technol, Inst Comp Graph \\& Knowledge Visualizat, Graz, Austria.\nPreiner, R.; Schreck, T., Graz Univ Technol, Inst Comp Graph \\& Knowledge Visualizat, Graz, Austria.\nSchmidt, J.; Kroesl, K., Virtual Real \\& Visualisierung Forsch GmbH, VRVis Zentrum, Vienna, Austria.\nKroesl, K., TU Wien, Inst Visual Comp \\& Human Ctr Technol, Vienna, Austria.\nMistelbauer, G., Otto von Guericke Univ, Dept Simulat \\& Graph, Magdeburg, Germany.", "countries": "Germany;Austria", "abstract": "We propose a novel visualization technique for graphs that are attributed with scalar data. In many scenarios, these attributes (e.g., birth date in a family network) provide ambient context information for the graph structure, whose consideration is important for different visual graph analysis tasks. Graph attributes are usually conveyed using different visual representations (e.g., color, size, shape) or by reordering the graph structure according to the attribute domain (e.g., timelines). While visual encodings allow graphs to be arranged in a readable layout, assessing contextual information such as the relative similarities of attributes across the graph is often cumbersome. In contrast, attribute-based graph reordering serves the comparison task of attributes, but typically strongly impairs the readability of the structural information given by the graph's topology. In this work, we augment force-directed node-link diagrams with a continuous ambient representation of the attribute context. This way, we provide a consistent overview of the graph's topological structure as well as its attributes, supporting a wide range of graph-related analysis tasks. We resort to an intuitive height field metaphor, illustrated by a topographic map rendering using contour lines and suitable color maps. Contour lines visually connect nodes of similar attribute values, and depict their relative arrangement within the global context. Moreover, our contextual representation supports visualizing attribute value ranges associated with graph nodes (e.g., lifespans in a family network) as trajectories routed through this height field. We discuss how user interaction with both the structural and the contextual information fosters exploratory graph analysis tasks. The effectiveness and versatility of our technique is confirmed in a user study and case studies from various application domains.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13987", "refList": ["10.1109/tvcg.2013.269", "10.1109/pacificvis.2010.5429590", "10.1073/pnas.0307654100", "10.1145/2505515.2505758", "10.1559/152304082783948286", "10.1109/pacificvis.2014.47", "10.1093/bioinformatics/btp432", "10.1111/j.1467-8659.2011.01898.x", "10.1111/cgf.12931", "10.1111/cgf.12880", "10.1109/tvcg.2014.2346422", "10.1111/j.1467-8659.2009.01706.x", "10.1109/tvcg.2016.2598795", "10.1111/cgf.12800", "10.1109/tvcg.2014.2315995", "10.1111/cgf.12656", "10.1111/cgf.13728", "10.1109/tvcg.2009.122", "10.1111/cgf.13211", "10.1109/tvcg.2007.70596", "10.1109/infvis.2002.1173152", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2003.1196007", "10.1109/infvis.2005.1532150", "10.1145/3243250.3243266", "10.1080/02693799008941549", "10.1371/journal.pone.0058779", "10.1109/infvis.1995.528686", "10.1111/cgf.12872", "10.1002/spe.4380211102", "10.1109/38.974518", "10.1145/3097983.3098130", "10.1002/aris.1440370106", "10.1145/1360612.1360691", "10.1109/mc.2016.145", "10.2307/3006914", "10.1111/j.1467-8659.2009.01683.x", "10.1145/1639714.1639784"], "wos": 1, "children": [], "len": 1}], "len": 31}, {"keywords": "Social media data; visual analytics; visualization", "doi": "10.1109/tmm.2016.2614220", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou 310027, Zhejiang, Peoples R China.\nWu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou 310027, Zhejiang, Peoples R China.\nCao, Nan, NYU, Tandon Sch Engn, New York, NY 11201 USA.\nCao, Nan, New York Univ ShangHai, Pudong 200122, Peoples R China.\nGotz, David, Univ N Carolina, Chapel Hill, NC 27514 USA.\nTan, Yap-Peng, Nanyang Technol Univ, Sch Elect \\& Elect Engn, Singapore, Singapore.\nKeim, Daniel A., Univ Konstanz, D-78464 Constance, Germany.", "abstract": "The unprecedented availability of social media data offers substantial opportunities for data owners, system operators, solution providers, and end users to explore and understand social dynamics. However, the exponential growth in the volume, velocity, and variability of social media data prevents people from fully utilizing such data. Visual analytics, which is an emerging research direction, has received considerable attention in recent years. Many visual analytics methods have been proposed across disciplines to understand large-scale structured and unstructured social media data. This objective, however, also poses significant challenges for researchers to obtain a comprehensive picture of the area, understand research challenges, and develop new techniques. In this paper, we present a comprehensive survey to characterize this fast-growing area and summarize the state-of-the-art techniques for analyzing social media data. In particular, we classify existing techniques into two categories: gathering information and understanding user behaviors. We aim to provide a clear overview of the research area through the established taxonomy. We then explore the design space and identify the research trends. Finally, we discuss challenges and open questions for future studies.", "year": "2016", "title": "A Survey on Visual Analytics of Social Media Data", "conferenceName": "IEEE TRANSACTIONS ON MULTIMEDIA", "authors": "Yingcai Wu;Nan Cao;David Gotz;Yap-Peng Tan;Daniel A. Keim", "citationCount": "50", "countries": "China;USA;Germany;Singapore", "refList": ["10.1109/infvis.2004.43", "10.1109/vast.2010.5652922", "10.1109/tmm.2008.2009684", "10.1145/2207676.2208409", "10.1109/tmm.2015.2425143", "10.1109/cse.2009.186", "10.1109/icde.2009.140", "10.1109/tvcg.2008.135", "10.1109/tmm.2009.2012912", "10.1109/tvcg.2010.129", "10.1109/tvcg.2012.291", "10.1109/tvcg.2006.122", "10.1109/tvcg.2015.2467196", "10.1007/s12650-014-0246-x", "10.1109/vast.2012.6400485", "10.1109/tmm.2015.2430819", "10.1109/tmm.2014.2384912", "10.1109/vast.2012.6400557", "10.1109/tmm.2010.2050649", "10.1109/tvcg.2011.185", "10.1109/tvcg.2007.70582", "10.1109/tmm.2013.2265079", "10.1109/tvcg.2006.147", "10.1145/2470654.2466478", "10.1109/tmm.2014.2340133", "10.1109/tvcg.2013.221", "10.1136/qshc.2004.010033", "10.1145/2733373.2806246", "10.1109/vast.2011.6102456", "10.1145/985692.985765", "10.1109/tmm.2009.2012916", "10.1109/tvcg.2015.2467554", "10.1109/vast.2011.6102443", "10.1109/tvcg.2015.2509990", "10.1109/tmm.2015.2510333", "10.1145/2488388.2488504", "10.1109/mcom.2013.6588663", "10.1145/2449396.2449424", "10.1109/tvcg.2013.186", "10.1109/tmm.2015.2510329", "10.1145/1866029.1866077", "10.1109/tvcg.2014.2346919", "10.1109/tmm.2016.2515362", "10.1109/tvcg.2006.166", "10.1145/2187980.2188035", "10.1109/mcg.2015.73", "10.1145/2733373.2806236", "10.1109/tvcg.2014.2346920", "10.1145/2470654.2466444", "10.1109/tmm.2013.2265078", "10.1109/pacificvis.2009.4906836", "10.1109/tvcg.2006.120", "10.1109/tvcg.2015.2467619", "10.1109/tmm.2015.2477277", "10.1109/tvcg.2013.223", "10.1109/tvcg.2014.2346922", "10.1109/pacificvis.2016.7465266", "10.1145/2536798"], "children": [{"doi": "10.1109/vast.2017.8585638", "title": "E-Map: A Visual Analytics Approach for Exploring Significant Event Evolutions in Social Media", "year": "2017", "conferenceName": "VAST", "authors": "Siming Chen;Shuai Chen;Lijing Lin;Xiaoru Yuan;Jie Liang 0004;Xiaolong Zhang", "citationCount": "10", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing, Peoples R China. Yuan, XR (Corresponding Author), Peking Univ, Sch EECS, Beijing, Peoples R China. Chen, Siming; Chen, Shuai; Lin, Lijing; Yuan, Xiaoru, Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing, Peoples R China. Chen, Siming; Chen, Shuai; Lin, Lijing; Yuan, Xiaoru, Peking Univ, Sch EECS, Beijing, Peoples R China. Liang, Jie, Univ Technol, Fac Engn \\& Informat Technol, Sydney, NSW, Australia. Zhang, Xiaolong, Penn State Univ, Coll Informat Sci \\& Technol, University Pk, PA 16802 USA.", "countries": "USA;China;Australia", "abstract": "Significant events are often discussed and spread through social media, involving many people. Reposting activities and opinions expressed in social media offer good opportunities to understand the evolution of events. However, the dynamics of reposting activities and the diversity of user comments pose challenges to understand event-related social media data. We propose E-Map, a visual analytics approach that uses map-like visualization tools to help multi-faceted analysis of social media data on a significant event and in-depth understanding of the development of the event. E-Map transforms extracted keywords, messages, and reposting behaviors into map features such as cities, towns, and rivers to build a structured and semantic space for users to explore. It also visualizes complex posting and reposting behaviors as simple trajectories and connections that can be easily followed. By supporting multi-level spatial temporal exploration, E-Map helps to reveal the patterns of event development and key players in an event, disclosing the ways they shape and affect the development of the event. Two cases analysing real-world events confirm the capacities of E-Map in facilitating the analysis of event evolution with social media data.", "keywords": "Social Media,Event Analysis,Map-like Visual Metaphor,Spatial Temporal Visual Analytics", "link": "http://dx.doi.org/10.1109/VAST.2017.8585638", "refList": ["10.1109/tvcg.2007.70582", "10.1109/tvcg.2016.2598919", "10.1109/pacificvis.2010.5429590", "10.1057/ivs.2008.23", "10.1109/vast.2014.7042496", "10.1016/j.cag.2013.11.003", "10.1109/vast.2011.6102456", "10.1109/tvcg.2013.221", "10.1109/tvcg.2011.185", "10.1109/vast.2016.7883510", "10.1111/j.1467-8659.2012.03120.x", "10.1109/tvcg.2013.186", "10.1109/tmm.2016.2614220", "10.7155/jgaa.00302", "10.1109/mc.2012.430", "10.1109/tvcg.2015.2467619", "10.1109/tvcg.2010.129", "10.1016/s0341-8162(01)00164-3", "10.1109/tvcg.2016.2598590", "10.1145/2065023.2065041", "10.1111/cgf.13211", "10.1109/tvcg.2013.162", "10.1109/tvcg.2011.288", "10.1145/1963405.1963504", "10.5670/oceanog.2016.66", "10.1109/tvcg.2015.2467554", "10.1109/tvcg.2015.2467691", "10.1016/j.cag.2013.10.008", "10.1109/mcg.2015.73", "10.1109/vast.2012.6400557", "10.1109/tvcg.2016.2539960", "10.1109/tit.1982.1056489", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2014.2346922", "10.1109/tmm.2014.2384912", "10.1109/vast.2016.7883511", "10.1002/spe.4380211102", "10.1145/2488388.2488504", "10.2312/eurovisstar.20141176", "10.1109/bigdata.2013.6691714", "10.1109/tvcg.2009.171", "10.1109/tvcg.2013.196", "10.1109/vast.2008.4677356", "10.1145/2207676.2208672", "10.1111/j.1467-8659.2011.01955.x", "10.1109/pacificvis.2014.38", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2015.7156376", "10.1145/1348549.1348556", "10.1109/vast.2015.7347632", "10.1109/tvcg.2014.2346919", "10.1109/tvcg.2014.2346433"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934263", "title": "R-Map: A Map Metaphor for Visualizing Information Reposting Process in Social Media", "year": "2019", "conferenceName": "VAST", "authors": "Shuai Chen;Sihang Li;Siming Chen;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Chen, Shuai; Li, Sihang, Peking Univ, Sch EECS, Minist Educ, Key Lab Machine Petrept, Beijing, Peoples R China. Yuan, Xiaoru, Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Chen, Siming, Fraunhofer Inst IAIS, St Augustin, Germany. Chen, Siming, Univ Bonn, Bonn, Germany.", "countries": "Germany;China", "abstract": "We propose R-Map (Reposting Map), a visual analytical approach with a map metaphor to support interactive exploration and analysis of the information reposting process in social media. A single original social media post can cause large cascades of repostings (i.e., retweets) on online networks, involving thousands, even millions of people with different opinions. Such reposting behaviors form the reposting tree, in which a node represents a message and a link represents the reposting relation. In R-Map, the reposting tree structure can be spatialized with highlighted key players and tiled nodes. The important reposting behaviors, the following relations and the semantics relations are represented as rivers, routes and bridges, respectively, in a virtual geographical space. R-Map supports a scalable overview of a large number of information repostings with semantics. Additional interactions on the map are provided to support the investigation of temporal patterns and user behaviors in the information diffusion process. We evaluate the usability and effectiveness of our system with two use cases and a formal user study.", "keywords": "Social Media,Information Diffusion,Map-like Visual Metaphor", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934263", "refList": ["10.1109/infvis.2000.885091", "10.1109/pacificvis.2010.5429590", "10.1111/j.0020-2754.1998.00269.x", "10.1109/vast.2017.8585638", "10.1145/2700398", "10.1109/tmm.2016.2614229", "10.1109/visual.1991.175815", "10.1145/3183347", "10.1109/infvis.2001.963290", "10.1109/vast.2016.7883510", "10.1109/access.2016.2605009", "10.1109/mcg.2011.103", "10.1145/1124772.1124851", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2010.79", "10.1109/infvis.2000.885095", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1002/(sici)1097-0266(199606)17:6", "10.5670/oceanog.2016.66", "10.1559/152304003100011081", "10.1109/tit.1982.1056489", "10.1559/152304098782383034", "10.1109/tvcg.2014.2346922", "10.1007/978-3-540-85567-5\\_9", "10.1145/2488388.2488504", "10.1109/38.974518", "10.1109/bigdata.2013.6691714", "10.1109/pacificvis.2014.38", "10.1109/tvcg.2012.291", "10.1109/infvis.2005.1532128", "10.2307/2685881", "10.1007/1-4020-4179-9\\_91", "10.1109/infvis.1999.801860", "10.1109/asonam.2011.37"], "wos": 1, "children": [{"doi": "10.1111/cgf.14031", "year": "2020", "title": "The State of the Art in Map-Like Visualization", "conferenceName": "EuroVis", "authors": "Marius Hogr{\\\"{a}}fer;Magnus Heitzler;Hans{-}J{\\\"{o}}rg Schulz", "citationCount": "0", "affiliation": "Hografer, M (Corresponding Author), Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHografer, Marius; Schulz, Hans-Jorg, Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHeitzler, Magnus, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Denmark", "abstract": "Cartographic maps have been shown to provide cognitive benefits when interpreting data in relation to a geographic location. In visualization, the term map-like describes techniques that incorporate characteristics of cartographic maps in their representation of abstract data. However, the field of map-like visualization is vast and currently lacks a clear classification of the existing techniques. Moreover, choosing the right technique to support a particular visualization task is further complicated, as techniques are scattered across different domains, with each considering different characteristics as map-like. In this paper, we give an overview of the literature on map-like visualization and provide a hierarchical classification of existing techniques along two general perspectives: imitation and schematization of cartographic maps. Each perspective is further divided into four principal categories that group common map-like techniques along the visual primitives they affect. We further discuss this classification from a task-centered view and highlight open research questions.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14031", "refList": ["10.1111/j.0020-2754.1998.00269.x.21", "10.1109/iv.2005.26", "10.2307/2980460", "10.1109/tvcg.2017.2743959", "10.1016/j.jvlc.2011.11.004", "10.1145/2501988.2502046", "10.1559/1523040042742402", "10.1007/s00779-011-0500-3", "10.1111/cgf.12932", "10.1109/tvcg.2010.89", "10.1111/cgf.13200", "10.1080/15230406.2016.1160797", "10.1179/000870410x12825500202896", "10.1109/tvcg.2007.70596", "10.5167/80972.19uzh-80972", "10.1007/978-3-319-11593-1\\_2", "10.1037/aca0000175", "10.1080/17538947.2014.923942", "10.1109/tvcg.2015.2467321", "10.1109/vissof.2005.1684299", "10.1559/152304098782383034", "10.1007/978-3-642-36763-2\\_38", "10.1109/tvcg.2013.130", "10.1109/tvcg.2014.2346274", "10.1111/cgf.12648", "10.1111/0004-5608.00242", "10.1177/1473871617724212", "10.1109/iv.2004.1320123", "10.1371/journal.pcbi.1006907", "10.1109/tvcg.2016.2599030", "10.1559/1523040042742411", "10.3390/informatics5030031", "10.1111/cgf.13079", "10.1109/tvcg.2015.2467811", "10.1016/j.tics.2003.12.004", "10.1007/s00799-016-0168-4", "10.1109/infvis.2004.57", "10.1007/s10021-007-9038-7", "10.1109/pacificvis.2015.7156366", "10.1007/978-3-319-27261-0\\_1", "10.1007/978-1-4471-2804-5\\_6", "10.1109/access.2019.2939977", "10.1109/tvcg.2017.2747545", "10.1109/icdm.2003.1250978", "10.1111/cgf.13167", "10.1016/0010-0285(78)90006-3", "10.3138/nj8v-8514-871t-221k", "10.1016/j.cag.2009.06.002", "10.1111/j.1467-8659.2011.01937.x", "10.1016/b978-044451020-4/50035-9", "10.1111/1467-8659.00566", "10.5220/0006618101080119", "10.1109/pacificvis.2012.6183571", "10.1145/2038558.2038579", "10.1109/tvcg.2010.191", "10.1111/j.0033-0124.1985.00075.x", "10.1109/tvcg.2004.1260761", "10.1007/978-3-642-34848-8\\_6", "10.1145/2968220.2968239", "10.1145/3002151.3002160", "10.1109/tst.2013.6509098", "10.1109/tvcg.2013.91", "10.1177/1473871615597077", "10.1016/j.jvlc.2011.02.001", "10.1080/17445647.2014.935502", "10.1177/1687814017740710", "10.1111/1744-7917.12601", "10.1073/pnas.0400280101", "10.1111/cgf.13672", "10.1109/mcg.2006.90", "10.1002/asi.21712", "10.1007/978-3-642-33024-7\\_3", "10.1145/2801040.2801056", "10.1109/mcg.2010.101", "10.1179/003962607x165041", "10.1057/palgrave.ivs.9500039", "10.1109/vast.2016.7883510", "10.1559/152304009788988288", "10.1080/23729333.2017.1301346", "10.1109/tvcg.2013.66", "10.1111/j.1467-8659.2012.03085.x", "10.1109/tvcg.2011.288", "10.3390/ijgi9040253", "10.1109/infvis.2005.1532150", "10.1145/2254556.2254636", "10.20382//jocg.v4i1a9", "10.1016/0010-0285(81)90016-5", "10.1145/2556288.2557224", "10.1109/iv.2001.942043", "10.1021/ed1000203", "10.1016/0169-7439(87)80084-9", "10.1109/tvcg.2010.154", "10.1016/j.jvlc.2015.10.003", "10.1109/tvcg.2019.2903945", "10.1109/tvcg.2013.120", "10.1109/tvcg.2019.2934263", "10.1146/annurev-ecolsys-102209-144718", "10.1109/tvcg.2008.165", "10.3138/a477-3202-7876-n514", "10.1080/23729333.2017.1288535", "10.1111/j.0020-2754.1998.00269.x", "10.1109/mcg.2004.41", "10.1109/vast.2009.5332593", "10.1002/smr.414", "10.1007/s12650-019-00584-3", "10.1145/22949.22950", "10.1179/1743277413y.0000000036", "10.1080/15230406.2016.1262280", "10.1016/s0341-8162(01)00164-3", "10.22224/gistbok/2017.3.8", "10.1109/tvcg.2008.155", "10.1057/ivs.2008.31", "10.1016/j.cag.2004.03.012", "10.1179/1743277412y.0000000007", "10.1016/j.soncn.2011.02.001", "10.1179/caj.1987.24.1.27", "10.3138/carto.48.3.1691", "10.5220/0004267205150524", "10.1109/38.974518", "10.1145/102377.115768", "10.1057/palgrave.ivs.9500186", "10.1109/5.58325", "10.5167/80972.uzh-80972", "10.1177/030913339602000204", "10.1007/978-3-642-22300-6\\_14", "10.1109/iv.2004.1320189", "10.1111/cgf.13447", "10.1007/s11192-017-2596-3", "10.1559/1523040053722150", "10.1007/978-3-662-45803-7\\_34"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2018.2865139", "title": "Structure-Based Suggestive Exploration: A New Approach for Effective Exploration of Large Networks", "year": "2018", "conferenceName": "InfoVis", "authors": "Wei Chen;Fangzhou Guo;Dongming Han;Jacheng Pan;Xiaotao Nie;Jiazhi Xia;Xiaolong Zhang", "citationCount": "5", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Labo CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Chen, Wei; Guo, Fangzhou; Han, Dongming; Pan, Jacheng; Nie, Xiaotao, Zhejiang Univ, State Key Labo CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Xia, Jiazhi, Cent S Univ, Changsha, Hunan, Peoples R China. Zhang, Xiaolong, Penn State Univ, University Pk, PA 16802 USA.", "countries": "USA;China", "abstract": "When analyzing a visualized network, users need to explore different sections of the network to gain insight. However, effective exploration of large networks is often a challenge. While various tools are available for users to explore the global and local features of a network, these tools usually require significant interaction activities, such as repetitive navigation actions to follow network nodes and edges. In this paper, we propose a structure-based suggestive exploration approach to support effective exploration of large networks by suggesting appropriate structures upon user request. Encoding nodes with vectorized representations by transforming information of surrounding structures of nodes into a high dimensional space, our approach can identify similar structures within a large network, enable user interaction with multiple similar structures simultaneously, and guide the exploration of unexplored structures. We develop a web-based visual exploration system to incorporate this suggestive exploration approach and compare performances of our approach under different vectorizing methods and networks. We also present the usability and effectiveness of our approach through a controlled user study with two datasets.", "keywords": "Large Network Exploration,Structure-Based Exploration,Suggestive Exploration", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865139", "refList": ["10.1109/tvcg.2007.70582", "10.1109/tvcg.2006.120", "10.1109/mc.2013.242", "10.1109/tvcg.2009.108", "10.1145/2939672.2939754", "10.1016/j.comnet.2011.08.019", "10.1145/2702123.2702476", "10.1109/tvcg.2017.2744938", "10.1007/s00371-013-0892-3", "10.1137/1.9781611974973.67", "10.1109/vast.2009.5333893", "10.1109/tst.2013.6509098", "10.1109/35021bigcomp.2015.7072812", "10.1145/2909132.2909246", "10.1111/j.1467-8659.2011.01957.x", "10.1111/j.1467-8659.2011.01898.x", "10.1177/1473871612455749", "10.1109/tvcg.2013.167", "10.1145/3097983.3098061", "10.1007/978-1-4613-0303-9\\_28", "10.1109/vast.2014.7042485", "10.1109/tmm.2016.2614220", "10.1145/1376616.1376675", "10.1145/2623330.2623732", "10.14778/1920841.1920887", "10.1109/tvcg.2017.2745219", "10.1109/tvcg.2017.2743858", "10.1109/tvcg.2008.151", "10.1145/1150402.1150479", "10.1093/bioinformatics/bth436", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2016.2598958", "10.1111/cgf.12883", "10.1145/1556262.1556300", "10.1109/icdm.2012.159", "10.1145/2470654.2466444", "10.1109/tvcg.2013.109", "10.1109/infvis.2004.1", "10.1109/icdmw.2008.99", "10.1002/aris.1440370106", "10.1007/978-3-319-05813-9\\_11", "10.1371/journal.pone.0098679", "10.1109/tvcg.2006.106", "10.1111/j.1467-8659.2011.01935.x", "10.1111/cgf.12397", "10.1111/cgf.13184", "10.1111/cgf.12642", "10.1109/tvcg.2016.2598831", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934396", "title": "A Deep Generative Model for Graph Layout", "year": "2019", "conferenceName": "InfoVis", "authors": "Oh-Hyun Kwon;Kwan-Liu Ma", "citationCount": "4", "affiliation": "Kwon, OH (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Kwon, Oh-Hyun; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "Different layouts can characterize different aspects of the same graph. Finding a \u201cgood\u201d layout of a graph is thus an important task for graph visualization. In practice, users often visualize a graph in multiple layouts by using different methods and varying parameter settings until they find a layout that best suits the purpose of the visualization. However, this trial-and-error process is often haphazard and time-consuming. To provide users with an intuitive way to navigate the layout design space, we present a technique to systematically visualize a graph in diverse layouts using deep generative models. We design an encoder-decoder architecture to learn a model from a collection of example layouts, where the encoder represents training examples in a latent space and the decoder produces layouts from the latent space. In particular, we train the model to construct a two-dimensional latent space for users to easily explore and generate various layouts. We demonstrate our approach through quantitative and qualitative evaluations of the generated layouts. The results of our evaluations show that our model is capable of learning and generalizing abstract concepts of graph layouts, not just memorizing the training examples. In summary, this paper presents a fundamentally new approach to graph visualization where a machine learning model learns to visualize a graph from examples without manually-defined heuristics.", "keywords": "Graph,network,visualization,layout,machine learning,deep learning,neural network,generative model,autoencoder", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934396", "refList": ["10.1103/physreve.74.036104", "10.1145/234535.234538", "10.2307/2412323", "10.1109/tvcg.2007.70580", "10.7155/jgaa.00405", "10.1177/1473871612455749", "10.1109/tvcg.2011.185", "10.1073/pnas.122653799", "10.1007/3-540-58950-3", "10.1145/2897824.2925974", "10.1007/3-540-44541-2\\_17", "10.1007/bf00410640", "10.1021/acscentsci.7b00572", "10.1007/s10208-011-9093-5", "10.1109/tvcg.2015.2467451", "10.1016/0020-0190(89)90102-6", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13187", "10.1214/aoms/1177729586", "10.1109/tvcg.2014.2346277", "10.1109/tvcg.2017.2743858", "10.1007/978-3-662-44043-8\\_3", "10.1038/30918", "10.1109/mcg.2018.2881501", "10.1016/j.camwa.2004.08.015", "10.1109/tvcg.2010.269", "10.1006/s1045-926x(02)00016-2", "10.1016/0925-7721(94)00014-x", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1103/physrevx.4.011047", "10.1371/journal.pone.0098679", "10.1145/2487788.2488173", "10.1007/978-3-030-01418-6\\_41", "10.1007/978-3-030-04414-5\\_12", "10.1109/tvcg.2018.2865139", "10.1142/s0219525903001067", "10.7155/jgaa.00051"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030367", "title": "Lyra 2: Designing Interactive Visualizations by Demonstration", "year": "2020", "conferenceName": "InfoVis", "authors": "Jonathan Zong;Dhiraj Barnwal;Rupayan Neogy;Arvind Satyanarayan", "citationCount": "0", "affiliation": "Zong, J (Corresponding Author), MIT, Cambridge, MA 02139 USA. Zong, Jonathan; Neogy, Rupayan; Satyanarayan, Arvind, MIT, Cambridge, MA 02139 USA. Barnwal, Dhiraj, Indian Inst Technol Kharagpur, Kharagpur, W Bengal, India.", "countries": "India;USA", "abstract": "Recent graphical interfaces offer direct manipulation mechanisms for authoring visualizations, but are largely restricted to static output. To author interactive visualizations, users must instead turn to textual specification, but such approaches impose a higher technical burden. To bridge this gap, we introduce Lyra 2, a system that extends a prior visualization design environment with novel methods for authoring interaction techniques by demonstration. Users perform an interaction (e.g., button clicks, drags, or key presses) directly on the visualization they are editing. The system interprets this performance using a set of heuristics and enumerates suggestions of possible interaction designs. These heuristics account for the properties of the interaction (e.g., target and event type) as well as the visualization (e.g., mark and scale types, and multiple views). Interaction design suggestions are displayed as thumbnails; users can preview and test these suggestions, iteratively refine them through additional demonstrations, and finally apply and customize them via property inspectors. We evaluate our approach through a gallery of diverse examples, and evaluate its usability through a first-use study and via an analysis of its cognitive dimensions. We find that, in Lyra 2, interaction design by demonstration enables users to rapidly express a wide range of interactive visualizations.", "keywords": "Direct manipulation,interactive visualization,interaction design by demonstration", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030367", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030467", "title": "PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch;Lingyun Yu;Peiran Ren;Thomas Ertl;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Knittel, Johannes; Koch, Steffen; Ertl, Thomas, Univ Stuttgart, VIS VISUS, Stuttgart, Germany. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Dept Comp Sci \\& Software Engn, Suzhou, Peoples R China. Ren, Peiran, Alibaba Grp, Hangzhou, Peoples R China.", "countries": "Germany;China", "abstract": "Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.", "keywords": "Storyline visualization,reinforcement learning,mixed-initiative design", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030467", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030459", "title": "Scalability of Network Visualisation from a Cognitive Load Perspective", "year": "2020", "conferenceName": "InfoVis", "authors": "Vahan Yoghourdjian;Yalong Yang;Tim Dwyer;Lawrence Lee;Michael Wybrow;Kim Marriott", "citationCount": "0", "affiliation": "Yoghourdjian, V (Corresponding Author), Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Yoghourdjian, Vahan; Yang, Yalong; Dwyer, Tim; Wybrow, Michael; Marriott, Kim, Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Lawrence, Lee, Monash Univ, Fac Business \\& Econ, Melbourne, Vic, Australia. Yang, Yalong, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA.", "countries": "USA;Australia", "abstract": "Node-link diagrams are widely used to visualise networks. However, even the best network layout algorithms ultimately result in \u2018hairball\u2019 visualisations when the graph reaches a certain degree of complexity, requiring simplification through aggregation or interaction (such as filtering) to remain usable. Until now, there has been little data to indicate at what level of complexity node-link diagrams become ineffective or how visual complexity affects cognitive load. To this end, we conducted a controlled study to understand workload limits for a task that requires a detailed understanding of the network topology-finding the shortest path between two nodes. We tested performance on graphs with 25 to 175 nodes with varying density. We collected performance measures (accuracy and response time), subjective feedback, and physiological measures (EEG, pupil dilation, and heart rate variability). To the best of our knowledge this is the first network visualisation study to include physiological measures. Our results show that people have significant difficulty finding the shortest path in high density node-link diagrams with more than 50 nodes and even low density graphs with more than 100 nodes. From our collected EEG data we observe functional differences in brain activity between hard and easy tasks. We found that cognitive load increased up to certain level of difficulty after which it decreased, likely because participants had given up. We also explored the effects of global network layout features such as size or number of crossings, and features of the shortest path such as length or straightness on task difficulty. We found that global features generally had a greater impact than those of the shortest path.", "keywords": "Data Visualisation,Network Visualisation,Cognitive Load,EEG", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030459", "refList": ["10.1109/tvcg.2019.2934396", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2016.2570755", "10.1007/s00371-013-0892-3", "10.1007/b98835", "10.1109/isda.2014.7066252", "10.1109/tvcg.2015.2467251", "10.1177/1473871612455749", "10.1109/tvcg.2012.299", "10.1007/3-540-58950-3", "10.1111/cgf.12878", "10.1109/mcse.2007.55", "10.1109/tvcg.2012.238", "10.1145/264645.264657", "10.1109/tvcg.2015.2467451", "10.1109/tvcg.2013.151", "10.1016/0020-0190(89)90102-6", "10.1109/tvcg.2019.2934307", "10.1109/tvcg.2015.2468151", "10.1109/tvcg.2017.2745919", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13440", "10.1109/tvcg.2011.220", "10.1111/cgf.13187", "10.1109/t-c.1969.222678", "10.1109/tvcg.2017.2743858", "10.1126/science.290.5500.2319", "10.1109/cahpc.2018.8645912", "10.1109/tvcg.2015.2465151", "10.1109/tvcg.2016.2598958", "10.1109/tvcg.2017.2751473", "10.1002/spe.4380211102", "10.1109/tpds.2018.2869805", "10.1016/j.jpdc.2019.04.008", "10.1109/pacificvis.2017.8031574", "10.1006/s1045-926x(02)00016-2", "10.1109/tvcg.2017.2674999", "10.1145/2872427.2883041", "10.1145/3292500.3330989", "10.1109/tvcg.2017.2744878", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1109/sbac-pad.2018.00060", "10.1007/978-3-662-45803-7\\_27", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1111/j.1469-1809.1936.tb02137.x", "10.1007/bf02289565", "10.1109/tvcg.2017.2689016", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.4722", "year": "2020", "title": "A Study of Mental Maps in Immersive Network Visualization", "conferenceName": "PacificVis", "authors": "Joseph Kotlarek;Oh{-}Hyun Kwon;Kwan{-}Liu Ma;Peter Eades;Andreas Kerren;Karsten Klein;Falk Schreiber", "citationCount": "0", "affiliation": "Kotlarek, J (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA.\nKotlarek, Joseph; Kwon, Oh-Hyun; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.\nEades, Peter, Univ Sydney, Sydney, NSW, Australia.\nKerren, Andreas, Linnaeus Univ, Vaxjo, Sweden.\nKlein, Karsten; Schreiber, Falk, Univ Konstanz, Constance, Germany.", "countries": "Sweden;Germany;USA;Australia", "abstract": "The visualization of a network influences the quality of the mental map that the viewer develops to understand the network. In this study, we investigate the effects of a 3D immersive visualization environment compared to a traditional 2D desktop environment on the comprehension of a network's structure. We compare the two visualization environments using three tasks-interpreting network structure, memorizing a set of nodes, and identifying the structural changes-commonly used for evaluating the quality of a mental map in network visualization. The results show that participants were able to interpret network structure more accurately when viewing the network in an immersive environment, particularly for larger networks. However, we found that 2D visualizations performed better than immersive visualization for tasks that required spatial memory.", "keywords": "Human-centered computing; Visualization; Visualization techniques; Graph drawings; Human-centered computing; Visualization; Empirical studies in visualization", "link": "https://doi.org/10.1109/PacificVis48177.2020.4722", "refList": ["10.1103/physreve.74.036104", "10.1109/tvcg.2019.2934396", "10.1007/978-3-540-87730-1\\_9", "10.1117/12.2005484", "10.1177/1473871612455749", "10.1109/pacificvis.2017.8031577", "10.1007/978-3-319-73207-7", "10.1109/38.888006", "10.1109/2945.841119", "10.1109/mc.2005.297", "10.1007/s10055-018-0346-3", "10.1109/tvcg.2016.2599107", "10.1109/icsmc.1992.271688", "10.1038/30918", "10.1006/jvlc.1995.1010", "10.1089/109493101300117938", "10.1109/vrais.1998.658488", "10.1109/pacificvis.2015.7156357", "10.1109/tvcg.2010.78", "10.1109/tvcg.2016.2520921", "10.1007/978-3-030-01388-22", "10.1145/229459.229467", "10.1145/1056808.1056875", "10.1109/tvcg.2017.2744079", "10.1109/bdva.2015.7314293", "10.1086/jar.33.4.3629752", "10.1016/j.ijhcs.2013.08.004"], "wos": 1, "children": [], "len": 1}], "len": 11}, {"doi": "10.1109/tvcg.2020.3030440", "title": "Context-aware Sampling of Large Networks via Graph Representation Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Zhiguang Zhou;Chen Shi;Xilong Shen;Lihong Cai;Haoxuan Wang;Yuhua Liu;Ying Zhao;Wei Chen", "citationCount": "0", "affiliation": "Zhao, Y (Corresponding Author), Cent South Univ, Changsha, Peoples R China. Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Zhou, Zhiguang; Shi, Chen; Shen, Xilong; Cai, Lihong; Wang, Haoxuan; Liu, Yuhua, Zhejiang Univ Finance \\& Econ, Sch Informat, Hangzhou, Peoples R China. Zhao, Ying, Cent South Univ, Changsha, Peoples R China. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China.", "countries": "China", "abstract": "Numerous sampling strategies have been proposed to simplify large-scale networks for highly readable visualizations. It is of great challenge to preserve contextual structures formed by nodes and edges with tight relationships in a sampled graph, because they are easily overlooked during the process of sampling due to their irregular distribution and immunity to scale. In this paper, a new graph sampling method is proposed oriented to the preservation of contextual structures. We first utilize a graph representation learning (GRL) model to transform nodes into vectors so that the contextual structures in a network can be effectively extracted and organized. Then, we propose a multi-objective blue noise sampling model to select a subset of nodes in the vectorized space to preserve contextual structures with the retention of relative data and cluster densities in addition to those features of significance, such as bridging nodes and graph connections. We also design a set of visual interfaces enabling users to interactively conduct context-aware sampling, visually compare results with various sampling strategies, and deeply explore large networks. Case studies and quantitative comparisons based on real-world datasets have demonstrated the effectiveness of our method in the abstraction and exploration of large networks.", "keywords": "Graph sampling,Graph representation learning,Blue noise sampling,Graph evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030440", "refList": ["10.1145/2491159.2491168", "10.1016/j.physa.2015.04.035", "10.1145/1830252.1830274", "10.1109/icdmw.2007.91", "10.1002/net.21834", "10.1109/tvcg.2018.2864503", "10.1016/j.cag.2018.01.010", "10.1109/icc.2016.7511156", "10.1111/cgf.13444", "10.1145/956750.956831", "10.1145/364099.364331", "10.1007/s00180-016-0663-5", "10.1109/tvcg.2013.223", "10.1007/s12650-018-0530-2", "10.1103/physreve.73.016102", "10.1109/access.2018.2870684", "10.1007/978-3-319-06793-3\\_1", "10.2312/vissym/eurovis05/239-246", "10.1016/j.ins.2015.02.014", "10.1145/2339530.2339723", "10.1109/icde.2015.7113345", "10.1109/tvcg.2011.233", "10.14778/2809974.2809980", "10.1109/glocom.2015.7417471", "10.1145/2578153.2578175", "10.1214/aoms/1177705148", "10.1109/tvcg.2008.130", "10.14232/actacyb.20.1.2011.6", "10.1504/ijitm.2019.099809", "10.1109/tvcg.2018.2865020", "10.1145/956750", "10.1002/cpe.4330060203", "10.1145/1150402.1150479", "10.1103/physreve.72.036118", "10.1109/tvcg.2017.2744098", "10.1145/2020408.2020512", "10.1142/s0129183114400075", "10.1109/jsac.2011.111005", "10.1016/j.camwa.2011.11.057", "10.1145/2470654.2466444", "10.1109/tvcg.2017.2674999", "10.1214/aos/1013203451", "10.1109/icdcsw.2011.34", "10.1016/j.physa.2013.11.015", "10.1145/1081870.1081893", "10.1109/tnet.2008.2001730", "10.1109/access.2016.2633485", "10.1145/1879141.1879192", "10.1371/journal.pone.0098679", "10.1126/science.220.4598.671", "10.1109/pacificvis.2015.7156355", "10.1088/1475-7516/2011/08/011", "10.1007/978-3-319-27261-0\\_41", "10.1111/cgf.13410", "10.1109/tvcg.2018.2865139", "10.1109/tvcg.2016.2598831", "10.1016/j.physa.2014.06.065"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030447", "title": "DRGraph: An Efficient Graph Layout Algorithm for Large-scale Graphs by Dimensionality Reduction", "year": "2020", "conferenceName": "InfoVis", "authors": "Minfeng Zhu;Wei Chen;Yuanzhe Hu;Yuxuan Hou;Liangjun Liu;Kaiyuan Zhang", "citationCount": "0", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Zhu, Minfeng; Chen, Wei; Hu, Yuanzhe; Hou, Yuxuan; Liu, Liangjun; Zhang, Kaiyuan, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China.", "countries": "China", "abstract": "Efficient layout of large-scale graphs remains a challenging problem: the force-directed and dimensionality reduction-based methods suffer from high overhead for graph distance and gradient computation. In this paper, we present a new graph layout algorithm, called DRGraph, that enhances the nonlinear dimensionality reduction process with three schemes: approximating graph distances by means of a sparse distance matrix, estimating the gradient by using the negative sampling technique, and accelerating the optimization process through a multi-level layout scheme. DRGraph achieves a linear complexity for the computation and memory consumption, and scales up to large-scale graphs with millions of nodes. Experimental results and comparisons with state-of-the-art graph layout methods demonstrate that DRGraph can generate visually comparable layouts with a faster running time and a lower memory requirement.", "keywords": "graph visualization,graph layout,dimensionality reduction,force-directed layout", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030447", "refList": ["10.1109/tvcg.2007.70582", "10.1371/journal.pone.0136497", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/3018661.3018731", "10.1109/34.491619", "10.1145/2939672.2939754", "10.1145/3269206.3271788", "10.1016/j.comnet.2011.08.019", "10.1007/s11263-011-0442-2", "10.1016/s0020-0190(98)00108-2", "10.1109/tvcg.2018.2865151", "10.1007/978-3-319-61188-4\\_2", "10.1177/1473871612455749", "10.1177/1473871616666394", "10.1109/tvcg.2015.2467035", "10.1186/1471-2105-10-375", "10.1145/3097983.3098061", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1109/tvcg.2015.2467451", "10.1109/2945.841119", "10.1016/j.swevo.2015.10.002", "10.1016/0020-0190(89)90102-6", "10.1109/infvis.2003.1249009", "10.1109/tvcg.2017.2745919", "10.1111/cgf.13187", "10.1111/cgf.13440", "10.1109/tvcg.2012.245", "10.1109/tvcg.2017.2743858", "10.1177/1473871618821740", "10.1186/s12859-015-0585-1", "10.1002/nav.3800020109", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1006/s1045-926x(02)00016-2", "10.1109/cvpr.2012.6247667", "10.1023/b:jogo.0000042115.44455.f3", "10.1109/pacificvis.2017.8031607", "10.1002/nav.3800030404", "10.1109/cvpr.2008.4587500", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1090/s0002-9904-1920-03322-7", "10.1109/iv.2013.3", "10.1145/568522.568523", "10.1109/tvcg.2006.156", "10.1109/tvcg.2012.236", "10.1109/tvcg.2018.2865139", "10.1145/3219819.3220025", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030393", "title": "Exemplar-based Layout Fine-tuning for Node-link Diagrams", "year": "2020", "conferenceName": "InfoVis", "authors": "Jiacheng Pan;Wei Chen;Xiaodong Zhao;Shuyue Zhou;Wei Zeng;Minfeng Zhu;Jian Chen;Siwei Fu;Yingcai Wu", "citationCount": "1", "affiliation": "Chen, W; Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Lab, Hangzhou, Peoples R China. Pan, Jiacheng; Chen, Wei; Zhao, Xiaodong; Zhou, Shuyue; Zhu, Minfeng; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Fu, Siwei; Wu, Yingcai, Zhejiang Lab, Hangzhou, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Jian, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA;China", "abstract": "We design and evaluate a novel layout fine-tuning technique for node-link diagrams that facilitates exemplar-based adjustment of a group of substructures in batching mode. The key idea is to transfer user modifications on a local substructure to other substructures in the entire graph that are topologically similar to the exemplar. We first precompute a canonical representation for each substructure with node embedding techniques and then use it for on-the-fly substructure retrieval. We design and develop a light-weight interactive system to enable intuitive adjustment, modification transfer, and visual graph exploration. We also report some results of quantitative comparisons, three case studies, and a within-participant user study.", "keywords": "Node-link diagram,graph layout,graph visualization,user interactions", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030393", "refList": ["10.1109/tvcg.2007.70582", "10.1371/journal.pone.0136497", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/3018661.3018731", "10.1109/34.491619", "10.1145/2939672.2939754", "10.1145/3269206.3271788", "10.1016/j.comnet.2011.08.019", "10.1007/s11263-011-0442-2", "10.1016/s0020-0190(98)00108-2", "10.1109/tvcg.2018.2865151", "10.1177/1473871612455749", "10.1177/1473871616666394", "10.1109/tvcg.2015.2467035", "10.1186/1471-2105-10-375", "10.1145/3097983.3098061", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1109/tvcg.2015.2467451", "10.1016/j.swevo.2015.10.002", "10.1016/0020-0190(89)90102-6", "10.1109/infvis.2003.1249009", "10.1109/tvcg.2017.2745919", "10.1111/cgf.13187", "10.1111/cgf.13440", "10.1109/tvcg.2012.245", "10.1109/tvcg.2017.2743858", "10.1177/1473871618821740", "10.1186/s12859-015-0585-1", "10.1002/nav.3800020109", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1006/s1045-926x(02)00016-2", "10.1109/cvpr.2012.6247667", "10.1023/b:jogo.0000042115.44455.f3", "10.1109/pacificvis.2017.8031607", "10.1002/nav.3800030404", "10.1109/infvis.2004.1", "10.1109/cvpr.2008.4587500", "10.1109/pacificvis.2011.5742389", "10.1140/epjb/e2011-10979-2", "10.1371/journal.pone.0098679", "10.1090/s0002-9904-1920-03322-7", "10.1109/iv.2013.3", "10.1145/568522.568523", "10.1109/tvcg.2006.156", "10.1109/tvcg.2012.236", "10.1109/tvcg.2018.2865139", "10.1145/3219819.3220025", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}], "len": 19}, {"doi": "10.1109/tvcg.2019.2934263", "title": "R-Map: A Map Metaphor for Visualizing Information Reposting Process in Social Media", "year": "2019", "conferenceName": "VAST", "authors": "Shuai Chen;Sihang Li;Siming Chen;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Chen, Shuai; Li, Sihang, Peking Univ, Sch EECS, Minist Educ, Key Lab Machine Petrept, Beijing, Peoples R China. Yuan, Xiaoru, Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Chen, Siming, Fraunhofer Inst IAIS, St Augustin, Germany. Chen, Siming, Univ Bonn, Bonn, Germany.", "countries": "Germany;China", "abstract": "We propose R-Map (Reposting Map), a visual analytical approach with a map metaphor to support interactive exploration and analysis of the information reposting process in social media. A single original social media post can cause large cascades of repostings (i.e., retweets) on online networks, involving thousands, even millions of people with different opinions. Such reposting behaviors form the reposting tree, in which a node represents a message and a link represents the reposting relation. In R-Map, the reposting tree structure can be spatialized with highlighted key players and tiled nodes. The important reposting behaviors, the following relations and the semantics relations are represented as rivers, routes and bridges, respectively, in a virtual geographical space. R-Map supports a scalable overview of a large number of information repostings with semantics. Additional interactions on the map are provided to support the investigation of temporal patterns and user behaviors in the information diffusion process. We evaluate the usability and effectiveness of our system with two use cases and a formal user study.", "keywords": "Social Media,Information Diffusion,Map-like Visual Metaphor", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934263", "refList": ["10.1109/infvis.2000.885091", "10.1109/pacificvis.2010.5429590", "10.1111/j.0020-2754.1998.00269.x", "10.1109/vast.2017.8585638", "10.1145/2700398", "10.1109/tmm.2016.2614229", "10.1109/visual.1991.175815", "10.1145/3183347", "10.1109/infvis.2001.963290", "10.1109/vast.2016.7883510", "10.1109/access.2016.2605009", "10.1109/mcg.2011.103", "10.1145/1124772.1124851", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2010.79", "10.1109/infvis.2000.885095", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1002/(sici)1097-0266(199606)17:6", "10.5670/oceanog.2016.66", "10.1559/152304003100011081", "10.1109/tit.1982.1056489", "10.1559/152304098782383034", "10.1109/tvcg.2014.2346922", "10.1007/978-3-540-85567-5\\_9", "10.1145/2488388.2488504", "10.1109/38.974518", "10.1109/bigdata.2013.6691714", "10.1109/pacificvis.2014.38", "10.1109/tvcg.2012.291", "10.1109/infvis.2005.1532128", "10.2307/2685881", "10.1007/1-4020-4179-9\\_91", "10.1109/infvis.1999.801860", "10.1109/asonam.2011.37"], "wos": 1, "children": [{"doi": "10.1111/cgf.14031", "year": "2020", "title": "The State of the Art in Map-Like Visualization", "conferenceName": "EuroVis", "authors": "Marius Hogr{\\\"{a}}fer;Magnus Heitzler;Hans{-}J{\\\"{o}}rg Schulz", "citationCount": "0", "affiliation": "Hografer, M (Corresponding Author), Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHografer, Marius; Schulz, Hans-Jorg, Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHeitzler, Magnus, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Denmark", "abstract": "Cartographic maps have been shown to provide cognitive benefits when interpreting data in relation to a geographic location. In visualization, the term map-like describes techniques that incorporate characteristics of cartographic maps in their representation of abstract data. However, the field of map-like visualization is vast and currently lacks a clear classification of the existing techniques. Moreover, choosing the right technique to support a particular visualization task is further complicated, as techniques are scattered across different domains, with each considering different characteristics as map-like. In this paper, we give an overview of the literature on map-like visualization and provide a hierarchical classification of existing techniques along two general perspectives: imitation and schematization of cartographic maps. Each perspective is further divided into four principal categories that group common map-like techniques along the visual primitives they affect. We further discuss this classification from a task-centered view and highlight open research questions.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14031", "refList": ["10.1111/j.0020-2754.1998.00269.x.21", "10.1109/iv.2005.26", "10.2307/2980460", "10.1109/tvcg.2017.2743959", "10.1016/j.jvlc.2011.11.004", "10.1145/2501988.2502046", "10.1559/1523040042742402", "10.1007/s00779-011-0500-3", "10.1111/cgf.12932", "10.1109/tvcg.2010.89", "10.1111/cgf.13200", "10.1080/15230406.2016.1160797", "10.1179/000870410x12825500202896", "10.1109/tvcg.2007.70596", "10.5167/80972.19uzh-80972", "10.1007/978-3-319-11593-1\\_2", "10.1037/aca0000175", "10.1080/17538947.2014.923942", "10.1109/tvcg.2015.2467321", "10.1109/vissof.2005.1684299", "10.1559/152304098782383034", "10.1007/978-3-642-36763-2\\_38", "10.1109/tvcg.2013.130", "10.1109/tvcg.2014.2346274", "10.1111/cgf.12648", "10.1111/0004-5608.00242", "10.1177/1473871617724212", "10.1109/iv.2004.1320123", "10.1371/journal.pcbi.1006907", "10.1109/tvcg.2016.2599030", "10.1559/1523040042742411", "10.3390/informatics5030031", "10.1111/cgf.13079", "10.1109/tvcg.2015.2467811", "10.1016/j.tics.2003.12.004", "10.1007/s00799-016-0168-4", "10.1109/infvis.2004.57", "10.1007/s10021-007-9038-7", "10.1109/pacificvis.2015.7156366", "10.1007/978-3-319-27261-0\\_1", "10.1007/978-1-4471-2804-5\\_6", "10.1109/access.2019.2939977", "10.1109/tvcg.2017.2747545", "10.1109/icdm.2003.1250978", "10.1111/cgf.13167", "10.1016/0010-0285(78)90006-3", "10.3138/nj8v-8514-871t-221k", "10.1016/j.cag.2009.06.002", "10.1111/j.1467-8659.2011.01937.x", "10.1016/b978-044451020-4/50035-9", "10.1111/1467-8659.00566", "10.5220/0006618101080119", "10.1109/pacificvis.2012.6183571", "10.1145/2038558.2038579", "10.1109/tvcg.2010.191", "10.1111/j.0033-0124.1985.00075.x", "10.1109/tvcg.2004.1260761", "10.1007/978-3-642-34848-8\\_6", "10.1145/2968220.2968239", "10.1145/3002151.3002160", "10.1109/tst.2013.6509098", "10.1109/tvcg.2013.91", "10.1177/1473871615597077", "10.1016/j.jvlc.2011.02.001", "10.1080/17445647.2014.935502", "10.1177/1687814017740710", "10.1111/1744-7917.12601", "10.1073/pnas.0400280101", "10.1111/cgf.13672", "10.1109/mcg.2006.90", "10.1002/asi.21712", "10.1007/978-3-642-33024-7\\_3", "10.1145/2801040.2801056", "10.1109/mcg.2010.101", "10.1179/003962607x165041", "10.1057/palgrave.ivs.9500039", "10.1109/vast.2016.7883510", "10.1559/152304009788988288", "10.1080/23729333.2017.1301346", "10.1109/tvcg.2013.66", "10.1111/j.1467-8659.2012.03085.x", "10.1109/tvcg.2011.288", "10.3390/ijgi9040253", "10.1109/infvis.2005.1532150", "10.1145/2254556.2254636", "10.20382//jocg.v4i1a9", "10.1016/0010-0285(81)90016-5", "10.1145/2556288.2557224", "10.1109/iv.2001.942043", "10.1021/ed1000203", "10.1016/0169-7439(87)80084-9", "10.1109/tvcg.2010.154", "10.1016/j.jvlc.2015.10.003", "10.1109/tvcg.2019.2903945", "10.1109/tvcg.2013.120", "10.1109/tvcg.2019.2934263", "10.1146/annurev-ecolsys-102209-144718", "10.1109/tvcg.2008.165", "10.3138/a477-3202-7876-n514", "10.1080/23729333.2017.1288535", "10.1111/j.0020-2754.1998.00269.x", "10.1109/mcg.2004.41", "10.1109/vast.2009.5332593", "10.1002/smr.414", "10.1007/s12650-019-00584-3", "10.1145/22949.22950", "10.1179/1743277413y.0000000036", "10.1080/15230406.2016.1262280", "10.1016/s0341-8162(01)00164-3", "10.22224/gistbok/2017.3.8", "10.1109/tvcg.2008.155", "10.1057/ivs.2008.31", "10.1016/j.cag.2004.03.012", "10.1179/1743277412y.0000000007", "10.1016/j.soncn.2011.02.001", "10.1179/caj.1987.24.1.27", "10.3138/carto.48.3.1691", "10.5220/0004267205150524", "10.1109/38.974518", "10.1145/102377.115768", "10.1057/palgrave.ivs.9500186", "10.1109/5.58325", "10.5167/80972.uzh-80972", "10.1177/030913339602000204", "10.1007/978-3-642-22300-6\\_14", "10.1109/iv.2004.1320189", "10.1111/cgf.13447", "10.1007/s11192-017-2596-3", "10.1559/1523040053722150", "10.1007/978-3-662-45803-7\\_34"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934266", "title": "VASSL: A Visual Analytics Toolkit for Social Spambot Labeling", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;Jieqiong Zhao;David S. Ebert", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Zhao, Jieqiong; Ebert, David S., Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Purdue Univ, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Social media platforms are filled with social spambots. Detecting these malicious accounts is essential, yet challenging, as they continually evolve to evade detection techniques. In this article, we present VASSL, a visual analytics system that assists in the process of detecting and labeling spambots. Our tool enhances the performance and scalability of manual labeling by providing multiple connected views and utilizing dimensionality reduction, sentiment analysis and topic modeling, enabling insights for the identification of spambots. The system allows users to select and analyze groups of accounts in an interactive manner, which enables the detection of spambots that may not be identified when examined individually. We present a user study to objectively evaluate the performance of VASSL users, as well as capturing subjective opinions about the usefulness and the ease of use of the tool.", "keywords": "Spambot,Labeling,Detection,Visual Analytics,Social Media Annotation", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934266", "refList": ["10.7326/0003-4819-110-11-916", "10.1109/tifs.2013.2267732", "10.1111/cgf.12106", "10.1016/j.ins.2013.11.016", "10.1109/tvcg.2017.2752166", "10.1109/vast.2016.7883510", "10.1109/vl.1996.545307", "10.1145/2872518.2889302", "10.1109/tmm.2016.2614220", "10.1145/2818717", "10.1109/tdsc.2017.2681672", "10.1111/cgf.13211", "10.2307/2685478", "10.1109/tvcg.2014.2346920", "10.1109/tdsc.2016.2641441", "10.1109/tvcg.2017.2745080", "10.1109/vast.2012.6400557", "10.1109/asonam.2016.7752287", "10.1109/tvcg.2014.2346922", "10.1111/cgf.13217", "10.2307/249008", "10.1109/tvcg.2017.2711030", "10.1109/mcse.2013.70", "10.1109/tvcg.2015.2467196", "10.1016/j.comcom.2013.04.004", "10.1109/asonam.2014.6921650", "10.1109/mc.2016.183", "10.1126/science.290.5500.2323", "10.1145/3041021.3055135", "10.1162/jmlr.2003.3.4-5.993", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2013.153", "10.1109/iv.2008.89", "10.1109/mcom.2013.6588663", "10.1179/000870403235002042", "10.1145/3047010"], "wos": 1, "children": [{"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934655", "title": "Visual Analytics for Electromagnetic Situation Awareness in Radio Monitoring and Management", "year": "2019", "conferenceName": "VAST", "authors": "Ying Zhao;Xiaobo Luo;Xiaoru Lin;Hairong Wang;Xiaoyan Kui;Fangfang Zhou;Jinsong Wang;Yi Chen 0007;Wei Chen", "citationCount": "3", "affiliation": "Kui, XY; Zhou, FF (Corresponding Author), Cent S Univ, Sch Comp Sci \\& Engn, Changsha, Hunan, Peoples R China. Zhao, Ying; Luo, Xiaobo; Lin, Xiaoru; Kui, Xiaoyan; Zhou, Fangfang, Cent S Univ, Sch Comp Sci \\& Engn, Changsha, Hunan, Peoples R China. Wang, Hairong, Cent S Univ, Sch Automat, Changsha, Hunan, Peoples R China. Wang, Jinsong, Southwest Elect \\& Telecom Engn Inst, Shanghai, Peoples R China. Chen, Yi, Beijing Technol \\& Business Univ, Beijing Key Lab Big Data Technol Food Safety, Beijing, Peoples R China. Chen, Wei, Zhejiang Univ, Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China.", "countries": "China", "abstract": "Traditional radio monitoring and management largely depend on radio spectrum data analysis, which requires considerable domain experience and heavy cognition effort and frequently results in incorrect signal judgment and incomprehensive situation awareness. Faced with increasingly complicated electromagnetic environments, radio supervisors urgently need additional data sources and advanced analytical technologies to enhance their situation awareness ability. This paper introduces a visual analytics approach for electromagnetic situation awareness. Guided by a detailed scenario and requirement analysis, we first propose a signal clustering method to process radio signal data and a situation assessment model to obtain qualitative and quantitative descriptions of the electromagnetic situations. We then design a two-module interface with a set of visualization views and interactions to help radio supervisors perceive and understand the electromagnetic situations by a joint analysis of radio signal data and radio spectrum data. Evaluations on real-world data sets and an interview with actual users demonstrate the effectiveness of our prototype system. Finally, we discuss the limitations of the proposed approach and provide future work directions.", "keywords": "Radio monitoring and management,radio signal data,radio spectrum data,situation awareness,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934655", "refList": ["10.1016/j.newast.2010.07.009", "10.1109/tvcg.2017.2744459", "10.1109/tvcg.2018.2864503", "10.1145/1029208.1029219", "10.1109/pacificvis.2014.54", "10.1109/tvcg.2018.2829750", "10.1016/j.jvlc.2017.11.004", "10.1109/tcyb.2015.2448236", "10.1016/j.patrec.2017.11.011", "10.1109/tvcg.2015.2505305", "10.1109/vast.2014.7042528", "10.1007/s12650-018-0530-2", "10.1109/tvcg.2018.2816203", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2017.2745180", "10.1109/mcg.2018.2879067", "10.1109/tvcg.2018.2859973", "10.1109/tvcg.2016.2598460", "10.1109/infvis.2005.1532134", "10.1109/tvcg.2018.2851227", "10.1155/2012/920671", "10.1109/vast.2014.7042479", "10.1109/tvcg.2013.228", "10.1109/tvcg.2018.2865077", "10.1016/j.csda.2005.10.001", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2011.239", "10.1145/3173574.3174237", "10.1007/s13042-016-0603-2", "10.1109/tvcg.2018.2865020", "10.1017/s1041610219000024", "10.1109/2945.981848", "10.1109/tvcg.2010.193", "10.1109/tvcg.2018.2802520", "10.1145/3200766", "10.1109/wcncw.2015.7122557", "10.1145/3006299.3006312", "10.1109/icsssm.2007.4280175", "10.1126/science.1242072", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2015.2467196", "10.1109/tvcg.2016.2598619", "10.1109/tvcg.2016.2598664", "10.1109/tvcg.2013.196", "10.1109/comst.2016.2631080", "10.1109/tvcg.2018.2865029", "10.1007/s10816-016-9307-x", "10.1109/tvcg.2008.166", "10.1109/tvcg.2017.2758362", "10.1109/vizsec.2005.1532072", "10.1518/001872095779049543", "10.1109/tvcg.2007.70415", "10.1016/j.ins.2018.01.013", "10.1109/tvcg.2014.2346911", "10.1109/mim.2013.6616284", "10.1109/jsyst.2014.2358997", "10.1111/cgf.12910", "10.1109/isi.2009.5137305", "10.1109/tvcg.2011.179", "10.1007/s11277-015-2631-8", "10.1111/cgf.12396", "10.1109/tvcg.2013.2297933", "10.1109/tvcg.2014.2346926", "10.1109/tvcg.2014.2346913", "10.1109/pacificvis.2018.00030", "10.1109/tvcg.2014.2346433", "10.1109/tvcg.2016.2614803"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13211", "year": "2017", "title": "Social Media Visual Analytics", "conferenceName": "EuroVis", "authors": "Siming Chen;Lijing Lin;Xiaoru Yuan", "citationCount": "29", "affiliation": "Chen, SM (Corresponding Author), Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.\nChen, SM (Corresponding Author), Peking Univ, Sch EECS, Beijing, Peoples R China.\nChen, Siming; Lin, Lijing; Yuan, Xiaoru, Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.\nChen, Siming; Lin, Lijing; Yuan, Xiaoru, Peking Univ, Sch EECS, Beijing, Peoples R China.", "countries": "China", "abstract": "With the development of social media (e.g. Twitter, Flickr, Foursquare, Sina Weibo, etc.), a large number of people are now using them and post microblogs, messages and multi-media information. The everyday usage of social media results in big open social media data. The data offer fruitful information and reflect social behaviors of people. There is much visualization and visual analytics research on such data. We collect state-of-the-art research and put it into three main categories: social network, spatial temporal information and text analysis. We further summarize the visual analytics pipeline for the social media, combining the above categories and supporting complex tasks. With these techniques, social media analytics can apply to multiple disciplines. We summarize the applications and public tools to further investigate the challenges and trends.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13211", "refList": ["10.1016/j.cag.2013.11.003", "10.1109/vast.2010.5652922", "10.1007/s12650-015-0277-y", "10.1007/978-3-540-70956-5", "10.1007/s00146-014-0549-4", "10.1109/tmm.2014.2340133", "10.1109/tmm.2015.2510329", "10.1057/palgrave.ivs.9500116", "10.1109/tvcg.2010.129", "10.1109/tvcg.2016.2598590", "10.1145/2065023.2065041", "10.1109/tvcg.2013.162", "10.1145/1963405.1963504", "10.1177/1473871613490678", "10.1007/978-0-85729-436-4\\_9", "10.1145/2488388.2488504", "10.1109/bigdata.2015.7363826", "10.1109/tvcg.2009.171", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/tvcg.2014.2371856", "10.1145/2089094.2089102", "10.1109/tvcg.2014.2346919", "10.1109/tvcg.2014.2346433", "10.1109/tvcg.2015.2509990", "10.1109/pacificvis.2015.7156366", "10.1016/j.jocs.2010.12.007", "10.1109/tvcg.2011.169", "10.1109/tvcg.2015.2467619", "10.1016/j.cag.2013.10.008", "10.1007/978-3-319-06793-3\\_3", "10.1109/tmm.2009.2012912", "10.1109/pacificvis.2015.7156367", "10.1109/tvcg.2014.2346922", "10.1371/journal.pone.0101837", "10.1109/tmm.2015.2425143", "10.1145/2493102.2493108", "10.1109/tvcg.2015.2467196", "10.1109/tvcg.2013.196", "10.1109/mcg.2014.61", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2006.160", "10.1057/palgrave.ivs.9500092", "10.1016/j.giq.2012.06.002", "10.1016/b978-0-12-382229-1.00002-3", "10.1109/vast.2016.7883510", "10.1111/j.1467-8659.2012.03120.x", "10.1109/tmm.2016.2614220", "10.7155/jgaa.00302", "10.1109/tvcg.2014.2346920", "10.1109/vast.2015.7347631", "10.1371/journal.pone.0095043", "10.1109/vast.2014.7042495", "10.1511/2001.4.344", "10.1109/vast.2012.6400557", "10.1109/pacificvis.2012.6183572", "10.1145/2567948.2577020", "10.1016/j.joi.2014.07.006", "10.1111/j.1467-8659.2009.01687.x", "10.1145/2801040.2801054", "10.1016/j.bushor.2009.09.003", "10.1109/infvis.2000.885098", "10.1109/ldav.2013.6675163", "10.1007/s13218-012-0177-4", "10.1080/13658816.2013.825724", "10.1109/tvcg.2007.70582", "10.1109/mc.2013.152", "10.1109/vast.2014.7042496", "10.1109/vast.2011.6102456", "10.1145/2733373.2806236", "10.1109/tvcg.2013.221", "10.1177/1473871615576925", "10.1109/vast.2016.7883513", "10.1109/tvcg.2013.186", "10.1109/mc.2012.430", "10.1109/tvcg.2015.2467554", "10.1109/pacificvis.2014.48", "10.1145/2212776.2212796", "10.1109/tvcg.2006.107", "10.1109/pacificvis.2014.38", "10.1109/pacificvis.2015.7156376", "10.1109/vast.2014.7042535", "10.1109/tvcg.2014.2359887"], "wos": 1, "children": [{"doi": "10.1109/vast.2017.8585638", "title": "E-Map: A Visual Analytics Approach for Exploring Significant Event Evolutions in Social Media", "year": "2017", "conferenceName": "VAST", "authors": "Siming Chen;Shuai Chen;Lijing Lin;Xiaoru Yuan;Jie Liang 0004;Xiaolong Zhang", "citationCount": "10", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing, Peoples R China. Yuan, XR (Corresponding Author), Peking Univ, Sch EECS, Beijing, Peoples R China. Chen, Siming; Chen, Shuai; Lin, Lijing; Yuan, Xiaoru, Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing, Peoples R China. Chen, Siming; Chen, Shuai; Lin, Lijing; Yuan, Xiaoru, Peking Univ, Sch EECS, Beijing, Peoples R China. Liang, Jie, Univ Technol, Fac Engn \\& Informat Technol, Sydney, NSW, Australia. Zhang, Xiaolong, Penn State Univ, Coll Informat Sci \\& Technol, University Pk, PA 16802 USA.", "countries": "USA;China;Australia", "abstract": "Significant events are often discussed and spread through social media, involving many people. Reposting activities and opinions expressed in social media offer good opportunities to understand the evolution of events. However, the dynamics of reposting activities and the diversity of user comments pose challenges to understand event-related social media data. We propose E-Map, a visual analytics approach that uses map-like visualization tools to help multi-faceted analysis of social media data on a significant event and in-depth understanding of the development of the event. E-Map transforms extracted keywords, messages, and reposting behaviors into map features such as cities, towns, and rivers to build a structured and semantic space for users to explore. It also visualizes complex posting and reposting behaviors as simple trajectories and connections that can be easily followed. By supporting multi-level spatial temporal exploration, E-Map helps to reveal the patterns of event development and key players in an event, disclosing the ways they shape and affect the development of the event. Two cases analysing real-world events confirm the capacities of E-Map in facilitating the analysis of event evolution with social media data.", "keywords": "Social Media,Event Analysis,Map-like Visual Metaphor,Spatial Temporal Visual Analytics", "link": "http://dx.doi.org/10.1109/VAST.2017.8585638", "refList": ["10.1109/tvcg.2007.70582", "10.1109/tvcg.2016.2598919", "10.1109/pacificvis.2010.5429590", "10.1057/ivs.2008.23", "10.1109/vast.2014.7042496", "10.1016/j.cag.2013.11.003", "10.1109/vast.2011.6102456", "10.1109/tvcg.2013.221", "10.1109/tvcg.2011.185", "10.1109/vast.2016.7883510", "10.1111/j.1467-8659.2012.03120.x", "10.1109/tvcg.2013.186", "10.1109/tmm.2016.2614220", "10.7155/jgaa.00302", "10.1109/mc.2012.430", "10.1109/tvcg.2015.2467619", "10.1109/tvcg.2010.129", "10.1016/s0341-8162(01)00164-3", "10.1109/tvcg.2016.2598590", "10.1145/2065023.2065041", "10.1111/cgf.13211", "10.1109/tvcg.2013.162", "10.1109/tvcg.2011.288", "10.1145/1963405.1963504", "10.5670/oceanog.2016.66", "10.1109/tvcg.2015.2467554", "10.1109/tvcg.2015.2467691", "10.1016/j.cag.2013.10.008", "10.1109/mcg.2015.73", "10.1109/vast.2012.6400557", "10.1109/tvcg.2016.2539960", "10.1109/tit.1982.1056489", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2014.2346922", "10.1109/tmm.2014.2384912", "10.1109/vast.2016.7883511", "10.1002/spe.4380211102", "10.1145/2488388.2488504", "10.2312/eurovisstar.20141176", "10.1109/bigdata.2013.6691714", "10.1109/tvcg.2009.171", "10.1109/tvcg.2013.196", "10.1109/vast.2008.4677356", "10.1145/2207676.2208672", "10.1111/j.1467-8659.2011.01955.x", "10.1109/pacificvis.2014.38", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2015.7156376", "10.1145/1348549.1348556", "10.1109/vast.2015.7347632", "10.1109/tvcg.2014.2346919", "10.1109/tvcg.2014.2346433"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934263", "title": "R-Map: A Map Metaphor for Visualizing Information Reposting Process in Social Media", "year": "2019", "conferenceName": "VAST", "authors": "Shuai Chen;Sihang Li;Siming Chen;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Chen, Shuai; Li, Sihang, Peking Univ, Sch EECS, Minist Educ, Key Lab Machine Petrept, Beijing, Peoples R China. Yuan, Xiaoru, Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Chen, Siming, Fraunhofer Inst IAIS, St Augustin, Germany. Chen, Siming, Univ Bonn, Bonn, Germany.", "countries": "Germany;China", "abstract": "We propose R-Map (Reposting Map), a visual analytical approach with a map metaphor to support interactive exploration and analysis of the information reposting process in social media. A single original social media post can cause large cascades of repostings (i.e., retweets) on online networks, involving thousands, even millions of people with different opinions. Such reposting behaviors form the reposting tree, in which a node represents a message and a link represents the reposting relation. In R-Map, the reposting tree structure can be spatialized with highlighted key players and tiled nodes. The important reposting behaviors, the following relations and the semantics relations are represented as rivers, routes and bridges, respectively, in a virtual geographical space. R-Map supports a scalable overview of a large number of information repostings with semantics. Additional interactions on the map are provided to support the investigation of temporal patterns and user behaviors in the information diffusion process. We evaluate the usability and effectiveness of our system with two use cases and a formal user study.", "keywords": "Social Media,Information Diffusion,Map-like Visual Metaphor", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934263", "refList": ["10.1109/infvis.2000.885091", "10.1109/pacificvis.2010.5429590", "10.1111/j.0020-2754.1998.00269.x", "10.1109/vast.2017.8585638", "10.1145/2700398", "10.1109/tmm.2016.2614229", "10.1109/visual.1991.175815", "10.1145/3183347", "10.1109/infvis.2001.963290", "10.1109/vast.2016.7883510", "10.1109/access.2016.2605009", "10.1109/mcg.2011.103", "10.1145/1124772.1124851", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2010.79", "10.1109/infvis.2000.885095", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1002/(sici)1097-0266(199606)17:6", "10.5670/oceanog.2016.66", "10.1559/152304003100011081", "10.1109/tit.1982.1056489", "10.1559/152304098782383034", "10.1109/tvcg.2014.2346922", "10.1007/978-3-540-85567-5\\_9", "10.1145/2488388.2488504", "10.1109/38.974518", "10.1109/bigdata.2013.6691714", "10.1109/pacificvis.2014.38", "10.1109/tvcg.2012.291", "10.1109/infvis.2005.1532128", "10.2307/2685881", "10.1007/1-4020-4179-9\\_91", "10.1109/infvis.1999.801860", "10.1109/asonam.2011.37"], "wos": 1, "children": [{"doi": "10.1111/cgf.14031", "year": "2020", "title": "The State of the Art in Map-Like Visualization", "conferenceName": "EuroVis", "authors": "Marius Hogr{\\\"{a}}fer;Magnus Heitzler;Hans{-}J{\\\"{o}}rg Schulz", "citationCount": "0", "affiliation": "Hografer, M (Corresponding Author), Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHografer, Marius; Schulz, Hans-Jorg, Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHeitzler, Magnus, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Denmark", "abstract": "Cartographic maps have been shown to provide cognitive benefits when interpreting data in relation to a geographic location. In visualization, the term map-like describes techniques that incorporate characteristics of cartographic maps in their representation of abstract data. However, the field of map-like visualization is vast and currently lacks a clear classification of the existing techniques. Moreover, choosing the right technique to support a particular visualization task is further complicated, as techniques are scattered across different domains, with each considering different characteristics as map-like. In this paper, we give an overview of the literature on map-like visualization and provide a hierarchical classification of existing techniques along two general perspectives: imitation and schematization of cartographic maps. Each perspective is further divided into four principal categories that group common map-like techniques along the visual primitives they affect. We further discuss this classification from a task-centered view and highlight open research questions.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14031", "refList": ["10.1111/j.0020-2754.1998.00269.x.21", "10.1109/iv.2005.26", "10.2307/2980460", "10.1109/tvcg.2017.2743959", "10.1016/j.jvlc.2011.11.004", "10.1145/2501988.2502046", "10.1559/1523040042742402", "10.1007/s00779-011-0500-3", "10.1111/cgf.12932", "10.1109/tvcg.2010.89", "10.1111/cgf.13200", "10.1080/15230406.2016.1160797", "10.1179/000870410x12825500202896", "10.1109/tvcg.2007.70596", "10.5167/80972.19uzh-80972", "10.1007/978-3-319-11593-1\\_2", "10.1037/aca0000175", "10.1080/17538947.2014.923942", "10.1109/tvcg.2015.2467321", "10.1109/vissof.2005.1684299", "10.1559/152304098782383034", "10.1007/978-3-642-36763-2\\_38", "10.1109/tvcg.2013.130", "10.1109/tvcg.2014.2346274", "10.1111/cgf.12648", "10.1111/0004-5608.00242", "10.1177/1473871617724212", "10.1109/iv.2004.1320123", "10.1371/journal.pcbi.1006907", "10.1109/tvcg.2016.2599030", "10.1559/1523040042742411", "10.3390/informatics5030031", "10.1111/cgf.13079", "10.1109/tvcg.2015.2467811", "10.1016/j.tics.2003.12.004", "10.1007/s00799-016-0168-4", "10.1109/infvis.2004.57", "10.1007/s10021-007-9038-7", "10.1109/pacificvis.2015.7156366", "10.1007/978-3-319-27261-0\\_1", "10.1007/978-1-4471-2804-5\\_6", "10.1109/access.2019.2939977", "10.1109/tvcg.2017.2747545", "10.1109/icdm.2003.1250978", "10.1111/cgf.13167", "10.1016/0010-0285(78)90006-3", "10.3138/nj8v-8514-871t-221k", "10.1016/j.cag.2009.06.002", "10.1111/j.1467-8659.2011.01937.x", "10.1016/b978-044451020-4/50035-9", "10.1111/1467-8659.00566", "10.5220/0006618101080119", "10.1109/pacificvis.2012.6183571", "10.1145/2038558.2038579", "10.1109/tvcg.2010.191", "10.1111/j.0033-0124.1985.00075.x", "10.1109/tvcg.2004.1260761", "10.1007/978-3-642-34848-8\\_6", "10.1145/2968220.2968239", "10.1145/3002151.3002160", "10.1109/tst.2013.6509098", "10.1109/tvcg.2013.91", "10.1177/1473871615597077", "10.1016/j.jvlc.2011.02.001", "10.1080/17445647.2014.935502", "10.1177/1687814017740710", "10.1111/1744-7917.12601", "10.1073/pnas.0400280101", "10.1111/cgf.13672", "10.1109/mcg.2006.90", "10.1002/asi.21712", "10.1007/978-3-642-33024-7\\_3", "10.1145/2801040.2801056", "10.1109/mcg.2010.101", "10.1179/003962607x165041", "10.1057/palgrave.ivs.9500039", "10.1109/vast.2016.7883510", "10.1559/152304009788988288", "10.1080/23729333.2017.1301346", "10.1109/tvcg.2013.66", "10.1111/j.1467-8659.2012.03085.x", "10.1109/tvcg.2011.288", "10.3390/ijgi9040253", "10.1109/infvis.2005.1532150", "10.1145/2254556.2254636", "10.20382//jocg.v4i1a9", "10.1016/0010-0285(81)90016-5", "10.1145/2556288.2557224", "10.1109/iv.2001.942043", "10.1021/ed1000203", "10.1016/0169-7439(87)80084-9", "10.1109/tvcg.2010.154", "10.1016/j.jvlc.2015.10.003", "10.1109/tvcg.2019.2903945", "10.1109/tvcg.2013.120", "10.1109/tvcg.2019.2934263", "10.1146/annurev-ecolsys-102209-144718", "10.1109/tvcg.2008.165", "10.3138/a477-3202-7876-n514", "10.1080/23729333.2017.1288535", "10.1111/j.0020-2754.1998.00269.x", "10.1109/mcg.2004.41", "10.1109/vast.2009.5332593", "10.1002/smr.414", "10.1007/s12650-019-00584-3", "10.1145/22949.22950", "10.1179/1743277413y.0000000036", "10.1080/15230406.2016.1262280", "10.1016/s0341-8162(01)00164-3", "10.22224/gistbok/2017.3.8", "10.1109/tvcg.2008.155", "10.1057/ivs.2008.31", "10.1016/j.cag.2004.03.012", "10.1179/1743277412y.0000000007", "10.1016/j.soncn.2011.02.001", "10.1179/caj.1987.24.1.27", "10.3138/carto.48.3.1691", "10.5220/0004267205150524", "10.1109/38.974518", "10.1145/102377.115768", "10.1057/palgrave.ivs.9500186", "10.1109/5.58325", "10.5167/80972.uzh-80972", "10.1177/030913339602000204", "10.1007/978-3-642-22300-6\\_14", "10.1109/iv.2004.1320189", "10.1111/cgf.13447", "10.1007/s11192-017-2596-3", "10.1559/1523040053722150", "10.1007/978-3-662-45803-7\\_34"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934263", "title": "R-Map: A Map Metaphor for Visualizing Information Reposting Process in Social Media", "year": "2019", "conferenceName": "VAST", "authors": "Shuai Chen;Sihang Li;Siming Chen;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Chen, Shuai; Li, Sihang, Peking Univ, Sch EECS, Minist Educ, Key Lab Machine Petrept, Beijing, Peoples R China. Yuan, Xiaoru, Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Chen, Siming, Fraunhofer Inst IAIS, St Augustin, Germany. Chen, Siming, Univ Bonn, Bonn, Germany.", "countries": "Germany;China", "abstract": "We propose R-Map (Reposting Map), a visual analytical approach with a map metaphor to support interactive exploration and analysis of the information reposting process in social media. A single original social media post can cause large cascades of repostings (i.e., retweets) on online networks, involving thousands, even millions of people with different opinions. Such reposting behaviors form the reposting tree, in which a node represents a message and a link represents the reposting relation. In R-Map, the reposting tree structure can be spatialized with highlighted key players and tiled nodes. The important reposting behaviors, the following relations and the semantics relations are represented as rivers, routes and bridges, respectively, in a virtual geographical space. R-Map supports a scalable overview of a large number of information repostings with semantics. Additional interactions on the map are provided to support the investigation of temporal patterns and user behaviors in the information diffusion process. We evaluate the usability and effectiveness of our system with two use cases and a formal user study.", "keywords": "Social Media,Information Diffusion,Map-like Visual Metaphor", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934263", "refList": ["10.1109/infvis.2000.885091", "10.1109/pacificvis.2010.5429590", "10.1111/j.0020-2754.1998.00269.x", "10.1109/vast.2017.8585638", "10.1145/2700398", "10.1109/tmm.2016.2614229", "10.1109/visual.1991.175815", "10.1145/3183347", "10.1109/infvis.2001.963290", "10.1109/vast.2016.7883510", "10.1109/access.2016.2605009", "10.1109/mcg.2011.103", "10.1145/1124772.1124851", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2010.79", "10.1109/infvis.2000.885095", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1002/(sici)1097-0266(199606)17:6", "10.5670/oceanog.2016.66", "10.1559/152304003100011081", "10.1109/tit.1982.1056489", "10.1559/152304098782383034", "10.1109/tvcg.2014.2346922", "10.1007/978-3-540-85567-5\\_9", "10.1145/2488388.2488504", "10.1109/38.974518", "10.1109/bigdata.2013.6691714", "10.1109/pacificvis.2014.38", "10.1109/tvcg.2012.291", "10.1109/infvis.2005.1532128", "10.2307/2685881", "10.1007/1-4020-4179-9\\_91", "10.1109/infvis.1999.801860", "10.1109/asonam.2011.37"], "wos": 1, "children": [{"doi": "10.1111/cgf.14031", "year": "2020", "title": "The State of the Art in Map-Like Visualization", "conferenceName": "EuroVis", "authors": "Marius Hogr{\\\"{a}}fer;Magnus Heitzler;Hans{-}J{\\\"{o}}rg Schulz", "citationCount": "0", "affiliation": "Hografer, M (Corresponding Author), Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHografer, Marius; Schulz, Hans-Jorg, Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.\nHeitzler, Magnus, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Denmark", "abstract": "Cartographic maps have been shown to provide cognitive benefits when interpreting data in relation to a geographic location. In visualization, the term map-like describes techniques that incorporate characteristics of cartographic maps in their representation of abstract data. However, the field of map-like visualization is vast and currently lacks a clear classification of the existing techniques. Moreover, choosing the right technique to support a particular visualization task is further complicated, as techniques are scattered across different domains, with each considering different characteristics as map-like. In this paper, we give an overview of the literature on map-like visualization and provide a hierarchical classification of existing techniques along two general perspectives: imitation and schematization of cartographic maps. Each perspective is further divided into four principal categories that group common map-like techniques along the visual primitives they affect. We further discuss this classification from a task-centered view and highlight open research questions.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14031", "refList": ["10.1111/j.0020-2754.1998.00269.x.21", "10.1109/iv.2005.26", "10.2307/2980460", "10.1109/tvcg.2017.2743959", "10.1016/j.jvlc.2011.11.004", "10.1145/2501988.2502046", "10.1559/1523040042742402", "10.1007/s00779-011-0500-3", "10.1111/cgf.12932", "10.1109/tvcg.2010.89", "10.1111/cgf.13200", "10.1080/15230406.2016.1160797", "10.1179/000870410x12825500202896", "10.1109/tvcg.2007.70596", "10.5167/80972.19uzh-80972", "10.1007/978-3-319-11593-1\\_2", "10.1037/aca0000175", "10.1080/17538947.2014.923942", "10.1109/tvcg.2015.2467321", "10.1109/vissof.2005.1684299", "10.1559/152304098782383034", "10.1007/978-3-642-36763-2\\_38", "10.1109/tvcg.2013.130", "10.1109/tvcg.2014.2346274", "10.1111/cgf.12648", "10.1111/0004-5608.00242", "10.1177/1473871617724212", "10.1109/iv.2004.1320123", "10.1371/journal.pcbi.1006907", "10.1109/tvcg.2016.2599030", "10.1559/1523040042742411", "10.3390/informatics5030031", "10.1111/cgf.13079", "10.1109/tvcg.2015.2467811", "10.1016/j.tics.2003.12.004", "10.1007/s00799-016-0168-4", "10.1109/infvis.2004.57", "10.1007/s10021-007-9038-7", "10.1109/pacificvis.2015.7156366", "10.1007/978-3-319-27261-0\\_1", "10.1007/978-1-4471-2804-5\\_6", "10.1109/access.2019.2939977", "10.1109/tvcg.2017.2747545", "10.1109/icdm.2003.1250978", "10.1111/cgf.13167", "10.1016/0010-0285(78)90006-3", "10.3138/nj8v-8514-871t-221k", "10.1016/j.cag.2009.06.002", "10.1111/j.1467-8659.2011.01937.x", "10.1016/b978-044451020-4/50035-9", "10.1111/1467-8659.00566", "10.5220/0006618101080119", "10.1109/pacificvis.2012.6183571", "10.1145/2038558.2038579", "10.1109/tvcg.2010.191", "10.1111/j.0033-0124.1985.00075.x", "10.1109/tvcg.2004.1260761", "10.1007/978-3-642-34848-8\\_6", "10.1145/2968220.2968239", "10.1145/3002151.3002160", "10.1109/tst.2013.6509098", "10.1109/tvcg.2013.91", "10.1177/1473871615597077", "10.1016/j.jvlc.2011.02.001", "10.1080/17445647.2014.935502", "10.1177/1687814017740710", "10.1111/1744-7917.12601", "10.1073/pnas.0400280101", "10.1111/cgf.13672", "10.1109/mcg.2006.90", "10.1002/asi.21712", "10.1007/978-3-642-33024-7\\_3", "10.1145/2801040.2801056", "10.1109/mcg.2010.101", "10.1179/003962607x165041", "10.1057/palgrave.ivs.9500039", "10.1109/vast.2016.7883510", "10.1559/152304009788988288", "10.1080/23729333.2017.1301346", "10.1109/tvcg.2013.66", "10.1111/j.1467-8659.2012.03085.x", "10.1109/tvcg.2011.288", "10.3390/ijgi9040253", "10.1109/infvis.2005.1532150", "10.1145/2254556.2254636", "10.20382//jocg.v4i1a9", "10.1016/0010-0285(81)90016-5", "10.1145/2556288.2557224", "10.1109/iv.2001.942043", "10.1021/ed1000203", "10.1016/0169-7439(87)80084-9", "10.1109/tvcg.2010.154", "10.1016/j.jvlc.2015.10.003", "10.1109/tvcg.2019.2903945", "10.1109/tvcg.2013.120", "10.1109/tvcg.2019.2934263", "10.1146/annurev-ecolsys-102209-144718", "10.1109/tvcg.2008.165", "10.3138/a477-3202-7876-n514", "10.1080/23729333.2017.1288535", "10.1111/j.0020-2754.1998.00269.x", "10.1109/mcg.2004.41", "10.1109/vast.2009.5332593", "10.1002/smr.414", "10.1007/s12650-019-00584-3", "10.1145/22949.22950", "10.1179/1743277413y.0000000036", "10.1080/15230406.2016.1262280", "10.1016/s0341-8162(01)00164-3", "10.22224/gistbok/2017.3.8", "10.1109/tvcg.2008.155", "10.1057/ivs.2008.31", "10.1016/j.cag.2004.03.012", "10.1179/1743277412y.0000000007", "10.1016/j.soncn.2011.02.001", "10.1179/caj.1987.24.1.27", "10.3138/carto.48.3.1691", "10.5220/0004267205150524", "10.1109/38.974518", "10.1145/102377.115768", "10.1057/palgrave.ivs.9500186", "10.1109/5.58325", "10.5167/80972.uzh-80972", "10.1177/030913339602000204", "10.1007/978-3-642-22300-6\\_14", "10.1109/iv.2004.1320189", "10.1111/cgf.13447", "10.1007/s11192-017-2596-3", "10.1559/1523040053722150", "10.1007/978-3-662-45803-7\\_34"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934266", "title": "VASSL: A Visual Analytics Toolkit for Social Spambot Labeling", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;Jieqiong Zhao;David S. Ebert", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Zhao, Jieqiong; Ebert, David S., Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Purdue Univ, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Social media platforms are filled with social spambots. Detecting these malicious accounts is essential, yet challenging, as they continually evolve to evade detection techniques. In this article, we present VASSL, a visual analytics system that assists in the process of detecting and labeling spambots. Our tool enhances the performance and scalability of manual labeling by providing multiple connected views and utilizing dimensionality reduction, sentiment analysis and topic modeling, enabling insights for the identification of spambots. The system allows users to select and analyze groups of accounts in an interactive manner, which enables the detection of spambots that may not be identified when examined individually. We present a user study to objectively evaluate the performance of VASSL users, as well as capturing subjective opinions about the usefulness and the ease of use of the tool.", "keywords": "Spambot,Labeling,Detection,Visual Analytics,Social Media Annotation", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934266", "refList": ["10.7326/0003-4819-110-11-916", "10.1109/tifs.2013.2267732", "10.1111/cgf.12106", "10.1016/j.ins.2013.11.016", "10.1109/tvcg.2017.2752166", "10.1109/vast.2016.7883510", "10.1109/vl.1996.545307", "10.1145/2872518.2889302", "10.1109/tmm.2016.2614220", "10.1145/2818717", "10.1109/tdsc.2017.2681672", "10.1111/cgf.13211", "10.2307/2685478", "10.1109/tvcg.2014.2346920", "10.1109/tdsc.2016.2641441", "10.1109/tvcg.2017.2745080", "10.1109/vast.2012.6400557", "10.1109/asonam.2016.7752287", "10.1109/tvcg.2014.2346922", "10.1111/cgf.13217", "10.2307/249008", "10.1109/tvcg.2017.2711030", "10.1109/mcse.2013.70", "10.1109/tvcg.2015.2467196", "10.1016/j.comcom.2013.04.004", "10.1109/asonam.2014.6921650", "10.1109/mc.2016.183", "10.1126/science.290.5500.2323", "10.1145/3041021.3055135", "10.1162/jmlr.2003.3.4-5.993", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2013.153", "10.1109/iv.2008.89", "10.1109/mcom.2013.6588663", "10.1179/000870403235002042", "10.1145/3047010"], "wos": 1, "children": [{"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 7}, {"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13677", "year": "2019", "title": "An Ontological Framework for Supporting the Design and Evaluation of Visual Analytics Systems", "conferenceName": "EuroVis", "authors": "Min Chen;David S. Ebert", "citationCount": "5", "affiliation": "Chen, M (Corresponding Author), Univ Oxford, Oxford, England.\nChen, Min, Univ Oxford, Oxford, England.\nEbert, David S., Purdue Univ, W Lafayette, IN 47907 USA.", "countries": "USA;England", "abstract": "Designing, evaluating, and improving visual analytics (VA) systems is a primary area of activities in our discipline. In this paper, we present an ontological framework for recording and categorizing technical shortcomings to be addressed in a VA workflow, reasoning about the causes of such problems, identifying technical solutions, and anticipating secondary effects of the solutions. The methodology is built on the theoretical premise that designing a VA workflow is an optimization of the cost-benefit ratio of the processes in the workflow. It makes uses three fundamental measures to group and connect symptoms, causes, remedies, and side-effects, and guide the search for potential solutions to the problems. In terms of requirement analysis and system design, the proposed methodology can enable system designers to explore the decision space in a structured manner. In terms of evaluation, the proposed methodology is time-efficient and complementary to various forms of empirical studies, such as user surveys, controlled experiments, observational studies, focus group discussions, and so on. In general, it reduces the amount of trial-and-error in the lifecycle of VA system development.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13677", "refList": ["10.1109/tvcg.2006.178", "10.1109/infvis.2000.885092", "10.1111/cgf.12920", "10.1057/ivs.2009.26", "10.1109/mcg.2017.3271463", "10.1007/978-3-319-10578-9\\_1", "10.1109/icdmw.2008.62", "10.1109/mcg.2017.51", "10.1145/3011141.3011207", "10.1109/tvcg.2013.134", "10.1080/10618600.1996.10474696", "10.1007/978-3-540-70956-5", "10.1109/visual.1990.146375", "10.1109/tvcg.2012.219", "10.1109/vl.1996.545307", "10.1057/ivs.2009.23", "10.1002/j.1538-7305.1948.tb00917.x", "10.1109/tvcg.2010.79", "10.1109/tvcg.2013.124", "10.1111/cgf.13211", "10.1007/978-3-642-40897-7\\_9", "10.1103/physrev.108.171", "10.1109/infvis.2004.59", "10.1109/iv.2008.36", "10.1111/cgf.13210", "10.1111/j.1467-8659.2008.01230.x", "10.1001/jama.293.10.1223", "10.1177/1473871611407399", "10.1109/visual.1995.480821", "10.1117/12.539227", "10.1109/tvcg.2015.2513410", "10.1109/vast.2011.6102463", "10.7749/citiescommunitiesterritories.dec2014.029.art01", "10.1109/mcg.2005.55", "10.1109/tvcg.2012.234", "10.1109/pacificvis.2012.6183556", "10.1103/physrev.106.620", "10.1109/infvis.1997.636792", "10.2307/2104491", "10.1145/2468356.2468677", "10.1145/3173574.3173611", "10.1109/tvcg.2018.2864838", "10.1111/cgf.13092", "10.1109/visual.2004.10", "10.1109/tvcg.2017.2744319", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2603178"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934264", "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;David S. Ebert;Arif Ghafoor", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Karimzadeh, Morteza; Ebert, David S.; Ghafoor, Arif, Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.", "keywords": "Summative evaluation,usefulness,evaluation process,taxonomy,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934264", "refList": ["10.1109/tvcg.2017.2744478", "10.1109/tvcg.2018.2865025", "10.1109/tvcg.2006.85", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2017.2745181", "10.1111/cgf.13677", "10.1109/tvcg.2018.2864844", "10.1109/tvcg.2013.126", "10.1109/tvcg.2018.2864811", "10.1109/infvis.2005.1532147", "10.1177/0956797613504966", "10.1145/2669557.2669579", "10.1109/mcg.2005.102", "10.1109/visual.2003.1250426", "10.1136/bmj.39489.470347.ad", "10.1109/tvcg.2017.2744080", "10.1109/mcg.2009.53", "10.1111/j.1467-8527.2005.00307.x", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2864886", "10.1109/tvcg.2018.2864843", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2018.2865051", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2018.2865026", "10.1007/978-3-540-71080-6\\_6", "10.1109/tvcg.2018.2865020", "10.1177/1473871611407399", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2018.2864526", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2864905", "10.1049/sej.1991.0040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2711030", "10.1109/tvcg.2011.279", "10.1109/vast.2017.8585505", "10.1147/jrd.2010.2042914", "10.1016/s0378-7206(98)00044-5", "10.1145/2993901.2993913", "10.1109/tvcg.2018.2865041", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744758", "10.1145/1168149.1168158", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2012.213", "10.1109/tvcg.2017.2744738", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2018.2864826", "10.1145/1377966.1377974", "10.1109/apec.2009.4802646", "10.1145/1168149.1168152", "10.1016/j.jss.2008.03.059", "10.1109/vast.2017.8585484", "10.1109/tvcg.2017.2744818", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2865042", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030388", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "year": "2020", "conferenceName": "SciVis", "authors": "Pepe Eulzer;Sabine Bauer;Francis Kilian;Kai Lawonn", "citationCount": "0", "affiliation": "Eulzer, P (Corresponding Author), Univ Jena, Jena, Germany. Eulzer, Pepe; Lawonn, Kai, Univ Jena, Jena, Germany. Bauer, Sabine, Univ Koblenz Landau, Koblenz, Germany. Kilian, Francis, Cath Clin Koblenz Montabaur, Dept Spine Surg, Koblenz, Germany.", "countries": "Germany", "abstract": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. $\\mathrm{By}$ linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. $\\mathrm{in}$ a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.", "keywords": "Medical visualization,bioinformatics,coordinated views,focus and context,biomechanical simulation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030388", "refList": ["10.1109/tvcg.2018.2864903", "10.1177/1473871613510429", "10.1093/ehjqcco/qcz052", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2018.2865076", "10.1055/s-0039-1687862", "10.1109/visual.1990.146375", "10.1109/tvcg.2017.2744198", "10.1016/j.ijmedinf.2014.10.001", "10.1109/tvcg.2013.124", "10.1016/j.jacc", "10.1111/cgf.13167", "10.17705/1thci.00055", "10.1136/bmjqs.2009.037895", "10.1109/tvcg.2013.238", "10.1109/tvcg.2018.2865240", "10.1186/1471-2261-6-34", "10.1109/tvcg.2019.2934264", "10.1109/tvcg.2013.200", "10.1109/tvcg.2011.209", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467091", "10.1136/bmjopen-2019-033208", "10.1109/beliv.2018.8634027", "10.1109/tvcg.2012.213", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2015.2467325", "10.1145/2133806.2133821", "10.1145/1806799.1806866", "10.1108/02635570610688869", "10.1002/hbm.20701", "10.1561/1100000039", "10.1145/3025453.3025645", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 5}, {"doi": "10.1111/cgf.13987", "year": "2020", "title": "Augmenting Node-Link Diagrams with Topographic Attribute Maps", "conferenceName": "EuroVis", "authors": "Reinhold Preiner;Johanna Schmidt;Katharina Kr{\\\"{o}}sl;Tobias Schreck;Gabriel Mistelbauer", "citationCount": "0", "affiliation": "Preiner, R (Corresponding Author), Graz Univ Technol, Inst Comp Graph \\& Knowledge Visualizat, Graz, Austria.\nPreiner, R.; Schreck, T., Graz Univ Technol, Inst Comp Graph \\& Knowledge Visualizat, Graz, Austria.\nSchmidt, J.; Kroesl, K., Virtual Real \\& Visualisierung Forsch GmbH, VRVis Zentrum, Vienna, Austria.\nKroesl, K., TU Wien, Inst Visual Comp \\& Human Ctr Technol, Vienna, Austria.\nMistelbauer, G., Otto von Guericke Univ, Dept Simulat \\& Graph, Magdeburg, Germany.", "countries": "Germany;Austria", "abstract": "We propose a novel visualization technique for graphs that are attributed with scalar data. In many scenarios, these attributes (e.g., birth date in a family network) provide ambient context information for the graph structure, whose consideration is important for different visual graph analysis tasks. Graph attributes are usually conveyed using different visual representations (e.g., color, size, shape) or by reordering the graph structure according to the attribute domain (e.g., timelines). While visual encodings allow graphs to be arranged in a readable layout, assessing contextual information such as the relative similarities of attributes across the graph is often cumbersome. In contrast, attribute-based graph reordering serves the comparison task of attributes, but typically strongly impairs the readability of the structural information given by the graph's topology. In this work, we augment force-directed node-link diagrams with a continuous ambient representation of the attribute context. This way, we provide a consistent overview of the graph's topological structure as well as its attributes, supporting a wide range of graph-related analysis tasks. We resort to an intuitive height field metaphor, illustrated by a topographic map rendering using contour lines and suitable color maps. Contour lines visually connect nodes of similar attribute values, and depict their relative arrangement within the global context. Moreover, our contextual representation supports visualizing attribute value ranges associated with graph nodes (e.g., lifespans in a family network) as trajectories routed through this height field. We discuss how user interaction with both the structural and the contextual information fosters exploratory graph analysis tasks. The effectiveness and versatility of our technique is confirmed in a user study and case studies from various application domains.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13987", "refList": ["10.1109/tvcg.2013.269", "10.1109/pacificvis.2010.5429590", "10.1073/pnas.0307654100", "10.1145/2505515.2505758", "10.1559/152304082783948286", "10.1109/pacificvis.2014.47", "10.1093/bioinformatics/btp432", "10.1111/j.1467-8659.2011.01898.x", "10.1111/cgf.12931", "10.1111/cgf.12880", "10.1109/tvcg.2014.2346422", "10.1111/j.1467-8659.2009.01706.x", "10.1109/tvcg.2016.2598795", "10.1111/cgf.12800", "10.1109/tvcg.2014.2315995", "10.1111/cgf.12656", "10.1111/cgf.13728", "10.1109/tvcg.2009.122", "10.1111/cgf.13211", "10.1109/tvcg.2007.70596", "10.1109/infvis.2002.1173152", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2003.1196007", "10.1109/infvis.2005.1532150", "10.1145/3243250.3243266", "10.1080/02693799008941549", "10.1371/journal.pone.0058779", "10.1109/infvis.1995.528686", "10.1111/cgf.12872", "10.1002/spe.4380211102", "10.1109/38.974518", "10.1145/3097983.3098130", "10.1002/aris.1440370106", "10.1145/1360612.1360691", "10.1109/mc.2016.145", "10.2307/3006914", "10.1111/j.1467-8659.2009.01683.x", "10.1145/1639714.1639784"], "wos": 1, "children": [], "len": 1}], "len": 31}], "len": 81}], "len": 127}, {"doi": "10.1111/cgf.13713", "year": "2019", "title": "Bird's-Eye - Large-Scale Visual Analytics of City Dynamics using Social Location Data", "conferenceName": "EuroVis", "authors": "Robert Kr{\\\"{u}}ger;Qi Han;Nikolay Ivanov;Sanae Mahtal;Dennis Thom;Hanspeter Pfister;Thomas Ertl", "citationCount": "2", "affiliation": "Krueger, R (Corresponding Author), Harvard Univ, Visual Comp Grp, Cambridge, MA 02138 USA.\nKrueger, Robert; Pfister, Hanspeter, Harvard Univ, Visual Comp Grp, Cambridge, MA 02138 USA.\nHang, Qi; Ivanov, Nikolay; Mahtal, Sanae; Thom, Dennis; Ertl, Thomas, Univ Stuttgart, Inst Visualizat \\& Interact Syst, Stuttgart, Germany.", "countries": "Germany;USA", "abstract": "The analysis of behavioral city dynamics, such as temporal patterns of visited places and citizens' mobility routines, is an essential task for urban and transportation planning. Social media applications such as Foursquare and Twitter provide access to large-scale and up-to-date dynamic movement data that not only help to understand the social life and pulse of a city but also to maintain and improve urban infrastructure. However, the fast growth rate of this data poses challenges for conventional methods to provide up-to-date, flexible analysis. Therefore, planning authorities barely consider it. We present a system and design study to leverage social media data that assist urban and transportation planners to achieve better monitoring and analysis of city dynamics such as visited places and mobility patterns in large metropolitan areas. We conducted a goal-and-task analysis with urban planning experts. To address these goals, we designed a system with a scalable data monitoring back-end and an interactive visual analytics interface. The monitoring component uses intelligent pre-aggregation to allow dynamic queries in near real-time. The visual analytics interface leverages unsupervised learning to reveal clusters, routines, and unusual behavior in massive data, allowing to understand patterns in time and space. We evaluated our approach based on a qualitative user study with urban planning experts which demonstrates that intuitive integration of advanced analytical tools with visual interfaces is pivotal in making behavioral city dynamics accessible to practitioners. Our interviews also revealed areas for future research.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13713", "refList": ["10.1016/j.jvlc.2014.10.028", "10.1109/tvcg.2015.2468111", "10.1111/cgf.12129", "10.1109/tvcg.2011.127", "10.1145/2516604.2516616", "10.1126/science.1200970", "10.1145/2629592", "10.4000/netcom.2725", "10.1109/tvcg.2016.2598585", "10.1145/2365952.2366028", "10.1109/tvcg.2015.2467619", "10.1109/tvcg.2009.100", "10.1109/iv.2010.94", "10.1145/2424321.2424395", "10.1109/pacificvis.2012.6183572", "10.1109/tits.2017.2727281", "10.1177/1473871617692841", "10.1109/tvcg.2013.179", "10.1080/09669582.2010.502576", "10.1145/2378023.2378027", "10.1109/ictai.2016.0063", "10.1109/tvcg.2017.2758362", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2012.213", "10.1057/udi.2010.20", "10.1109/tvcg.2014.2371856", "10.1109/ictai.2016.60", "10.1109/tits.2016.2639320", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2008.152", "10.1109/tvcg.2015.2511733"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13712", "year": "2019", "title": "Route-Aware Edge Bundling for Visualizing Origin-Destination Trails in Urban Traffic", "conferenceName": "EuroVis", "authors": "Wei Zeng;Q. Shen;Y. Jiang;A. Telea", "citationCount": "1", "affiliation": "Shen, Q (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.\nZeng, W., Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China.\nShen, Q.; Jiang, Y., Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.\nTelea, A., Univ Groningen, Groningen, Netherlands.", "countries": "China;Netherlands", "abstract": "Origin-destination (OD) trails describe movements across space. Typical visualizations thereof use either straight lines or plot the actual trajectories. To reduce clutter inherent to visualizing large OD datasets, bundling methods can be used. Yet, bundling OD trails in urban traffic data remains challenging. Two specific reasons hereof are the constraints implied by the underlying road network and the difficulty of finding good bundling settings. To cope with these issues, we propose a new approach called Route Aware Edge Bundling (RAEB). To handle road constraints, we first generate a hierarchical model of the road-and-trajectory data. Next, we derive optimal bundling parameters, including kernel size and number of iterations, for a user-selected level of detail of this model, thereby allowing users to explicitly trade off simplification vs accuracy. We demonstrate the added value of RAEB compared to state-of-the-art trail bundling methods on both synthetic and real-world traffic data for tasks that include the preservation of road network topology and the support of multiscale exploration.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13712", "refList": ["10.1109/tvcg.2008.135", "10.1145/2833165.2833168", "10.1111/cgf.13213", "10.1109/mcg.2011.88", "10.1109/tvcg.2013.226", "10.1109/tvcg.2010.44", "10.1145/1653771.1653820", "10.1109/tvcg.2015.2468111", "10.1109/tst.2013.6509098", "10.1111/j.1467-8659.2009.01440.x", "10.1111/cgf.12922", "10.1109/tvcg.2018.2816219", "10.1109/vast.2014.7042486", "10.1198/tas.2009.0033", "10.1111/cgf.12132", "10.1111/j.1467-8659.2009.01700.x", "10.1016/j.trc.2007.05.002", "10.1007/978-3-540-72680-7\\_22", "10.1109/tvcg.2011.233", "10.1109/tvcg.2014.2346271", "10.1109/tvcg.2016.2515611", "10.1109/infvis.2005.1532150", "10.1109/tvcg.2016.2598472", "10.1109/tvcg.2006.147", "10.1109/tvcg.2016.2598958", "10.1109/tits.2017.2683539", "10.1109/tits.2015.2436897", "10.1111/j.1467-8659.2012.03079.x", "10.1109/tvcg.2017.2744338", "10.1109/pacificvis.2017.8031594", "10.1179/000870410x12658023467367", "10.1057/palgrave.ivs.9500182", "10.1109/tvcg.2011.104", "10.1109/tvcg.2011.190", "10.1109/tvcg.2013.114", "10.1111/cgf.12778", "10.1109/tvcg.2015.2467112", "10.1111/cgf.12107", "10.1109/iv.2010.53", "10.1109/42.563664", "10.1109/tvcg.2017.2666146", "10.1145/2530531", "10.1111/j.1467-8659.2009.01450.x", "10.4028/www.scientific.net/kem.342-343.593", "10.1109/tvcg.2011.202", "10.1038/srep00612"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030410", "title": "Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Wei Zeng 0002;Chengqiao Lin;Juncong Lin;Jincheng Jiang;Jiazhi Xia;Cagatay Turkay;Wei Chen", "citationCount": "0", "affiliation": "Lin, JC (Corresponding Author), Xiamen Univ, Xiamen, Peoples R China. Zeng, Wei; Jiang, Jincheng, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Lin, Chengqiao; Lin, Juncong, Xiamen Univ, Xiamen, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Turkay, Cagatay, Univ Warwick, Coventry, W Midlands, England. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China.", "countries": "China;England", "abstract": "Deep learning methods are being increasingly used for urban traffic prediction where spatiotemporal traffic data is aggregated into sequentially organized matrices that are then fed into convolution-based residual neural networks. However, the widely known modifiable areal unit problem within such aggregation processes can lead to perturbations in the network inputs. This issue can significantly destabilize the feature embeddings and the predictions - rendering deep networks much less useful for the experts. This paper approaches this challenge by leveraging unit visualization techniques that enable the investigation of many-to-many relationships between dynamically varied multi-scalar aggregations of urban traffic data and neural network predictions. Through regular exchanges with a domain expert, we design and develop a visual analytics solution that integrates 1) a Bivariate Map equipped with an advanced bivariate colormap to simultaneously depict input traffic and prediction errors across space, 2) a Moran's I Scatterplot that provides local indicators of spatial association analysis, and 3) a Multi-scale Attribution View that arranges non-linear dot plots in a tree layout to promote model analysis and comparison across scales. We evaluate our approach through a series of case studies involving a real-world dataset of Shenzhen taxi trips, and through interviews with domain experts. We observe that geographical scale variations have important impact on prediction performances, and interactive visual exploration of dynamically varying inputs and outputs benefit experts in the development of deep traffic prediction models.", "keywords": "MAUP,traffic prediction,deep learning,model diagnostic,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030410", "refList": ["10.1038/srep26377", "10.1109/mcg.2011.88", "10.1080/13658816.2015.1119279", "10.1109/tvcg.2013.226", "10.1109/pacificvis.2011.5742387", "10.1038/s41467-017-01882-w", "10.1109/tvcg.2019.2934670", "10.1111/j.1467-8659.2009.01440.x", "10.1111/cgf.13712", "10.1016/j.compenvurbsys.2008.09.006", "10.1109/pacificvis.2014.50", "10.1109/tvcg.2018.2816219", "10.1109/tvcg.2016.2535234", "10.1109/tvcg.2014.2346893", "10.3390/ijgi8080344", "10.1109/tvcg.2013.246", "10.1007/s10940-005-9003-6", "10.1016/j.compenvurbsys.2008.05.001", "10.1007/s10661-019-7831-3", "10.1111/j.1538-4632.2007.00699.x", "10.1016/j.aap.2016.08.015", "10.1080/13658816.2018.1541177", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2011.181", "10.1137/090759069", "10.1109/pacificvis.2011.5742390", "10.1214/10-aos799", "10.1109/tits.2017.2683539", "10.1109/tits.2015.2436897", "10.3390/ijerph16071150", "10.1109/tvcg.2009.145", "10.1109/tvcg.2012.265", "10.1080/10106049.2017.1404140", "10.3390/ijgi8020063", "10.3390/info6020134", "10.1080/13658816.2014.955027", "10.1109/tits.2016.2639320", "10.2307/143141", "10.1109/tvcg.2016.2598432"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030469", "title": "Topology Density Map for Urban Data Visualization and Analysis", "year": "2020", "conferenceName": "VAST", "authors": "Zezheng Feng;Haotian Li;Wei Zeng 0004;Shuang-Hua Yang;Huamin Qu", "citationCount": "0", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Feng, Zezheng; Li, Haotian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yang, Shuang-Hua, Southern Univ Sci \\& Technol, Shenzhen, Peoples R China.", "countries": "China", "abstract": "Density map is an effective visualization technique for depicting the scalar field distribution in 2D space. Conventional methods for constructing density maps are mainly based on Euclidean distance, limiting their applicability in urban analysis that shall consider road network and urban traffic. In this work, we propose a new method named Topology Density Map, targeting for accurate and intuitive density maps in the context of urban environment. Based on the various constraints of road connections and traffic conditions, the method first constructs a directed acyclic graph (DAG) that propagates nonlinear scalar fields along 1D road networks. Next, the method extends the scalar fields to a 2D space by identifying key intersecting points in the DAG and calculating the scalar fields for every point, yielding a weighted Voronoi diagram like effect of space division. Two case studies demonstrate that the Topology Density Map supplies accurate information to users and provides an intuitive visualization for decision making. An interview with domain experts demonstrates the feasibility, usability, and effectiveness of our method.", "keywords": "Density map,network topology,urban data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030469", "refList": ["10.1109/vast.2009.5332584", "10.1109/tvcg.2013.193", "10.1080/03081060.2013.844903", "10.1109/tvcg.2018.2864503", "10.1145/2702123.2702419", "10.1109/tvcg.2019.2934670", "10.1109/tits.2015.2496783", "10.1177/1473871615581216", "10.3141/1617-02", "10.1145/2024156.2024169", "10.1111/cgf.13712", "10.1016/j.ejor.2007.02.005", "10.1109/tvcg.2014.2346893", "10.1007/11871842\\_29", "10.1109/vast.2010.5652478", "10.1016/j.visinf.2019.10.002", "10.1109/tvcg.2016.2616404", "10.1109/vl.1996.545307", "10.1145/2629592", "10.1155/2018/2696037", "10.1061/(asce)0733-947x(1998)124:4(368", "10.3141/1899-21", "10.1023/a:1026123329433", "10.1109/mcg.2010.79", "10.1057/palgrave.ivs.9500174", "10.1109/tcyb.2019.2963681", "10.1109/tvcg.2015.2467554", "10.1111/cgf.12114", "10.1145/2814575", "10.1016/j.jcps.2014.08.002", "10.1109/2945.981847", "10.1080/03052150210909", "10.1109/tciaig.2012.2186810", "10.1109/tits.2017.2683539", "10.1109/iv.2004.1320137", "10.1016/0377-2217(80)90126-5", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2015.2467196", "10.1145/3097983.3098056", "10.1007/s11432-018-9801-4", "10.1109/vast.2014.7042490", "10.1061/(asce)0733-947x(2006)132:2(122", "10.1016/j.tra.2008.03.011", "10.1109/tits.2014.2298892", "10.1016/j.trb.2005.12.003", "10.1007/bf01840357", "10.1109/vast.2011.6102454", "10.1109/tvcg.2013.145", "10.1007/bf02289588", "10.1109/pacificvis.2014.56", "10.1109/mcg.2018.053491730", "10.1109/tvcg.2009.111", "10.1057/palgrave.ivs.9500184", "10.1109/tvcg.2013.173", "10.1109/tvcg.2016.2598432", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13170", "year": "2017", "title": "Visual Comparison of Eye Movement Patterns", "conferenceName": "EuroVis", "authors": "Tanja Blascheck;Markus Schweizer;Fabian Beck;Thomas Ertl", "citationCount": "3", "affiliation": "Blascheck, T (Corresponding Author), Univ Stuttgart, Inst Visualizat \\& Interact Syst, Stuttgart, Germany.\nBlascheck, Tanja; Schweizer, Markus; Ertl, Thomas, Univ Stuttgart, Inst Visualizat \\& Interact Syst, Stuttgart, Germany.\nBeck, Fabian, Univ Duisburg Essen, Inst Comp Sci \\& Business Informat Syst, Essen, Germany.", "countries": "Germany", "abstract": "In eye tracking research, finding eye movement patterns and similar strategies between participants' eye movements is important to understand task solving strategies and obstacles. In this application paper, we present a graph comparison method using radial graphs that show Areas of Interest (AOIs) and their transitions. An analyst investigates a single graph based on dwell times, directed transitions, and temporal AOI sequences. Two graphs can be compared directly and temporal changes may be analyzed. A list and matrix approach facilitate the analyst to contrast more than two graphs guided by visually encoded graph similarities. We evaluated our approach in case studies with three eye tracking and visualization experts. They identified temporal transition patterns of eye movements across participants, groups of participants, and outliers.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13170", "refList": ["10.1145/2857491.2857524", "10.1109/tvcg.2008.117", "10.1177/1473871611416549", "10.1109/tvcg.2015.2468111", "10.1111/cgf.12791", "10.1109/iv.2011.49", "10.1109/tvcg.2012.276", "10.1109/infvis.2001.963281", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2013.263", "10.1109/tvcg.2009.181", "10.1080/13875868.2016.1226839", "10.1109/tvcg.2015.2467871", "10.1109/vissoft.2013.6650549", "10.1145/2578153.2578175", "10.1109/iv.2009.108", "10.1177/1473871612455983", "10.1145/1743666.1743721", "10.1109/vast.2016.7883520", "10.1007/978-3-319-47024-5\\_7", "10.1016/b978-044451020-4/50035-9", "10.1109/iv.2016.28", "10.1109/tvcg.2014.2346677", "10.1145/2470654.2470724", "10.1016/s0169-8141(98)00068-7", "10.1109/tvcg.2007.70521", "10.1111/cgf.12115", "10.1145/2509315.2509326", "10.1109/pacificvis.2016.7465266", "10.1145/2669557.2669558", "10.1109/tvcg.2010.149", "10.1109/tvcg.2009.150", "10.1179/000870403235002042"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00037", "year": "2019", "title": "A Visual Approach for the Comparative Analysis of Character Networks in Narrative Texts", "conferenceName": "PacificVis", "authors": "Markus John;Martin Baumann", "citationCount": "1", "affiliation": "John, M (Corresponding Author), Univ Stuttgart, Inst Visualizat \\& Interact Syst VIS, Stuttgart, Germany.\nJohn, Markus; Baumann, Martin; Schuetz, David; Koch, Steffen; Ertl, Thomas, Univ Stuttgart, Inst Visualizat \\& Interact Syst VIS, Stuttgart, Germany.", "countries": "Germany", "abstract": "The analysis of a novel's plot and characters are challenging and time-consuming tasks in literary criticism. Typically, humanities scholars want to describe and compare characters' personality traits, their roles, their relationships, and the evolution of these aspects over the course of a novel. Nowadays, due to the digitization of literature, humanities scholars can be supported in these endeavors with computational methods. In this paper, we present an approach that offers several means to analyze the plot and characters of a novel visually. Analysts can easily switch between an adjacency matrix and a node-link representation, which provide an overview of the characters and the relationships between them. Both views enable analysts to select different text ranges of the novel for studying the commonalities and differences of the character constellations within these ranges. We offer interactive visual representations to help investigate the relationships between the characters in more detail. Additionally, we link the visual representations with the novels' texts to support the inspection and verification of previously generated ideas and hypotheses. To demonstrate the benefits and limitations of our approach, we present two usage scenarios. The first one is based on a fictitious analysis and the second one discusses applications that were carried out during joint workshops with humanities scholars. Finally, we present and discuss the insights gained by an expert study and the design decisions of our approach.", "keywords": "Visual text analysis; document analysis; close reading; distant reading; digital humanities; graph comparison", "link": "https://doi.org/10.1109/PacificVis.2019.00037", "refList": ["10.1177/1473871611416549", "10.1145/2702123.2702476", "10.1111/cgf.13181", "10.1111/cgf.12791", "10.1111/cgf.13170", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2011.185", "10.1109/vast.2009.5333248", "10.1109/tvcg.2015.2467971", "10.1080/10447318.2010.516722", "10.1136/qshc.2004.010033", "10.1109/tvcg.2011.169", "10.3115/v1/p14-5010", "10.1109/tmm.2016.2614184", "10.1109/tvcg.2009.106", "10.1111/cgf.12124", "10.1109/iv.2016.28", "10.1145/2470654.2470724", "10.1057/palgrave.ivs.9500180", "10.1109/pacificvis.2011.5742388", "10.2190/ec.44.1.a", "10.13140/2.1.1341.1520", "10.1109/tvcg.2011.232", "10.1109/vast.2007.4389004", "10.1057/palgrave.ivs.9500092", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.13190", "year": "2017", "title": "Visualization of Delay Uncertainty and its Impact on Train Trip Planning: A Design Study", "conferenceName": "EuroVis", "authors": "Marcel Wunderlich;Kathrin Ballweg;Georg Fuchs;Tatiana von Landesberger", "citationCount": "4", "affiliation": "Wunderlich, M (Corresponding Author), Tech Univ Darmstadt, Darmstadt, Germany.\nWunderlich, M.; Ballweg, K.; von Landesberger, T., Tech Univ Darmstadt, Darmstadt, Germany.\nFuchs, G., Fraunhofer IAIS, St Augustin, Germany.", "countries": "Germany", "abstract": "Uncertainty about possible train delays has an impact on train trips, as the exact arrival time is unknown during trip planning. Delays can lead to missing a connecting train at the transfer station, or to coming too late to an appointment at the destination. Facing this uncertainty, the traveler may wish to use an earlier train or a different connection arriving well before the appointment. Currently, train trip planning is based on scheduled times of connections between two stations. Information about approximate delays is only available shortly before train departure. Although several visualization approaches can show temporal uncertainty, we are not aware of any visual design specifically supporting trip planning, which can show delay uncertainty and its impact on the connections. We propose and evaluate a visual design which extends train trip planning with delay uncertainty. It shows the scheduled train connections together with their expected train delays as well as their impacts on both the arrival time, and the potential of missing a transfer. The visualization also includes information about alternative connections in case of these critical transfers. In this way the user is able to judge which train connection is suitable for a trip. We conducted a user study with 76 participants to evaluate our design. We compared it to two alternative presentations that are prominent in Germany. The study showed that our design performs comparably well for tasks concerning train schedules. The additional uncertainty display as well as the visualization of alternative connections was appreciated and well understood. The participants were able to estimate when they would likely arrive at their destination despite possible train delays while they were unable to estimate this with existing presentations. The users would prefer to use the new design for their trip planning.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13190", "refList": ["10.1109/tvcg.2008.141", "10.1111/cgf.12113", "10.1109/tvcg.2015.2468111", "10.1179/1743277414y.0000000099", "10.1037/1082-989x.4.3.243", "10.1145/2993901.2993919", "10.1109/tvcg.2014.2346893", "10.1007/978-3-540-70956-5\\_2", "10.1007/978-1-4471-2804-5\\_6", "10.1109/tvcg.2011.205", "10.1007/978-3-642-38527-8\\_6", "10.1109/tvcg.2015.2467592", "10.1094/pdis-08-11-0707", "10.1559/1523040054738936", "10.1111/j.1467-8659.2012.03085.x", "10.1007/978-1-84800-269-2\\_2", "10.1109/metrics.2005.24", "10.1016/s1053-8100(02)00007-7", "10.1007/978-0-85729-079-3", "10.1016/j.jebo.2011.08.009", "10.1559/152304009789786326", "10.1109/tvcg.2015.2467752", "10.1111/j.1467-8659.2009.01441.x", "10.1109/tits.2015.2436897", "10.1109/tvcg.2012.279", "10.1109/tvcg.2012.220", "10.1007/978-3-642-36763-2\\_41", "10.1145/2858036.2858558", "10.1179/000870403235002042"], "wos": 1, "children": [{"doi": "10.1111/cgf.13698", "year": "2019", "title": "Visual-Interactive Preprocessing of Multivariate Time Series Data", "conferenceName": "EuroVis", "authors": "J{\\\"{u}}rgen Bernard;Marco Hutter;Heiko Reinemuth;Hendrik Pfeifer;Christian Bors;J{\\\"{o}}rn Kohlhammer", "citationCount": "2", "affiliation": "Bernard, J (Corresponding Author), Tech Univ Darmstadt, Darmstadt, Germany.\nBernard, Juergen; Hutter, Marco; Reinemuth, Heiko; Pfeifer, Hendrik, Tech Univ Darmstadt, Darmstadt, Germany.\nBors, Christian, TU Wien, Vienna, Austria.\nKohlhammer, Joern, Fraunhofer IGD, Darmstadt, Germany.", "countries": "Germany;Austria", "abstract": "Pre-processing is a prerequisite to conduct effective and efficient downstream data analysis. Pre-processing pipelines often require multiple routines to address data quality challenges and to bring the data into a usable form. For both the construction and the refinement of pre-processing pipelines, human-in-the-loop approaches are highly beneficial. This particularly applies to multivariate time series, a complex data type with multiple values developing over time. Due to the high specificity of this domain, it has not been subject to in-depth research in visual analytics. We present a visual-interactive approach for preprocessing multivariate time series data with the following aspects. Our approach supports analysts to carry out six core analysis tasks related to pre-processing of multivariate time series. To support these tasks, we identify requirements to baseline toolkits that may help practitioners in their choice. We characterize the space of visualization designs for uncertainty-aware pre-processing and justify our decisions. Two usage scenarios demonstrate applicability of our approach, design choices, and uncertainty visualizations for the six analysis tasks. This work is one step towards strengthening the visual analytics support for data pre-processing in general and for uncertainty-aware pre-processing of multivariate time series in particular.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13698", "refList": ["10.1016/j.engappai.2010.09.007", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13190", "10.1177/1473871611416549", "10.1007/3-7908-1701-5\\_9", "10.1109/icdmw.2009.55", "10.3758/s13423-012-0247-5", "10.1109/pacificvis.2010.5429596", "10.1177/1473871611415994", "10.1117/12.2080001", "10.1145/2379776.2379788", "10.1145/3105971.3105984", "10.1016/j.patcog.2005.01.025", "10.1594/pangaea.150017", "10.1109/tvcg.2013.178", "10.1109/tvcg.2018.2859973", "10.1109/tvcg.2011.195", "10.1145/1126004.1126005", "10.1145/1656274.1656278", "10.1594/pangaea.804156", "10.1594/pangaea.787726", "10.1109/tvcg.2018.2865077", "10.1007/978-1-4471-6497-5\\_1", "10.2312/eurova.20151104", "10.1109/tvcg.2012.285", "10.1023/a:1024988512476", "10.1109/tvcg.2010.225", "10.1109/vast.2015.7347672", "10.1007/s00799-014-0134-y", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2018.2864907", "10.1007/s40430-018-1079-7", "10.1007/978-3-642-32498-7\\_5", "10.1111/cgf.13237", "10.1109/vast.2009.5332611", "10.1109/tvcg.2015.2467752", "10.1016/j.cviu.2006.08.002", "10.1109/tvcg.2014.2346321", "10.1145/1014052.1014104", "10.1007/978-3-540-71949-6", "10.2312/sca/sca10/001-010", "10.1145/3025453.3025738", "10.2312/eurova.20181112", "10.1109/pacificvis.2018.00034", "10.1007/978-0-85729-079-3\\_8", "10.1109/tvcg.2017.2779501", "10.1109/tvcg.2008.166", "10.1007/s10618-012-0285-7", "10.1109/tvcg.2013.222", "10.1109/tvcg.2018.2864889", "10.1109/tvcg.2016.2598592", "10.1109/vast.2010.5652530", "10.1145/2723372.2731081", "10.1109/tvcg.2018.2864914", "10.1109/infvis.2000.885098", "10.1016/j.neucom.2017.01.105", "10.2352/issn.2470-1173.2017.1.vda-387", "10.1109/tvcg.2015.2467851", "10.1109/tvcg.2016.2598468", "10.1109/tvcg.2016.2603178"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030364", "title": "Supporting the Problem-Solving Loop: Designing Highly Interactive Optimisation Systems", "year": "2020", "conferenceName": "VAST", "authors": "Jie Liu;Tim Dwyer;Guido Tack;Samuel Gratzl;Kim Marriott", "citationCount": "0", "affiliation": "Liu, J (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Liu, Jie; Dwyer, Tim; Tack, Guido; Gratzl, Samuel; Marriott, Kim, Monash Univ, Clayton, Vic, Australia.", "countries": "Australia", "abstract": "Efficient optimisation algorithms have become important tools for finding high-quality solutions to hard, real-world problems such as production scheduling, timetabling, or vehicle routing. These algorithms are typically \u201cblack boxes\u201d that work on mathematical models of the problem to solve. However, many problems are difficult to fully specify, and require a \u201chuman in the loop\u201d who collaborates with the algorithm by refining the model and guiding the search to produce acceptable solutions. Recently, the Problem-Solving Loop was introduced as a high-level model of such interactive optimisation. Here, we present and evaluate nine recommendations for the design of interactive visualisation tools supporting the Problem-Solving Loop. They range from the choice of visual representation for solutions and constraints to the use of a solution gallery to support exploration of alternate solutions. We first examined the applicability of the recommendations by investigating how well they had been supported in previous interactive optimisation tools. We then evaluated the recommendations in the context of the vehicle routing problem with time windows (VRPTW). To do so we built a sophisticated interactive visual system for solving VRPTW that was informed by the recommendations. Ten participants then used this system to solve a variety of routing problems. We report on participant comments and interaction patterns with the tool. These showed the tool was regarded as highly usable and the results generally supported the usefulness of the underlying recommendations.", "keywords": "Interactive optimisation,Interface design,Usability,Interactive systems and tools,Vehicle routing", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030364", "refList": ["10.1176/appi.ajp.2018.18070881", "10.1006/obhd.1997.2679", "10.1109/vlhcc.2018.8506576", "10.1177/0956797611417632", "10.1177/0956797619830649", "10.1177/0049124115610347", "10.1177/1948550618775108", "10.1145/3313831.3376533", "10.1511/2014.111.460", "10.1145/2858036.2858465", "10.1145/3173574.3173606", "10.1177/1745691616658637", "10.1038/483531a", "10.1145/3173574.3174053", "10.1037/pspp0000186", "10.1016/j.jclinepi.2015.05.029", "10.1126/science.aac4716", "10.1145/985692.985782", "10.1080/02699931.2018.1524747", "10.1073/pnas.1402786111", "10.1145/3290605.3300295", "10.1111/cgf.13698", "10.1145/3290605.3300709", "10.2307/2686111", "10.1214/17-ba1091", "10.1145/1449715.1449732", "10.2139/ssrn.2694998", "10.1145/3313831.3376777", "10.1177/2515245917747646", "10.1145/3290605.3300754", "10.1145/3025453.3025626", "10.1109/tvcg.2014.2346321", "10.1037/gpr0000123", "10.2139/ssrn.2535453", "10.1145/3025453.3025738", "10.1038/533452a", "10.1177/1948550617714584", "10.1007/s11222-013-9416-2", "10.3389/fpsyg.2016.01832", "10.1038/s41562-018-0506-1", "10.1007/s11222-016-9696-4", "10.1177/1367006918763132", "10.1109/tsmc.1981.4308636", "10.1038/nrd3439-c1", "10.1177/0956797617723726", "10.1111/j.1467-8659.2012.03116.x", "10.1146/annurev-psych-122216-011836", "10.1145/3290605.3300432"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 5}], "len": 199}, {"doi": "10.1109/tvcg.2015.2468078", "title": "Reducing Snapshots to Points: A Visual Analytics Approach to Dynamic Network Exploration", "year": "2015", "conferenceName": "VAST", "authors": "Stef van den Elzen;Danny Holten;Jorik Blaas;Jarke J. van Wijk", "citationCount": "68", "affiliation": "van den Elzen, S (Corresponding Author), Univ Technol \\& SynerScope, Eindhoven, Netherlands. van den Elzen, Stef, Univ Technol \\& SynerScope, Eindhoven, Netherlands. Holten, Danny; Blaas, Jorik, SynerScope, Helvoirt, Netherlands. van Wijk, Jarke J., Eindhoven Univ Technol, NL-5600 MB Eindhoven, Netherlands.", "countries": "Netherlands", "abstract": "We propose a visual analytics approach for the exploration and analysis of dynamic networks. We consider snapshots of the network as points in high-dimensional space and project these to two dimensions for visualization and interaction using two juxtaposed views: one for showing a snapshot and one for showing the evolution of the network. With this approach users are enabled to detect stable states, recurring states, outlier topologies, and gain knowledge about the transitions between states and the network evolution in general. The components of our approach are discretization, vectorization and normalization, dimensionality reduction, and visualization and interaction, which are discussed in detail. The effectiveness of the approach is shown by applying it to artificial and real-world dynamic networks.", "keywords": "Dynamic Networks, Exploration, Dimensionality Reduction", "link": "http://dx.doi.org/10.1109/TVCG.2015.2468078", "refList": ["10.1109/infvis.2004.18", "10.1111/j.1467-8659.2009.01664.x", "10.1016/0025-5564(79)90080-4", "10.1111/cgf.12106", "10.1109/tvcg.2011.226", "10.1111/j.1467-8659.2009.01451.x", "10.1109/tvcg.2013.263", "10.1109/vissoft.2014.30", "10.1002/sam.10071", "10.1109/tvcg.2013.254", "10.1109/infvis.1999.801851", "10.1371/journal.pone.0008694", "10.1109/tvcg.2006.193", "10.1002/1097-024x(200009)30:11", "10.1007/978-3-319-06793-3\\_8", "10.1126/science.1116869", "10.1080/14786440109462720", "10.1109/pacificvis.2013.6596125", "10.1371/journal.pone.0107878", "10.1109/tvcg.2013.198", "10.2312/eurovisstar.20141174", "10.1145/2556288.2557010", "10.1109/iv.2002.1028770", "10.1038/30918", "10.1109/tvcg.2006.147", "10.1109/tvcg.2011.178", "10.1109/wi.2006.118", "10.1109/tvcg.2013.109", "10.1111/j.1467-8659.2008.01213.x", "10.1016/s0375-9601(99)00757-4", "10.1109/tvcg.2008.125", "10.1007/3-540-36151-0", "10.1109/passat/socialcom.2011.216", "10.1109/vlhcc.2010.31", "10.1109/iv.2009.24", "10.1109/iv.2011.93", "10.1111/cgf.12396", "10.1007/bf02289565", "10.1109/sibgrapi.2013.11", "10.1137/080736417", "10.1177/1473871613487087"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2017.2744898", "title": "VIGOR: Interactive Visual Exploration of Graph Query Results", "year": "2017", "conferenceName": "VAST", "authors": "Robert Pienta;Fred Hohman;Alex Endert;Acar Tamersoy;Kevin A. Roundy;Christopher Gates 0002;Shamkant B. Navathe;Duen Horng Chau", "citationCount": "5", "affiliation": "Pienta, R (Corresponding Author), Georgia Tech, Atlanta, GA 30332 USA. Pienta, Robert; Hohman, Fred; Endert, Alex; Navathe, Shamkant; Chau, Duen Horng, Georgia Tech, Atlanta, GA 30332 USA. Tamersoy, Acar; Roundy, Kevin; Gates, Chris, Symantec Res Labs, Mountain View, CA USA.", "countries": "USA", "abstract": "Finding patterns in graphs has become a vital challenge in many domains from biological systems, network security, to finance (e.g., finding money laundering rings of bankers and business owners). While there is significant interest in graph databases and querying techniques, less research has focused on helping analysts make sense of underlying patterns within a group of subgraph results. Visualizing graph query results is challenging, requiring effective summarization of a large number of subgraphs, each having potentially shared node-values, rich node features, and flexible structure across queries. We present VIGOR, a novel interactive visual analytics system, for exploring and making sense of query results. VIGOR uses multiple coordinated views, leveraging different data representations and organizations to streamline analysts sensemaking process. VIGOR contributes: (1) an exemplar-based interaction technique, where an analyst starts with a specific result and relaxes constraints to find other similar results or starts with only the structure (i.e., without node value constraints), and adds constraints to narrow in on specific results; and (2) a novel feature-aware subgraph result summarization. Through a collaboration with Symantec, we demonstrate how VIGOR helps tackle real-world problems through the discovery of security blindspots in a cybersecurity dataset with over 11,000 incidents. We also evaluate VIGOR with a within-subjects study, demonstrating VIGOR's ease of use over a leading graph database management system, and its ability to help analysts understand their results at higher speed and make fewer errors.", "keywords": "graph querying,subgraph results,query result visualization", "link": "http://dx.doi.org/10.1109/TVCG.2017.2744898", "refList": ["10.1145/1322432.1322433", "10.1145/2702123.2702476", "10.1109/35021bigcomp.2015.7072812", "10.1145/2909132.2909246", "10.1111/j.1467-8659.2011.01898.x", "10.1006/jvlc.1997.0037", "10.1023/a:1009745219419", "10.1002/widm.30", "10.1109/bigdata.2014.7004278", "10.1109/52.329404", "10.1109/2945.841119", "10.1137/1.9781611973440.11", "10.1109/icde.2008.4497505", "10.1016/j.aml.2007.01.006", "10.1037/h0071325", "10.1093/bioinformatics/bti556", "10.2312/eurovisstar.20141174", "10.1109/tvcg.2015.2467717", "10.1109/tvcg.2015.2468078", "10.1145/2470654.2466444", "10.1002/qre.441", "10.1145/345513.345282", "10.1109/icdmw.2008.99", "10.1145/1242572.1242600", "10.1147/sj.164.0324", "10.1007/978-1-84996-074-8\\_4", "10.1002/spe.4380180302", "10.1073/pnas.0307545100", "10.1007/bf02289565", "10.1145/1541880.1541882", "10.1093/comjnl/9.1.60"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865139", "title": "Structure-Based Suggestive Exploration: A New Approach for Effective Exploration of Large Networks", "year": "2018", "conferenceName": "InfoVis", "authors": "Wei Chen;Fangzhou Guo;Dongming Han;Jacheng Pan;Xiaotao Nie;Jiazhi Xia;Xiaolong Zhang", "citationCount": "5", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Labo CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Chen, Wei; Guo, Fangzhou; Han, Dongming; Pan, Jacheng; Nie, Xiaotao, Zhejiang Univ, State Key Labo CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Xia, Jiazhi, Cent S Univ, Changsha, Hunan, Peoples R China. Zhang, Xiaolong, Penn State Univ, University Pk, PA 16802 USA.", "countries": "USA;China", "abstract": "When analyzing a visualized network, users need to explore different sections of the network to gain insight. However, effective exploration of large networks is often a challenge. While various tools are available for users to explore the global and local features of a network, these tools usually require significant interaction activities, such as repetitive navigation actions to follow network nodes and edges. In this paper, we propose a structure-based suggestive exploration approach to support effective exploration of large networks by suggesting appropriate structures upon user request. Encoding nodes with vectorized representations by transforming information of surrounding structures of nodes into a high dimensional space, our approach can identify similar structures within a large network, enable user interaction with multiple similar structures simultaneously, and guide the exploration of unexplored structures. We develop a web-based visual exploration system to incorporate this suggestive exploration approach and compare performances of our approach under different vectorizing methods and networks. We also present the usability and effectiveness of our approach through a controlled user study with two datasets.", "keywords": "Large Network Exploration,Structure-Based Exploration,Suggestive Exploration", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865139", "refList": ["10.1109/tvcg.2007.70582", "10.1109/tvcg.2006.120", "10.1109/mc.2013.242", "10.1109/tvcg.2009.108", "10.1145/2939672.2939754", "10.1016/j.comnet.2011.08.019", "10.1145/2702123.2702476", "10.1109/tvcg.2017.2744938", "10.1007/s00371-013-0892-3", "10.1137/1.9781611974973.67", "10.1109/vast.2009.5333893", "10.1109/tst.2013.6509098", "10.1109/35021bigcomp.2015.7072812", "10.1145/2909132.2909246", "10.1111/j.1467-8659.2011.01957.x", "10.1111/j.1467-8659.2011.01898.x", "10.1177/1473871612455749", "10.1109/tvcg.2013.167", "10.1145/3097983.3098061", "10.1007/978-1-4613-0303-9\\_28", "10.1109/vast.2014.7042485", "10.1109/tmm.2016.2614220", "10.1145/1376616.1376675", "10.1145/2623330.2623732", "10.14778/1920841.1920887", "10.1109/tvcg.2017.2745219", "10.1109/tvcg.2017.2743858", "10.1109/tvcg.2008.151", "10.1145/1150402.1150479", "10.1093/bioinformatics/bth436", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2016.2598958", "10.1111/cgf.12883", "10.1145/1556262.1556300", "10.1109/icdm.2012.159", "10.1145/2470654.2466444", "10.1109/tvcg.2013.109", "10.1109/infvis.2004.1", "10.1109/icdmw.2008.99", "10.1002/aris.1440370106", "10.1007/978-3-319-05813-9\\_11", "10.1371/journal.pone.0098679", "10.1109/tvcg.2006.106", "10.1111/j.1467-8659.2011.01935.x", "10.1111/cgf.12397", "10.1111/cgf.13184", "10.1111/cgf.12642", "10.1109/tvcg.2016.2598831", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934396", "title": "A Deep Generative Model for Graph Layout", "year": "2019", "conferenceName": "InfoVis", "authors": "Oh-Hyun Kwon;Kwan-Liu Ma", "citationCount": "4", "affiliation": "Kwon, OH (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Kwon, Oh-Hyun; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "Different layouts can characterize different aspects of the same graph. Finding a \u201cgood\u201d layout of a graph is thus an important task for graph visualization. In practice, users often visualize a graph in multiple layouts by using different methods and varying parameter settings until they find a layout that best suits the purpose of the visualization. However, this trial-and-error process is often haphazard and time-consuming. To provide users with an intuitive way to navigate the layout design space, we present a technique to systematically visualize a graph in diverse layouts using deep generative models. We design an encoder-decoder architecture to learn a model from a collection of example layouts, where the encoder represents training examples in a latent space and the decoder produces layouts from the latent space. In particular, we train the model to construct a two-dimensional latent space for users to easily explore and generate various layouts. We demonstrate our approach through quantitative and qualitative evaluations of the generated layouts. The results of our evaluations show that our model is capable of learning and generalizing abstract concepts of graph layouts, not just memorizing the training examples. In summary, this paper presents a fundamentally new approach to graph visualization where a machine learning model learns to visualize a graph from examples without manually-defined heuristics.", "keywords": "Graph,network,visualization,layout,machine learning,deep learning,neural network,generative model,autoencoder", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934396", "refList": ["10.1103/physreve.74.036104", "10.1145/234535.234538", "10.2307/2412323", "10.1109/tvcg.2007.70580", "10.7155/jgaa.00405", "10.1177/1473871612455749", "10.1109/tvcg.2011.185", "10.1073/pnas.122653799", "10.1007/3-540-58950-3", "10.1145/2897824.2925974", "10.1007/3-540-44541-2\\_17", "10.1007/bf00410640", "10.1021/acscentsci.7b00572", "10.1007/s10208-011-9093-5", "10.1109/tvcg.2015.2467451", "10.1016/0020-0190(89)90102-6", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13187", "10.1214/aoms/1177729586", "10.1109/tvcg.2014.2346277", "10.1109/tvcg.2017.2743858", "10.1007/978-3-662-44043-8\\_3", "10.1038/30918", "10.1109/mcg.2018.2881501", "10.1016/j.camwa.2004.08.015", "10.1109/tvcg.2010.269", "10.1006/s1045-926x(02)00016-2", "10.1016/0925-7721(94)00014-x", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1103/physrevx.4.011047", "10.1371/journal.pone.0098679", "10.1145/2487788.2488173", "10.1007/978-3-030-01418-6\\_41", "10.1007/978-3-030-04414-5\\_12", "10.1109/tvcg.2018.2865139", "10.1142/s0219525903001067", "10.7155/jgaa.00051"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030367", "title": "Lyra 2: Designing Interactive Visualizations by Demonstration", "year": "2020", "conferenceName": "InfoVis", "authors": "Jonathan Zong;Dhiraj Barnwal;Rupayan Neogy;Arvind Satyanarayan", "citationCount": "0", "affiliation": "Zong, J (Corresponding Author), MIT, Cambridge, MA 02139 USA. Zong, Jonathan; Neogy, Rupayan; Satyanarayan, Arvind, MIT, Cambridge, MA 02139 USA. Barnwal, Dhiraj, Indian Inst Technol Kharagpur, Kharagpur, W Bengal, India.", "countries": "India;USA", "abstract": "Recent graphical interfaces offer direct manipulation mechanisms for authoring visualizations, but are largely restricted to static output. To author interactive visualizations, users must instead turn to textual specification, but such approaches impose a higher technical burden. To bridge this gap, we introduce Lyra 2, a system that extends a prior visualization design environment with novel methods for authoring interaction techniques by demonstration. Users perform an interaction (e.g., button clicks, drags, or key presses) directly on the visualization they are editing. The system interprets this performance using a set of heuristics and enumerates suggestions of possible interaction designs. These heuristics account for the properties of the interaction (e.g., target and event type) as well as the visualization (e.g., mark and scale types, and multiple views). Interaction design suggestions are displayed as thumbnails; users can preview and test these suggestions, iteratively refine them through additional demonstrations, and finally apply and customize them via property inspectors. We evaluate our approach through a gallery of diverse examples, and evaluate its usability through a first-use study and via an analysis of its cognitive dimensions. We find that, in Lyra 2, interaction design by demonstration enables users to rapidly express a wide range of interactive visualizations.", "keywords": "Direct manipulation,interactive visualization,interaction design by demonstration", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030367", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030467", "title": "PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch;Lingyun Yu;Peiran Ren;Thomas Ertl;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Knittel, Johannes; Koch, Steffen; Ertl, Thomas, Univ Stuttgart, VIS VISUS, Stuttgart, Germany. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Dept Comp Sci \\& Software Engn, Suzhou, Peoples R China. Ren, Peiran, Alibaba Grp, Hangzhou, Peoples R China.", "countries": "Germany;China", "abstract": "Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.", "keywords": "Storyline visualization,reinforcement learning,mixed-initiative design", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030467", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030459", "title": "Scalability of Network Visualisation from a Cognitive Load Perspective", "year": "2020", "conferenceName": "InfoVis", "authors": "Vahan Yoghourdjian;Yalong Yang;Tim Dwyer;Lawrence Lee;Michael Wybrow;Kim Marriott", "citationCount": "0", "affiliation": "Yoghourdjian, V (Corresponding Author), Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Yoghourdjian, Vahan; Yang, Yalong; Dwyer, Tim; Wybrow, Michael; Marriott, Kim, Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Lawrence, Lee, Monash Univ, Fac Business \\& Econ, Melbourne, Vic, Australia. Yang, Yalong, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA.", "countries": "USA;Australia", "abstract": "Node-link diagrams are widely used to visualise networks. However, even the best network layout algorithms ultimately result in \u2018hairball\u2019 visualisations when the graph reaches a certain degree of complexity, requiring simplification through aggregation or interaction (such as filtering) to remain usable. Until now, there has been little data to indicate at what level of complexity node-link diagrams become ineffective or how visual complexity affects cognitive load. To this end, we conducted a controlled study to understand workload limits for a task that requires a detailed understanding of the network topology-finding the shortest path between two nodes. We tested performance on graphs with 25 to 175 nodes with varying density. We collected performance measures (accuracy and response time), subjective feedback, and physiological measures (EEG, pupil dilation, and heart rate variability). To the best of our knowledge this is the first network visualisation study to include physiological measures. Our results show that people have significant difficulty finding the shortest path in high density node-link diagrams with more than 50 nodes and even low density graphs with more than 100 nodes. From our collected EEG data we observe functional differences in brain activity between hard and easy tasks. We found that cognitive load increased up to certain level of difficulty after which it decreased, likely because participants had given up. We also explored the effects of global network layout features such as size or number of crossings, and features of the shortest path such as length or straightness on task difficulty. We found that global features generally had a greater impact than those of the shortest path.", "keywords": "Data Visualisation,Network Visualisation,Cognitive Load,EEG", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030459", "refList": ["10.1109/tvcg.2019.2934396", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2016.2570755", "10.1007/s00371-013-0892-3", "10.1007/b98835", "10.1109/isda.2014.7066252", "10.1109/tvcg.2015.2467251", "10.1177/1473871612455749", "10.1109/tvcg.2012.299", "10.1007/3-540-58950-3", "10.1111/cgf.12878", "10.1109/mcse.2007.55", "10.1109/tvcg.2012.238", "10.1145/264645.264657", "10.1109/tvcg.2015.2467451", "10.1109/tvcg.2013.151", "10.1016/0020-0190(89)90102-6", "10.1109/tvcg.2019.2934307", "10.1109/tvcg.2015.2468151", "10.1109/tvcg.2017.2745919", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13440", "10.1109/tvcg.2011.220", "10.1111/cgf.13187", "10.1109/t-c.1969.222678", "10.1109/tvcg.2017.2743858", "10.1126/science.290.5500.2319", "10.1109/cahpc.2018.8645912", "10.1109/tvcg.2015.2465151", "10.1109/tvcg.2016.2598958", "10.1109/tvcg.2017.2751473", "10.1002/spe.4380211102", "10.1109/tpds.2018.2869805", "10.1016/j.jpdc.2019.04.008", "10.1109/pacificvis.2017.8031574", "10.1006/s1045-926x(02)00016-2", "10.1109/tvcg.2017.2674999", "10.1145/2872427.2883041", "10.1145/3292500.3330989", "10.1109/tvcg.2017.2744878", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1109/sbac-pad.2018.00060", "10.1007/978-3-662-45803-7\\_27", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1111/j.1469-1809.1936.tb02137.x", "10.1007/bf02289565", "10.1109/tvcg.2017.2689016", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.4722", "year": "2020", "title": "A Study of Mental Maps in Immersive Network Visualization", "conferenceName": "PacificVis", "authors": "Joseph Kotlarek;Oh{-}Hyun Kwon;Kwan{-}Liu Ma;Peter Eades;Andreas Kerren;Karsten Klein;Falk Schreiber", "citationCount": "0", "affiliation": "Kotlarek, J (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA.\nKotlarek, Joseph; Kwon, Oh-Hyun; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.\nEades, Peter, Univ Sydney, Sydney, NSW, Australia.\nKerren, Andreas, Linnaeus Univ, Vaxjo, Sweden.\nKlein, Karsten; Schreiber, Falk, Univ Konstanz, Constance, Germany.", "countries": "Sweden;Germany;USA;Australia", "abstract": "The visualization of a network influences the quality of the mental map that the viewer develops to understand the network. In this study, we investigate the effects of a 3D immersive visualization environment compared to a traditional 2D desktop environment on the comprehension of a network's structure. We compare the two visualization environments using three tasks-interpreting network structure, memorizing a set of nodes, and identifying the structural changes-commonly used for evaluating the quality of a mental map in network visualization. The results show that participants were able to interpret network structure more accurately when viewing the network in an immersive environment, particularly for larger networks. However, we found that 2D visualizations performed better than immersive visualization for tasks that required spatial memory.", "keywords": "Human-centered computing; Visualization; Visualization techniques; Graph drawings; Human-centered computing; Visualization; Empirical studies in visualization", "link": "https://doi.org/10.1109/PacificVis48177.2020.4722", "refList": ["10.1103/physreve.74.036104", "10.1109/tvcg.2019.2934396", "10.1007/978-3-540-87730-1\\_9", "10.1117/12.2005484", "10.1177/1473871612455749", "10.1109/pacificvis.2017.8031577", "10.1007/978-3-319-73207-7", "10.1109/38.888006", "10.1109/2945.841119", "10.1109/mc.2005.297", "10.1007/s10055-018-0346-3", "10.1109/tvcg.2016.2599107", "10.1109/icsmc.1992.271688", "10.1038/30918", "10.1006/jvlc.1995.1010", "10.1089/109493101300117938", "10.1109/vrais.1998.658488", "10.1109/pacificvis.2015.7156357", "10.1109/tvcg.2010.78", "10.1109/tvcg.2016.2520921", "10.1007/978-3-030-01388-22", "10.1145/229459.229467", "10.1145/1056808.1056875", "10.1109/tvcg.2017.2744079", "10.1109/bdva.2015.7314293", "10.1086/jar.33.4.3629752", "10.1016/j.ijhcs.2013.08.004"], "wos": 1, "children": [], "len": 1}], "len": 11}, {"doi": "10.1109/tvcg.2020.3030440", "title": "Context-aware Sampling of Large Networks via Graph Representation Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Zhiguang Zhou;Chen Shi;Xilong Shen;Lihong Cai;Haoxuan Wang;Yuhua Liu;Ying Zhao;Wei Chen", "citationCount": "0", "affiliation": "Zhao, Y (Corresponding Author), Cent South Univ, Changsha, Peoples R China. Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Zhou, Zhiguang; Shi, Chen; Shen, Xilong; Cai, Lihong; Wang, Haoxuan; Liu, Yuhua, Zhejiang Univ Finance \\& Econ, Sch Informat, Hangzhou, Peoples R China. Zhao, Ying, Cent South Univ, Changsha, Peoples R China. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China.", "countries": "China", "abstract": "Numerous sampling strategies have been proposed to simplify large-scale networks for highly readable visualizations. It is of great challenge to preserve contextual structures formed by nodes and edges with tight relationships in a sampled graph, because they are easily overlooked during the process of sampling due to their irregular distribution and immunity to scale. In this paper, a new graph sampling method is proposed oriented to the preservation of contextual structures. We first utilize a graph representation learning (GRL) model to transform nodes into vectors so that the contextual structures in a network can be effectively extracted and organized. Then, we propose a multi-objective blue noise sampling model to select a subset of nodes in the vectorized space to preserve contextual structures with the retention of relative data and cluster densities in addition to those features of significance, such as bridging nodes and graph connections. We also design a set of visual interfaces enabling users to interactively conduct context-aware sampling, visually compare results with various sampling strategies, and deeply explore large networks. Case studies and quantitative comparisons based on real-world datasets have demonstrated the effectiveness of our method in the abstraction and exploration of large networks.", "keywords": "Graph sampling,Graph representation learning,Blue noise sampling,Graph evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030440", "refList": ["10.1145/2491159.2491168", "10.1016/j.physa.2015.04.035", "10.1145/1830252.1830274", "10.1109/icdmw.2007.91", "10.1002/net.21834", "10.1109/tvcg.2018.2864503", "10.1016/j.cag.2018.01.010", "10.1109/icc.2016.7511156", "10.1111/cgf.13444", "10.1145/956750.956831", "10.1145/364099.364331", "10.1007/s00180-016-0663-5", "10.1109/tvcg.2013.223", "10.1007/s12650-018-0530-2", "10.1103/physreve.73.016102", "10.1109/access.2018.2870684", "10.1007/978-3-319-06793-3\\_1", "10.2312/vissym/eurovis05/239-246", "10.1016/j.ins.2015.02.014", "10.1145/2339530.2339723", "10.1109/icde.2015.7113345", "10.1109/tvcg.2011.233", "10.14778/2809974.2809980", "10.1109/glocom.2015.7417471", "10.1145/2578153.2578175", "10.1214/aoms/1177705148", "10.1109/tvcg.2008.130", "10.14232/actacyb.20.1.2011.6", "10.1504/ijitm.2019.099809", "10.1109/tvcg.2018.2865020", "10.1145/956750", "10.1002/cpe.4330060203", "10.1145/1150402.1150479", "10.1103/physreve.72.036118", "10.1109/tvcg.2017.2744098", "10.1145/2020408.2020512", "10.1142/s0129183114400075", "10.1109/jsac.2011.111005", "10.1016/j.camwa.2011.11.057", "10.1145/2470654.2466444", "10.1109/tvcg.2017.2674999", "10.1214/aos/1013203451", "10.1109/icdcsw.2011.34", "10.1016/j.physa.2013.11.015", "10.1145/1081870.1081893", "10.1109/tnet.2008.2001730", "10.1109/access.2016.2633485", "10.1145/1879141.1879192", "10.1371/journal.pone.0098679", "10.1126/science.220.4598.671", "10.1109/pacificvis.2015.7156355", "10.1088/1475-7516/2011/08/011", "10.1007/978-3-319-27261-0\\_41", "10.1111/cgf.13410", "10.1109/tvcg.2018.2865139", "10.1109/tvcg.2016.2598831", "10.1016/j.physa.2014.06.065"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030447", "title": "DRGraph: An Efficient Graph Layout Algorithm for Large-scale Graphs by Dimensionality Reduction", "year": "2020", "conferenceName": "InfoVis", "authors": "Minfeng Zhu;Wei Chen;Yuanzhe Hu;Yuxuan Hou;Liangjun Liu;Kaiyuan Zhang", "citationCount": "0", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Zhu, Minfeng; Chen, Wei; Hu, Yuanzhe; Hou, Yuxuan; Liu, Liangjun; Zhang, Kaiyuan, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China.", "countries": "China", "abstract": "Efficient layout of large-scale graphs remains a challenging problem: the force-directed and dimensionality reduction-based methods suffer from high overhead for graph distance and gradient computation. In this paper, we present a new graph layout algorithm, called DRGraph, that enhances the nonlinear dimensionality reduction process with three schemes: approximating graph distances by means of a sparse distance matrix, estimating the gradient by using the negative sampling technique, and accelerating the optimization process through a multi-level layout scheme. DRGraph achieves a linear complexity for the computation and memory consumption, and scales up to large-scale graphs with millions of nodes. Experimental results and comparisons with state-of-the-art graph layout methods demonstrate that DRGraph can generate visually comparable layouts with a faster running time and a lower memory requirement.", "keywords": "graph visualization,graph layout,dimensionality reduction,force-directed layout", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030447", "refList": ["10.1109/tvcg.2007.70582", "10.1371/journal.pone.0136497", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/3018661.3018731", "10.1109/34.491619", "10.1145/2939672.2939754", "10.1145/3269206.3271788", "10.1016/j.comnet.2011.08.019", "10.1007/s11263-011-0442-2", "10.1016/s0020-0190(98)00108-2", "10.1109/tvcg.2018.2865151", "10.1007/978-3-319-61188-4\\_2", "10.1177/1473871612455749", "10.1177/1473871616666394", "10.1109/tvcg.2015.2467035", "10.1186/1471-2105-10-375", "10.1145/3097983.3098061", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1109/tvcg.2015.2467451", "10.1109/2945.841119", "10.1016/j.swevo.2015.10.002", "10.1016/0020-0190(89)90102-6", "10.1109/infvis.2003.1249009", "10.1109/tvcg.2017.2745919", "10.1111/cgf.13187", "10.1111/cgf.13440", "10.1109/tvcg.2012.245", "10.1109/tvcg.2017.2743858", "10.1177/1473871618821740", "10.1186/s12859-015-0585-1", "10.1002/nav.3800020109", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1006/s1045-926x(02)00016-2", "10.1109/cvpr.2012.6247667", "10.1023/b:jogo.0000042115.44455.f3", "10.1109/pacificvis.2017.8031607", "10.1002/nav.3800030404", "10.1109/cvpr.2008.4587500", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1090/s0002-9904-1920-03322-7", "10.1109/iv.2013.3", "10.1145/568522.568523", "10.1109/tvcg.2006.156", "10.1109/tvcg.2012.236", "10.1109/tvcg.2018.2865139", "10.1145/3219819.3220025", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030393", "title": "Exemplar-based Layout Fine-tuning for Node-link Diagrams", "year": "2020", "conferenceName": "InfoVis", "authors": "Jiacheng Pan;Wei Chen;Xiaodong Zhao;Shuyue Zhou;Wei Zeng;Minfeng Zhu;Jian Chen;Siwei Fu;Yingcai Wu", "citationCount": "1", "affiliation": "Chen, W; Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Lab, Hangzhou, Peoples R China. Pan, Jiacheng; Chen, Wei; Zhao, Xiaodong; Zhou, Shuyue; Zhu, Minfeng; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Fu, Siwei; Wu, Yingcai, Zhejiang Lab, Hangzhou, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Jian, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA;China", "abstract": "We design and evaluate a novel layout fine-tuning technique for node-link diagrams that facilitates exemplar-based adjustment of a group of substructures in batching mode. The key idea is to transfer user modifications on a local substructure to other substructures in the entire graph that are topologically similar to the exemplar. We first precompute a canonical representation for each substructure with node embedding techniques and then use it for on-the-fly substructure retrieval. We design and develop a light-weight interactive system to enable intuitive adjustment, modification transfer, and visual graph exploration. We also report some results of quantitative comparisons, three case studies, and a within-participant user study.", "keywords": "Node-link diagram,graph layout,graph visualization,user interactions", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030393", "refList": ["10.1109/tvcg.2007.70582", "10.1371/journal.pone.0136497", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/3018661.3018731", "10.1109/34.491619", "10.1145/2939672.2939754", "10.1145/3269206.3271788", "10.1016/j.comnet.2011.08.019", "10.1007/s11263-011-0442-2", "10.1016/s0020-0190(98)00108-2", "10.1109/tvcg.2018.2865151", "10.1177/1473871612455749", "10.1177/1473871616666394", "10.1109/tvcg.2015.2467035", "10.1186/1471-2105-10-375", "10.1145/3097983.3098061", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1109/tvcg.2015.2467451", "10.1016/j.swevo.2015.10.002", "10.1016/0020-0190(89)90102-6", "10.1109/infvis.2003.1249009", "10.1109/tvcg.2017.2745919", "10.1111/cgf.13187", "10.1111/cgf.13440", "10.1109/tvcg.2012.245", "10.1109/tvcg.2017.2743858", "10.1177/1473871618821740", "10.1186/s12859-015-0585-1", "10.1002/nav.3800020109", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1006/s1045-926x(02)00016-2", "10.1109/cvpr.2012.6247667", "10.1023/b:jogo.0000042115.44455.f3", "10.1109/pacificvis.2017.8031607", "10.1002/nav.3800030404", "10.1109/infvis.2004.1", "10.1109/cvpr.2008.4587500", "10.1109/pacificvis.2011.5742389", "10.1140/epjb/e2011-10979-2", "10.1371/journal.pone.0098679", "10.1090/s0002-9904-1920-03322-7", "10.1109/iv.2013.3", "10.1145/568522.568523", "10.1109/tvcg.2006.156", "10.1109/tvcg.2012.236", "10.1109/tvcg.2018.2865139", "10.1145/3219819.3220025", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}], "len": 19}, {"doi": "10.1109/tvcg.2018.2865149", "title": "Juniper: A Tree+Table Approach to Multivariate Graph Visualization", "year": "2018", "conferenceName": "InfoVis", "authors": "Carolina Nobre;Marc Streit;Alexander Lex", "citationCount": "7", "affiliation": "Nobre, C (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Nobre, Carolina; Lex, Alexander, Univ Utah, Salt Lake City, UT 84112 USA. Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.", "countries": "USA;Austria", "abstract": "Analyzing large, multivariate graphs is an important problem in many domains, yet such graphs are challenging to visualize. In this paper, we introduce a novel, scalable, tree-table multivariate graph visualization technique, which makes many tasks related to multivariate graph analysis easier to achieve. The core principle we follow is to selectively query for nodes or subgraphs of interest and visualize these subgraphs as a spanning tree of the graph. The tree is laid out linearly, which enables us to juxtapose the nodes with a table visualization where diverse attributes can be shown. We also use this table as an adjacency matrix, so that the resulting technique is a hybrid node-link/adjacency matrix technique. We implement this concept in Juniper and complement it with a set of interaction techniques that enable analysts to dynamically grow, restructure, and aggregate the tree, as well as change the layout or show paths between nodes. We demonstrate the utility of our tool in usage scenarios for different multivariate networks: a bipartite network of scholars, papers, and citation metrics and a multitype network of story characters, places, books, etc.", "keywords": "Multivariate graphs,networks,tree-based graph visualization,adjacency matrix,spanning trees,visualization", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865149", "refList": ["10.1109/tvcg.2007.70582", "10.1109/infvis.2000.885091", "10.1109/tvcg.2016.2615308", "10.1109/tvcg.2008.117", "10.1109/tvcg.2009.108", "10.1109/pacificvis.2013.6596127", "10.1111/j.1467-8659.2011.01898.x", "10.1109/visual.1991.175815", "10.1093/bioinformatics/btp454", "10.1093/bioinformatics/btq675", "10.1109/tvcg.2011.247", "10.1111/j.1467-8659.2012.03110.x", "10.1145/2207676.2208293", "10.1056/nejmsa066082", "10.1109/infvis.2003.1249009", "10.1038/nmeth.1436", "10.1073/pnas.95.25.14863", "10.1109/cw.2002.1180907", "10.1109/tvcg.2014.2346248", "10.1111/j.1467-8659.2009.01710.x", "10.1109/tvcg.2006.147", "10.1109/tvcg.2015.2468078", "10.1093/bioinformatics/btx324", "10.1111/cgf.12883", "10.1145/1168149.1168168", "10.1109/pacificvis.2010.5429609", "10.1093/bioinformatics/btn068", "10.1109/tvcg.2006.106", "10.2307/2685881", "10.1057/palgrave.ivs.9500092", "10.1109/tvcg.2016.2598885", "10.1111/j.1467-8659.2009.01687.x", "10.1109/tvcg.2018.2811488", "10.1145/22339.22342", "10.1111/cgf.13184", "10.1111/cgf.12642", "10.1109/biovis.2012.6378600", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934670", "title": "AirVis: Visual Analytics of Air Pollution Propagation", "year": "2019", "conferenceName": "VAST", "authors": "Zikun Deng;Di Weng;Jiahui Chen;Ren Liu;Zhibin Wang;Jie Bao 0003;Yu Zheng 0004;Yingcai Wu", "citationCount": "6", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Deng, Zikun; Weng, Di; Chen, Jiahui; Liu, Ren; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Wang, Zhibin, Zhejiang Univ, Res Ctr Air Pollut \\& Hlth, Hangzhou, Peoples R China. Bao, Jie; Zheng, Yu, JD Intelligent City Res, Beijing, Peoples R China.", "countries": "China", "abstract": "Air pollution has become a serious public health problem for many cities around the world. To find the causes of air pollution, the propagation processes of air pollutants must be studied at a large spatial scale. However, the complex and dynamic wind fields lead to highly uncertain pollutant transportation. The state-of-the-art data mining approaches cannot fully support the extensive analysis of such uncertain spatiotemporal propagation processes across multiple districts without the integration of domain knowledge. The limitation of these automated approaches motivates us to design and develop AirVis, a novel visual analytics system that assists domain experts in efficiently capturing and interpreting the uncertain propagation patterns of air pollution based on graph visualizations. Designing such a system poses three challenges: a) the extraction of propagation patterns; b) the scalability of pattern presentations; and c) the analysis of propagation processes. To address these challenges, we develop a novel pattern mining framework to model pollutant transportation and extract frequent propagation patterns efficiently from large-scale atmospheric data. Furthermore, we organize the extracted patterns hierarchically based on the minimum description length (MDL) principle and empower expert users to explore and analyze these patterns effectively on the basis of pattern topologies. We demonstrated the effectiveness of our approach through two case studies conducted with a real-world dataset and positive feedback from domain experts.", "keywords": "Air pollution propagation,pattern mining,graph visualization", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934670", "refList": ["10.1109/tvcg.2016.2598919", "10.1093/bib/bbr069", "10.1145/2487575.2488188", "10.1109/tvcg.2013.193", "10.1016/j.atmosenv.2014.12.011", "10.1109/tvcg.2013.226", "10.1109/tvcg.2018.2864503", "10.1109/tvcg.2015.2468111", "10.1109/icicta.2015.183", "10.1016/j.atmosenv.2014.05.039", "10.1111/cgf.12791", "10.1111/j.1467-8659.2009.01451.x", "10.1111/j.1467-8659.2011.01898.x", "10.5194/acp-12-5031-2012", "10.1016/j.atmosenv.2008.05.053", "10.1109/tvcg.2016.2535234", "10.1016/j.atmosres.2014.12.003", "10.1109/tvcg.2015.2467194", "10.1109/tvcg.2013.263", "10.1109/icdm.2002.1184038", "10.1145/2783258.2788573", "10.1109/tvcg.2018.2865149", "10.1109/tbdata.2017.2723899", "10.1109/tvcg.2012.311", "10.1109/vl.1996.545307", "10.1007/s12650-018-0481-7", "10.1016/j.envpol.2007.06.012", "10.3155/1047-3289.61.6.660", "10.1080/13658810701349037", "10.1145/3097983.3098090", "10.1109/tvcg.2007.70523", "10.1115/1.2128636", "10.1109/tvcg.2015.2467619", "10.3978/j.issn.2072-1439.2016.01.19", "10.1109/tkde.2005.127", "10.1017/s0269888912000331", "10.2312/conf/eg2013/stars/039-063", "10.1109/tvcg.2014.2346271", "10.1175/bams-d-14-00110.1", "10.1016/0005-1098(78)90005-5", "10.1007/s10618-006-0044-8", "10.1109/tvcg.2018.2865126", "10.2307/1912791", "10.3390/su6085322", "10.1007/s12650-018-0489-z", "10.2312/eurovisstar.20151109", "10.1109/tvcg.2011.181", "10.1126/science.298.5594.824", "10.1162/jmlr.2003.3.4-5.951", "10.1109/asonam.2014.6921638", "10.1111/j.1467-8659.2008.01213.x", "10.1038/s41598-017-18107-1", "10.1109/tvcg.2018.2865041", "10.1109/tits.2019.2901117", "10.1038/srep20668", "10.1109/tvcg.2012.265", "10.1109/tpami.2016.2608884", "10.1109/tvcg.2012.213", "10.1145/1376616.1376661", "10.1007/s00521-019-04567-1", "10.1109/tvcg.2017.2745083", "10.1126/science.243.4892.745", "10.1109/tvcg.2018.2864826", "10.1109/tnn.2003.820440", "10.1109/tvcg.2016.2598885", "10.1145/3219819.3219822", "10.1073/pnas.1502596112", "10.1016/j.envsoft.2009.01.004", "10.1002/pmic.200700095", "10.1145/2254556.2254651", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2011.202"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028958", "title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Tiankai Xie;Yuxin Ma;Hanghang Tong;My T. Thai;Ross Maciejewski", "citationCount": "0", "affiliation": "Xie, TK (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Xie, Tiankai; Ma, Yuxin; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. Tong, Hanghang, Univ Illinois, Urbana, IL USA. Thai, My T., Univ Florida, Gainesville, FL 32611 USA.", "countries": "USA", "abstract": "Graph mining plays a pivotal role across a number of disciplines, and a variety of algorithms have been developed to answer who/what type questions. For example, what items shall we recommend to a given user on an e-commerce platform? The answers to such questions are typically returned in the form of a ranked list, and graph-based ranking methods are widely used in industrial information retrieval settings. However, these ranking algorithms have a variety of sensitivities, and even small changes in rank can lead to vast reductions in product sales and page hits. As such, there is a need for tools and methods that can help model developers and analysts explore the sensitivities of graph ranking algorithms with respect to perturbations within the graph structure. In this paper, we present a visual analytics framework for explaining and exploring the sensitivity of any graph-based ranking algorithm by performing perturbation-based what-if analysis. We demonstrate our framework through three case studies inspecting the sensitivity of two classic graph-based ranking algorithms (PageRank and HITS) as applied to rankings in political news media and social networks.", "keywords": "Graph-based ranking,sensitivity analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028958", "refList": ["10.1109/wsc.2017.8247800", "10.1023/a:1022649401552", "10.1515/1559-0410.11416", "10.1109/tvcg.2016.2598919", "10.1177/1473871611416549", "10.1109/tvcg.2019.2934630", "10.1140/epjds29", "10.1109/tvcg.2019.2934670", "10.1016/j.eswa.2015.09.004", "10.1145/2702123.2702509", "10.1016/j.visinf.2018.12.001", "10.2307/3002000", "10.1109/tvcg.2019.2934399", "10.1007/s41060-016-0032-z", "10.1111/cgf.13198", "10.14778/2350229.2350254", "10.1145/2939672.2939764", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/vast.2017.8585647", "10.1007/bf01187020", "10.1109/icdm.2015.26", "10.1145/2362383.2362387", "10.1177/0049124104268644", "10.1109/vast.2011.6102442", "10.1109/infvis.2003.1249025", "10.1109/tvcg.2018.2864475", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2015.2467691", "10.1111/cgf.13210", "10.1214/aos/1176344136", "10.1109/tvcg.2015.2424872", "10.1016/j.visinf.2018.09.001", "10.1177/089443939100900106", "10.1109/tvcg.2015.2467931", "10.1162/neco.1997.9.8.1735", "10.1007/s11162-011-9241-4", "10.1111/cgf.13680", "10.1145/3065386", "10.1109/tvcg.2018.2864889", "10.1177/003804070808100402", "10.1109/icdm.2010.62", "10.1038/s41598-020-59669-x", "10.1162/153244303321897717", "10.1109/tvcg.2019.2934619", "10.1007/bf00356088", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030410", "title": "Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Wei Zeng 0002;Chengqiao Lin;Juncong Lin;Jincheng Jiang;Jiazhi Xia;Cagatay Turkay;Wei Chen", "citationCount": "0", "affiliation": "Lin, JC (Corresponding Author), Xiamen Univ, Xiamen, Peoples R China. Zeng, Wei; Jiang, Jincheng, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Lin, Chengqiao; Lin, Juncong, Xiamen Univ, Xiamen, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Turkay, Cagatay, Univ Warwick, Coventry, W Midlands, England. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China.", "countries": "China;England", "abstract": "Deep learning methods are being increasingly used for urban traffic prediction where spatiotemporal traffic data is aggregated into sequentially organized matrices that are then fed into convolution-based residual neural networks. However, the widely known modifiable areal unit problem within such aggregation processes can lead to perturbations in the network inputs. This issue can significantly destabilize the feature embeddings and the predictions - rendering deep networks much less useful for the experts. This paper approaches this challenge by leveraging unit visualization techniques that enable the investigation of many-to-many relationships between dynamically varied multi-scalar aggregations of urban traffic data and neural network predictions. Through regular exchanges with a domain expert, we design and develop a visual analytics solution that integrates 1) a Bivariate Map equipped with an advanced bivariate colormap to simultaneously depict input traffic and prediction errors across space, 2) a Moran's I Scatterplot that provides local indicators of spatial association analysis, and 3) a Multi-scale Attribution View that arranges non-linear dot plots in a tree layout to promote model analysis and comparison across scales. We evaluate our approach through a series of case studies involving a real-world dataset of Shenzhen taxi trips, and through interviews with domain experts. We observe that geographical scale variations have important impact on prediction performances, and interactive visual exploration of dynamically varying inputs and outputs benefit experts in the development of deep traffic prediction models.", "keywords": "MAUP,traffic prediction,deep learning,model diagnostic,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030410", "refList": ["10.1038/srep26377", "10.1109/mcg.2011.88", "10.1080/13658816.2015.1119279", "10.1109/tvcg.2013.226", "10.1109/pacificvis.2011.5742387", "10.1038/s41467-017-01882-w", "10.1109/tvcg.2019.2934670", "10.1111/j.1467-8659.2009.01440.x", "10.1111/cgf.13712", "10.1016/j.compenvurbsys.2008.09.006", "10.1109/pacificvis.2014.50", "10.1109/tvcg.2018.2816219", "10.1109/tvcg.2016.2535234", "10.1109/tvcg.2014.2346893", "10.3390/ijgi8080344", "10.1109/tvcg.2013.246", "10.1007/s10940-005-9003-6", "10.1016/j.compenvurbsys.2008.05.001", "10.1007/s10661-019-7831-3", "10.1111/j.1538-4632.2007.00699.x", "10.1016/j.aap.2016.08.015", "10.1080/13658816.2018.1541177", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2011.181", "10.1137/090759069", "10.1109/pacificvis.2011.5742390", "10.1214/10-aos799", "10.1109/tits.2017.2683539", "10.1109/tits.2015.2436897", "10.3390/ijerph16071150", "10.1109/tvcg.2009.145", "10.1109/tvcg.2012.265", "10.1080/10106049.2017.1404140", "10.3390/ijgi8020063", "10.3390/info6020134", "10.1080/13658816.2014.955027", "10.1109/tits.2016.2639320", "10.2307/143141", "10.1109/tvcg.2016.2598432"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030370", "title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups", "year": "2020", "conferenceName": "VAST", "authors": "Yating Lin;Kamkwai Wong;Yong Wang;Rong Zhang;Bo Dong;Huamin Qu;Qinghua Zheng", "citationCount": "0", "affiliation": "Lin, YT (Corresponding Author), Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian, Shaanxi, Peoples R China. Lin, Yating; Zheng, Qinghua, Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian, Shaanxi, Peoples R China. Wong, Kamkwai; Wang, Yong; Zhang, Rong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Dong, Bo, Xi An Jiao Tong Univ, Natl Engn Lab Big Data Analyt, Xian, Shaanxi, Peoples R China.", "countries": "China", "abstract": "Tax evasion is a serious economic problem for many countries, as it can undermine the government's tax system and lead to an unfair business competition environment. Recent research has applied data analytics techniques to analyze and detect tax evasion behaviors of individual taxpayers. However, they have failed to support the analysis and exploration of the related party transaction tax evasion (RPTTE) behaviors (e.g., transfer pricing), where a group of taxpayers is involved. In this paper, we present TaxThemis, an interactive visual analytics system to help tax officers mine and explore suspicious tax evasion groups through analyzing heterogeneous tax-related data. A taxpayer network is constructed and fused with the respective trade network to detect suspicious RPTTE groups. Rich visualizations are designed to facilitate the exploration and investigation of suspicious transactions between related taxpayers with profit and topological data analysis. Specifically, we propose a calendar heatmap with a carefully-designed encoding scheme to intuitively show the evidence of transferring revenue through related party transactions. We demonstrate the usefulness and effectiveness of TaxThemis through two case studies on real-world tax-related data and interviews with domain experts.", "keywords": "Visual Analytics,Tax Network,Tax Evasion Detection,Anomaly detection,Multidimensional data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030370", "refList": ["10.1111/cgf.12886", "10.2307/2277827", "10.1109/tvcg.2010.44", "10.1109/tits.2014.2315794", "10.1109/tvcg.2019.2934670", "10.1038/s41467-019-08987-4", "10.1111/cgf.12920", "10.1109/vast.2017.8585721", "10.1080/15230406.2015.1093431", "10.1109/tvcg.2018.2843369", "10.1038/srep01001", "10.1109/tvcg.2017.2744018", "10.1109/tvcg.2017.2744159", "10.1068/b130199p", "10.1109/tvcg.2009.143", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2019.2892483", "10.1109/pacificvis.2017.8031583", "10.1109/pacificvis48177.2020.2785", "10.2307/2686111", "10.1109/tvcg.2015.2467199", "10.1111/cgf.12114", "10.1109/tvcg.2018.2865126", "10.1111/j.1538-4632.1996.tb00936.x", "10.1109/tvcg.2019.2934619", "10.2307/2332142", "10.1007/978-3-319-10590-1\\_53", "10.1109/cvpr.2016.485", "10.1109/cvpr.2017.17", "10.2307/2986645", "10.1109/tvcg.2014.2346321", "10.1017/s0140525x16001837", "10.1109/tvcg.2016.2598541", "10.1371/journal.pone.0207377", "10.1109/tvcg.2014.2346265", "10.1007/s4095-020-0191-7", "10.3141/1644-14", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2016.90", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2785807", "10.1109/tvcg.2017.2744358", "10.1111/j.1538-4632.1995.tb00338.x", "10.1080/03081068808717359", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030469", "title": "Topology Density Map for Urban Data Visualization and Analysis", "year": "2020", "conferenceName": "VAST", "authors": "Zezheng Feng;Haotian Li;Wei Zeng 0004;Shuang-Hua Yang;Huamin Qu", "citationCount": "0", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Feng, Zezheng; Li, Haotian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yang, Shuang-Hua, Southern Univ Sci \\& Technol, Shenzhen, Peoples R China.", "countries": "China", "abstract": "Density map is an effective visualization technique for depicting the scalar field distribution in 2D space. Conventional methods for constructing density maps are mainly based on Euclidean distance, limiting their applicability in urban analysis that shall consider road network and urban traffic. In this work, we propose a new method named Topology Density Map, targeting for accurate and intuitive density maps in the context of urban environment. Based on the various constraints of road connections and traffic conditions, the method first constructs a directed acyclic graph (DAG) that propagates nonlinear scalar fields along 1D road networks. Next, the method extends the scalar fields to a 2D space by identifying key intersecting points in the DAG and calculating the scalar fields for every point, yielding a weighted Voronoi diagram like effect of space division. Two case studies demonstrate that the Topology Density Map supplies accurate information to users and provides an intuitive visualization for decision making. An interview with domain experts demonstrates the feasibility, usability, and effectiveness of our method.", "keywords": "Density map,network topology,urban data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030469", "refList": ["10.1109/vast.2009.5332584", "10.1109/tvcg.2013.193", "10.1080/03081060.2013.844903", "10.1109/tvcg.2018.2864503", "10.1145/2702123.2702419", "10.1109/tvcg.2019.2934670", "10.1109/tits.2015.2496783", "10.1177/1473871615581216", "10.3141/1617-02", "10.1145/2024156.2024169", "10.1111/cgf.13712", "10.1016/j.ejor.2007.02.005", "10.1109/tvcg.2014.2346893", "10.1007/11871842\\_29", "10.1109/vast.2010.5652478", "10.1016/j.visinf.2019.10.002", "10.1109/tvcg.2016.2616404", "10.1109/vl.1996.545307", "10.1145/2629592", "10.1155/2018/2696037", "10.1061/(asce)0733-947x(1998)124:4(368", "10.3141/1899-21", "10.1023/a:1026123329433", "10.1109/mcg.2010.79", "10.1057/palgrave.ivs.9500174", "10.1109/tcyb.2019.2963681", "10.1109/tvcg.2015.2467554", "10.1111/cgf.12114", "10.1145/2814575", "10.1016/j.jcps.2014.08.002", "10.1109/2945.981847", "10.1080/03052150210909", "10.1109/tciaig.2012.2186810", "10.1109/tits.2017.2683539", "10.1109/iv.2004.1320137", "10.1016/0377-2217(80)90126-5", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2015.2467196", "10.1145/3097983.3098056", "10.1007/s11432-018-9801-4", "10.1109/vast.2014.7042490", "10.1061/(asce)0733-947x(2006)132:2(122", "10.1016/j.tra.2008.03.011", "10.1109/tits.2014.2298892", "10.1016/j.trb.2005.12.003", "10.1007/bf01840357", "10.1109/vast.2011.6102454", "10.1109/tvcg.2013.145", "10.1007/bf02289588", "10.1109/pacificvis.2014.56", "10.1109/mcg.2018.053491730", "10.1109/tvcg.2009.111", "10.1057/palgrave.ivs.9500184", "10.1109/tvcg.2013.173", "10.1109/tvcg.2016.2598432", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/vast47406.2019.8986909", "title": "Origraph: Interactive Network Wrangling", "year": "2019", "conferenceName": "VAST", "authors": "Alex Bigelow;Carolina Nobre;Miriah D. Meyer;Alexander Lex", "citationCount": "2", "affiliation": "Bigelow, A (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bigelow, Alex; Nobre, Carolina; Meyer, Miriah; Lex, Alexander, Univ Utah, Salt Lake City, UT 84112 USA.", "countries": "USA", "abstract": "Networks are a natural way of thinking about many datasets. The data on which a network is based, however, is rarely collected in a form that suits the analysis process, making it necessary to create and reshape networks. Data wrangling is widely acknowledged to be a critical part of the data analysis pipeline, yet interactive network wrangling has received little attention in the visualization research community. In this paper, we discuss a set of operations that are important for wrangling network datasets and introduce a visual data wrangling tool, Origraph, that enables analysts to apply these operations to their datasets. Key operations include creating a network from source data such as tables, reshaping a network by introducing new node or edge classes, filtering nodes or edges, and deriving new node or edge attributes. Our tool, Origraph, enables analysts to execute these operations with little to no programming, and to immediately visualize the results. Origraph provides views to investigate the network model, a sample of the network, and node and edge attributes. In addition, we introduce interfaces designed to aid analysts in specifying arguments for sensible network wrangling operations. We demonstrate the usefulness of Origraph in two Use Cases: first, we investigate gender bias in the film industry, and then the influence of money on the political support for the war in Yemen.", "keywords": "Graph visualization,Data abstraction,Data wrangling,Human-centered computing [Information visualization],[Human-centered computing]: Visualization systems and tools,Information systems [Graph-based database models]", "link": "http://dx.doi.org/10.1109/VAST47406.2019.8986909", "refList": ["10.1101/gr.1239303", "10.1145/1054972.1055032", "10.1016/b978-0-12-382229-1.00002-3", "10.1109/tvcg.2017.2744843", "10.1109/tvcg.2013.154", "10.1007/978-1-4614-7163-9315-1", "10.1073/pnas.1607151113", "10.18637/jss.v040.i01", "10.1145/1124772.1124891", "10.1177/1473871611415994", "10.1109/tvcg.2018.2865149", "10.1145/2598153.2598175", "10.1177/1473871613488591", "10.1109/tvcg.2018.2859973", "10.1186/1471-2105-14-s19-s3", "10.1056/nejmsa066082", "10.1007/978-3-642-36763-2\\_48", "10.1016/j.socscimed.2016.01.049", "10.1111/j.1467-8659.2008.01231.x", "10.1002/cne.24084", "10.2138/am-2017-6104ccbyncnd", "10.1109/tvcg.2014.2346248", "10.1111/j.1467-8659.2009.01710.x", "10.1007/978-3-642-03658-3\\_47", "10.1007/978-3-319-06793-3\\_5", "10.3390/genes9110519", "10.1145/3290605.3300356", "10.1111/cgf.12883", "10.1177/1473871612462152", "10.1016/j.jelectrocard.2010.09.003", "10.1111/cgf.13610", "10.1109/vast.2010.5652520", "10.1111/evo.13318", "10.1109/tvcg.2018.2811488", "10.1109/tvcg.2009.111", "10.1111/cgf.13184", "10.1109/tvcg.2009.116", "10.1109/biovis.2012.6378600"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030462", "title": "Table Scraps: An Actionable Framework for Multi-Table Data Wrangling From An Artifact Study of Computational Journalism", "year": "2020", "conferenceName": "InfoVis", "authors": "Stephen Kasica;Charles Berret;Tamara Munzner", "citationCount": "0", "affiliation": "Kasica, S (Corresponding Author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada. Kasica, Stephen; Munzner, Tamara, Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada. Berret, Charles, Univ British Columbia, Sch Journalism Writing \\& Media, Vancouver, BC, Canada.", "countries": "Canada", "abstract": "For the many journalists who use data and computation to report the news, data wrangling is an integral part of their work. Despite an abundance of literature on data wrangling in the context of enterprise data analysis, little is known about the specific operations, processes, and pain points journalists encounter while performing this tedious, time-consuming task. To better understand the needs of this user group, we conduct a technical observation study of 50 public repositories of data and analysis code authored by 33 professional journalists at 26 news organizations. We develop two detailed and cross-cutting taxonomies of data wrangling in computational journalism, for actions and for processes. We observe the extensive use of multiple tables, a notable gap in previous wrangling analyses. We develop a concise, actionable framework for general multi-table data wrangling that includes wrangling operations documented in our taxonomy that are without clear parallels in other work. This framework, the first to incorporate tables as first-class objects, will support future interactive wrangling tools for both computational journalism and general-purpose use. We assess the generative and descriptive power of our framework through discussion of its relationship to our set of taxonomies.", "keywords": "Computational journalism,Data journalism,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030462", "refList": ["10.1145/1378773.1378792", "10.1109/tvcg.2012.219", "10.1109/vast47406.2019.8986909", "10.1145/1084805.1084812", "10.1007/s00778-008-0098-x", "10.1016/j.websem.2008.09.005", "10.18637/jss.v040.i01", "10.1145/989863.989865", "10.1109/tvcg.2015.2467551", "10.5281/zenodo.3509134", "10.1109/tvcg.2019.2934539", "10.1109/tvcg.2019.2934593", "10.1109/tse.2018.2796554", "10.17349/jmc117309", "10.1109/2945.981851", "10.1109/vast.2011.6102440", "10.1177/1473871611415994", "10.1145/2001269.2001288"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934285", "title": "Visualizing a Moving Target: A Design Study on Task Parallel Programs in the Presence of Evolving Data and Concerns", "year": "2019", "conferenceName": "InfoVis", "authors": "Katy Williams;Alex Bigelow;Katherine E. Isaacs", "citationCount": "5", "affiliation": "Williams, K (Corresponding Author), Univ Arizona, Tucson, AZ 85721 USA. Williams, Katy; Bigelow, Alex; Isaacs, Kate, Univ Arizona, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "Common pitfalls in visualization projects include lack of data availability and the domain users' needs and focus changing too rapidly for the design process to complete. While it is often prudent to avoid such projects, we argue it can be beneficial to engage them in some cases as the visualization process can help refine data collection, solving a \u201cchicken and egg\u201d problem of having the data and tools to analyze it. We found this to be the case in the domain of task parallel computing where such data and tooling is an open area of research. Despite these hurdles, we conducted a design study. Through a tightly-coupled iterative design process, we built Atria, a multi-view execution graph visualization to support performance analysis. Atria simplifies the initial representation of the execution graph by aggregating nodes as related to their line of code. We deployed Atria on multiple platforms, some requiring design alteration. We describe how we adapted the design study methodology to the \u201cmoving target\u201d of both the data and the domain experts' concerns and how this movement kept both the visualization and programming project healthy. We reflect on our process and discuss what factors allow the project to be successful in the presence of changing data and user needs.", "keywords": "design studies,software visualization,parallel computing,graph visualization", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934285", "refList": ["10.1007/978-3-642-31476-6\\_7", "10.1109/tvcg.2018.2859974", "10.1145/3337821.3337915", "10.1109/tse.1981.234519", "10.1109/32.221135", "10.1111/cgf.13433", "10.1109/tvcg.2011.185", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2018.2865149", "10.1109/tvcg.2014.2346323", "10.3233/978-1-61499-649-1-87", "10.1145/2993901.2993911", "10.1109/mcg.2011.103", "10.1002/1097-024x(200009)30:11", "10.1177/1094342006064482", "10.1057/palgrave.ivs.9500116", "10.1177/1473871615621602", "10.1109/tvcg.2013.124", "10.1109/infvis.2003.1249009", "10.1145/642611.642616", "10.1109/tvcg.2015.2467452", "10.1109/cw.2002.1180907", "10.1145/3125571.3125585", "10.1145/2807591.2807634", "10.1109/infvis.2002.1173148", "10.1145/2676870.2676883", "10.1145/1168149.1168168", "10.1007/978-3-030-17872-7\\_14", "10.1145/2993901.2993916", "10.1109/iotdi.2015.41", "10.1109/infvis.2004.1", "10.1080/14639220903165169", "10.1109/hpdc.2000.868632", "10.1145/882262.882291", "10.1109/mcse.2013.98", "10.1109/tvcg.2012.213", "10.1109/tvcg.2018.2811488", "10.1109/tvcg.2017.2744319", "10.1007/978-3-642-31476-67", "10.1109/mcg.2018.2874523", "10.1109/sc.2012.71"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030355", "title": "Guidelines For Pursuing and Revealing Data Abstractions", "year": "2020", "conferenceName": "InfoVis", "authors": "Alex Bigelow;Katy Williams;Katherine E. Isaacs", "citationCount": "0", "affiliation": "Bigelow, A (Corresponding Author), Univ Arizona, Tucson, AZ 85721 USA. Bigelow, Alex; Williams, Katy; Isaacs, Katherine E., Univ Arizona, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "Many data abstraction types, such as networks or set relationships, remain unfamiliar to data workers beyond the visualization research community. We conduct a survey and series of interviews about how people describe their data, either directly or indirectly. We refer to the latter as latent data abstractions. We conduct a Grounded Theory analysis that (1) interprets the extent to which latent data abstractions exist, (2) reveals the far-reaching effects that the interventionist pursuit of such abstractions can have on data workers, (3) describes why and when data workers may resist such explorations, and (4) suggests how to take advantage of opportunities and mitigate risks through transparency about visualization research perspectives and agendas. We then use the themes and codes discovered in the Grounded Theory analysis to develop guidelines for data abstraction in visualization projects. To continue the discussion, we make our dataset open along with a visual interface for further exploration.", "keywords": "Data abstraction,Grounded theory,Survey design,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030355", "refList": ["10.1080/2159676x.2016.1251701", "10.1109/infvis.2000.885092", "10.1145/2702123.2702298", "10.4135/9781848607941.n14", "10.1007/978-1-4939", "10.1109/tvcg.2014.2346331", "10.1109/tvcg.2017.2744843", "10.1177/1473871613510429", "10.1007/978-1-4939-0378-8\\_2", "10.1145/2598153.2598175", "10.1109/tvcg.2019.2934285", "10.1177/1473871613488591", "10.1145/2501105.2501106", "10.1109/tvcg.2019.2934538", "10.1109/tvcg.2019.2934539", "10.1017/s1049096510990781", "10.1145/3025453.3025837", "10.1145/3290605.3300474", "10.1145/3290605.3300356", "10.1002/nur.1025", "10.1145/2993901.2993916", "10.1145/3392826", "10.1086/269268", "10.1109/tvcg.2018.2865241", "10.1145/2998181.2998331", "10.1145/291224.291229", "10.1057/ivs.2009.13", "10.1145/2047196.2047205", "10.1109/tvcg.2012.213", "10.1145/3274405", "10.1109/tvcg.2013.145", "10.1016/0040-6031(92)85160-w", "10.1109/iv.2013.45", "10.1109/tvcg.2009.111", "10.1109/mcg.2019.2914844", "10.1109/tvcg.2009.116"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2020.3029413", "title": "A Design Space of Vision Science Methods for Visualization Research", "year": "2020", "conferenceName": "InfoVis", "authors": "Madison A. Elliott;Christine Nothelfer;Cindy Xiong;Danielle Albers Szafir", "citationCount": "0", "affiliation": "Elliott, MA (Corresponding Author), Univ British Columbia, Vancouver, BC, Canada. Elliott, Madison A., Univ British Columbia, Vancouver, BC, Canada. Nothelfer, Christine, Northwestern Univ, Evanston, IL 60208 USA. Xiong, Cindy, Univ Massachusetts, Amherst, MA 01003 USA. Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "Canada;USA", "abstract": "A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.", "keywords": "Perception,human vision,empirical research,evaluation,HCI", "link": "http://dx.doi.org/10.1109/TVCG.2020.3029413", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1177/0886109909354981", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1093/bioinformatics/btq110", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1177/1744987107081254", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1177/174498710501000305", "10.1017/s1049096513001789", "10.1109/tvcg.2012.213", "10.1093/nar/gkz239", "10.1093/sysbio/sys062", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030405", "title": "Insights From Experiments With Rigor in an EvoBio Design Study", "year": "2020", "conferenceName": "InfoVis", "authors": "Jennifer Rogers;Austin H. Patton;Luke Harmon;Alexander Lex;Miriah D. Meyer", "citationCount": "0", "affiliation": "Rogers, J (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Rogers, Jen; Lex, Alexander; Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Patton, Austin H., Washington State Univ, Pullman, WA 99164 USA. Harmon, Luke, Univ Idaho, Moscow, ID 83843 USA.", "countries": "USA", "abstract": "Design study is an established approach of conducting problem-driven visualization research. The academic visualization community has produced a large body of work for reporting on design studies, informed by a handful of theoretical frameworks, and applied to a broad range of application areas. The result is an abundance of reported insights into visualization design, with an emphasis on novel visualization techniques and systems as the primary contribution of these studies. In recent work we proposed a new, interpretivist perspective on design study and six companion criteria for rigor that highlight the opportunities for researchers to contribute knowledge that extends beyond visualization idioms and software. In this work we conducted a year-long collaboration with evolutionary biologists to develop an interactive tool for visual exploration of multivariate datasets and phylogenetic trees. During this design study we experimented with methods to support three of the rigor criteria: ABUNDANT, REFLEXIVE, and TRANSPARENT. As a result we contribute two novel visualization techniques for the analysis of multivariate phylogenetic datasets, three methodological recommendations for conducting design studies drawn from reflections over our process of experimentation, and two writing devices for reporting interpretivist design study. We offer this work as an example for implementing the rigor criteria to produce a diverse range of knowledge contributions.", "keywords": "Methodologies,Application Motivated Visualization,Guidelines,Life Sciences Visualization,Health,Medicine,Biology,Bioinformatics,Genomics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030405", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1080/17493460802276893", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1109/tvcg.2012.213", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13728", "year": "2019", "title": "The State of the Art in Visualizing Multivariate Networks", "conferenceName": "EuroVis", "authors": "Carolina Nobre;Miriah D. Meyer;Marc Streit;Alexander Lex", "citationCount": "5", "affiliation": "Nobre, C (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA.\nNobre, C.; Meyer, M.; Lex, A., Univ Utah, Salt Lake City, UT 84112 USA.\nStreit, M., Johannes Kepler Univ Linz, Linz, Austria.", "countries": "USA;Austria", "abstract": "Multivariate networks are made up of nodes and their relationships (links), but also data about those nodes and links as attributes. Most real-world networks are associated with several attributes, and many analysis tasks depend on analyzing both, relationships and attributes. Visualization of multivariate networks, however, is challenging, especially when both the topology of the network and the attributes need to be considered concurrently. In this state-of-the-art report, we analyze current practices and classify techniques along four axes: layouts, view operations, layout operations, and data operations. We also provide an analysis of tasks specific to multivariate networks and give recommendations for which technique to use in which scenario. Finally, we survey application areas and evaluation methodologies.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13728", "refList": ["10.2312/eurovisstar.20151110", "10.1111/cgf.12106", "10.1109/iv.2016.19", "10.1111/j.1467-8659.2011.02087.x", "10.1111/j.1467-8659.2008.01214.x", "10.1109/tvcg.2014.2346893", "10.1145/1124772.1124891", "10.1109/vast.2014.7042484", "10.1109/tvcg.2018.2865149", "10.1080/10447318.2010.516722", "10.1109/icsmc.2011.6084125", "10.1109/mcg.2011.103", "10.1117/12.378894", "10.1145/2207676.2208293", "10.1109/tvcg.2009.122", "10.1111/cgf.12935", "10.1016/s0020-0255(02)00191-3", "10.1038/nmeth.1436", "10.1109/tvcg.2011.187", "10.1109/32.177365", "10.1007/978-3-642-03658-3\\_47", "10.1109/tvcg.2006.147", "10.1109/pacificvis.2011.5742390", "10.1145/2470654.2466444", "10.1109/tvcg.2006.166", "10.1145/2470654.2470724", "10.1109/tvcg.2014.2346441", "10.1109/vizsec.2005.1532070", "10.1101/gr.092759.109", "10.1111/cgf.13184", "10.1109/tvcg.2018.2865940", "10.1016/j.scico.2012.05.002", "10.1109/biovis.2012.6378600", "10.1111/cgf.13213", "10.1109/noms.2006.1687547", "10.1109/tvcg.2008.141", "10.1007/978-3-540-78243-8\\_13", "10.1145/1029208.1029217", "10.1145/345513.345271", "10.1109/tvcg.2015.2467811", "10.1109/visual.1991.175815", "10.1186/1471-2105-10-375", "10.1007/978-3-319-06793-3\\_1", "10.1186/1471-2105-13-275", "10.1109/tvcg.2010.79", "10.1109/tvcg.2011.217", "10.1186/1471-2105-14-s19-s3", "10.3389/fmicb.2017.00010", "10.1109/infvis.2003.1249009", "10.1111/cgf.13187", "10.1111/j.1467-8659.2008.01231.x", "10.1109/tvcg.2011.144", "10.1145/2556288.2557010", "10.1111/j.1467-8659.2009.01710.x", "10.1109/infvis.2004.46", "10.1109/iv.2009.97", "10.1109/tvcg.2008.34", "10.1109/infvis.2003.1249011", "10.1109/iv.2016.41", "10.1109/pacificvis.2010.5429609", "10.1109/tvcg.2006.160", "10.1109/tvcg.2010.205", "10.1109/38.486685", "10.1109/tvcg.2006.106", "10.1057/palgrave.ivs.9500092", "10.1145/568522.568523", "10.1111/j.1467-8659.2008.01221.x", "10.1109/iv.2010.15", "10.1145/22339.22342", "10.1109/csmr.2009.17", "10.1109/tvcg.2017.2744898", "10.1109/tvcg.2008.117", "10.1186/1752-0509-3-82", "10.1109/tvcg.2008.61", "10.1109/pacificvis.2013.6596127", "10.1109/tvcg.2007.70529", "10.1186/1471-2105-15-198", "10.1109/tvcg.2009.128", "10.2312/eurovisshort.20151124", "10.1093/bioinformatics/btq675", "10.1109/tvcg.2009.143", "10.1109/tvcg.2014.2346752", "10.1109/tvcg.2013.124", "10.1007/s00450-007-0036-y", "10.1093/bioinformatics/17.suppl\\_1.s22", "10.2312/eurovisstar.20151109", "10.1007/978-3-319-06793-3\\_5", "10.1109/vissof.2007.4290706", "10.1109/tvcg.2009.145", "10.1109/mcas.2003.1228503", "10.1109/infvis.2005.1532128", "10.1145/989863.989941", "10.1109/infvis.1999.801860", "10.1111/j.1467-8659.2009.01687.x", "10.1109/tvcg.2013.120", "10.1109/tvcg.2007.70582", "10.1109/tvcg.2009.167", "10.1109/pacificvis.2010.5429590", "10.1109/infvis.2000.885091", "10.1109/tvcg.2012.189", "10.1109/tvcg.2009.108", "10.1109/tvcg.2013.154", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2010.159", "10.1016/s0306-4573(98)00024-7", "10.1109/tvcg.2013.223", "10.1002/sam.10071", "10.2312/vissym/eurovis07/083-090", "10.1109/infvis.2002.1173156", "10.1186/1471-2105-7-109", "10.1177/1473871612455983", "10.1073/pnas.95.25.14863", "10.1007/978-3-319-06793-3\\_2", "10.1111/cgf.12883", "10.1007/978-3-540-78243-8\\_9", "10.1057/palgrave.ivs.9500180", "10.1109/pacificvis.2012.6183556", "10.1117/12.872578", "10.1145/1168149.1168169", "10.1109/iv.2013.3", "10.1109/tvcg.2011.186", "10.2307/2685881", "10.1109/pacificvis.2015.7156354", "10.1109/tvcg.2016.2598885", "10.1109/tvcg.2018.2811488", "10.1109/infvis.2005.1532129", "10.1111/j.1467-8659.2009.01450.x", "10.1057/palgrave.ivs.9500162", "10.1109/tvcg.2009.116", "10.1109/2945.468391"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3029413", "title": "A Design Space of Vision Science Methods for Visualization Research", "year": "2020", "conferenceName": "InfoVis", "authors": "Madison A. Elliott;Christine Nothelfer;Cindy Xiong;Danielle Albers Szafir", "citationCount": "0", "affiliation": "Elliott, MA (Corresponding Author), Univ British Columbia, Vancouver, BC, Canada. Elliott, Madison A., Univ British Columbia, Vancouver, BC, Canada. Nothelfer, Christine, Northwestern Univ, Evanston, IL 60208 USA. Xiong, Cindy, Univ Massachusetts, Amherst, MA 01003 USA. Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "Canada;USA", "abstract": "A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.", "keywords": "Perception,human vision,empirical research,evaluation,HCI", "link": "http://dx.doi.org/10.1109/TVCG.2020.3029413", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1177/0886109909354981", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1093/bioinformatics/btq110", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1177/1744987107081254", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1177/174498710501000305", "10.1017/s1049096513001789", "10.1109/tvcg.2012.213", "10.1093/nar/gkz239", "10.1093/sysbio/sys062", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030405", "title": "Insights From Experiments With Rigor in an EvoBio Design Study", "year": "2020", "conferenceName": "InfoVis", "authors": "Jennifer Rogers;Austin H. Patton;Luke Harmon;Alexander Lex;Miriah D. Meyer", "citationCount": "0", "affiliation": "Rogers, J (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Rogers, Jen; Lex, Alexander; Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Patton, Austin H., Washington State Univ, Pullman, WA 99164 USA. Harmon, Luke, Univ Idaho, Moscow, ID 83843 USA.", "countries": "USA", "abstract": "Design study is an established approach of conducting problem-driven visualization research. The academic visualization community has produced a large body of work for reporting on design studies, informed by a handful of theoretical frameworks, and applied to a broad range of application areas. The result is an abundance of reported insights into visualization design, with an emphasis on novel visualization techniques and systems as the primary contribution of these studies. In recent work we proposed a new, interpretivist perspective on design study and six companion criteria for rigor that highlight the opportunities for researchers to contribute knowledge that extends beyond visualization idioms and software. In this work we conducted a year-long collaboration with evolutionary biologists to develop an interactive tool for visual exploration of multivariate datasets and phylogenetic trees. During this design study we experimented with methods to support three of the rigor criteria: ABUNDANT, REFLEXIVE, and TRANSPARENT. As a result we contribute two novel visualization techniques for the analysis of multivariate phylogenetic datasets, three methodological recommendations for conducting design studies drawn from reflections over our process of experimentation, and two writing devices for reporting interpretivist design study. We offer this work as an example for implementing the rigor criteria to produce a diverse range of knowledge contributions.", "keywords": "Methodologies,Application Motivated Visualization,Guidelines,Life Sciences Visualization,Health,Medicine,Biology,Bioinformatics,Genomics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030405", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1080/17493460802276893", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1109/tvcg.2012.213", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13963", "year": "2020", "title": "MotionGlyphs: Visual Abstraction of Spatio-Temporal Networks in Collective Animal Behavior", "conferenceName": "EuroVis", "authors": "Eren Cakmak;Hanna Sch{\\\"{a}}fer;Juri Buchm{\\\"{u}}ller;Johannes Fuchs;Tobias Schreck;A. Jordan;Daniel A. Keim", "citationCount": "0", "affiliation": "Cakmak, E (Corresponding Author), Univ Konstanz, Constance, Germany.\nCakmak, E (Corresponding Author), Ctr Adv Study Collect Behav, Constance, Germany.\nCakmak, E.; Schaefer, H.; Buchmueller, J.; Fuchs, J.; Jordan, A.; Keim, D., Univ Konstanz, Constance, Germany.\nCakmak, E.; Jordan, A.; Keim, D., Ctr Adv Study Collect Behav, Constance, Germany.\nSchreck, T., Graz Univ Technol, Graz, Austria.\nJordan, A., Max Planck Inst Anim Behav, Radolfzell am Bodensee, Germany.", "countries": "Germany;Austria", "abstract": "Domain experts for collective animal behavior analyze relationships between single animal movers and groups of animals over time and space to detect emergent group properties. A common way to interpret this type of data is to visualize it as a spatio-temporal network. Collective behavior data sets are often large, and may hence result in dense and highly connected node-link diagrams, resulting in issues of node-overlap and edge clutter. In this design study, in an iterative design process, we developed glyphs as a design for seamlessly encoding relationships and movement characteristics of a single mover or clusters of movers. Based on these glyph designs, we developed a visual exploration prototype, MotionGlyphs, that supports domain experts in interactively filtering, clustering, and animating spatio-temporal networks for collective animal behavior analysis. By means of an expert evaluation, we show how MotionGlyphs supports important tasks and analysis goals of our domain experts, and we give evidence of the usefulness for analyzing spatio-temporal networks of collective animal behavior.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13963", "refList": ["10.1109/tvcg.2007.70582", "10.1111/cgf.13213", "10.1109/pacificvis.2014.13", "10.1145/2093973.2094038", "10.1111/j.1467-8659.2009.01664.x", "10.1109/tvcg.2010.44", "10.1111/cgf.12106", "10.1111/cgf.12791", "10.1080/15230406.2014.890071", "10.1111/tgis.12100", "10.1111/j.1467-8659.2009.01451.x", "10.1179/000870409x12525737905042", "10.1111/cgf.12923", "10.1006/ijhc.2002.1017", "10.1145/1124772.1124891", "10.1109/tvcg.2011.213", "10.1109/vast.2014.7042484", "10.1007/s12650-016-0375-5", "10.1109/vlhcc.2012.6344514", "10.1073/pnas.1420068112", "10.1006/ijhc.1017", "10.1007/s00371-017-1461-y", "10.1109/infvis.2003.1249008", "10.1111/cgf.13728", "10.2312/conf/eg2013/stars/039-063", "10.1109/tvcg.2014.2346271", "10.1109/hicss.2011.339", "10.1068/p3104", "10.1145/2931002.2931012", "10.1109/asonam.2012.39", "10.1179/000870403235002042", "10.1145/2556288.2557010", "10.1145/2470654.2466443", "10.1109/tvcg.2011.209", "10.1016/j.tree.2013.06.002", "10.1111/1365-2656.12418", "10.1111/cgf.12872", "10.1145/2470654.2466444", "10.1109/tvcg.2014.2322594", "10.1109/tvcg.2008.125", "10.1109/tvcg.2014.2346426", "10.1109/tvcg.2006.166", "10.1111/cgf.12615", "10.1057/palgrave.ivs.9500170", "10.1007/3-540-36151-0", "10.1117/12.872578", "10.1016/j.tics.2008.10.002", "10.1006/jtbi.2002.3065", "10.1109/tvcg.2010.78", "10.1109/iv.2013.3", "10.1073/pnas.1001763107", "10.1109/pacificvis.2015.7156354", "10.1371/journal.pbio.1001805", "10.5220/0005303801230130", "10.1111/j.1467-8659.2009.01687.x", "10.1007/s10844-011-0159-2", "10.1007/s12650-018-00543-4", "10.1145/2669557.2669572", "10.1111/cgf.13184", "10.1186/s40462-015-0032-y", "10.1016/j.ins.2016.06.048", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13987", "year": "2020", "title": "Augmenting Node-Link Diagrams with Topographic Attribute Maps", "conferenceName": "EuroVis", "authors": "Reinhold Preiner;Johanna Schmidt;Katharina Kr{\\\"{o}}sl;Tobias Schreck;Gabriel Mistelbauer", "citationCount": "0", "affiliation": "Preiner, R (Corresponding Author), Graz Univ Technol, Inst Comp Graph \\& Knowledge Visualizat, Graz, Austria.\nPreiner, R.; Schreck, T., Graz Univ Technol, Inst Comp Graph \\& Knowledge Visualizat, Graz, Austria.\nSchmidt, J.; Kroesl, K., Virtual Real \\& Visualisierung Forsch GmbH, VRVis Zentrum, Vienna, Austria.\nKroesl, K., TU Wien, Inst Visual Comp \\& Human Ctr Technol, Vienna, Austria.\nMistelbauer, G., Otto von Guericke Univ, Dept Simulat \\& Graph, Magdeburg, Germany.", "countries": "Germany;Austria", "abstract": "We propose a novel visualization technique for graphs that are attributed with scalar data. In many scenarios, these attributes (e.g., birth date in a family network) provide ambient context information for the graph structure, whose consideration is important for different visual graph analysis tasks. Graph attributes are usually conveyed using different visual representations (e.g., color, size, shape) or by reordering the graph structure according to the attribute domain (e.g., timelines). While visual encodings allow graphs to be arranged in a readable layout, assessing contextual information such as the relative similarities of attributes across the graph is often cumbersome. In contrast, attribute-based graph reordering serves the comparison task of attributes, but typically strongly impairs the readability of the structural information given by the graph's topology. In this work, we augment force-directed node-link diagrams with a continuous ambient representation of the attribute context. This way, we provide a consistent overview of the graph's topological structure as well as its attributes, supporting a wide range of graph-related analysis tasks. We resort to an intuitive height field metaphor, illustrated by a topographic map rendering using contour lines and suitable color maps. Contour lines visually connect nodes of similar attribute values, and depict their relative arrangement within the global context. Moreover, our contextual representation supports visualizing attribute value ranges associated with graph nodes (e.g., lifespans in a family network) as trajectories routed through this height field. We discuss how user interaction with both the structural and the contextual information fosters exploratory graph analysis tasks. The effectiveness and versatility of our technique is confirmed in a user study and case studies from various application domains.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13987", "refList": ["10.1109/tvcg.2013.269", "10.1109/pacificvis.2010.5429590", "10.1073/pnas.0307654100", "10.1145/2505515.2505758", "10.1559/152304082783948286", "10.1109/pacificvis.2014.47", "10.1093/bioinformatics/btp432", "10.1111/j.1467-8659.2011.01898.x", "10.1111/cgf.12931", "10.1111/cgf.12880", "10.1109/tvcg.2014.2346422", "10.1111/j.1467-8659.2009.01706.x", "10.1109/tvcg.2016.2598795", "10.1111/cgf.12800", "10.1109/tvcg.2014.2315995", "10.1111/cgf.12656", "10.1111/cgf.13728", "10.1109/tvcg.2009.122", "10.1111/cgf.13211", "10.1109/tvcg.2007.70596", "10.1109/infvis.2002.1173152", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2003.1196007", "10.1109/infvis.2005.1532150", "10.1145/3243250.3243266", "10.1080/02693799008941549", "10.1371/journal.pone.0058779", "10.1109/infvis.1995.528686", "10.1111/cgf.12872", "10.1002/spe.4380211102", "10.1109/38.974518", "10.1145/3097983.3098130", "10.1002/aris.1440370106", "10.1145/1360612.1360691", "10.1109/mc.2016.145", "10.2307/3006914", "10.1111/j.1467-8659.2009.01683.x", "10.1145/1639714.1639784"], "wos": 1, "children": [], "len": 1}], "len": 9}], "len": 33}, {"doi": "10.1109/tvcg.2019.2934264", "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;David S. Ebert;Arif Ghafoor", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Karimzadeh, Morteza; Ebert, David S.; Ghafoor, Arif, Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.", "keywords": "Summative evaluation,usefulness,evaluation process,taxonomy,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934264", "refList": ["10.1109/tvcg.2017.2744478", "10.1109/tvcg.2018.2865025", "10.1109/tvcg.2006.85", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2017.2745181", "10.1111/cgf.13677", "10.1109/tvcg.2018.2864844", "10.1109/tvcg.2013.126", "10.1109/tvcg.2018.2864811", "10.1109/infvis.2005.1532147", "10.1177/0956797613504966", "10.1145/2669557.2669579", "10.1109/mcg.2005.102", "10.1109/visual.2003.1250426", "10.1136/bmj.39489.470347.ad", "10.1109/tvcg.2017.2744080", "10.1109/mcg.2009.53", "10.1111/j.1467-8527.2005.00307.x", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2864886", "10.1109/tvcg.2018.2864843", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2018.2865051", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2018.2865026", "10.1007/978-3-540-71080-6\\_6", "10.1109/tvcg.2018.2865020", "10.1177/1473871611407399", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2018.2864526", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2864905", "10.1049/sej.1991.0040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2711030", "10.1109/tvcg.2011.279", "10.1109/vast.2017.8585505", "10.1147/jrd.2010.2042914", "10.1016/s0378-7206(98)00044-5", "10.1145/2993901.2993913", "10.1109/tvcg.2018.2865041", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744758", "10.1145/1168149.1168158", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2012.213", "10.1109/tvcg.2017.2744738", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2018.2864826", "10.1145/1377966.1377974", "10.1109/apec.2009.4802646", "10.1145/1168149.1168152", "10.1016/j.jss.2008.03.059", "10.1109/vast.2017.8585484", "10.1109/tvcg.2017.2744818", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2865042", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030388", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "year": "2020", "conferenceName": "SciVis", "authors": "Pepe Eulzer;Sabine Bauer;Francis Kilian;Kai Lawonn", "citationCount": "0", "affiliation": "Eulzer, P (Corresponding Author), Univ Jena, Jena, Germany. Eulzer, Pepe; Lawonn, Kai, Univ Jena, Jena, Germany. Bauer, Sabine, Univ Koblenz Landau, Koblenz, Germany. Kilian, Francis, Cath Clin Koblenz Montabaur, Dept Spine Surg, Koblenz, Germany.", "countries": "Germany", "abstract": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. $\\mathrm{By}$ linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. $\\mathrm{in}$ a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.", "keywords": "Medical visualization,bioinformatics,coordinated views,focus and context,biomechanical simulation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030388", "refList": ["10.1109/tvcg.2018.2864903", "10.1177/1473871613510429", "10.1093/ehjqcco/qcz052", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2018.2865076", "10.1055/s-0039-1687862", "10.1109/visual.1990.146375", "10.1109/tvcg.2017.2744198", "10.1016/j.ijmedinf.2014.10.001", "10.1109/tvcg.2013.124", "10.1016/j.jacc", "10.1111/cgf.13167", "10.17705/1thci.00055", "10.1136/bmjqs.2009.037895", "10.1109/tvcg.2013.238", "10.1109/tvcg.2018.2865240", "10.1186/1471-2261-6-34", "10.1109/tvcg.2019.2934264", "10.1109/tvcg.2013.200", "10.1109/tvcg.2011.209", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467091", "10.1136/bmjopen-2019-033208", "10.1109/beliv.2018.8634027", "10.1109/tvcg.2012.213", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2015.2467325", "10.1145/2133806.2133821", "10.1145/1806799.1806866", "10.1108/02635570610688869", "10.1002/hbm.20701", "10.1561/1100000039", "10.1145/3025453.3025645", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.13728", "year": "2019", "title": "The State of the Art in Visualizing Multivariate Networks", "conferenceName": "EuroVis", "authors": "Carolina Nobre;Miriah D. Meyer;Marc Streit;Alexander Lex", "citationCount": "5", "affiliation": "Nobre, C (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA.\nNobre, C.; Meyer, M.; Lex, A., Univ Utah, Salt Lake City, UT 84112 USA.\nStreit, M., Johannes Kepler Univ Linz, Linz, Austria.", "countries": "USA;Austria", "abstract": "Multivariate networks are made up of nodes and their relationships (links), but also data about those nodes and links as attributes. Most real-world networks are associated with several attributes, and many analysis tasks depend on analyzing both, relationships and attributes. Visualization of multivariate networks, however, is challenging, especially when both the topology of the network and the attributes need to be considered concurrently. In this state-of-the-art report, we analyze current practices and classify techniques along four axes: layouts, view operations, layout operations, and data operations. We also provide an analysis of tasks specific to multivariate networks and give recommendations for which technique to use in which scenario. Finally, we survey application areas and evaluation methodologies.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13728", "refList": ["10.2312/eurovisstar.20151110", "10.1111/cgf.12106", "10.1109/iv.2016.19", "10.1111/j.1467-8659.2011.02087.x", "10.1111/j.1467-8659.2008.01214.x", "10.1109/tvcg.2014.2346893", "10.1145/1124772.1124891", "10.1109/vast.2014.7042484", "10.1109/tvcg.2018.2865149", "10.1080/10447318.2010.516722", "10.1109/icsmc.2011.6084125", "10.1109/mcg.2011.103", "10.1117/12.378894", "10.1145/2207676.2208293", "10.1109/tvcg.2009.122", "10.1111/cgf.12935", "10.1016/s0020-0255(02)00191-3", "10.1038/nmeth.1436", "10.1109/tvcg.2011.187", "10.1109/32.177365", "10.1007/978-3-642-03658-3\\_47", "10.1109/tvcg.2006.147", "10.1109/pacificvis.2011.5742390", "10.1145/2470654.2466444", "10.1109/tvcg.2006.166", "10.1145/2470654.2470724", "10.1109/tvcg.2014.2346441", "10.1109/vizsec.2005.1532070", "10.1101/gr.092759.109", "10.1111/cgf.13184", "10.1109/tvcg.2018.2865940", "10.1016/j.scico.2012.05.002", "10.1109/biovis.2012.6378600", "10.1111/cgf.13213", "10.1109/noms.2006.1687547", "10.1109/tvcg.2008.141", "10.1007/978-3-540-78243-8\\_13", "10.1145/1029208.1029217", "10.1145/345513.345271", "10.1109/tvcg.2015.2467811", "10.1109/visual.1991.175815", "10.1186/1471-2105-10-375", "10.1007/978-3-319-06793-3\\_1", "10.1186/1471-2105-13-275", "10.1109/tvcg.2010.79", "10.1109/tvcg.2011.217", "10.1186/1471-2105-14-s19-s3", "10.3389/fmicb.2017.00010", "10.1109/infvis.2003.1249009", "10.1111/cgf.13187", "10.1111/j.1467-8659.2008.01231.x", "10.1109/tvcg.2011.144", "10.1145/2556288.2557010", "10.1111/j.1467-8659.2009.01710.x", "10.1109/infvis.2004.46", "10.1109/iv.2009.97", "10.1109/tvcg.2008.34", "10.1109/infvis.2003.1249011", "10.1109/iv.2016.41", "10.1109/pacificvis.2010.5429609", "10.1109/tvcg.2006.160", "10.1109/tvcg.2010.205", "10.1109/38.486685", "10.1109/tvcg.2006.106", "10.1057/palgrave.ivs.9500092", "10.1145/568522.568523", "10.1111/j.1467-8659.2008.01221.x", "10.1109/iv.2010.15", "10.1145/22339.22342", "10.1109/csmr.2009.17", "10.1109/tvcg.2017.2744898", "10.1109/tvcg.2008.117", "10.1186/1752-0509-3-82", "10.1109/tvcg.2008.61", "10.1109/pacificvis.2013.6596127", "10.1109/tvcg.2007.70529", "10.1186/1471-2105-15-198", "10.1109/tvcg.2009.128", "10.2312/eurovisshort.20151124", "10.1093/bioinformatics/btq675", "10.1109/tvcg.2009.143", "10.1109/tvcg.2014.2346752", "10.1109/tvcg.2013.124", "10.1007/s00450-007-0036-y", "10.1093/bioinformatics/17.suppl\\_1.s22", "10.2312/eurovisstar.20151109", "10.1007/978-3-319-06793-3\\_5", "10.1109/vissof.2007.4290706", "10.1109/tvcg.2009.145", "10.1109/mcas.2003.1228503", "10.1109/infvis.2005.1532128", "10.1145/989863.989941", "10.1109/infvis.1999.801860", "10.1111/j.1467-8659.2009.01687.x", "10.1109/tvcg.2013.120", "10.1109/tvcg.2007.70582", "10.1109/tvcg.2009.167", "10.1109/pacificvis.2010.5429590", "10.1109/infvis.2000.885091", "10.1109/tvcg.2012.189", "10.1109/tvcg.2009.108", "10.1109/tvcg.2013.154", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2010.159", "10.1016/s0306-4573(98)00024-7", "10.1109/tvcg.2013.223", "10.1002/sam.10071", "10.2312/vissym/eurovis07/083-090", "10.1109/infvis.2002.1173156", "10.1186/1471-2105-7-109", "10.1177/1473871612455983", "10.1073/pnas.95.25.14863", "10.1007/978-3-319-06793-3\\_2", "10.1111/cgf.12883", "10.1007/978-3-540-78243-8\\_9", "10.1057/palgrave.ivs.9500180", "10.1109/pacificvis.2012.6183556", "10.1117/12.872578", "10.1145/1168149.1168169", "10.1109/iv.2013.3", "10.1109/tvcg.2011.186", "10.2307/2685881", "10.1109/pacificvis.2015.7156354", "10.1109/tvcg.2016.2598885", "10.1109/tvcg.2018.2811488", "10.1109/infvis.2005.1532129", "10.1111/j.1467-8659.2009.01450.x", "10.1057/palgrave.ivs.9500162", "10.1109/tvcg.2009.116", "10.1109/2945.468391"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3029413", "title": "A Design Space of Vision Science Methods for Visualization Research", "year": "2020", "conferenceName": "InfoVis", "authors": "Madison A. Elliott;Christine Nothelfer;Cindy Xiong;Danielle Albers Szafir", "citationCount": "0", "affiliation": "Elliott, MA (Corresponding Author), Univ British Columbia, Vancouver, BC, Canada. Elliott, Madison A., Univ British Columbia, Vancouver, BC, Canada. Nothelfer, Christine, Northwestern Univ, Evanston, IL 60208 USA. Xiong, Cindy, Univ Massachusetts, Amherst, MA 01003 USA. Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "Canada;USA", "abstract": "A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.", "keywords": "Perception,human vision,empirical research,evaluation,HCI", "link": "http://dx.doi.org/10.1109/TVCG.2020.3029413", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1177/0886109909354981", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1093/bioinformatics/btq110", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1177/1744987107081254", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1177/174498710501000305", "10.1017/s1049096513001789", "10.1109/tvcg.2012.213", "10.1093/nar/gkz239", "10.1093/sysbio/sys062", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030405", "title": "Insights From Experiments With Rigor in an EvoBio Design Study", "year": "2020", "conferenceName": "InfoVis", "authors": "Jennifer Rogers;Austin H. Patton;Luke Harmon;Alexander Lex;Miriah D. Meyer", "citationCount": "0", "affiliation": "Rogers, J (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Rogers, Jen; Lex, Alexander; Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Patton, Austin H., Washington State Univ, Pullman, WA 99164 USA. Harmon, Luke, Univ Idaho, Moscow, ID 83843 USA.", "countries": "USA", "abstract": "Design study is an established approach of conducting problem-driven visualization research. The academic visualization community has produced a large body of work for reporting on design studies, informed by a handful of theoretical frameworks, and applied to a broad range of application areas. The result is an abundance of reported insights into visualization design, with an emphasis on novel visualization techniques and systems as the primary contribution of these studies. In recent work we proposed a new, interpretivist perspective on design study and six companion criteria for rigor that highlight the opportunities for researchers to contribute knowledge that extends beyond visualization idioms and software. In this work we conducted a year-long collaboration with evolutionary biologists to develop an interactive tool for visual exploration of multivariate datasets and phylogenetic trees. During this design study we experimented with methods to support three of the rigor criteria: ABUNDANT, REFLEXIVE, and TRANSPARENT. As a result we contribute two novel visualization techniques for the analysis of multivariate phylogenetic datasets, three methodological recommendations for conducting design studies drawn from reflections over our process of experimentation, and two writing devices for reporting interpretivist design study. We offer this work as an example for implementing the rigor criteria to produce a diverse range of knowledge contributions.", "keywords": "Methodologies,Application Motivated Visualization,Guidelines,Life Sciences Visualization,Health,Medicine,Biology,Bioinformatics,Genomics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030405", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1080/17493460802276893", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1109/tvcg.2012.213", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13963", "year": "2020", "title": "MotionGlyphs: Visual Abstraction of Spatio-Temporal Networks in Collective Animal Behavior", "conferenceName": "EuroVis", "authors": "Eren Cakmak;Hanna Sch{\\\"{a}}fer;Juri Buchm{\\\"{u}}ller;Johannes Fuchs;Tobias Schreck;A. Jordan;Daniel A. Keim", "citationCount": "0", "affiliation": "Cakmak, E (Corresponding Author), Univ Konstanz, Constance, Germany.\nCakmak, E (Corresponding Author), Ctr Adv Study Collect Behav, Constance, Germany.\nCakmak, E.; Schaefer, H.; Buchmueller, J.; Fuchs, J.; Jordan, A.; Keim, D., Univ Konstanz, Constance, Germany.\nCakmak, E.; Jordan, A.; Keim, D., Ctr Adv Study Collect Behav, Constance, Germany.\nSchreck, T., Graz Univ Technol, Graz, Austria.\nJordan, A., Max Planck Inst Anim Behav, Radolfzell am Bodensee, Germany.", "countries": "Germany;Austria", "abstract": "Domain experts for collective animal behavior analyze relationships between single animal movers and groups of animals over time and space to detect emergent group properties. A common way to interpret this type of data is to visualize it as a spatio-temporal network. Collective behavior data sets are often large, and may hence result in dense and highly connected node-link diagrams, resulting in issues of node-overlap and edge clutter. In this design study, in an iterative design process, we developed glyphs as a design for seamlessly encoding relationships and movement characteristics of a single mover or clusters of movers. Based on these glyph designs, we developed a visual exploration prototype, MotionGlyphs, that supports domain experts in interactively filtering, clustering, and animating spatio-temporal networks for collective animal behavior analysis. By means of an expert evaluation, we show how MotionGlyphs supports important tasks and analysis goals of our domain experts, and we give evidence of the usefulness for analyzing spatio-temporal networks of collective animal behavior.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13963", "refList": ["10.1109/tvcg.2007.70582", "10.1111/cgf.13213", "10.1109/pacificvis.2014.13", "10.1145/2093973.2094038", "10.1111/j.1467-8659.2009.01664.x", "10.1109/tvcg.2010.44", "10.1111/cgf.12106", "10.1111/cgf.12791", "10.1080/15230406.2014.890071", "10.1111/tgis.12100", "10.1111/j.1467-8659.2009.01451.x", "10.1179/000870409x12525737905042", "10.1111/cgf.12923", "10.1006/ijhc.2002.1017", "10.1145/1124772.1124891", "10.1109/tvcg.2011.213", "10.1109/vast.2014.7042484", "10.1007/s12650-016-0375-5", "10.1109/vlhcc.2012.6344514", "10.1073/pnas.1420068112", "10.1006/ijhc.1017", "10.1007/s00371-017-1461-y", "10.1109/infvis.2003.1249008", "10.1111/cgf.13728", "10.2312/conf/eg2013/stars/039-063", "10.1109/tvcg.2014.2346271", "10.1109/hicss.2011.339", "10.1068/p3104", "10.1145/2931002.2931012", "10.1109/asonam.2012.39", "10.1179/000870403235002042", "10.1145/2556288.2557010", "10.1145/2470654.2466443", "10.1109/tvcg.2011.209", "10.1016/j.tree.2013.06.002", "10.1111/1365-2656.12418", "10.1111/cgf.12872", "10.1145/2470654.2466444", "10.1109/tvcg.2014.2322594", "10.1109/tvcg.2008.125", "10.1109/tvcg.2014.2346426", "10.1109/tvcg.2006.166", "10.1111/cgf.12615", "10.1057/palgrave.ivs.9500170", "10.1007/3-540-36151-0", "10.1117/12.872578", "10.1016/j.tics.2008.10.002", "10.1006/jtbi.2002.3065", "10.1109/tvcg.2010.78", "10.1109/iv.2013.3", "10.1073/pnas.1001763107", "10.1109/pacificvis.2015.7156354", "10.1371/journal.pbio.1001805", "10.5220/0005303801230130", "10.1111/j.1467-8659.2009.01687.x", "10.1007/s10844-011-0159-2", "10.1007/s12650-018-00543-4", "10.1145/2669557.2669572", "10.1111/cgf.13184", "10.1186/s40462-015-0032-y", "10.1016/j.ins.2016.06.048", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13987", "year": "2020", "title": "Augmenting Node-Link Diagrams with Topographic Attribute Maps", "conferenceName": "EuroVis", "authors": "Reinhold Preiner;Johanna Schmidt;Katharina Kr{\\\"{o}}sl;Tobias Schreck;Gabriel Mistelbauer", "citationCount": "0", "affiliation": "Preiner, R (Corresponding Author), Graz Univ Technol, Inst Comp Graph \\& Knowledge Visualizat, Graz, Austria.\nPreiner, R.; Schreck, T., Graz Univ Technol, Inst Comp Graph \\& Knowledge Visualizat, Graz, Austria.\nSchmidt, J.; Kroesl, K., Virtual Real \\& Visualisierung Forsch GmbH, VRVis Zentrum, Vienna, Austria.\nKroesl, K., TU Wien, Inst Visual Comp \\& Human Ctr Technol, Vienna, Austria.\nMistelbauer, G., Otto von Guericke Univ, Dept Simulat \\& Graph, Magdeburg, Germany.", "countries": "Germany;Austria", "abstract": "We propose a novel visualization technique for graphs that are attributed with scalar data. In many scenarios, these attributes (e.g., birth date in a family network) provide ambient context information for the graph structure, whose consideration is important for different visual graph analysis tasks. Graph attributes are usually conveyed using different visual representations (e.g., color, size, shape) or by reordering the graph structure according to the attribute domain (e.g., timelines). While visual encodings allow graphs to be arranged in a readable layout, assessing contextual information such as the relative similarities of attributes across the graph is often cumbersome. In contrast, attribute-based graph reordering serves the comparison task of attributes, but typically strongly impairs the readability of the structural information given by the graph's topology. In this work, we augment force-directed node-link diagrams with a continuous ambient representation of the attribute context. This way, we provide a consistent overview of the graph's topological structure as well as its attributes, supporting a wide range of graph-related analysis tasks. We resort to an intuitive height field metaphor, illustrated by a topographic map rendering using contour lines and suitable color maps. Contour lines visually connect nodes of similar attribute values, and depict their relative arrangement within the global context. Moreover, our contextual representation supports visualizing attribute value ranges associated with graph nodes (e.g., lifespans in a family network) as trajectories routed through this height field. We discuss how user interaction with both the structural and the contextual information fosters exploratory graph analysis tasks. The effectiveness and versatility of our technique is confirmed in a user study and case studies from various application domains.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13987", "refList": ["10.1109/tvcg.2013.269", "10.1109/pacificvis.2010.5429590", "10.1073/pnas.0307654100", "10.1145/2505515.2505758", "10.1559/152304082783948286", "10.1109/pacificvis.2014.47", "10.1093/bioinformatics/btp432", "10.1111/j.1467-8659.2011.01898.x", "10.1111/cgf.12931", "10.1111/cgf.12880", "10.1109/tvcg.2014.2346422", "10.1111/j.1467-8659.2009.01706.x", "10.1109/tvcg.2016.2598795", "10.1111/cgf.12800", "10.1109/tvcg.2014.2315995", "10.1111/cgf.12656", "10.1111/cgf.13728", "10.1109/tvcg.2009.122", "10.1111/cgf.13211", "10.1109/tvcg.2007.70596", "10.1109/infvis.2002.1173152", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2003.1196007", "10.1109/infvis.2005.1532150", "10.1145/3243250.3243266", "10.1080/02693799008941549", "10.1371/journal.pone.0058779", "10.1109/infvis.1995.528686", "10.1111/cgf.12872", "10.1002/spe.4380211102", "10.1109/38.974518", "10.1145/3097983.3098130", "10.1002/aris.1440370106", "10.1145/1360612.1360691", "10.1109/mc.2016.145", "10.2307/3006914", "10.1111/j.1467-8659.2009.01683.x", "10.1145/1639714.1639784"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1111/cgf.13963", "year": "2020", "title": "MotionGlyphs: Visual Abstraction of Spatio-Temporal Networks in Collective Animal Behavior", "conferenceName": "EuroVis", "authors": "Eren Cakmak;Hanna Sch{\\\"{a}}fer;Juri Buchm{\\\"{u}}ller;Johannes Fuchs;Tobias Schreck;A. Jordan;Daniel A. Keim", "citationCount": "0", "affiliation": "Cakmak, E (Corresponding Author), Univ Konstanz, Constance, Germany.\nCakmak, E (Corresponding Author), Ctr Adv Study Collect Behav, Constance, Germany.\nCakmak, E.; Schaefer, H.; Buchmueller, J.; Fuchs, J.; Jordan, A.; Keim, D., Univ Konstanz, Constance, Germany.\nCakmak, E.; Jordan, A.; Keim, D., Ctr Adv Study Collect Behav, Constance, Germany.\nSchreck, T., Graz Univ Technol, Graz, Austria.\nJordan, A., Max Planck Inst Anim Behav, Radolfzell am Bodensee, Germany.", "countries": "Germany;Austria", "abstract": "Domain experts for collective animal behavior analyze relationships between single animal movers and groups of animals over time and space to detect emergent group properties. A common way to interpret this type of data is to visualize it as a spatio-temporal network. Collective behavior data sets are often large, and may hence result in dense and highly connected node-link diagrams, resulting in issues of node-overlap and edge clutter. In this design study, in an iterative design process, we developed glyphs as a design for seamlessly encoding relationships and movement characteristics of a single mover or clusters of movers. Based on these glyph designs, we developed a visual exploration prototype, MotionGlyphs, that supports domain experts in interactively filtering, clustering, and animating spatio-temporal networks for collective animal behavior analysis. By means of an expert evaluation, we show how MotionGlyphs supports important tasks and analysis goals of our domain experts, and we give evidence of the usefulness for analyzing spatio-temporal networks of collective animal behavior.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13963", "refList": ["10.1109/tvcg.2007.70582", "10.1111/cgf.13213", "10.1109/pacificvis.2014.13", "10.1145/2093973.2094038", "10.1111/j.1467-8659.2009.01664.x", "10.1109/tvcg.2010.44", "10.1111/cgf.12106", "10.1111/cgf.12791", "10.1080/15230406.2014.890071", "10.1111/tgis.12100", "10.1111/j.1467-8659.2009.01451.x", "10.1179/000870409x12525737905042", "10.1111/cgf.12923", "10.1006/ijhc.2002.1017", "10.1145/1124772.1124891", "10.1109/tvcg.2011.213", "10.1109/vast.2014.7042484", "10.1007/s12650-016-0375-5", "10.1109/vlhcc.2012.6344514", "10.1073/pnas.1420068112", "10.1006/ijhc.1017", "10.1007/s00371-017-1461-y", "10.1109/infvis.2003.1249008", "10.1111/cgf.13728", "10.2312/conf/eg2013/stars/039-063", "10.1109/tvcg.2014.2346271", "10.1109/hicss.2011.339", "10.1068/p3104", "10.1145/2931002.2931012", "10.1109/asonam.2012.39", "10.1179/000870403235002042", "10.1145/2556288.2557010", "10.1145/2470654.2466443", "10.1109/tvcg.2011.209", "10.1016/j.tree.2013.06.002", "10.1111/1365-2656.12418", "10.1111/cgf.12872", "10.1145/2470654.2466444", "10.1109/tvcg.2014.2322594", "10.1109/tvcg.2008.125", "10.1109/tvcg.2014.2346426", "10.1109/tvcg.2006.166", "10.1111/cgf.12615", "10.1057/palgrave.ivs.9500170", "10.1007/3-540-36151-0", "10.1117/12.872578", "10.1016/j.tics.2008.10.002", "10.1006/jtbi.2002.3065", "10.1109/tvcg.2010.78", "10.1109/iv.2013.3", "10.1073/pnas.1001763107", "10.1109/pacificvis.2015.7156354", "10.1371/journal.pbio.1001805", "10.5220/0005303801230130", "10.1111/j.1467-8659.2009.01687.x", "10.1007/s10844-011-0159-2", "10.1007/s12650-018-00543-4", "10.1145/2669557.2669572", "10.1111/cgf.13184", "10.1186/s40462-015-0032-y", "10.1016/j.ins.2016.06.048", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [], "len": 1}], "len": 71}, {"doi": "10.1109/tvcg.2018.2864811", "title": "Analysis of Flight Variability: a Systematic Approach", "year": "2018", "conferenceName": "VAST", "authors": "Natalia V. Andrienko;Gennady L. Andrienko;Jose Manuel Cordero Garcia;David Scarlatti", "citationCount": "2", "affiliation": "Andrienko, G (Corresponding Author), Fraunhofer IAIS, St Augustin, Germany. Andrienko, G (Corresponding Author), City Univ London, London, England. Andrienko, Natalia; Andrienko, Gennady, Fraunhofer IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Garcia, Jose Manuel Cordero, CRIDA Reference Ctr Res Dev \\& Innovat ATM, Madrid, Spain. Scarlatti, David, Boeing Res \\& Dev Europe, Madrid, Spain.", "countries": "Spain;Germany;England", "abstract": "In movement data analysis, there exists a problem of comparing multiple trajectories of moving objects to common or distinct reference trajectories. We introduce a general conceptual framework for comparative analysis of trajectories and an analytical procedure, which consists of (1) finding corresponding points in pairs of trajectories, (2) computation of pairwise difference measures, and (3) interactive visual analysis of the distributions of the differences with respect to space, time, set of moving objects, trajectory structures, and spatio-temporal context. We propose a combination of visualisation, interaction, and data transformation techniques supporting the analysis and demonstrate the use of our approach for solving a challenging problem from the aviation domain.", "keywords": "Visual analytics,movement data,flight trajectories", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864811", "refList": ["10.1007/s00371-015-1185-9", "10.1109/tvcg.2013.193", "10.1177/1473871611416549", "10.1109/tvcg.2010.44", "10.1109/pacificvis.2011.5742387", "10.1016/j.compenvurbsys.2014.01.005", "10.1007/978-3-642-37583-5", "10.1145/1653771.1653820", "10.1111/j.1467-8659.2009.01440.x", "10.1177/1473871615581216", "10.2312/pe/eurovast/eurova12/079-083", "10.1109/icde.2002.994784", "10.1109/tvcg.2012.311", "10.1109/tvcg.2016.2616404", "10.1109/tvcg.2011.233", "10.1007/3-540-31190-4", "10.1145/3003965.3003970", "10.1080/13658816.2011.556120", "10.1016/j.trc.2008.11.004", "10.1109/tvcg.2015.2468078", "10.1109/tits.2017.2683539", "10.1109/dasc.2016.7777947", "10.1111/cgf.12630", "10.1145/1345448.1345454", "10.1109/tits.2015.2436897", "10.1145/2093973.2094060", "10.1109/vast.2008.4677356", "10.1559/152304000783547759", "10.1109/tvcg.2009.145", "10.1109/tvcg.2012.265", "10.1109/tits.2016.2547641", "10.1142/s0218195995000064", "10.2307/144477", "10.1080/13658810500105572", "10.1145/2525314.2525360", "10.1109/vast.2010.5653580", "10.1145/2782759.2782767", "10.1016/j.visinf.2017.01.004", "10.1007/s10844-011-0159-2", "10.1109/tvcg.2017.2744322", "10.1109/mcg.2006.93", "10.1145/1842993.1843034", "10.1109/tvcg.2015.2467851", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934264", "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;David S. Ebert;Arif Ghafoor", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Karimzadeh, Morteza; Ebert, David S.; Ghafoor, Arif, Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.", "keywords": "Summative evaluation,usefulness,evaluation process,taxonomy,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934264", "refList": ["10.1109/tvcg.2017.2744478", "10.1109/tvcg.2018.2865025", "10.1109/tvcg.2006.85", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2017.2745181", "10.1111/cgf.13677", "10.1109/tvcg.2018.2864844", "10.1109/tvcg.2013.126", "10.1109/tvcg.2018.2864811", "10.1109/infvis.2005.1532147", "10.1177/0956797613504966", "10.1145/2669557.2669579", "10.1109/mcg.2005.102", "10.1109/visual.2003.1250426", "10.1136/bmj.39489.470347.ad", "10.1109/tvcg.2017.2744080", "10.1109/mcg.2009.53", "10.1111/j.1467-8527.2005.00307.x", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2864886", "10.1109/tvcg.2018.2864843", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2018.2865051", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2018.2865026", "10.1007/978-3-540-71080-6\\_6", "10.1109/tvcg.2018.2865020", "10.1177/1473871611407399", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2018.2864526", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2864905", "10.1049/sej.1991.0040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2711030", "10.1109/tvcg.2011.279", "10.1109/vast.2017.8585505", "10.1147/jrd.2010.2042914", "10.1016/s0378-7206(98)00044-5", "10.1145/2993901.2993913", "10.1109/tvcg.2018.2865041", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744758", "10.1145/1168149.1168158", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2012.213", "10.1109/tvcg.2017.2744738", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2018.2864826", "10.1145/1377966.1377974", "10.1109/apec.2009.4802646", "10.1145/1168149.1168152", "10.1016/j.jss.2008.03.059", "10.1109/vast.2017.8585484", "10.1109/tvcg.2017.2744818", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2865042", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030388", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "year": "2020", "conferenceName": "SciVis", "authors": "Pepe Eulzer;Sabine Bauer;Francis Kilian;Kai Lawonn", "citationCount": "0", "affiliation": "Eulzer, P (Corresponding Author), Univ Jena, Jena, Germany. Eulzer, Pepe; Lawonn, Kai, Univ Jena, Jena, Germany. Bauer, Sabine, Univ Koblenz Landau, Koblenz, Germany. Kilian, Francis, Cath Clin Koblenz Montabaur, Dept Spine Surg, Koblenz, Germany.", "countries": "Germany", "abstract": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. $\\mathrm{By}$ linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. $\\mathrm{in}$ a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.", "keywords": "Medical visualization,bioinformatics,coordinated views,focus and context,biomechanical simulation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030388", "refList": ["10.1109/tvcg.2018.2864903", "10.1177/1473871613510429", "10.1093/ehjqcco/qcz052", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2018.2865076", "10.1055/s-0039-1687862", "10.1109/visual.1990.146375", "10.1109/tvcg.2017.2744198", "10.1016/j.ijmedinf.2014.10.001", "10.1109/tvcg.2013.124", "10.1016/j.jacc", "10.1111/cgf.13167", "10.17705/1thci.00055", "10.1136/bmjqs.2009.037895", "10.1109/tvcg.2013.238", "10.1109/tvcg.2018.2865240", "10.1186/1471-2261-6-34", "10.1109/tvcg.2019.2934264", "10.1109/tvcg.2013.200", "10.1109/tvcg.2011.209", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467091", "10.1136/bmjopen-2019-033208", "10.1109/beliv.2018.8634027", "10.1109/tvcg.2012.213", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2015.2467325", "10.1145/2133806.2133821", "10.1145/1806799.1806866", "10.1108/02635570610688869", "10.1002/hbm.20701", "10.1561/1100000039", "10.1145/3025453.3025645", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 5}, {"doi": "10.1109/tvcg.2018.2865139", "title": "Structure-Based Suggestive Exploration: A New Approach for Effective Exploration of Large Networks", "year": "2018", "conferenceName": "InfoVis", "authors": "Wei Chen;Fangzhou Guo;Dongming Han;Jacheng Pan;Xiaotao Nie;Jiazhi Xia;Xiaolong Zhang", "citationCount": "5", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Labo CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Chen, Wei; Guo, Fangzhou; Han, Dongming; Pan, Jacheng; Nie, Xiaotao, Zhejiang Univ, State Key Labo CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Xia, Jiazhi, Cent S Univ, Changsha, Hunan, Peoples R China. Zhang, Xiaolong, Penn State Univ, University Pk, PA 16802 USA.", "countries": "USA;China", "abstract": "When analyzing a visualized network, users need to explore different sections of the network to gain insight. However, effective exploration of large networks is often a challenge. While various tools are available for users to explore the global and local features of a network, these tools usually require significant interaction activities, such as repetitive navigation actions to follow network nodes and edges. In this paper, we propose a structure-based suggestive exploration approach to support effective exploration of large networks by suggesting appropriate structures upon user request. Encoding nodes with vectorized representations by transforming information of surrounding structures of nodes into a high dimensional space, our approach can identify similar structures within a large network, enable user interaction with multiple similar structures simultaneously, and guide the exploration of unexplored structures. We develop a web-based visual exploration system to incorporate this suggestive exploration approach and compare performances of our approach under different vectorizing methods and networks. We also present the usability and effectiveness of our approach through a controlled user study with two datasets.", "keywords": "Large Network Exploration,Structure-Based Exploration,Suggestive Exploration", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865139", "refList": ["10.1109/tvcg.2007.70582", "10.1109/tvcg.2006.120", "10.1109/mc.2013.242", "10.1109/tvcg.2009.108", "10.1145/2939672.2939754", "10.1016/j.comnet.2011.08.019", "10.1145/2702123.2702476", "10.1109/tvcg.2017.2744938", "10.1007/s00371-013-0892-3", "10.1137/1.9781611974973.67", "10.1109/vast.2009.5333893", "10.1109/tst.2013.6509098", "10.1109/35021bigcomp.2015.7072812", "10.1145/2909132.2909246", "10.1111/j.1467-8659.2011.01957.x", "10.1111/j.1467-8659.2011.01898.x", "10.1177/1473871612455749", "10.1109/tvcg.2013.167", "10.1145/3097983.3098061", "10.1007/978-1-4613-0303-9\\_28", "10.1109/vast.2014.7042485", "10.1109/tmm.2016.2614220", "10.1145/1376616.1376675", "10.1145/2623330.2623732", "10.14778/1920841.1920887", "10.1109/tvcg.2017.2745219", "10.1109/tvcg.2017.2743858", "10.1109/tvcg.2008.151", "10.1145/1150402.1150479", "10.1093/bioinformatics/bth436", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2016.2598958", "10.1111/cgf.12883", "10.1145/1556262.1556300", "10.1109/icdm.2012.159", "10.1145/2470654.2466444", "10.1109/tvcg.2013.109", "10.1109/infvis.2004.1", "10.1109/icdmw.2008.99", "10.1002/aris.1440370106", "10.1007/978-3-319-05813-9\\_11", "10.1371/journal.pone.0098679", "10.1109/tvcg.2006.106", "10.1111/j.1467-8659.2011.01935.x", "10.1111/cgf.12397", "10.1111/cgf.13184", "10.1111/cgf.12642", "10.1109/tvcg.2016.2598831", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934396", "title": "A Deep Generative Model for Graph Layout", "year": "2019", "conferenceName": "InfoVis", "authors": "Oh-Hyun Kwon;Kwan-Liu Ma", "citationCount": "4", "affiliation": "Kwon, OH (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Kwon, Oh-Hyun; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "Different layouts can characterize different aspects of the same graph. Finding a \u201cgood\u201d layout of a graph is thus an important task for graph visualization. In practice, users often visualize a graph in multiple layouts by using different methods and varying parameter settings until they find a layout that best suits the purpose of the visualization. However, this trial-and-error process is often haphazard and time-consuming. To provide users with an intuitive way to navigate the layout design space, we present a technique to systematically visualize a graph in diverse layouts using deep generative models. We design an encoder-decoder architecture to learn a model from a collection of example layouts, where the encoder represents training examples in a latent space and the decoder produces layouts from the latent space. In particular, we train the model to construct a two-dimensional latent space for users to easily explore and generate various layouts. We demonstrate our approach through quantitative and qualitative evaluations of the generated layouts. The results of our evaluations show that our model is capable of learning and generalizing abstract concepts of graph layouts, not just memorizing the training examples. In summary, this paper presents a fundamentally new approach to graph visualization where a machine learning model learns to visualize a graph from examples without manually-defined heuristics.", "keywords": "Graph,network,visualization,layout,machine learning,deep learning,neural network,generative model,autoencoder", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934396", "refList": ["10.1103/physreve.74.036104", "10.1145/234535.234538", "10.2307/2412323", "10.1109/tvcg.2007.70580", "10.7155/jgaa.00405", "10.1177/1473871612455749", "10.1109/tvcg.2011.185", "10.1073/pnas.122653799", "10.1007/3-540-58950-3", "10.1145/2897824.2925974", "10.1007/3-540-44541-2\\_17", "10.1007/bf00410640", "10.1021/acscentsci.7b00572", "10.1007/s10208-011-9093-5", "10.1109/tvcg.2015.2467451", "10.1016/0020-0190(89)90102-6", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13187", "10.1214/aoms/1177729586", "10.1109/tvcg.2014.2346277", "10.1109/tvcg.2017.2743858", "10.1007/978-3-662-44043-8\\_3", "10.1038/30918", "10.1109/mcg.2018.2881501", "10.1016/j.camwa.2004.08.015", "10.1109/tvcg.2010.269", "10.1006/s1045-926x(02)00016-2", "10.1016/0925-7721(94)00014-x", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1103/physrevx.4.011047", "10.1371/journal.pone.0098679", "10.1145/2487788.2488173", "10.1007/978-3-030-01418-6\\_41", "10.1007/978-3-030-04414-5\\_12", "10.1109/tvcg.2018.2865139", "10.1142/s0219525903001067", "10.7155/jgaa.00051"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030367", "title": "Lyra 2: Designing Interactive Visualizations by Demonstration", "year": "2020", "conferenceName": "InfoVis", "authors": "Jonathan Zong;Dhiraj Barnwal;Rupayan Neogy;Arvind Satyanarayan", "citationCount": "0", "affiliation": "Zong, J (Corresponding Author), MIT, Cambridge, MA 02139 USA. Zong, Jonathan; Neogy, Rupayan; Satyanarayan, Arvind, MIT, Cambridge, MA 02139 USA. Barnwal, Dhiraj, Indian Inst Technol Kharagpur, Kharagpur, W Bengal, India.", "countries": "India;USA", "abstract": "Recent graphical interfaces offer direct manipulation mechanisms for authoring visualizations, but are largely restricted to static output. To author interactive visualizations, users must instead turn to textual specification, but such approaches impose a higher technical burden. To bridge this gap, we introduce Lyra 2, a system that extends a prior visualization design environment with novel methods for authoring interaction techniques by demonstration. Users perform an interaction (e.g., button clicks, drags, or key presses) directly on the visualization they are editing. The system interprets this performance using a set of heuristics and enumerates suggestions of possible interaction designs. These heuristics account for the properties of the interaction (e.g., target and event type) as well as the visualization (e.g., mark and scale types, and multiple views). Interaction design suggestions are displayed as thumbnails; users can preview and test these suggestions, iteratively refine them through additional demonstrations, and finally apply and customize them via property inspectors. We evaluate our approach through a gallery of diverse examples, and evaluate its usability through a first-use study and via an analysis of its cognitive dimensions. We find that, in Lyra 2, interaction design by demonstration enables users to rapidly express a wide range of interactive visualizations.", "keywords": "Direct manipulation,interactive visualization,interaction design by demonstration", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030367", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030467", "title": "PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch;Lingyun Yu;Peiran Ren;Thomas Ertl;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Knittel, Johannes; Koch, Steffen; Ertl, Thomas, Univ Stuttgart, VIS VISUS, Stuttgart, Germany. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Dept Comp Sci \\& Software Engn, Suzhou, Peoples R China. Ren, Peiran, Alibaba Grp, Hangzhou, Peoples R China.", "countries": "Germany;China", "abstract": "Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.", "keywords": "Storyline visualization,reinforcement learning,mixed-initiative design", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030467", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030459", "title": "Scalability of Network Visualisation from a Cognitive Load Perspective", "year": "2020", "conferenceName": "InfoVis", "authors": "Vahan Yoghourdjian;Yalong Yang;Tim Dwyer;Lawrence Lee;Michael Wybrow;Kim Marriott", "citationCount": "0", "affiliation": "Yoghourdjian, V (Corresponding Author), Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Yoghourdjian, Vahan; Yang, Yalong; Dwyer, Tim; Wybrow, Michael; Marriott, Kim, Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Lawrence, Lee, Monash Univ, Fac Business \\& Econ, Melbourne, Vic, Australia. Yang, Yalong, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA.", "countries": "USA;Australia", "abstract": "Node-link diagrams are widely used to visualise networks. However, even the best network layout algorithms ultimately result in \u2018hairball\u2019 visualisations when the graph reaches a certain degree of complexity, requiring simplification through aggregation or interaction (such as filtering) to remain usable. Until now, there has been little data to indicate at what level of complexity node-link diagrams become ineffective or how visual complexity affects cognitive load. To this end, we conducted a controlled study to understand workload limits for a task that requires a detailed understanding of the network topology-finding the shortest path between two nodes. We tested performance on graphs with 25 to 175 nodes with varying density. We collected performance measures (accuracy and response time), subjective feedback, and physiological measures (EEG, pupil dilation, and heart rate variability). To the best of our knowledge this is the first network visualisation study to include physiological measures. Our results show that people have significant difficulty finding the shortest path in high density node-link diagrams with more than 50 nodes and even low density graphs with more than 100 nodes. From our collected EEG data we observe functional differences in brain activity between hard and easy tasks. We found that cognitive load increased up to certain level of difficulty after which it decreased, likely because participants had given up. We also explored the effects of global network layout features such as size or number of crossings, and features of the shortest path such as length or straightness on task difficulty. We found that global features generally had a greater impact than those of the shortest path.", "keywords": "Data Visualisation,Network Visualisation,Cognitive Load,EEG", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030459", "refList": ["10.1109/tvcg.2019.2934396", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2016.2570755", "10.1007/s00371-013-0892-3", "10.1007/b98835", "10.1109/isda.2014.7066252", "10.1109/tvcg.2015.2467251", "10.1177/1473871612455749", "10.1109/tvcg.2012.299", "10.1007/3-540-58950-3", "10.1111/cgf.12878", "10.1109/mcse.2007.55", "10.1109/tvcg.2012.238", "10.1145/264645.264657", "10.1109/tvcg.2015.2467451", "10.1109/tvcg.2013.151", "10.1016/0020-0190(89)90102-6", "10.1109/tvcg.2019.2934307", "10.1109/tvcg.2015.2468151", "10.1109/tvcg.2017.2745919", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13440", "10.1109/tvcg.2011.220", "10.1111/cgf.13187", "10.1109/t-c.1969.222678", "10.1109/tvcg.2017.2743858", "10.1126/science.290.5500.2319", "10.1109/cahpc.2018.8645912", "10.1109/tvcg.2015.2465151", "10.1109/tvcg.2016.2598958", "10.1109/tvcg.2017.2751473", "10.1002/spe.4380211102", "10.1109/tpds.2018.2869805", "10.1016/j.jpdc.2019.04.008", "10.1109/pacificvis.2017.8031574", "10.1006/s1045-926x(02)00016-2", "10.1109/tvcg.2017.2674999", "10.1145/2872427.2883041", "10.1145/3292500.3330989", "10.1109/tvcg.2017.2744878", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1109/sbac-pad.2018.00060", "10.1007/978-3-662-45803-7\\_27", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1111/j.1469-1809.1936.tb02137.x", "10.1007/bf02289565", "10.1109/tvcg.2017.2689016", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.4722", "year": "2020", "title": "A Study of Mental Maps in Immersive Network Visualization", "conferenceName": "PacificVis", "authors": "Joseph Kotlarek;Oh{-}Hyun Kwon;Kwan{-}Liu Ma;Peter Eades;Andreas Kerren;Karsten Klein;Falk Schreiber", "citationCount": "0", "affiliation": "Kotlarek, J (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA.\nKotlarek, Joseph; Kwon, Oh-Hyun; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.\nEades, Peter, Univ Sydney, Sydney, NSW, Australia.\nKerren, Andreas, Linnaeus Univ, Vaxjo, Sweden.\nKlein, Karsten; Schreiber, Falk, Univ Konstanz, Constance, Germany.", "countries": "Sweden;Germany;USA;Australia", "abstract": "The visualization of a network influences the quality of the mental map that the viewer develops to understand the network. In this study, we investigate the effects of a 3D immersive visualization environment compared to a traditional 2D desktop environment on the comprehension of a network's structure. We compare the two visualization environments using three tasks-interpreting network structure, memorizing a set of nodes, and identifying the structural changes-commonly used for evaluating the quality of a mental map in network visualization. The results show that participants were able to interpret network structure more accurately when viewing the network in an immersive environment, particularly for larger networks. However, we found that 2D visualizations performed better than immersive visualization for tasks that required spatial memory.", "keywords": "Human-centered computing; Visualization; Visualization techniques; Graph drawings; Human-centered computing; Visualization; Empirical studies in visualization", "link": "https://doi.org/10.1109/PacificVis48177.2020.4722", "refList": ["10.1103/physreve.74.036104", "10.1109/tvcg.2019.2934396", "10.1007/978-3-540-87730-1\\_9", "10.1117/12.2005484", "10.1177/1473871612455749", "10.1109/pacificvis.2017.8031577", "10.1007/978-3-319-73207-7", "10.1109/38.888006", "10.1109/2945.841119", "10.1109/mc.2005.297", "10.1007/s10055-018-0346-3", "10.1109/tvcg.2016.2599107", "10.1109/icsmc.1992.271688", "10.1038/30918", "10.1006/jvlc.1995.1010", "10.1089/109493101300117938", "10.1109/vrais.1998.658488", "10.1109/pacificvis.2015.7156357", "10.1109/tvcg.2010.78", "10.1109/tvcg.2016.2520921", "10.1007/978-3-030-01388-22", "10.1145/229459.229467", "10.1145/1056808.1056875", "10.1109/tvcg.2017.2744079", "10.1109/bdva.2015.7314293", "10.1086/jar.33.4.3629752", "10.1016/j.ijhcs.2013.08.004"], "wos": 1, "children": [], "len": 1}], "len": 11}, {"doi": "10.1109/tvcg.2020.3030440", "title": "Context-aware Sampling of Large Networks via Graph Representation Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Zhiguang Zhou;Chen Shi;Xilong Shen;Lihong Cai;Haoxuan Wang;Yuhua Liu;Ying Zhao;Wei Chen", "citationCount": "0", "affiliation": "Zhao, Y (Corresponding Author), Cent South Univ, Changsha, Peoples R China. Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Zhou, Zhiguang; Shi, Chen; Shen, Xilong; Cai, Lihong; Wang, Haoxuan; Liu, Yuhua, Zhejiang Univ Finance \\& Econ, Sch Informat, Hangzhou, Peoples R China. Zhao, Ying, Cent South Univ, Changsha, Peoples R China. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China.", "countries": "China", "abstract": "Numerous sampling strategies have been proposed to simplify large-scale networks for highly readable visualizations. It is of great challenge to preserve contextual structures formed by nodes and edges with tight relationships in a sampled graph, because they are easily overlooked during the process of sampling due to their irregular distribution and immunity to scale. In this paper, a new graph sampling method is proposed oriented to the preservation of contextual structures. We first utilize a graph representation learning (GRL) model to transform nodes into vectors so that the contextual structures in a network can be effectively extracted and organized. Then, we propose a multi-objective blue noise sampling model to select a subset of nodes in the vectorized space to preserve contextual structures with the retention of relative data and cluster densities in addition to those features of significance, such as bridging nodes and graph connections. We also design a set of visual interfaces enabling users to interactively conduct context-aware sampling, visually compare results with various sampling strategies, and deeply explore large networks. Case studies and quantitative comparisons based on real-world datasets have demonstrated the effectiveness of our method in the abstraction and exploration of large networks.", "keywords": "Graph sampling,Graph representation learning,Blue noise sampling,Graph evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030440", "refList": ["10.1145/2491159.2491168", "10.1016/j.physa.2015.04.035", "10.1145/1830252.1830274", "10.1109/icdmw.2007.91", "10.1002/net.21834", "10.1109/tvcg.2018.2864503", "10.1016/j.cag.2018.01.010", "10.1109/icc.2016.7511156", "10.1111/cgf.13444", "10.1145/956750.956831", "10.1145/364099.364331", "10.1007/s00180-016-0663-5", "10.1109/tvcg.2013.223", "10.1007/s12650-018-0530-2", "10.1103/physreve.73.016102", "10.1109/access.2018.2870684", "10.1007/978-3-319-06793-3\\_1", "10.2312/vissym/eurovis05/239-246", "10.1016/j.ins.2015.02.014", "10.1145/2339530.2339723", "10.1109/icde.2015.7113345", "10.1109/tvcg.2011.233", "10.14778/2809974.2809980", "10.1109/glocom.2015.7417471", "10.1145/2578153.2578175", "10.1214/aoms/1177705148", "10.1109/tvcg.2008.130", "10.14232/actacyb.20.1.2011.6", "10.1504/ijitm.2019.099809", "10.1109/tvcg.2018.2865020", "10.1145/956750", "10.1002/cpe.4330060203", "10.1145/1150402.1150479", "10.1103/physreve.72.036118", "10.1109/tvcg.2017.2744098", "10.1145/2020408.2020512", "10.1142/s0129183114400075", "10.1109/jsac.2011.111005", "10.1016/j.camwa.2011.11.057", "10.1145/2470654.2466444", "10.1109/tvcg.2017.2674999", "10.1214/aos/1013203451", "10.1109/icdcsw.2011.34", "10.1016/j.physa.2013.11.015", "10.1145/1081870.1081893", "10.1109/tnet.2008.2001730", "10.1109/access.2016.2633485", "10.1145/1879141.1879192", "10.1371/journal.pone.0098679", "10.1126/science.220.4598.671", "10.1109/pacificvis.2015.7156355", "10.1088/1475-7516/2011/08/011", "10.1007/978-3-319-27261-0\\_41", "10.1111/cgf.13410", "10.1109/tvcg.2018.2865139", "10.1109/tvcg.2016.2598831", "10.1016/j.physa.2014.06.065"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030447", "title": "DRGraph: An Efficient Graph Layout Algorithm for Large-scale Graphs by Dimensionality Reduction", "year": "2020", "conferenceName": "InfoVis", "authors": "Minfeng Zhu;Wei Chen;Yuanzhe Hu;Yuxuan Hou;Liangjun Liu;Kaiyuan Zhang", "citationCount": "0", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Zhu, Minfeng; Chen, Wei; Hu, Yuanzhe; Hou, Yuxuan; Liu, Liangjun; Zhang, Kaiyuan, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China.", "countries": "China", "abstract": "Efficient layout of large-scale graphs remains a challenging problem: the force-directed and dimensionality reduction-based methods suffer from high overhead for graph distance and gradient computation. In this paper, we present a new graph layout algorithm, called DRGraph, that enhances the nonlinear dimensionality reduction process with three schemes: approximating graph distances by means of a sparse distance matrix, estimating the gradient by using the negative sampling technique, and accelerating the optimization process through a multi-level layout scheme. DRGraph achieves a linear complexity for the computation and memory consumption, and scales up to large-scale graphs with millions of nodes. Experimental results and comparisons with state-of-the-art graph layout methods demonstrate that DRGraph can generate visually comparable layouts with a faster running time and a lower memory requirement.", "keywords": "graph visualization,graph layout,dimensionality reduction,force-directed layout", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030447", "refList": ["10.1109/tvcg.2007.70582", "10.1371/journal.pone.0136497", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/3018661.3018731", "10.1109/34.491619", "10.1145/2939672.2939754", "10.1145/3269206.3271788", "10.1016/j.comnet.2011.08.019", "10.1007/s11263-011-0442-2", "10.1016/s0020-0190(98)00108-2", "10.1109/tvcg.2018.2865151", "10.1007/978-3-319-61188-4\\_2", "10.1177/1473871612455749", "10.1177/1473871616666394", "10.1109/tvcg.2015.2467035", "10.1186/1471-2105-10-375", "10.1145/3097983.3098061", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1109/tvcg.2015.2467451", "10.1109/2945.841119", "10.1016/j.swevo.2015.10.002", "10.1016/0020-0190(89)90102-6", "10.1109/infvis.2003.1249009", "10.1109/tvcg.2017.2745919", "10.1111/cgf.13187", "10.1111/cgf.13440", "10.1109/tvcg.2012.245", "10.1109/tvcg.2017.2743858", "10.1177/1473871618821740", "10.1186/s12859-015-0585-1", "10.1002/nav.3800020109", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1006/s1045-926x(02)00016-2", "10.1109/cvpr.2012.6247667", "10.1023/b:jogo.0000042115.44455.f3", "10.1109/pacificvis.2017.8031607", "10.1002/nav.3800030404", "10.1109/cvpr.2008.4587500", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1090/s0002-9904-1920-03322-7", "10.1109/iv.2013.3", "10.1145/568522.568523", "10.1109/tvcg.2006.156", "10.1109/tvcg.2012.236", "10.1109/tvcg.2018.2865139", "10.1145/3219819.3220025", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030393", "title": "Exemplar-based Layout Fine-tuning for Node-link Diagrams", "year": "2020", "conferenceName": "InfoVis", "authors": "Jiacheng Pan;Wei Chen;Xiaodong Zhao;Shuyue Zhou;Wei Zeng;Minfeng Zhu;Jian Chen;Siwei Fu;Yingcai Wu", "citationCount": "1", "affiliation": "Chen, W; Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Lab, Hangzhou, Peoples R China. Pan, Jiacheng; Chen, Wei; Zhao, Xiaodong; Zhou, Shuyue; Zhu, Minfeng; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Fu, Siwei; Wu, Yingcai, Zhejiang Lab, Hangzhou, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Jian, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA;China", "abstract": "We design and evaluate a novel layout fine-tuning technique for node-link diagrams that facilitates exemplar-based adjustment of a group of substructures in batching mode. The key idea is to transfer user modifications on a local substructure to other substructures in the entire graph that are topologically similar to the exemplar. We first precompute a canonical representation for each substructure with node embedding techniques and then use it for on-the-fly substructure retrieval. We design and develop a light-weight interactive system to enable intuitive adjustment, modification transfer, and visual graph exploration. We also report some results of quantitative comparisons, three case studies, and a within-participant user study.", "keywords": "Node-link diagram,graph layout,graph visualization,user interactions", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030393", "refList": ["10.1109/tvcg.2007.70582", "10.1371/journal.pone.0136497", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/3018661.3018731", "10.1109/34.491619", "10.1145/2939672.2939754", "10.1145/3269206.3271788", "10.1016/j.comnet.2011.08.019", "10.1007/s11263-011-0442-2", "10.1016/s0020-0190(98)00108-2", "10.1109/tvcg.2018.2865151", "10.1177/1473871612455749", "10.1177/1473871616666394", "10.1109/tvcg.2015.2467035", "10.1186/1471-2105-10-375", "10.1145/3097983.3098061", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1109/tvcg.2015.2467451", "10.1016/j.swevo.2015.10.002", "10.1016/0020-0190(89)90102-6", "10.1109/infvis.2003.1249009", "10.1109/tvcg.2017.2745919", "10.1111/cgf.13187", "10.1111/cgf.13440", "10.1109/tvcg.2012.245", "10.1109/tvcg.2017.2743858", "10.1177/1473871618821740", "10.1186/s12859-015-0585-1", "10.1002/nav.3800020109", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1006/s1045-926x(02)00016-2", "10.1109/cvpr.2012.6247667", "10.1023/b:jogo.0000042115.44455.f3", "10.1109/pacificvis.2017.8031607", "10.1002/nav.3800030404", "10.1109/infvis.2004.1", "10.1109/cvpr.2008.4587500", "10.1109/pacificvis.2011.5742389", "10.1140/epjb/e2011-10979-2", "10.1371/journal.pone.0098679", "10.1090/s0002-9904-1920-03322-7", "10.1109/iv.2013.3", "10.1145/568522.568523", "10.1109/tvcg.2006.156", "10.1109/tvcg.2012.236", "10.1109/tvcg.2018.2865139", "10.1145/3219819.3220025", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}], "len": 19}, {"doi": "10.1109/tvcg.2018.2865149", "title": "Juniper: A Tree+Table Approach to Multivariate Graph Visualization", "year": "2018", "conferenceName": "InfoVis", "authors": "Carolina Nobre;Marc Streit;Alexander Lex", "citationCount": "7", "affiliation": "Nobre, C (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Nobre, Carolina; Lex, Alexander, Univ Utah, Salt Lake City, UT 84112 USA. Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.", "countries": "USA;Austria", "abstract": "Analyzing large, multivariate graphs is an important problem in many domains, yet such graphs are challenging to visualize. In this paper, we introduce a novel, scalable, tree-table multivariate graph visualization technique, which makes many tasks related to multivariate graph analysis easier to achieve. The core principle we follow is to selectively query for nodes or subgraphs of interest and visualize these subgraphs as a spanning tree of the graph. The tree is laid out linearly, which enables us to juxtapose the nodes with a table visualization where diverse attributes can be shown. We also use this table as an adjacency matrix, so that the resulting technique is a hybrid node-link/adjacency matrix technique. We implement this concept in Juniper and complement it with a set of interaction techniques that enable analysts to dynamically grow, restructure, and aggregate the tree, as well as change the layout or show paths between nodes. We demonstrate the utility of our tool in usage scenarios for different multivariate networks: a bipartite network of scholars, papers, and citation metrics and a multitype network of story characters, places, books, etc.", "keywords": "Multivariate graphs,networks,tree-based graph visualization,adjacency matrix,spanning trees,visualization", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865149", "refList": ["10.1109/tvcg.2007.70582", "10.1109/infvis.2000.885091", "10.1109/tvcg.2016.2615308", "10.1109/tvcg.2008.117", "10.1109/tvcg.2009.108", "10.1109/pacificvis.2013.6596127", "10.1111/j.1467-8659.2011.01898.x", "10.1109/visual.1991.175815", "10.1093/bioinformatics/btp454", "10.1093/bioinformatics/btq675", "10.1109/tvcg.2011.247", "10.1111/j.1467-8659.2012.03110.x", "10.1145/2207676.2208293", "10.1056/nejmsa066082", "10.1109/infvis.2003.1249009", "10.1038/nmeth.1436", "10.1073/pnas.95.25.14863", "10.1109/cw.2002.1180907", "10.1109/tvcg.2014.2346248", "10.1111/j.1467-8659.2009.01710.x", "10.1109/tvcg.2006.147", "10.1109/tvcg.2015.2468078", "10.1093/bioinformatics/btx324", "10.1111/cgf.12883", "10.1145/1168149.1168168", "10.1109/pacificvis.2010.5429609", "10.1093/bioinformatics/btn068", "10.1109/tvcg.2006.106", "10.2307/2685881", "10.1057/palgrave.ivs.9500092", "10.1109/tvcg.2016.2598885", "10.1111/j.1467-8659.2009.01687.x", "10.1109/tvcg.2018.2811488", "10.1145/22339.22342", "10.1111/cgf.13184", "10.1111/cgf.12642", "10.1109/biovis.2012.6378600", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934670", "title": "AirVis: Visual Analytics of Air Pollution Propagation", "year": "2019", "conferenceName": "VAST", "authors": "Zikun Deng;Di Weng;Jiahui Chen;Ren Liu;Zhibin Wang;Jie Bao 0003;Yu Zheng 0004;Yingcai Wu", "citationCount": "6", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Deng, Zikun; Weng, Di; Chen, Jiahui; Liu, Ren; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Wang, Zhibin, Zhejiang Univ, Res Ctr Air Pollut \\& Hlth, Hangzhou, Peoples R China. Bao, Jie; Zheng, Yu, JD Intelligent City Res, Beijing, Peoples R China.", "countries": "China", "abstract": "Air pollution has become a serious public health problem for many cities around the world. To find the causes of air pollution, the propagation processes of air pollutants must be studied at a large spatial scale. However, the complex and dynamic wind fields lead to highly uncertain pollutant transportation. The state-of-the-art data mining approaches cannot fully support the extensive analysis of such uncertain spatiotemporal propagation processes across multiple districts without the integration of domain knowledge. The limitation of these automated approaches motivates us to design and develop AirVis, a novel visual analytics system that assists domain experts in efficiently capturing and interpreting the uncertain propagation patterns of air pollution based on graph visualizations. Designing such a system poses three challenges: a) the extraction of propagation patterns; b) the scalability of pattern presentations; and c) the analysis of propagation processes. To address these challenges, we develop a novel pattern mining framework to model pollutant transportation and extract frequent propagation patterns efficiently from large-scale atmospheric data. Furthermore, we organize the extracted patterns hierarchically based on the minimum description length (MDL) principle and empower expert users to explore and analyze these patterns effectively on the basis of pattern topologies. We demonstrated the effectiveness of our approach through two case studies conducted with a real-world dataset and positive feedback from domain experts.", "keywords": "Air pollution propagation,pattern mining,graph visualization", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934670", "refList": ["10.1109/tvcg.2016.2598919", "10.1093/bib/bbr069", "10.1145/2487575.2488188", "10.1109/tvcg.2013.193", "10.1016/j.atmosenv.2014.12.011", "10.1109/tvcg.2013.226", "10.1109/tvcg.2018.2864503", "10.1109/tvcg.2015.2468111", "10.1109/icicta.2015.183", "10.1016/j.atmosenv.2014.05.039", "10.1111/cgf.12791", "10.1111/j.1467-8659.2009.01451.x", "10.1111/j.1467-8659.2011.01898.x", "10.5194/acp-12-5031-2012", "10.1016/j.atmosenv.2008.05.053", "10.1109/tvcg.2016.2535234", "10.1016/j.atmosres.2014.12.003", "10.1109/tvcg.2015.2467194", "10.1109/tvcg.2013.263", "10.1109/icdm.2002.1184038", "10.1145/2783258.2788573", "10.1109/tvcg.2018.2865149", "10.1109/tbdata.2017.2723899", "10.1109/tvcg.2012.311", "10.1109/vl.1996.545307", "10.1007/s12650-018-0481-7", "10.1016/j.envpol.2007.06.012", "10.3155/1047-3289.61.6.660", "10.1080/13658810701349037", "10.1145/3097983.3098090", "10.1109/tvcg.2007.70523", "10.1115/1.2128636", "10.1109/tvcg.2015.2467619", "10.3978/j.issn.2072-1439.2016.01.19", "10.1109/tkde.2005.127", "10.1017/s0269888912000331", "10.2312/conf/eg2013/stars/039-063", "10.1109/tvcg.2014.2346271", "10.1175/bams-d-14-00110.1", "10.1016/0005-1098(78)90005-5", "10.1007/s10618-006-0044-8", "10.1109/tvcg.2018.2865126", "10.2307/1912791", "10.3390/su6085322", "10.1007/s12650-018-0489-z", "10.2312/eurovisstar.20151109", "10.1109/tvcg.2011.181", "10.1126/science.298.5594.824", "10.1162/jmlr.2003.3.4-5.951", "10.1109/asonam.2014.6921638", "10.1111/j.1467-8659.2008.01213.x", "10.1038/s41598-017-18107-1", "10.1109/tvcg.2018.2865041", "10.1109/tits.2019.2901117", "10.1038/srep20668", "10.1109/tvcg.2012.265", "10.1109/tpami.2016.2608884", "10.1109/tvcg.2012.213", "10.1145/1376616.1376661", "10.1007/s00521-019-04567-1", "10.1109/tvcg.2017.2745083", "10.1126/science.243.4892.745", "10.1109/tvcg.2018.2864826", "10.1109/tnn.2003.820440", "10.1109/tvcg.2016.2598885", "10.1145/3219819.3219822", "10.1073/pnas.1502596112", "10.1016/j.envsoft.2009.01.004", "10.1002/pmic.200700095", "10.1145/2254556.2254651", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2011.202"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028958", "title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Tiankai Xie;Yuxin Ma;Hanghang Tong;My T. Thai;Ross Maciejewski", "citationCount": "0", "affiliation": "Xie, TK (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Xie, Tiankai; Ma, Yuxin; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. Tong, Hanghang, Univ Illinois, Urbana, IL USA. Thai, My T., Univ Florida, Gainesville, FL 32611 USA.", "countries": "USA", "abstract": "Graph mining plays a pivotal role across a number of disciplines, and a variety of algorithms have been developed to answer who/what type questions. For example, what items shall we recommend to a given user on an e-commerce platform? The answers to such questions are typically returned in the form of a ranked list, and graph-based ranking methods are widely used in industrial information retrieval settings. However, these ranking algorithms have a variety of sensitivities, and even small changes in rank can lead to vast reductions in product sales and page hits. As such, there is a need for tools and methods that can help model developers and analysts explore the sensitivities of graph ranking algorithms with respect to perturbations within the graph structure. In this paper, we present a visual analytics framework for explaining and exploring the sensitivity of any graph-based ranking algorithm by performing perturbation-based what-if analysis. We demonstrate our framework through three case studies inspecting the sensitivity of two classic graph-based ranking algorithms (PageRank and HITS) as applied to rankings in political news media and social networks.", "keywords": "Graph-based ranking,sensitivity analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028958", "refList": ["10.1109/wsc.2017.8247800", "10.1023/a:1022649401552", "10.1515/1559-0410.11416", "10.1109/tvcg.2016.2598919", "10.1177/1473871611416549", "10.1109/tvcg.2019.2934630", "10.1140/epjds29", "10.1109/tvcg.2019.2934670", "10.1016/j.eswa.2015.09.004", "10.1145/2702123.2702509", "10.1016/j.visinf.2018.12.001", "10.2307/3002000", "10.1109/tvcg.2019.2934399", "10.1007/s41060-016-0032-z", "10.1111/cgf.13198", "10.14778/2350229.2350254", "10.1145/2939672.2939764", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/vast.2017.8585647", "10.1007/bf01187020", "10.1109/icdm.2015.26", "10.1145/2362383.2362387", "10.1177/0049124104268644", "10.1109/vast.2011.6102442", "10.1109/infvis.2003.1249025", "10.1109/tvcg.2018.2864475", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2015.2467691", "10.1111/cgf.13210", "10.1214/aos/1176344136", "10.1109/tvcg.2015.2424872", "10.1016/j.visinf.2018.09.001", "10.1177/089443939100900106", "10.1109/tvcg.2015.2467931", "10.1162/neco.1997.9.8.1735", "10.1007/s11162-011-9241-4", "10.1111/cgf.13680", "10.1145/3065386", "10.1109/tvcg.2018.2864889", "10.1177/003804070808100402", "10.1109/icdm.2010.62", "10.1038/s41598-020-59669-x", "10.1162/153244303321897717", "10.1109/tvcg.2019.2934619", "10.1007/bf00356088", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030410", "title": "Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Wei Zeng 0002;Chengqiao Lin;Juncong Lin;Jincheng Jiang;Jiazhi Xia;Cagatay Turkay;Wei Chen", "citationCount": "0", "affiliation": "Lin, JC (Corresponding Author), Xiamen Univ, Xiamen, Peoples R China. Zeng, Wei; Jiang, Jincheng, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Lin, Chengqiao; Lin, Juncong, Xiamen Univ, Xiamen, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Turkay, Cagatay, Univ Warwick, Coventry, W Midlands, England. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China.", "countries": "China;England", "abstract": "Deep learning methods are being increasingly used for urban traffic prediction where spatiotemporal traffic data is aggregated into sequentially organized matrices that are then fed into convolution-based residual neural networks. However, the widely known modifiable areal unit problem within such aggregation processes can lead to perturbations in the network inputs. This issue can significantly destabilize the feature embeddings and the predictions - rendering deep networks much less useful for the experts. This paper approaches this challenge by leveraging unit visualization techniques that enable the investigation of many-to-many relationships between dynamically varied multi-scalar aggregations of urban traffic data and neural network predictions. Through regular exchanges with a domain expert, we design and develop a visual analytics solution that integrates 1) a Bivariate Map equipped with an advanced bivariate colormap to simultaneously depict input traffic and prediction errors across space, 2) a Moran's I Scatterplot that provides local indicators of spatial association analysis, and 3) a Multi-scale Attribution View that arranges non-linear dot plots in a tree layout to promote model analysis and comparison across scales. We evaluate our approach through a series of case studies involving a real-world dataset of Shenzhen taxi trips, and through interviews with domain experts. We observe that geographical scale variations have important impact on prediction performances, and interactive visual exploration of dynamically varying inputs and outputs benefit experts in the development of deep traffic prediction models.", "keywords": "MAUP,traffic prediction,deep learning,model diagnostic,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030410", "refList": ["10.1038/srep26377", "10.1109/mcg.2011.88", "10.1080/13658816.2015.1119279", "10.1109/tvcg.2013.226", "10.1109/pacificvis.2011.5742387", "10.1038/s41467-017-01882-w", "10.1109/tvcg.2019.2934670", "10.1111/j.1467-8659.2009.01440.x", "10.1111/cgf.13712", "10.1016/j.compenvurbsys.2008.09.006", "10.1109/pacificvis.2014.50", "10.1109/tvcg.2018.2816219", "10.1109/tvcg.2016.2535234", "10.1109/tvcg.2014.2346893", "10.3390/ijgi8080344", "10.1109/tvcg.2013.246", "10.1007/s10940-005-9003-6", "10.1016/j.compenvurbsys.2008.05.001", "10.1007/s10661-019-7831-3", "10.1111/j.1538-4632.2007.00699.x", "10.1016/j.aap.2016.08.015", "10.1080/13658816.2018.1541177", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2011.181", "10.1137/090759069", "10.1109/pacificvis.2011.5742390", "10.1214/10-aos799", "10.1109/tits.2017.2683539", "10.1109/tits.2015.2436897", "10.3390/ijerph16071150", "10.1109/tvcg.2009.145", "10.1109/tvcg.2012.265", "10.1080/10106049.2017.1404140", "10.3390/ijgi8020063", "10.3390/info6020134", "10.1080/13658816.2014.955027", "10.1109/tits.2016.2639320", "10.2307/143141", "10.1109/tvcg.2016.2598432"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030370", "title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups", "year": "2020", "conferenceName": "VAST", "authors": "Yating Lin;Kamkwai Wong;Yong Wang;Rong Zhang;Bo Dong;Huamin Qu;Qinghua Zheng", "citationCount": "0", "affiliation": "Lin, YT (Corresponding Author), Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian, Shaanxi, Peoples R China. Lin, Yating; Zheng, Qinghua, Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian, Shaanxi, Peoples R China. Wong, Kamkwai; Wang, Yong; Zhang, Rong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Dong, Bo, Xi An Jiao Tong Univ, Natl Engn Lab Big Data Analyt, Xian, Shaanxi, Peoples R China.", "countries": "China", "abstract": "Tax evasion is a serious economic problem for many countries, as it can undermine the government's tax system and lead to an unfair business competition environment. Recent research has applied data analytics techniques to analyze and detect tax evasion behaviors of individual taxpayers. However, they have failed to support the analysis and exploration of the related party transaction tax evasion (RPTTE) behaviors (e.g., transfer pricing), where a group of taxpayers is involved. In this paper, we present TaxThemis, an interactive visual analytics system to help tax officers mine and explore suspicious tax evasion groups through analyzing heterogeneous tax-related data. A taxpayer network is constructed and fused with the respective trade network to detect suspicious RPTTE groups. Rich visualizations are designed to facilitate the exploration and investigation of suspicious transactions between related taxpayers with profit and topological data analysis. Specifically, we propose a calendar heatmap with a carefully-designed encoding scheme to intuitively show the evidence of transferring revenue through related party transactions. We demonstrate the usefulness and effectiveness of TaxThemis through two case studies on real-world tax-related data and interviews with domain experts.", "keywords": "Visual Analytics,Tax Network,Tax Evasion Detection,Anomaly detection,Multidimensional data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030370", "refList": ["10.1111/cgf.12886", "10.2307/2277827", "10.1109/tvcg.2010.44", "10.1109/tits.2014.2315794", "10.1109/tvcg.2019.2934670", "10.1038/s41467-019-08987-4", "10.1111/cgf.12920", "10.1109/vast.2017.8585721", "10.1080/15230406.2015.1093431", "10.1109/tvcg.2018.2843369", "10.1038/srep01001", "10.1109/tvcg.2017.2744018", "10.1109/tvcg.2017.2744159", "10.1068/b130199p", "10.1109/tvcg.2009.143", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2019.2892483", "10.1109/pacificvis.2017.8031583", "10.1109/pacificvis48177.2020.2785", "10.2307/2686111", "10.1109/tvcg.2015.2467199", "10.1111/cgf.12114", "10.1109/tvcg.2018.2865126", "10.1111/j.1538-4632.1996.tb00936.x", "10.1109/tvcg.2019.2934619", "10.2307/2332142", "10.1007/978-3-319-10590-1\\_53", "10.1109/cvpr.2016.485", "10.1109/cvpr.2017.17", "10.2307/2986645", "10.1109/tvcg.2014.2346321", "10.1017/s0140525x16001837", "10.1109/tvcg.2016.2598541", "10.1371/journal.pone.0207377", "10.1109/tvcg.2014.2346265", "10.1007/s4095-020-0191-7", "10.3141/1644-14", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2016.90", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2785807", "10.1109/tvcg.2017.2744358", "10.1111/j.1538-4632.1995.tb00338.x", "10.1080/03081068808717359", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030469", "title": "Topology Density Map for Urban Data Visualization and Analysis", "year": "2020", "conferenceName": "VAST", "authors": "Zezheng Feng;Haotian Li;Wei Zeng 0004;Shuang-Hua Yang;Huamin Qu", "citationCount": "0", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Feng, Zezheng; Li, Haotian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yang, Shuang-Hua, Southern Univ Sci \\& Technol, Shenzhen, Peoples R China.", "countries": "China", "abstract": "Density map is an effective visualization technique for depicting the scalar field distribution in 2D space. Conventional methods for constructing density maps are mainly based on Euclidean distance, limiting their applicability in urban analysis that shall consider road network and urban traffic. In this work, we propose a new method named Topology Density Map, targeting for accurate and intuitive density maps in the context of urban environment. Based on the various constraints of road connections and traffic conditions, the method first constructs a directed acyclic graph (DAG) that propagates nonlinear scalar fields along 1D road networks. Next, the method extends the scalar fields to a 2D space by identifying key intersecting points in the DAG and calculating the scalar fields for every point, yielding a weighted Voronoi diagram like effect of space division. Two case studies demonstrate that the Topology Density Map supplies accurate information to users and provides an intuitive visualization for decision making. An interview with domain experts demonstrates the feasibility, usability, and effectiveness of our method.", "keywords": "Density map,network topology,urban data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030469", "refList": ["10.1109/vast.2009.5332584", "10.1109/tvcg.2013.193", "10.1080/03081060.2013.844903", "10.1109/tvcg.2018.2864503", "10.1145/2702123.2702419", "10.1109/tvcg.2019.2934670", "10.1109/tits.2015.2496783", "10.1177/1473871615581216", "10.3141/1617-02", "10.1145/2024156.2024169", "10.1111/cgf.13712", "10.1016/j.ejor.2007.02.005", "10.1109/tvcg.2014.2346893", "10.1007/11871842\\_29", "10.1109/vast.2010.5652478", "10.1016/j.visinf.2019.10.002", "10.1109/tvcg.2016.2616404", "10.1109/vl.1996.545307", "10.1145/2629592", "10.1155/2018/2696037", "10.1061/(asce)0733-947x(1998)124:4(368", "10.3141/1899-21", "10.1023/a:1026123329433", "10.1109/mcg.2010.79", "10.1057/palgrave.ivs.9500174", "10.1109/tcyb.2019.2963681", "10.1109/tvcg.2015.2467554", "10.1111/cgf.12114", "10.1145/2814575", "10.1016/j.jcps.2014.08.002", "10.1109/2945.981847", "10.1080/03052150210909", "10.1109/tciaig.2012.2186810", "10.1109/tits.2017.2683539", "10.1109/iv.2004.1320137", "10.1016/0377-2217(80)90126-5", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2015.2467196", "10.1145/3097983.3098056", "10.1007/s11432-018-9801-4", "10.1109/vast.2014.7042490", "10.1061/(asce)0733-947x(2006)132:2(122", "10.1016/j.tra.2008.03.011", "10.1109/tits.2014.2298892", "10.1016/j.trb.2005.12.003", "10.1007/bf01840357", "10.1109/vast.2011.6102454", "10.1109/tvcg.2013.145", "10.1007/bf02289588", "10.1109/pacificvis.2014.56", "10.1109/mcg.2018.053491730", "10.1109/tvcg.2009.111", "10.1057/palgrave.ivs.9500184", "10.1109/tvcg.2013.173", "10.1109/tvcg.2016.2598432", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/vast47406.2019.8986909", "title": "Origraph: Interactive Network Wrangling", "year": "2019", "conferenceName": "VAST", "authors": "Alex Bigelow;Carolina Nobre;Miriah D. Meyer;Alexander Lex", "citationCount": "2", "affiliation": "Bigelow, A (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bigelow, Alex; Nobre, Carolina; Meyer, Miriah; Lex, Alexander, Univ Utah, Salt Lake City, UT 84112 USA.", "countries": "USA", "abstract": "Networks are a natural way of thinking about many datasets. The data on which a network is based, however, is rarely collected in a form that suits the analysis process, making it necessary to create and reshape networks. Data wrangling is widely acknowledged to be a critical part of the data analysis pipeline, yet interactive network wrangling has received little attention in the visualization research community. In this paper, we discuss a set of operations that are important for wrangling network datasets and introduce a visual data wrangling tool, Origraph, that enables analysts to apply these operations to their datasets. Key operations include creating a network from source data such as tables, reshaping a network by introducing new node or edge classes, filtering nodes or edges, and deriving new node or edge attributes. Our tool, Origraph, enables analysts to execute these operations with little to no programming, and to immediately visualize the results. Origraph provides views to investigate the network model, a sample of the network, and node and edge attributes. In addition, we introduce interfaces designed to aid analysts in specifying arguments for sensible network wrangling operations. We demonstrate the usefulness of Origraph in two Use Cases: first, we investigate gender bias in the film industry, and then the influence of money on the political support for the war in Yemen.", "keywords": "Graph visualization,Data abstraction,Data wrangling,Human-centered computing [Information visualization],[Human-centered computing]: Visualization systems and tools,Information systems [Graph-based database models]", "link": "http://dx.doi.org/10.1109/VAST47406.2019.8986909", "refList": ["10.1101/gr.1239303", "10.1145/1054972.1055032", "10.1016/b978-0-12-382229-1.00002-3", "10.1109/tvcg.2017.2744843", "10.1109/tvcg.2013.154", "10.1007/978-1-4614-7163-9315-1", "10.1073/pnas.1607151113", "10.18637/jss.v040.i01", "10.1145/1124772.1124891", "10.1177/1473871611415994", "10.1109/tvcg.2018.2865149", "10.1145/2598153.2598175", "10.1177/1473871613488591", "10.1109/tvcg.2018.2859973", "10.1186/1471-2105-14-s19-s3", "10.1056/nejmsa066082", "10.1007/978-3-642-36763-2\\_48", "10.1016/j.socscimed.2016.01.049", "10.1111/j.1467-8659.2008.01231.x", "10.1002/cne.24084", "10.2138/am-2017-6104ccbyncnd", "10.1109/tvcg.2014.2346248", "10.1111/j.1467-8659.2009.01710.x", "10.1007/978-3-642-03658-3\\_47", "10.1007/978-3-319-06793-3\\_5", "10.3390/genes9110519", "10.1145/3290605.3300356", "10.1111/cgf.12883", "10.1177/1473871612462152", "10.1016/j.jelectrocard.2010.09.003", "10.1111/cgf.13610", "10.1109/vast.2010.5652520", "10.1111/evo.13318", "10.1109/tvcg.2018.2811488", "10.1109/tvcg.2009.111", "10.1111/cgf.13184", "10.1109/tvcg.2009.116", "10.1109/biovis.2012.6378600"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030462", "title": "Table Scraps: An Actionable Framework for Multi-Table Data Wrangling From An Artifact Study of Computational Journalism", "year": "2020", "conferenceName": "InfoVis", "authors": "Stephen Kasica;Charles Berret;Tamara Munzner", "citationCount": "0", "affiliation": "Kasica, S (Corresponding Author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada. Kasica, Stephen; Munzner, Tamara, Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada. Berret, Charles, Univ British Columbia, Sch Journalism Writing \\& Media, Vancouver, BC, Canada.", "countries": "Canada", "abstract": "For the many journalists who use data and computation to report the news, data wrangling is an integral part of their work. Despite an abundance of literature on data wrangling in the context of enterprise data analysis, little is known about the specific operations, processes, and pain points journalists encounter while performing this tedious, time-consuming task. To better understand the needs of this user group, we conduct a technical observation study of 50 public repositories of data and analysis code authored by 33 professional journalists at 26 news organizations. We develop two detailed and cross-cutting taxonomies of data wrangling in computational journalism, for actions and for processes. We observe the extensive use of multiple tables, a notable gap in previous wrangling analyses. We develop a concise, actionable framework for general multi-table data wrangling that includes wrangling operations documented in our taxonomy that are without clear parallels in other work. This framework, the first to incorporate tables as first-class objects, will support future interactive wrangling tools for both computational journalism and general-purpose use. We assess the generative and descriptive power of our framework through discussion of its relationship to our set of taxonomies.", "keywords": "Computational journalism,Data journalism,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030462", "refList": ["10.1145/1378773.1378792", "10.1109/tvcg.2012.219", "10.1109/vast47406.2019.8986909", "10.1145/1084805.1084812", "10.1007/s00778-008-0098-x", "10.1016/j.websem.2008.09.005", "10.18637/jss.v040.i01", "10.1145/989863.989865", "10.1109/tvcg.2015.2467551", "10.5281/zenodo.3509134", "10.1109/tvcg.2019.2934539", "10.1109/tvcg.2019.2934593", "10.1109/tse.2018.2796554", "10.17349/jmc117309", "10.1109/2945.981851", "10.1109/vast.2011.6102440", "10.1177/1473871611415994", "10.1145/2001269.2001288"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934285", "title": "Visualizing a Moving Target: A Design Study on Task Parallel Programs in the Presence of Evolving Data and Concerns", "year": "2019", "conferenceName": "InfoVis", "authors": "Katy Williams;Alex Bigelow;Katherine E. Isaacs", "citationCount": "5", "affiliation": "Williams, K (Corresponding Author), Univ Arizona, Tucson, AZ 85721 USA. Williams, Katy; Bigelow, Alex; Isaacs, Kate, Univ Arizona, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "Common pitfalls in visualization projects include lack of data availability and the domain users' needs and focus changing too rapidly for the design process to complete. While it is often prudent to avoid such projects, we argue it can be beneficial to engage them in some cases as the visualization process can help refine data collection, solving a \u201cchicken and egg\u201d problem of having the data and tools to analyze it. We found this to be the case in the domain of task parallel computing where such data and tooling is an open area of research. Despite these hurdles, we conducted a design study. Through a tightly-coupled iterative design process, we built Atria, a multi-view execution graph visualization to support performance analysis. Atria simplifies the initial representation of the execution graph by aggregating nodes as related to their line of code. We deployed Atria on multiple platforms, some requiring design alteration. We describe how we adapted the design study methodology to the \u201cmoving target\u201d of both the data and the domain experts' concerns and how this movement kept both the visualization and programming project healthy. We reflect on our process and discuss what factors allow the project to be successful in the presence of changing data and user needs.", "keywords": "design studies,software visualization,parallel computing,graph visualization", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934285", "refList": ["10.1007/978-3-642-31476-6\\_7", "10.1109/tvcg.2018.2859974", "10.1145/3337821.3337915", "10.1109/tse.1981.234519", "10.1109/32.221135", "10.1111/cgf.13433", "10.1109/tvcg.2011.185", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2018.2865149", "10.1109/tvcg.2014.2346323", "10.3233/978-1-61499-649-1-87", "10.1145/2993901.2993911", "10.1109/mcg.2011.103", "10.1002/1097-024x(200009)30:11", "10.1177/1094342006064482", "10.1057/palgrave.ivs.9500116", "10.1177/1473871615621602", "10.1109/tvcg.2013.124", "10.1109/infvis.2003.1249009", "10.1145/642611.642616", "10.1109/tvcg.2015.2467452", "10.1109/cw.2002.1180907", "10.1145/3125571.3125585", "10.1145/2807591.2807634", "10.1109/infvis.2002.1173148", "10.1145/2676870.2676883", "10.1145/1168149.1168168", "10.1007/978-3-030-17872-7\\_14", "10.1145/2993901.2993916", "10.1109/iotdi.2015.41", "10.1109/infvis.2004.1", "10.1080/14639220903165169", "10.1109/hpdc.2000.868632", "10.1145/882262.882291", "10.1109/mcse.2013.98", "10.1109/tvcg.2012.213", "10.1109/tvcg.2018.2811488", "10.1109/tvcg.2017.2744319", "10.1007/978-3-642-31476-67", "10.1109/mcg.2018.2874523", "10.1109/sc.2012.71"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030355", "title": "Guidelines For Pursuing and Revealing Data Abstractions", "year": "2020", "conferenceName": "InfoVis", "authors": "Alex Bigelow;Katy Williams;Katherine E. Isaacs", "citationCount": "0", "affiliation": "Bigelow, A (Corresponding Author), Univ Arizona, Tucson, AZ 85721 USA. Bigelow, Alex; Williams, Katy; Isaacs, Katherine E., Univ Arizona, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "Many data abstraction types, such as networks or set relationships, remain unfamiliar to data workers beyond the visualization research community. We conduct a survey and series of interviews about how people describe their data, either directly or indirectly. We refer to the latter as latent data abstractions. We conduct a Grounded Theory analysis that (1) interprets the extent to which latent data abstractions exist, (2) reveals the far-reaching effects that the interventionist pursuit of such abstractions can have on data workers, (3) describes why and when data workers may resist such explorations, and (4) suggests how to take advantage of opportunities and mitigate risks through transparency about visualization research perspectives and agendas. We then use the themes and codes discovered in the Grounded Theory analysis to develop guidelines for data abstraction in visualization projects. To continue the discussion, we make our dataset open along with a visual interface for further exploration.", "keywords": "Data abstraction,Grounded theory,Survey design,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030355", "refList": ["10.1080/2159676x.2016.1251701", "10.1109/infvis.2000.885092", "10.1145/2702123.2702298", "10.4135/9781848607941.n14", "10.1007/978-1-4939", "10.1109/tvcg.2014.2346331", "10.1109/tvcg.2017.2744843", "10.1177/1473871613510429", "10.1007/978-1-4939-0378-8\\_2", "10.1145/2598153.2598175", "10.1109/tvcg.2019.2934285", "10.1177/1473871613488591", "10.1145/2501105.2501106", "10.1109/tvcg.2019.2934538", "10.1109/tvcg.2019.2934539", "10.1017/s1049096510990781", "10.1145/3025453.3025837", "10.1145/3290605.3300474", "10.1145/3290605.3300356", "10.1002/nur.1025", "10.1145/2993901.2993916", "10.1145/3392826", "10.1086/269268", "10.1109/tvcg.2018.2865241", "10.1145/2998181.2998331", "10.1145/291224.291229", "10.1057/ivs.2009.13", "10.1145/2047196.2047205", "10.1109/tvcg.2012.213", "10.1145/3274405", "10.1109/tvcg.2013.145", "10.1016/0040-6031(92)85160-w", "10.1109/iv.2013.45", "10.1109/tvcg.2009.111", "10.1109/mcg.2019.2914844", "10.1109/tvcg.2009.116"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2020.3029413", "title": "A Design Space of Vision Science Methods for Visualization Research", "year": "2020", "conferenceName": "InfoVis", "authors": "Madison A. Elliott;Christine Nothelfer;Cindy Xiong;Danielle Albers Szafir", "citationCount": "0", "affiliation": "Elliott, MA (Corresponding Author), Univ British Columbia, Vancouver, BC, Canada. Elliott, Madison A., Univ British Columbia, Vancouver, BC, Canada. Nothelfer, Christine, Northwestern Univ, Evanston, IL 60208 USA. Xiong, Cindy, Univ Massachusetts, Amherst, MA 01003 USA. Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "Canada;USA", "abstract": "A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.", "keywords": "Perception,human vision,empirical research,evaluation,HCI", "link": "http://dx.doi.org/10.1109/TVCG.2020.3029413", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1177/0886109909354981", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1093/bioinformatics/btq110", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1177/1744987107081254", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1177/174498710501000305", "10.1017/s1049096513001789", "10.1109/tvcg.2012.213", "10.1093/nar/gkz239", "10.1093/sysbio/sys062", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030405", "title": "Insights From Experiments With Rigor in an EvoBio Design Study", "year": "2020", "conferenceName": "InfoVis", "authors": "Jennifer Rogers;Austin H. Patton;Luke Harmon;Alexander Lex;Miriah D. Meyer", "citationCount": "0", "affiliation": "Rogers, J (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Rogers, Jen; Lex, Alexander; Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Patton, Austin H., Washington State Univ, Pullman, WA 99164 USA. Harmon, Luke, Univ Idaho, Moscow, ID 83843 USA.", "countries": "USA", "abstract": "Design study is an established approach of conducting problem-driven visualization research. The academic visualization community has produced a large body of work for reporting on design studies, informed by a handful of theoretical frameworks, and applied to a broad range of application areas. The result is an abundance of reported insights into visualization design, with an emphasis on novel visualization techniques and systems as the primary contribution of these studies. In recent work we proposed a new, interpretivist perspective on design study and six companion criteria for rigor that highlight the opportunities for researchers to contribute knowledge that extends beyond visualization idioms and software. In this work we conducted a year-long collaboration with evolutionary biologists to develop an interactive tool for visual exploration of multivariate datasets and phylogenetic trees. During this design study we experimented with methods to support three of the rigor criteria: ABUNDANT, REFLEXIVE, and TRANSPARENT. As a result we contribute two novel visualization techniques for the analysis of multivariate phylogenetic datasets, three methodological recommendations for conducting design studies drawn from reflections over our process of experimentation, and two writing devices for reporting interpretivist design study. We offer this work as an example for implementing the rigor criteria to produce a diverse range of knowledge contributions.", "keywords": "Methodologies,Application Motivated Visualization,Guidelines,Life Sciences Visualization,Health,Medicine,Biology,Bioinformatics,Genomics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030405", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1080/17493460802276893", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1109/tvcg.2012.213", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13728", "year": "2019", "title": "The State of the Art in Visualizing Multivariate Networks", "conferenceName": "EuroVis", "authors": "Carolina Nobre;Miriah D. Meyer;Marc Streit;Alexander Lex", "citationCount": "5", "affiliation": "Nobre, C (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA.\nNobre, C.; Meyer, M.; Lex, A., Univ Utah, Salt Lake City, UT 84112 USA.\nStreit, M., Johannes Kepler Univ Linz, Linz, Austria.", "countries": "USA;Austria", "abstract": "Multivariate networks are made up of nodes and their relationships (links), but also data about those nodes and links as attributes. Most real-world networks are associated with several attributes, and many analysis tasks depend on analyzing both, relationships and attributes. Visualization of multivariate networks, however, is challenging, especially when both the topology of the network and the attributes need to be considered concurrently. In this state-of-the-art report, we analyze current practices and classify techniques along four axes: layouts, view operations, layout operations, and data operations. We also provide an analysis of tasks specific to multivariate networks and give recommendations for which technique to use in which scenario. Finally, we survey application areas and evaluation methodologies.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13728", "refList": ["10.2312/eurovisstar.20151110", "10.1111/cgf.12106", "10.1109/iv.2016.19", "10.1111/j.1467-8659.2011.02087.x", "10.1111/j.1467-8659.2008.01214.x", "10.1109/tvcg.2014.2346893", "10.1145/1124772.1124891", "10.1109/vast.2014.7042484", "10.1109/tvcg.2018.2865149", "10.1080/10447318.2010.516722", "10.1109/icsmc.2011.6084125", "10.1109/mcg.2011.103", "10.1117/12.378894", "10.1145/2207676.2208293", "10.1109/tvcg.2009.122", "10.1111/cgf.12935", "10.1016/s0020-0255(02)00191-3", "10.1038/nmeth.1436", "10.1109/tvcg.2011.187", "10.1109/32.177365", "10.1007/978-3-642-03658-3\\_47", "10.1109/tvcg.2006.147", "10.1109/pacificvis.2011.5742390", "10.1145/2470654.2466444", "10.1109/tvcg.2006.166", "10.1145/2470654.2470724", "10.1109/tvcg.2014.2346441", "10.1109/vizsec.2005.1532070", "10.1101/gr.092759.109", "10.1111/cgf.13184", "10.1109/tvcg.2018.2865940", "10.1016/j.scico.2012.05.002", "10.1109/biovis.2012.6378600", "10.1111/cgf.13213", "10.1109/noms.2006.1687547", "10.1109/tvcg.2008.141", "10.1007/978-3-540-78243-8\\_13", "10.1145/1029208.1029217", "10.1145/345513.345271", "10.1109/tvcg.2015.2467811", "10.1109/visual.1991.175815", "10.1186/1471-2105-10-375", "10.1007/978-3-319-06793-3\\_1", "10.1186/1471-2105-13-275", "10.1109/tvcg.2010.79", "10.1109/tvcg.2011.217", "10.1186/1471-2105-14-s19-s3", "10.3389/fmicb.2017.00010", "10.1109/infvis.2003.1249009", "10.1111/cgf.13187", "10.1111/j.1467-8659.2008.01231.x", "10.1109/tvcg.2011.144", "10.1145/2556288.2557010", "10.1111/j.1467-8659.2009.01710.x", "10.1109/infvis.2004.46", "10.1109/iv.2009.97", "10.1109/tvcg.2008.34", "10.1109/infvis.2003.1249011", "10.1109/iv.2016.41", "10.1109/pacificvis.2010.5429609", "10.1109/tvcg.2006.160", "10.1109/tvcg.2010.205", "10.1109/38.486685", "10.1109/tvcg.2006.106", "10.1057/palgrave.ivs.9500092", "10.1145/568522.568523", "10.1111/j.1467-8659.2008.01221.x", "10.1109/iv.2010.15", "10.1145/22339.22342", "10.1109/csmr.2009.17", "10.1109/tvcg.2017.2744898", "10.1109/tvcg.2008.117", "10.1186/1752-0509-3-82", "10.1109/tvcg.2008.61", "10.1109/pacificvis.2013.6596127", "10.1109/tvcg.2007.70529", "10.1186/1471-2105-15-198", "10.1109/tvcg.2009.128", "10.2312/eurovisshort.20151124", "10.1093/bioinformatics/btq675", "10.1109/tvcg.2009.143", "10.1109/tvcg.2014.2346752", "10.1109/tvcg.2013.124", "10.1007/s00450-007-0036-y", "10.1093/bioinformatics/17.suppl\\_1.s22", "10.2312/eurovisstar.20151109", "10.1007/978-3-319-06793-3\\_5", "10.1109/vissof.2007.4290706", "10.1109/tvcg.2009.145", "10.1109/mcas.2003.1228503", "10.1109/infvis.2005.1532128", "10.1145/989863.989941", "10.1109/infvis.1999.801860", "10.1111/j.1467-8659.2009.01687.x", "10.1109/tvcg.2013.120", "10.1109/tvcg.2007.70582", "10.1109/tvcg.2009.167", "10.1109/pacificvis.2010.5429590", "10.1109/infvis.2000.885091", "10.1109/tvcg.2012.189", "10.1109/tvcg.2009.108", "10.1109/tvcg.2013.154", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2010.159", "10.1016/s0306-4573(98)00024-7", "10.1109/tvcg.2013.223", "10.1002/sam.10071", "10.2312/vissym/eurovis07/083-090", "10.1109/infvis.2002.1173156", "10.1186/1471-2105-7-109", "10.1177/1473871612455983", "10.1073/pnas.95.25.14863", "10.1007/978-3-319-06793-3\\_2", "10.1111/cgf.12883", "10.1007/978-3-540-78243-8\\_9", "10.1057/palgrave.ivs.9500180", "10.1109/pacificvis.2012.6183556", "10.1117/12.872578", "10.1145/1168149.1168169", "10.1109/iv.2013.3", "10.1109/tvcg.2011.186", "10.2307/2685881", "10.1109/pacificvis.2015.7156354", "10.1109/tvcg.2016.2598885", "10.1109/tvcg.2018.2811488", "10.1109/infvis.2005.1532129", "10.1111/j.1467-8659.2009.01450.x", "10.1057/palgrave.ivs.9500162", "10.1109/tvcg.2009.116", "10.1109/2945.468391"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3029413", "title": "A Design Space of Vision Science Methods for Visualization Research", "year": "2020", "conferenceName": "InfoVis", "authors": "Madison A. Elliott;Christine Nothelfer;Cindy Xiong;Danielle Albers Szafir", "citationCount": "0", "affiliation": "Elliott, MA (Corresponding Author), Univ British Columbia, Vancouver, BC, Canada. Elliott, Madison A., Univ British Columbia, Vancouver, BC, Canada. Nothelfer, Christine, Northwestern Univ, Evanston, IL 60208 USA. Xiong, Cindy, Univ Massachusetts, Amherst, MA 01003 USA. Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "Canada;USA", "abstract": "A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.", "keywords": "Perception,human vision,empirical research,evaluation,HCI", "link": "http://dx.doi.org/10.1109/TVCG.2020.3029413", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1177/0886109909354981", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1093/bioinformatics/btq110", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1177/1744987107081254", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1177/174498710501000305", "10.1017/s1049096513001789", "10.1109/tvcg.2012.213", "10.1093/nar/gkz239", "10.1093/sysbio/sys062", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030405", "title": "Insights From Experiments With Rigor in an EvoBio Design Study", "year": "2020", "conferenceName": "InfoVis", "authors": "Jennifer Rogers;Austin H. Patton;Luke Harmon;Alexander Lex;Miriah D. Meyer", "citationCount": "0", "affiliation": "Rogers, J (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Rogers, Jen; Lex, Alexander; Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Patton, Austin H., Washington State Univ, Pullman, WA 99164 USA. Harmon, Luke, Univ Idaho, Moscow, ID 83843 USA.", "countries": "USA", "abstract": "Design study is an established approach of conducting problem-driven visualization research. The academic visualization community has produced a large body of work for reporting on design studies, informed by a handful of theoretical frameworks, and applied to a broad range of application areas. The result is an abundance of reported insights into visualization design, with an emphasis on novel visualization techniques and systems as the primary contribution of these studies. In recent work we proposed a new, interpretivist perspective on design study and six companion criteria for rigor that highlight the opportunities for researchers to contribute knowledge that extends beyond visualization idioms and software. In this work we conducted a year-long collaboration with evolutionary biologists to develop an interactive tool for visual exploration of multivariate datasets and phylogenetic trees. During this design study we experimented with methods to support three of the rigor criteria: ABUNDANT, REFLEXIVE, and TRANSPARENT. As a result we contribute two novel visualization techniques for the analysis of multivariate phylogenetic datasets, three methodological recommendations for conducting design studies drawn from reflections over our process of experimentation, and two writing devices for reporting interpretivist design study. We offer this work as an example for implementing the rigor criteria to produce a diverse range of knowledge contributions.", "keywords": "Methodologies,Application Motivated Visualization,Guidelines,Life Sciences Visualization,Health,Medicine,Biology,Bioinformatics,Genomics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030405", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1080/17493460802276893", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1109/tvcg.2012.213", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13963", "year": "2020", "title": "MotionGlyphs: Visual Abstraction of Spatio-Temporal Networks in Collective Animal Behavior", "conferenceName": "EuroVis", "authors": "Eren Cakmak;Hanna Sch{\\\"{a}}fer;Juri Buchm{\\\"{u}}ller;Johannes Fuchs;Tobias Schreck;A. Jordan;Daniel A. Keim", "citationCount": "0", "affiliation": "Cakmak, E (Corresponding Author), Univ Konstanz, Constance, Germany.\nCakmak, E (Corresponding Author), Ctr Adv Study Collect Behav, Constance, Germany.\nCakmak, E.; Schaefer, H.; Buchmueller, J.; Fuchs, J.; Jordan, A.; Keim, D., Univ Konstanz, Constance, Germany.\nCakmak, E.; Jordan, A.; Keim, D., Ctr Adv Study Collect Behav, Constance, Germany.\nSchreck, T., Graz Univ Technol, Graz, Austria.\nJordan, A., Max Planck Inst Anim Behav, Radolfzell am Bodensee, Germany.", "countries": "Germany;Austria", "abstract": "Domain experts for collective animal behavior analyze relationships between single animal movers and groups of animals over time and space to detect emergent group properties. A common way to interpret this type of data is to visualize it as a spatio-temporal network. Collective behavior data sets are often large, and may hence result in dense and highly connected node-link diagrams, resulting in issues of node-overlap and edge clutter. In this design study, in an iterative design process, we developed glyphs as a design for seamlessly encoding relationships and movement characteristics of a single mover or clusters of movers. Based on these glyph designs, we developed a visual exploration prototype, MotionGlyphs, that supports domain experts in interactively filtering, clustering, and animating spatio-temporal networks for collective animal behavior analysis. By means of an expert evaluation, we show how MotionGlyphs supports important tasks and analysis goals of our domain experts, and we give evidence of the usefulness for analyzing spatio-temporal networks of collective animal behavior.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13963", "refList": ["10.1109/tvcg.2007.70582", "10.1111/cgf.13213", "10.1109/pacificvis.2014.13", "10.1145/2093973.2094038", "10.1111/j.1467-8659.2009.01664.x", "10.1109/tvcg.2010.44", "10.1111/cgf.12106", "10.1111/cgf.12791", "10.1080/15230406.2014.890071", "10.1111/tgis.12100", "10.1111/j.1467-8659.2009.01451.x", "10.1179/000870409x12525737905042", "10.1111/cgf.12923", "10.1006/ijhc.2002.1017", "10.1145/1124772.1124891", "10.1109/tvcg.2011.213", "10.1109/vast.2014.7042484", "10.1007/s12650-016-0375-5", "10.1109/vlhcc.2012.6344514", "10.1073/pnas.1420068112", "10.1006/ijhc.1017", "10.1007/s00371-017-1461-y", "10.1109/infvis.2003.1249008", "10.1111/cgf.13728", "10.2312/conf/eg2013/stars/039-063", "10.1109/tvcg.2014.2346271", "10.1109/hicss.2011.339", "10.1068/p3104", "10.1145/2931002.2931012", "10.1109/asonam.2012.39", "10.1179/000870403235002042", "10.1145/2556288.2557010", "10.1145/2470654.2466443", "10.1109/tvcg.2011.209", "10.1016/j.tree.2013.06.002", "10.1111/1365-2656.12418", "10.1111/cgf.12872", "10.1145/2470654.2466444", "10.1109/tvcg.2014.2322594", "10.1109/tvcg.2008.125", "10.1109/tvcg.2014.2346426", "10.1109/tvcg.2006.166", "10.1111/cgf.12615", "10.1057/palgrave.ivs.9500170", "10.1007/3-540-36151-0", "10.1117/12.872578", "10.1016/j.tics.2008.10.002", "10.1006/jtbi.2002.3065", "10.1109/tvcg.2010.78", "10.1109/iv.2013.3", "10.1073/pnas.1001763107", "10.1109/pacificvis.2015.7156354", "10.1371/journal.pbio.1001805", "10.5220/0005303801230130", "10.1111/j.1467-8659.2009.01687.x", "10.1007/s10844-011-0159-2", "10.1007/s12650-018-00543-4", "10.1145/2669557.2669572", "10.1111/cgf.13184", "10.1186/s40462-015-0032-y", "10.1016/j.ins.2016.06.048", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13987", "year": "2020", "title": "Augmenting Node-Link Diagrams with Topographic Attribute Maps", "conferenceName": "EuroVis", "authors": "Reinhold Preiner;Johanna Schmidt;Katharina Kr{\\\"{o}}sl;Tobias Schreck;Gabriel Mistelbauer", "citationCount": "0", "affiliation": "Preiner, R (Corresponding Author), Graz Univ Technol, Inst Comp Graph \\& Knowledge Visualizat, Graz, Austria.\nPreiner, R.; Schreck, T., Graz Univ Technol, Inst Comp Graph \\& Knowledge Visualizat, Graz, Austria.\nSchmidt, J.; Kroesl, K., Virtual Real \\& Visualisierung Forsch GmbH, VRVis Zentrum, Vienna, Austria.\nKroesl, K., TU Wien, Inst Visual Comp \\& Human Ctr Technol, Vienna, Austria.\nMistelbauer, G., Otto von Guericke Univ, Dept Simulat \\& Graph, Magdeburg, Germany.", "countries": "Germany;Austria", "abstract": "We propose a novel visualization technique for graphs that are attributed with scalar data. In many scenarios, these attributes (e.g., birth date in a family network) provide ambient context information for the graph structure, whose consideration is important for different visual graph analysis tasks. Graph attributes are usually conveyed using different visual representations (e.g., color, size, shape) or by reordering the graph structure according to the attribute domain (e.g., timelines). While visual encodings allow graphs to be arranged in a readable layout, assessing contextual information such as the relative similarities of attributes across the graph is often cumbersome. In contrast, attribute-based graph reordering serves the comparison task of attributes, but typically strongly impairs the readability of the structural information given by the graph's topology. In this work, we augment force-directed node-link diagrams with a continuous ambient representation of the attribute context. This way, we provide a consistent overview of the graph's topological structure as well as its attributes, supporting a wide range of graph-related analysis tasks. We resort to an intuitive height field metaphor, illustrated by a topographic map rendering using contour lines and suitable color maps. Contour lines visually connect nodes of similar attribute values, and depict their relative arrangement within the global context. Moreover, our contextual representation supports visualizing attribute value ranges associated with graph nodes (e.g., lifespans in a family network) as trajectories routed through this height field. We discuss how user interaction with both the structural and the contextual information fosters exploratory graph analysis tasks. The effectiveness and versatility of our technique is confirmed in a user study and case studies from various application domains.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13987", "refList": ["10.1109/tvcg.2013.269", "10.1109/pacificvis.2010.5429590", "10.1073/pnas.0307654100", "10.1145/2505515.2505758", "10.1559/152304082783948286", "10.1109/pacificvis.2014.47", "10.1093/bioinformatics/btp432", "10.1111/j.1467-8659.2011.01898.x", "10.1111/cgf.12931", "10.1111/cgf.12880", "10.1109/tvcg.2014.2346422", "10.1111/j.1467-8659.2009.01706.x", "10.1109/tvcg.2016.2598795", "10.1111/cgf.12800", "10.1109/tvcg.2014.2315995", "10.1111/cgf.12656", "10.1111/cgf.13728", "10.1109/tvcg.2009.122", "10.1111/cgf.13211", "10.1109/tvcg.2007.70596", "10.1109/infvis.2002.1173152", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2003.1196007", "10.1109/infvis.2005.1532150", "10.1145/3243250.3243266", "10.1080/02693799008941549", "10.1371/journal.pone.0058779", "10.1109/infvis.1995.528686", "10.1111/cgf.12872", "10.1002/spe.4380211102", "10.1109/38.974518", "10.1145/3097983.3098130", "10.1002/aris.1440370106", "10.1145/1360612.1360691", "10.1109/mc.2016.145", "10.2307/3006914", "10.1111/j.1467-8659.2009.01683.x", "10.1145/1639714.1639784"], "wos": 1, "children": [], "len": 1}], "len": 9}], "len": 33}, {"doi": "10.1109/vast.2018.8802415", "title": "Segue: Overviewing Evolution Patterns of Egocentric Networks by Interactive Construction of Spatial Layouts", "year": "2018", "conferenceName": "VAST", "authors": "Po-Ming Law;Yanhong Wu;Rahul C. Basole", "citationCount": "1", "affiliation": "Law, PM (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA. Law, Po-Ming; Basole, Rahul C., Georgia Inst Technol, Atlanta, GA 30332 USA. Wu, Yanhong, Visa Res, Palo Alto, CA USA.", "countries": "USA", "abstract": "Getting the overall picture of how a large number of ego-networks evolve is a common yet challenging task. Existing techniques often require analysts to inspect the evolution patterns of ego-networks one after another. In this study, we explore an approach that allows analysts to interactively create spatial layouts in which each dot is a dynamic ego-network. These spatial layouts provide overviews of the evolution patterns of ego-networks, thereby revealing different global patterns such as trends, clusters and outliers in evolution patterns. To let analysts interactively construct interpretable spatial layouts, we propose a data transformation pipeline, with which analysts can adjust the spatial layouts and convert dynamic ego-networks into event sequences to aid interpretations of the spatial positions. Based on this transformation pipeline, we develop Segue, a visual analysis system that supports thorough exploration of the evolution patterns of ego-networks. Through two usage scenarios, we demonstrate how analysts can gain insights into the overall evolution patterns of a large collection of ego-networks by interactively creating different spatial layouts.", "keywords": "Human-centered computing,Visualization,Visualization techniques,Graph drawings", "link": "http://dx.doi.org/10.1109/VAST.2018.8802415", "refList": ["10.1057/palgrave.ivs.9500099", "10.1007/b98835", "10.1109/vast.2009.5332595", "10.1109/tvcg.2011.226", "10.1111/j.1467-8659.2009.01451.x", "10.1145/1514888.1514891", "10.1016/j.socnet.2008.07.001", "10.1073/pnas.021544898", "10.1109/vast.2011.6102449", "10.1109/mcg.2005.102", "10.1109/tvcg.2013.263", "10.1109/tvcg.2016.2598446", "10.1109/tvcg.2013.254", "10.1109/tvcg.2017.2744198", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2207738", "10.1145/2207676.2207741", "10.1109/tvcg.2015.2468151", "10.1056/nejmsa066082", "10.1109/tvcg.2013.198", "10.1109/pacificvis.2017.8031576", "10.1145/2556288.2557010", "10.1109/tvcg.2015.2467615", "10.1109/tvcg.2013.200", "10.1109/icvrv.2016.88", "10.1109/34.682181", "10.1109/tvcg.2015.2465151", "10.1109/tvcg.2015.2468078", "10.1145/2512938.2512949", "10.1371/journal.pone.0036250", "10.1109/time.1997.600793", "10.1007/978-1-4757-1904-8", "10.1109/tvcg.2011.188", "10.1016/j.socnet.2008.12.002", "10.1109/tvcg.2014.2383380", "10.1109/pacificvis.2011.5742388", "10.1145/2133416.2146416", "10.1145/2858036.2858488", "10.1007/s13278-013-0123-y", "10.1109/tvcg.2010.78", "10.1109/vast.2015.7347632", "10.1109/mic.2005.114", "10.1109/tvcg.2015.2467851", "10.1109/vast.2012.6400486", "10.1145/3025453.3025777"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/vast.2018.8802486", "title": "SMARTexplore: Simplifying High-Dimensional Data Analysis through a Table-Based Visual Analytics Approach", "year": "2018", "conferenceName": "VAST", "authors": "Michael Blumenschein;Michael Behrisch;Stefanie Schmid;Simon Butscher;Deborah Wahl;Karoline Villinger;Britta Renner;Harald Reiterer;Daniel A. Keim", "citationCount": "0", "affiliation": "Blumenschein, M (Corresponding Author), Univ Konstanz, Constance, Germany. Blumenschein, Michael; Schmid, Stefanie; Butscher, Simon; Wahl, Deborah R.; Villinger, Karoline; Renner, Britta; Reiterer, Harald; Keim, Daniel A., Univ Konstanz, Constance, Germany. Behrisch, Michael, Harvard Univ, Cambridge, MA 02138 USA.", "countries": "Germany;USA", "abstract": "We present SMARTEXPLORE, a novel visual analytics technique that simplifies the identification and understanding of clusters, correlations, and complex patterns in high-dimensional data. The analysis is integrated into an interactive table-based visualization that maintains a consistent and familiar representation throughout the analysis. The visualization is tightly coupled with pattern matching, subspace analysis, reordering, and layout algorithms. To increase the analyst's trust in the revealed patterns, SMARTEXPLORE automatically selects and computes statistical measures based on dimension and data properties. While existing approaches to analyzing high-dimensional data (e.g., planar projections and Parallel coordinates) have proven effective, they typically have steep learning curves for non-visualization experts. Our evaluation, based on three expert case studies, confirms that non-visualization experts successfully reveal patterns in high-dimensional data when using SMARTEXPLORE.", "keywords": "High-dimensional data,visual exploration,pattern-driven analysis,tabular visualization,subspace,aggregation", "link": "http://dx.doi.org/10.1109/VAST.2018.8802486", "refList": ["10.1177/1473871612460526", "10.1109/tvcg.2015.2489649", "10.1111/j.1467-8659.2008.01241.x", "10.1007/978-3-319-25087-8\\_29", "10.1007/b98835", "10.13140/rg.2.2.16570.90567", "10.1109/tvcg.2014.2346260", "10.1109/vast.2009.5332628", "10.2307/2528823", "10.1109/tvcg.2010.184", "10.1145/1133265.1133318", "10.1057/palgrave.ivs.9500072", "10.1111/j.1467-8659.2012.03110.x", "10.1145/1007730.1007731", "10.1057/palgrave.ivs.9500086", "10.1111/cgf.12935", "10.1109/tvcg.2014.2346279", "10.1007/bf01898350", "10.1109/tvcg.2013.173", "10.1109/tvcg.2017.2672987", "10.1109/tvcg.2014.2346248", "10.1109/infvis.2004.46", "10.1109/vast.2010.5652433", "10.1109/vast.2009.5332611", "10.1109/tvcg.2015.2467553", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2013.150", "10.1109/tvcg.2016.2640960", "10.1111/cgf.12630", "10.1007/978-1-4757-1904-8", "10.1109/tvcg.2011.188", "10.1109/tvcg.2010.138", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2017.2743978", "10.1111/cgf.12879", "10.1109/iv.2008.33", "10.1111/cgf.13446", "10.1007/s00371-018-1483-0", "10.1109/infvis.1998.729559", "10.1145/2669557.2669572", "10.1111/j.1467-8659.2008.01239.x"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934433", "title": "An Incremental Dimensionality Reduction Method for Visualizing Streaming Multidimensional Data", "year": "2019", "conferenceName": "InfoVis", "authors": "Takanori Fujiwara;Jia-Kai Chou;Shilpika;Panpan Xu;Ren Liu;Kwan-Liu Ma", "citationCount": "3", "affiliation": "Fujiwara, T (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Fujiwara, Takanori; Chou, Jia-Kai; Shilpika; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA. Xu, Panpan; Ren, Liu, Bosch Res North Amer, Palo Alto, CA USA.", "countries": "USA", "abstract": "Dimensionality reduction (DR) methods are commonly used for analyzing and visualizing multidimensional data. However, when data is a live streaming feed, conventional DR methods cannot be directly used because of their computational complexity and inability to preserve the projected data positions at previous time points. In addition, the problem becomes even more challenging when the dynamic data records have a varying number of dimensions as often found in real-world applications. This paper presents an incremental DR solution. We enhance an existing incremental PCA method in several ways to ensure its usability for visualizing streaming multidimensional data. First, we use geometric transformation and animation methods to help preserve a viewer's mental map when visualizing the incremental results. Second, to handle data dimension variants, we use an optimization method to estimate the projected data positions, and also convey the resulting uncertainty in the visualization. We demonstrate the effectiveness of our design with two case studies using real-world datasets.", "keywords": "Dimensionality reduction,principal component analysis,streaming data,uncertainty,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934433", "refList": ["10.1109/tvcg.2015.2509990", "10.1109/tvcg.2014.2346578", "10.1007/978-3-642-11805-0.38", "10.1109/tpami.2006.56", "10.1109/tvcg.2015.2467591", "10.1109/tvcg.2016.2570755", "10.1109/tvcg.2016.2598838", "10.1007/s10994-009-5107-9", "10.1007/b98835", "10.1007/s00371-013-0892-3", "10.1080/00949657508810123", "10.1109/tvcg.2017.2744419", "10.1109/tvcg.2015.2392771", "10.1016/0022-247x(85)90131-3", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/tvcg.2013.186", "10.1109/tvcg.2013.254", "10.1109/iv.2013.96", "10.1111/j.1467-8659.2009.01475.x", "10.1109/bigdata.2013.6691713", "10.7155/jgaa.00302", "10.1007/s11263-007-0075-7", "10.1109/infvis.2003.1249004", "10.1109/iv.2004.1320173", "10.1109/infvis.2004.60", "10.1007/bf02291266", "10.2307/2394164", "10.2307/2286407", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2015.2467553", "10.1109/tvcg.2016.2598470", "10.1007/s11258-014-0406-z", "10.1109/tvcg.2016.2640960", "10.1109/tpami.2003.1217609", "10.1007/978-1-4757-1904-8", "10.1109/tvcg.2016.2598664", "10.1111/cgf.13264", "10.1109/tvcg.2017.2744318", "10.1016/j.eswa.2010.08.067", "10.1057/palgrave.ivs.9500040", "10.1007/978-4-431-68057-4\\_3", "10.1109/tst.2013.6509102", "10.1111/j.1469-1809.1936.tb02137.x", "10.1111/cgf.12396", "10.1109/tvcg.2014.2346574", "10.1016/j.patcog.2005.04.006", "10.1109/83.855432", "10.1109/tvcg.2015.2467851", "10.1002/cem.1122"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028889", "title": "A Visual Analytics Framework for Reviewing Multivariate Time-Series Data with Dimensionality Reduction", "year": "2020", "conferenceName": "VAST", "authors": "Takanori Fujiwara;Shilpika;Naohisa Sakamoto;Jorji Nonaka;Keiji Yamamoto;Kwan-Liu Ma", "citationCount": "0", "affiliation": "Fujiwara, T (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Fujiwara, Takanori; Shilpika; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA. Sakamoto, Naohisa, Kobe Univ, Kobe, Hyogo, Japan. Nonaka, Jorji; Yamamoto, Keiji, RIKEN R CCS, Kobe, Hyogo, Japan.", "countries": "Japan;USA", "abstract": "Data-driven problem solving in many real-world applications involves analysis of time-dependent multivariate data, for which dimensionality reduction (DR) methods are often used to uncover the intrinsic structure and features of the data. However, DR is usually applied to a subset of data that is either single-time-point multivariate or univariate time-series, resulting in the need to manually examine and correlate the DR results out of different data subsets. When the number of dimensions is large either in terms of the number of time points or attributes, this manual task becomes too tedious and infeasible. In this paper, we present MulTiDR, a new DR framework that enables processing of time-dependent multivariate data as a whole to provide a comprehensive overview of the data. With the framework, we employ DR in two steps. When treating the instances, time points, and attributes of the data as a 3D array, the first DR step reduces the three axes of the array to two, and the second DR step visualizes the data in a lower-dimensional space. In addition, by coupling with a contrastive learning method and interactive visualizations, our framework enhances analysts' ability to interpret DR results. We demonstrate the effectiveness of our framework with four case studies using real-world datasets.", "keywords": "Multivariate time-series,tensor,data cube,dimensionality reduction,interpretability,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028889", "refList": ["10.1016/j.neucom.2008.12.017", "10.1109/tvcg.2015.2467591", "10.1007/bf02289464", "10.1109/tvcg.2015.2468111", "10.1177/1350650119867242", "10.1109/pacificvis48177.2020.9280", "10.1109/tvcg.2017.2744419", "10.1109/tvcg.2011.185", "10.1109/bigdata.2015.7363807", "10.1145/2669557.2669559", "10.1016/j.patcog.2011.01.004", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1186/1475-925x-14-s2-s6", "10.1007/s12650-018-0530-2", "10.1109/tkde.2018.2878247", "10.1016/j.visinf.2018.04.010", "10.1002/1099-128x(200005/06)14:3", "10.1371/journal.pone.0107878", "10.1109/pacificvis.2017.8031601", "10.1016/j.jbi.2019.103291", "10.1109/daac49578.2019.00008", "10.1109/allerton.2019.8919886", "10.1007/978-3-319-13105-4\\_14", "10.4258/hir.2016.22.3.156", "10.1109/tvcg.2015.2467553", "10.1145/2245276.2245469", "10.1007/bf02294485", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2016.2598470", "10.1137/07070111x", "10.1109/tvcg.2019.2934433", "10.1109/tvcg.2016.2640960", "10.1007/bf02310791", "10.1109/tvcg.2016.2534558", "10.1109/access.2016.2529723", "10.1109/tvcg.2016.2598664", "10.2312/eurovisshort.20161164", "10.1109/tvcg.2018.2865018", "10.1111/cgf.12804", "10.1109/tvcg.2018.2846735", "10.1016/j.comnet.2017.06.013", "10.1146/annurev-statistics-041715-033624", "10.1109/pacificvis.2018.00026", "10.1109/tvcg.2015.2467851"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934300", "title": "GUIRO: User-Guided Matrix Reordering", "year": "2019", "conferenceName": "VAST", "authors": "Michael Behrisch;Tobias Schreck;Hanspeter Pfister", "citationCount": "1", "affiliation": "Behrisch, M (Corresponding Author), Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Behrisch, Michael; Pfister, Hanspeter, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Schreck, Tobias, Graz Univ Technol, Graz, Austria.", "countries": "USA;Austria", "abstract": "Matrix representations are one of the main established and empirically proven to be effective visualization techniques for relational (or network) data. However, matrices\u2014similar to node-link diagrams\u2014are most effective if their layout reveals the underlying data topology. Given the many developed algorithms, a practical problem arises: \u201cWhich matrix reordering algorithm should I choose for my dataset at hand?\u201d To make matters worse, different reordering algorithms applied to the same dataset may let significantly different visual matrix patterns emerge. This leads to the question of trustworthiness and explainability of these fully automated, often heuristic, black-box processes. We present GUIRO, a Visual Analytics system that helps novices, network analysts, and algorithm designers to open the black-box. Users can investigate the usefulness and expressiveness of 70 accessible matrix reordering algorithms. For network analysts, we introduce a novel model space representation and two interaction techniques for a user-guided reordering of rows or columns, and especially groups thereof (submatrix reordering). These novel techniques contribute to the understanding of the global and local dataset topology. We support algorithm designers by giving them access to 16 reordering quality metrics and visual exploration means for comparing reordering implementations on a row/column permutation level. We evaluated GUIRO in a guided explorative user study with 12 subjects, a case study demonstrating its usefulness in a real-world scenario, and through an expert study gathering feedback on our design decisions. We found that our proposed methods help even inexperienced users to understand matrix patterns and allow a user-guided steering of reordering algorithms. GUIRO helps to increase the transparency of matrix reordering algorithms, thus helping a broad range of users to get a better insight into the complex reordering process, in turn supporting data and reordering algorithm insights.", "keywords": "Visual Analytics,matrix,black-box algorithms,seriation,ordering,sorting,steerable algorithm,interaction,2D projection", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934300", "refList": ["10.1109/tvcg.2007.70582", "10.2307/276978", "10.1109/tvcg.2008.61", "10.3390/su10040973", "10.1101/121889", "10.1145/1345448.1345453", "10.1186/s12879-014-0695-9", "10.1186/1471-2105-9-155", "10.1145/1124772.1124891", "10.1109/tvcg.2010.159", "10.1111/j.2044-8317.1974.tb00534.x", "10.1109/tpami.2015.2470671", "10.1198/000313005x22770", "10.1109/tvcg.2012.219", "10.1002/sam.10071", "10.1016/j.ejor.2016.08.066", "10.1007/s00265-003-0651-y", "10.1109/biovis.2013.6664342", "10.3115/v1/p14-1062", "10.3389/fpsyg.2017.01349", "10.1109/tpami.2004.1265866", "10.1057/palgrave.ivs.9500086", "10.1007/s004260000031", "10.1111/cgf.12935", "10.1145/800195.805928", "10.1109/tvcg.2014.2346279", "10.2312/eurovisstar.20141174", "10.1109/tvcg.2012.256", "10.1093/bioinformatics/17.suppl\\_1.s22", "10.1109/infvis.2004.46", "10.1007/978-3-319-06793-3\\_5", "10.1109/tvcg.2006.147", "10.1109/tvcg.2015.2468078", "10.1093/bioinformatics/bti141", "10.1145/1168149.1168168", "10.1109/tvcg.2017.2745978", "10.1177/1473871613513228", "10.1109/mcg.2014.62", "10.1109/tvcg.2006.166", "10.1145/2470654.2470724", "10.1177/154193120605000909", "10.1109/sibgrapi.2007.21", "10.1145/3065386", "10.1109/tvcg.2006.160", "10.1287/opre.20.5.993", "10.1111/cgf.13446", "10.1057/palgrave.ivs.9500092", "10.1145/568522.568523", "10.1109/mcg.2013.66", "10.1109/tvcg.2018.2865940", "10.1109/tvcg.2012.208"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2020.3028889", "title": "A Visual Analytics Framework for Reviewing Multivariate Time-Series Data with Dimensionality Reduction", "year": "2020", "conferenceName": "VAST", "authors": "Takanori Fujiwara;Shilpika;Naohisa Sakamoto;Jorji Nonaka;Keiji Yamamoto;Kwan-Liu Ma", "citationCount": "0", "affiliation": "Fujiwara, T (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Fujiwara, Takanori; Shilpika; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA. Sakamoto, Naohisa, Kobe Univ, Kobe, Hyogo, Japan. Nonaka, Jorji; Yamamoto, Keiji, RIKEN R CCS, Kobe, Hyogo, Japan.", "countries": "Japan;USA", "abstract": "Data-driven problem solving in many real-world applications involves analysis of time-dependent multivariate data, for which dimensionality reduction (DR) methods are often used to uncover the intrinsic structure and features of the data. However, DR is usually applied to a subset of data that is either single-time-point multivariate or univariate time-series, resulting in the need to manually examine and correlate the DR results out of different data subsets. When the number of dimensions is large either in terms of the number of time points or attributes, this manual task becomes too tedious and infeasible. In this paper, we present MulTiDR, a new DR framework that enables processing of time-dependent multivariate data as a whole to provide a comprehensive overview of the data. With the framework, we employ DR in two steps. When treating the instances, time points, and attributes of the data as a 3D array, the first DR step reduces the three axes of the array to two, and the second DR step visualizes the data in a lower-dimensional space. In addition, by coupling with a contrastive learning method and interactive visualizations, our framework enhances analysts' ability to interpret DR results. We demonstrate the effectiveness of our framework with four case studies using real-world datasets.", "keywords": "Multivariate time-series,tensor,data cube,dimensionality reduction,interpretability,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028889", "refList": ["10.1016/j.neucom.2008.12.017", "10.1109/tvcg.2015.2467591", "10.1007/bf02289464", "10.1109/tvcg.2015.2468111", "10.1177/1350650119867242", "10.1109/pacificvis48177.2020.9280", "10.1109/tvcg.2017.2744419", "10.1109/tvcg.2011.185", "10.1109/bigdata.2015.7363807", "10.1145/2669557.2669559", "10.1016/j.patcog.2011.01.004", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1186/1475-925x-14-s2-s6", "10.1007/s12650-018-0530-2", "10.1109/tkde.2018.2878247", "10.1016/j.visinf.2018.04.010", "10.1002/1099-128x(200005/06)14:3", "10.1371/journal.pone.0107878", "10.1109/pacificvis.2017.8031601", "10.1016/j.jbi.2019.103291", "10.1109/daac49578.2019.00008", "10.1109/allerton.2019.8919886", "10.1007/978-3-319-13105-4\\_14", "10.4258/hir.2016.22.3.156", "10.1109/tvcg.2015.2467553", "10.1145/2245276.2245469", "10.1007/bf02294485", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2016.2598470", "10.1137/07070111x", "10.1109/tvcg.2019.2934433", "10.1109/tvcg.2016.2640960", "10.1007/bf02310791", "10.1109/tvcg.2016.2534558", "10.1109/access.2016.2529723", "10.1109/tvcg.2016.2598664", "10.2312/eurovisshort.20161164", "10.1109/tvcg.2018.2865018", "10.1111/cgf.12804", "10.1109/tvcg.2018.2846735", "10.1016/j.comnet.2017.06.013", "10.1146/annurev-statistics-041715-033624", "10.1109/pacificvis.2018.00026", "10.1109/tvcg.2015.2467851"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030398", "title": "Multiscale Snapshots: Visual Analysis of Temporal Summaries in Dynamic Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Eren Cakmak;Udo Schlegel;Dominik J\u00e4ckle;Daniel A. Keim;Tobias Schreck", "citationCount": "0", "affiliation": "Cakmak, E (Corresponding Author), Univ Konstanz, Constance, Germany. Cakmak, Eren; Schlegel, Udo; Keim, Daniel, Univ Konstanz, Constance, Germany. Schreck, Tobias, Graz Univ Technol, Graz, Austria.", "countries": "Germany;Austria", "abstract": "The overview-driven visual analysis of large-scale dynamic graphs poses a major challenge. We propose Multiscale Snapshots, a visual analytics approach to analyze temporal summaries of dynamic graphs at multiple temporal scales. First, we recursively generate temporal summaries to abstract overlapping sequences of graphs into compact snapshots. Second, we apply graph embeddings to the snapshots to learn low-dimensional representations of each sequence of graphs to speed up specific analytical tasks (e.g., similarity search). Third, we visualize the evolving data from a coarse to fine-granular snapshots to semi-automatically analyze temporal states, trends, and outliers. The approach enables us to discover similar temporal summaries (e.g., reoccurring states), reduces the temporal data to speed up automatic analysis, and to explore both structural and temporal properties of a dynamic graph. We demonstrate the usefulness of our approach by a quantitative evaluation and the application to a real-world dataset.", "keywords": "Dynamic Graph,Dynamic Network,Unsupervised Graph Learning,Graph Embedding,Multiscale Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030398", "refList": ["10.1007/s00442-002-1137-8", "10.1111/cgf.13668", "10.1086/282697", "10.1109/tvcg.2002.1044518", "10.1890/0012-9658(2000)081{[}0008:ppbatp]2.0.co", "2", "10.1101/2020.04.08.032524", "10.1111/j.1461-0248.2009.01391.x", "10.1016/0040-5809(80)90016-7", "10.1109/tvcg.2009.181", "10.1145/2669557.2669559", "10.1109/vl.1996.545307", "10.1109/tvcg.2013.254", "10.1109/tvcg.2019.2934251", "10.1073/pnas.93.6.2608", "10.2312/cgvc", "10.1080/14786440109462720", "10.1111/1440-1703.12057", "10.2307/1940591", "10.1037/h0071325", "10.1073/pnas.1215506110", "10.1126/science.283.5407.1528", "10.1109/tvcg.2013.198", "10.7717/peerj.824", "10.1126/science.290.5500.2319", "10.1038/nature06512", "10.1109/2945.981847", "10.1109/tvcg.2006.192", "10.1098/rsta.1994.0106", "10.1126/science.1227079", "10.1109/tvcg.2015.2468078", "10.1098/rspb.2015.2258", "10.2312/cgvc.20181210", "10.1177/1473871617692841", "10.1007/s11284-017-1469-9", "10.1109/wi.2006.118", "10.1038/nature25504", "10.1109/tvcg.2017.2745258", "10.1109/tvcg.2013.109", "10.1126/science.290.5500.2323", "10.1109/iv.2013.8", "10.1038/344734a0", "10.1890/0012-9658(1998)079{[}0201:acoams]2.0.co", "2", "10.1109/tvcg.2018.2846735", "10.1038/srep14750", "10.1111/cgf.12396", "10.1007/bf02289565", "10.1890/07-1246.1", "10.1046/j.1461-0248.2002.00312.x", "10.1145/3139295.3139303", "10.1177/1473871613487087"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030446", "title": "The Effectiveness of Interactive Visualization Techniques for Time Navigation of Dynamic Graphs on Large Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Alexandra Lee;Daniel Archambault;Miguel A. Nacenta", "citationCount": "0", "affiliation": "Lee, A (Corresponding Author), Swansea Univ, Dept Comp Sci, Swansea, W Glam, Wales. Lee, A (Corresponding Author), Swansea Univ, Med Sch, Swansea, W Glam, Wales. Lee, Alexandra; Archambault, Daniel, Swansea Univ, Dept Comp Sci, Swansea, W Glam, Wales. Lee, Alexandra, Swansea Univ, Med Sch, Swansea, W Glam, Wales. Nacenta, Miguel A., Univ Victoria, Dept Comp Sci, Victoria, BC, Canada.", "countries": "Canada;Wales", "abstract": "Dynamic networks can be challenging to analyze visually, especially if they span a large time range during which new nodes and edges can appear and disappear. Although it is straightforward to provide interfaces for visualization that represent multiple states of the network (i.e., multiple timeslices) either simultaneously (e.g., through small multiples) or interactively (e.g., through interactive animation), these interfaces might not support tasks in which disjoint timeslices need to be compared. Since these tasks are key for understanding the dynamic aspects of the network, understanding which interactive visualizations best support these tasks is important. We present the results of a series of laboratory experiments comparing two traditional approaches (small multiples and interactive animation), with a more recent approach based on interactive timeslicing. The tasks were performed on a large display through a touch interface. Participants completed 24 trials of three tasks with all techniques. The results show that interactive timeslicing brings benefit when comparing distant points in time, but less benefits when analyzing contiguous intervals of time.", "keywords": "Dynamic networks,Information visualization,Large displays", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030446", "refList": ["10.1145/3018661.3018731", "10.1111/cgf.12791", "10.1103/physreve.70.066111", "10.1145/3186727", "10.1109/tvcg.2015.2424889", "10.1109/visual.2019", "10.1109/tvcg.2011.213", "10.1109/vl.1996.545307", "10.1006/ijhc.1017", "10.1109/tvcg.2012.238", "10.1007/978-3-319-06793-3\\_8", "10.1016/0020-0190(89)90102-6", "10.1109/tvcg.2013.124", "10.1007/3-540-36151-03", "10.1109/tvcg.2013.198", "10.1111/cgf.12935", "10.1109/asonam.2012.39", "10.1145/3178876.3186141", "10.1109/tvcg.2006.161", "10.1109/pacificvis.2014.48", "10.1109/iv.2008.24", "10.1109/infvis.2004.46", "10.1109/tvcg.2013.149", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2012.323", "10.1007/978-3-319-14142-8\\_8", "10.1002/spe.4380211102", "10.1109/tvcg.2008.34", "10.1109/tvcg.2009.84", "10.1109/iv.2009.42", "10.1111/cgf.12615", "10.1109/tvcg.2007.70415", "10.1109/iv.2017.44", "10.1109/tvcg.2006.160", "10.1016/j.knosys.2018.03.022", "10.1109/vast.2009.5333880", "10.1109/tvcg.2010.78", "10.1111/j.1467-8659.2009.01687.x", "10.1109/tvcg.2015.2467851", "10.1179/000870403235002042"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2018.00024", "year": "2018", "title": "Visual Detection of Structural Changes in Time-Varying Graphs Using Persistent Homology", "conferenceName": "PacificVis", "authors": "Mustafa Hajij;Bei Wang;Carlos Scheidegger;Paul Rosen", "citationCount": "7", "affiliation": "Hajij, M (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA.\nHajij, Mustafa; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.\nWang, Bei, Univ Utah, Salt Lake City, UT 84112 USA.\nScheidegger, Carlos, Univ Arizona, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "Topological data analysis is an emerging area in exploratory data analysis and data mining. Its main tool, persistent homology, has become a popular technique to study the structure of complex, high-dimensional data. In this paper, we propose a novel method using persistent homology to quantify structural changes in time-varying graphs. Specifically, we transform each instance of the time-varying graph into a metric space, extract topological features using persistent homology, and compare those features over time. We provide a visualization that assists in time-varying graph exploration and helps to identify patterns of behavior within the data. To validate our approach, we conduct several case studies on real-world datasets and show how our method can find cyclic patterns, deviations from those patterns, and one-time events in time-varying graphs. We also examine whether a persistence-based similarity measure satisfies a set of well-established, desirable properties for graph metrics.", "keywords": "Topological data analysis; time-varying graph; persistent homology; graph visualization", "link": "https://doi.org/10.1109/PacificVis.2018.00024", "refList": ["10.1137/1.9781611972832.18", "10.1017/s0962492914000051", "10.1109/32.221135", "10.1111/j.1467-8659.2009.01451.x", "10.1111/j.1467-8659.2011.01898.x", "10.1109/infvis.2002.1173159", "10.1136/qshc.2004.010033", "10.1109/tvcg.2013.151", "10.1109/isbi.2015.7164127", "10.1145/1232722.1232727", "10.1090/s0273-0979-07-01191-3", "10.3402/qhw.v6i2.5918", "10.1371/journal.pone.0107878", "10.1109/tvcg.2011.177", "10.1109/tkde.2007.46", "10.1109/tvcg.2015.2468078", "10.1002/spe.4380211102", "10.1111/j.1467-8659.2012.03090.x", "10.1006/jvlc.1995.1010", "10.1371/journal.pcbi.1002581", "10.1371/journal.pone.0039464", "10.1007/s13174-010-0003-x", "10.1109/tvcg.2011.190", "10.1145/1542362.1542406", "10.1109/pacificvis.2012.6183556", "10.1145/1810959.1811004", "10.13140/2.1.1341.1520", "10.1109/pacificvis.2011.5742389", "10.1112/plms/s3-13.1.743", "10.1109/iv.2013.8", "10.1155/2013/815035", "10.1111/j.1467-8659.2009.01450.x", "10.1109/tvcg.2012.208"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934802", "title": "Persistent Homology Guided Force-Directed Graph Layouts", "year": "2019", "conferenceName": "InfoVis", "authors": "Ashley Suh;Mustafa Hajij;Bei Wang;Carlos Scheidegger;Paul A. Rosen", "citationCount": "1", "affiliation": "Suh, A (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Suh, A (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Suh, Ashley; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA. Suh, Ashley, Tufts Univ, Medford, MA 02155 USA. Hajij, Mustafa, Ohio State Univ, Columbus, OH 43210 USA. Wang, Bei, Univ Utah, Salt Lake City, UT 84112 USA. Scheidegger, Carlos, Univ Arizona, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "Graphs are commonly used to encode relationships among entities, yet their abstractness makes them difficult to analyze. Node-link diagrams are popular for drawing graphs, and force-directed layouts provide a flexible method for node arrangements that use local relationships in an attempt to reveal the global shape of the graph. However, clutter and overlap of unrelated structures can lead to confusing graph visualizations. This paper leverages the persistent homology features of an undirected graph as derived information for interactive manipulation of force-directed layouts. We first discuss how to efficiently extract 0-dimensional persistent homology features from both weighted and unweighted undirected graphs. We then introduce the interactive persistence barcode used to manipulate the force-directed graph layout. In particular, the user adds and removes contracting and repulsing forces generated by the persistent homology features, eventually selecting the set of persistent homology features that most improve the layout. Finally, we demonstrate the utility of our approach across a variety of synthetic and real datasets.", "keywords": "Graph drawing,force-directed layout,Topological Data Analysis,persistent homology", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934802", "refList": ["10.1109/lcn.2011.6115310", "10.1007/bf01386390", "10.1007/978-3-319-00395-5", "10.1109/pacificvis.2018.00024", "10.1109/tvcg.2016.2534559", "10.1109/32.221135", "10.1111/j.1467-8659.2011.01898.x", "10.1109/infvis.2002.1173159", "10.1007/978-3-662-45803-7\\_9", "10.1177/1473871612455749", "10.1109/2945.841121", "10.1088/1742-5468/2009/03/p03034", "10.1073/pnas.122653799", "10.1111/j.1467-8659.2012.03080.x", "10.1109/tvcg.2008.158", "10.1136/qshc.2004.010033", "10.1137/080734315", "10.1109/tvcg.2018.2864911", "10.1038/324446a0", "10.1109/2945.841119", "10.1109/tvcg.2013.151", "10.1109/infvis.2003.1249008", "10.1109/isbi.2011.5872535", "10.1109/isbi.2015.7164127", "10.2312/vissym/eurovis07/067-074", "10.1090/s0273-0979-07-01191-3", "10.1109/tvcg.2017.2745919", "10.1109/tvcg.2009.122", "10.3402/qhw.v6i2.5918", "10.1109/tmi.2012.2219590", "10.1371/journal.pone.0066506", "10.1090/s0002-9939-1956-0078686-7", "10.1007/978-3-642-02011-7\\_24", "10.1088/1478-3975/12/1/016007", "10.1109/tvcg.2016.2598958", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1109/tvcg.2008.34", "10.1109/tvcg.2011.223", "10.1006/s1045-926x(02)00016-2", "10.1111/j.1467-8659.2012.03090.x", "10.1371/journal.pcbi.1002581", "10.1371/journal.pone.0039464", "10.1109/tvcg.2014.2346441", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1063/1.3655371", "10.1109/tvcg.2011.190", "10.1109/pacificvis.2011.5742389", "10.1145/2254556.2254670", "10.1112/plms/s3-13.1.743", "10.1111/j.1467-8659.2011.01936.x", "10.1137/s003614450342480", "10.1155/2013/815035", "10.5169/seals-266450", "10.1016/j.cosrev.2007.05.001", "10.1111/j.1467-8659.2009.01450.x", "10.1016/s0378-8733(02)00039-4", "10.1109/tvcg.2012.208"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13185", "year": "2017", "title": "Visualizing a Sequence of a Thousand Graphs (or Even More)", "conferenceName": "EuroVis", "authors": "Michael Burch;Marcel Hlawatsch;Daniel Weiskopf", "citationCount": "7", "affiliation": "Burch, M (Corresponding Author), Univ Stuttgart, Visualizat Res Ctr VISUS, Stuttgart, Germany.\nBurch, M.; Hlawatsch, M.; Weiskopf, D., Univ Stuttgart, Visualizat Res Ctr VISUS, Stuttgart, Germany.", "countries": "Germany", "abstract": "The visualization of dynamic graphs demands visually encoding at least three major data dimensions: vertices, edges, and time steps. Many of the state-of-the-art techniques can show an overview of vertices and edges but lack a data-scalable visual representation of the time aspect. In this paper, we address the problem of displaying dynamic graphs with a thousand or more time steps. Our proposed interleaved parallel edge splatting technique uses a time-to-space mapping and shows the complete dynamic graph in a static visualization. It provides an overview of all data dimensions, allowing for visually detecting time-varying data patterns; hence, it serves as a starting point for further data exploration. By applying clustering and ordering techniques on the vertices, edge splatting on the links, and a dense time-to-space mapping, our approach becomes visually scalable in all three dynamic graph data dimensions. We illustrate the usefulness of our technique by applying it to call graphs and US domestic flight data with several hundred vertices, several thousand edges, and more than a thousand time steps.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13185", "refList": ["10.1007/3-540-36151-0", "10.1109/pacificvis.2014.48", "10.1177/1473871616661194", "10.1109/infvis.2004.18", "10.1109/vlhcc.2012.6344514", "10.1145/1054972.1055078", "10.1109/tvcg.2015.2468078", "10.1111/cgf.12791", "10.1057/palgrave/ivs/9500008", "10.1109/tvcg.2011.226", "10.1109/tvcg.2011.127", "10.1109/tvcg.2014.2322594", "10.1006/jvlc.1995.1010", "10.1109/iv.2016.28", "10.1007/bfb0021827", "10.1109/tvcg.2013.263", "10.2312/eurovisstar.20141174"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13210", "year": "2017", "title": "The State-of-the-Art in Predictive Visual Analytics", "conferenceName": "EuroVis", "authors": "Yafeng Lu;Rolando Garcia;Brett Hansen;Michael Gleicher;Ross Maciejewski", "citationCount": "22", "affiliation": "Lu, YF (Corresponding Author), Arizona State Univ, Sch Comp Informat \\& Decis Syst Engn, Tempe, AZ 85287 USA.\nLu, Yafeng; Garcia, Rolando; Hansen, Brett; Maciejewski, Ross, Arizona State Univ, Sch Comp Informat \\& Decis Syst Engn, Tempe, AZ 85287 USA.\nGleicher, Michael, Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Predictive analytics embraces an extensive range of techniques including statistical modeling, machine learning, and data mining and is applied in business intelligence, public health, disaster management and response, and many other fields. To date, visualization has been broadly used to support tasks in the predictive analytics pipeline. Primary uses have been in data cleaning, exploratory analysis, and diagnostics. For example, scatterplots and bar charts are used to illustrate class distributions and responses. More recently, extensive visual analytics systems for feature selection, incremental learning, and various prediction tasks have been proposed to support the growing use of complex models, agent-specific optimization, and comprehensive model comparison and result exploration. Such work is being driven by advances in interactive machine learning and the desire of end-users to understand and engage with the modeling process. In this state-of-the-art report, we catalogue recent advances in the visualization community for supporting predictive analytics. First, we define the scope of predictive analytics discussed in this article and describe how visual analytics can support predictive analytics tasks in a predictive visual analytics (PVA) pipeline. We then survey the literature and categorize the research with respect to the proposed PVA pipeline. Systems and techniques are evaluated in terms of their supported interactions, and interactions specific to predictive analytics are discussed. We end this report with a discussion of challenges and opportunities for future research in predictive visual analytics.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13210", "refList": ["10.1109/tvcg.2015.2467622", "10.1109/tvcg.2014.2346578", "10.1109/pacificvis.2016.7465248", "10.1109/tvcg.2016.2598828", "10.1109/vast.2015.7347637", "10.1109/tvcg.2013.125", "10.1109/vast.2011.6102451", "10.1214/10-sts330", "10.1109/mcg.2014.52", "10.1111/j.1467-8659.2011.01938.x", "10.1109/tvcg.2011.188", "10.1109/vast.2012.6400492", "10.1109/visual.2000.885740", "10.1126/science.1248506", "10.1109/jstars.2015.2388496", "10.1109/tvcg.2011.212", "10.1111/j.1467-8659.2009.01684.x", "10.1109/vast.2010.5652443", "10.1109/tvcg.2010.82", "10.1109/vast.2011.6102448", "10.1145/312129.312298", "10.1111/cgf.12886", "10.1109/tvcg.2014.2346926", "10.1109/tvcg.2016.2598838", "10.1109/pacificvis.2013.6596144", "10.1109/pacificvis.2015.7156366", "10.1109/icdmw.2010.181", "10.1109/vast.2015.7347626", "10.1007/978-1-4471-6497-5\\_1", "10.1109/pacificvis.2011.5742377", "10.1109/tvcg.2013.138", "10.1111/j.1467-8659.2011.01940.x", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2014.2346482", "10.1109/pacificvis.2014.33", "10.1109/tvcg.2014.2331979", "10.1109/tvcg.2012.277", "10.1111/cgf.12630", "10.1109/tvcg.2010.138", "10.1109/tvcg.2011.197", "10.1109/tvcg.2016.2598829", "10.1109/mcg.2014.61", "10.1109/tvcg.2014.2346660", "10.1109/vast.2011.6102457", "10.1093/bioinformatics/bth351", "10.1109/pacificvis.2015.7156353", "10.1007/978-3-319-46493-0\\_1", "10.1109/tvcg.2007.70515", "10.1057/palgrave.ivs.9500091", "10.1109/vast.2011.6102453", "10.1111/cgf.12899", "10.1145/2858036.2858150", "10.1145/2858036.2858529", "10.1109/pacificvis.2013.6596145", "10.1109/vast.2012.6400484", "10.1109/vast.2012.6400488", "10.1038/494155a", "10.1109/vast.2014.7042495", "10.1007/978-3-540-71080-6\\_6", "10.1007/978-3-642-00304-2\\_1", "10.1109/pacificvis.2013.6596154", "10.1109/iv.2013.21", "10.1007/978-3-540-71080-6\\_10", "10.1109/pacificvis.2016.7465260", "10.1145/1562849.1562851", "10.1111/cgf.12650", "10.1145/347090.347124", "10.1111/j.1467-8659.2011.01939.x", "10.1109/vast.2009.5333431", "10.1109/vast.2010.5654445", "10.1109/pacificvis.2015.7156389", "10.1109/vast.2012.6400486", "10.1109/tvcg.2016.2598831", "10.5591/978-1-57735-516-8/ijcai11-289", "10.1111/j.1467-8659.2012.03108.x", "10.1109/vast.2011.6102439", "10.1007/s11704-016-6028-y", "10.1109/tvcg.2012.64", "10.1145/2207676.2207741", "10.1109/vast.2010.5652484", "10.1109/vast.2010.5652392", "10.1109/tvcg.2016.2598541", "10.1109/tvcg.2012.258", "10.1111/cgf.12907", "10.1109/tvcg.2012.278", "10.1111/j.1467-8659.2011.01918.x", "10.1109/tvcg.2012.207", "10.1111/j.1467-8659.2011.01920.x", "10.1111/cgf.12520", "10.1007/s11390-016-1663-1", "10.1109/vast.2010.5652398"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865043", "title": "RegressionExplorer: Interactive Exploration of Logistic Regression Models with Subgroup Analysis", "year": "2018", "conferenceName": "VAST", "authors": "Dennis Dingen;Marcel van 't Veer;Patrick Houthuizen;Eveline H. J. Mestrom;Hendrikus H. M. Korsten;Arthur R. A. Bouwman;Jarke J. van Wijk", "citationCount": "5", "affiliation": "Dingen, D (Corresponding Author), Eindhoven Univ Technol, Eindhoven, Netherlands. Dingen, Dennis; van Wijk, Jarke, Eindhoven Univ Technol, Eindhoven, Netherlands. van't Veer, Marcel; Houthuizen, Patrick; Mestrom, Eveline H. J.; Korsten, Erik H. H. M.; Bouwman, Arthur R. A., Catharina Hosp, Eindhoven, Netherlands.", "countries": "Netherlands", "abstract": "We present RegressionExplorer, a Visual Analytics tool for the interactive exploration of logistic regression models. Our application domain is Clinical Biostatistics, where models are derived from patient data with the aim to obtain clinically meaningful insights and consequences. Development and interpretation of a proper model requires domain expertise and insight into model characteristics. Because of time constraints, often a limited number of candidate models is evaluated. RegressionExplorer enables experts to quickly generate, evaluate, and compare many different models, taking the workflow for model development as starting point. Global patterns in parameter values of candidate models can be explored effectively. In addition, experts are enabled to compare candidate models across multiple subpopulations. The insights obtained can be used to formulate new hypotheses or to steer model development. The effectiveness of the tool is demonstrated for two uses cases: prediction of a cardiac conduction disorder in patients after receiving a heart valve implant and prediction of hypernatremia in critically ill patients.", "keywords": "Visual analytics,Predictive visual analytics,Exploratory data analysis,Multivariate statistics,Regression analysis,Variable selection,Subgroup analysis", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865043", "refList": ["10.1214/12-aos1058", "10.21037/jtd.2016.06.46", "10.1093/jamia/ocv006", "10.1109/tvcg.2013.125", "10.1007/s11704-016-6028-y", "10.1109/tvcg.2014.2350494", "10.1145/2590349", "10.1038/srep06085", "10.1002/pds.4328", "10.1111/cgf.13210", "10.1109/pacificvis.2016.7465261", "10.1007/978-3-642-21716-6\\_15", "10.1109/tvcg.2015.2467931", "10.1145/2678025.2701407", "10.1109/tvcg.2015.2467325", "10.1109/vast.2012.6400537", "10.1111/j.1467-8659.2009.01684.x", "10.1561/1100000039", "10.1109/mcg.2016.59", "10.1007/s11390-016-1663-1", "10.1198/000313002533", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/pacificvis48177.2020.7090", "year": "2020", "title": "ExplainExplore: Visual Exploration of Machine Learning Explanations", "conferenceName": "PacificVis", "authors": "Dennis Collaris;Jarke J. van Wijk", "citationCount": "0", "affiliation": "Collaris, D (Corresponding Author), Eindhoven Univ Technol, Eindhoven, Netherlands.\nCollaris, Dennis; van Wijk, Jarke J., Eindhoven Univ Technol, Eindhoven, Netherlands.", "countries": "Netherlands", "abstract": "Machine learning models often exhibit complex behavior that is difficult to understand. Recent research in explainable AI has produced promising techniques to explain the inner workings of such models using feature contribution vectors. These vectors are helpful in a wide variety of applications. However, there are many parameters involved in this process and determining which settings are best is difficult due to the subjective nature of evaluating interpretability. To this end, we introduce EXPLAINEXPLORE: an interactive explanation system to explore explanations that fit the subjective preference of data scientists. We leverage the domain knowledge of the data scientist to find optimal parameter settings and instance perturbations, and enable the discussion of the model and its explanation with domain experts. We present a use case on a real-world dataset to demonstrate the effectiveness of our approach for the exploration and tuning of machine learning explanations.", "keywords": "Human-centered computing; Visualization; Computing methodologies; Machine learning", "link": "https://doi.org/10.1109/PacificVis48177.2020.7090", "refList": ["10.1145/3097983.3098047", "10.1109/tvcg.2016.2598838", "10.1145/2702123.2702509", "10.1109/tvcg.2016.2598828", "10.1609/aimag.v38i3.2741", "10.1137/1.9781611973440.78", "10.1109/tvcg.2013.125", "10.2307/2528823", "10.1111/j.1467-8659.2009.01475.x", "10.1145/2858036.2858529", "10.1080/13658810701349037", "10.1214/15-aoas848", "10.1145/2939672.2939778", "10.1109/tvcg.2015.2467554", "10.1038/494155a", "10.1145/2487575.2487579", "10.1145/2783258.2788613", "10.1109/icassp.1999.756335", "10.1111/cgf.12877", "10.1109/mlsp.2016.7738872", "10.1109/tvcg.2018.2865043", "10.1109/iv.2003.1217950", "10.1109/visual.1993.398859", "10.1109/tvcg.2014.2331979", "10.1016/j.ijhcs.2009.03.004", "10.1109/tvcg.2014.2346660", "10.1016/j.cor.2004.03.017", "10.1111/j.1467-8659.2009.01684.x", "10.1145/347090.347124", "10.1109/vast.2010.5652443", "10.1016/j.neucom.2017.01.105", "10.1007/978-3-319-04717-1\\_9", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2016.2598831", "10.1109/vast.2011.6102453"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934631", "title": "Explaining Vulnerabilities to Adversarial Machine Learning through Visual Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Yuxin Ma;Tiankai Xie;Jundong Li;Ross Maciejewski", "citationCount": "5", "affiliation": "Ma, YX (Corresponding Author), Arizona State Univ, Sch Comp Informat \\& Decis Syst Engn, Tempe, AZ 85287 USA. Ma, Yuxin; Xie, Tiankai; Maciejewski, Ross, Arizona State Univ, Sch Comp Informat \\& Decis Syst Engn, Tempe, AZ 85287 USA. Li, Jundong, Univ Virginia, Dept Elect \\& Comp Engn, Charlottesville, VA 22903 USA.", "countries": "USA", "abstract": "Machine learning models are currently being deployed in a variety of real-world applications where model predictions are used to make decisions about healthcare, bank loans, and numerous other critical tasks. As the deployment of artificial intelligence technologies becomes ubiquitous, it is unsurprising that adversaries have begun developing methods to manipulate machine learning models to their advantage. While the visual analytics community has developed methods for opening the black box of machine learning models, little work has focused on helping the user understand their model vulnerabilities in the context of adversarial attacks. In this paper, we present a visual analytics framework for explaining and exploring model vulnerabilities to adversarial attacks. Our framework employs a multi-faceted visualization scheme designed to support the analysis of data poisoning attacks from the perspective of models, data instances, features, and local structures. We demonstrate our framework through two case studies on binary classifiers and illustrate model vulnerabilities with respect to varying attack strategies.", "keywords": "Adversarial machine learning,data poisoning,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934631", "refList": ["10.1109/tvcg.2014.2346578", "10.1109/jbhi.2014.2344095", "10.1109/tvcg.2016.2598838", "10.3390/informatics5030031", "10.1145/1014052.1014066", "10.1109/tvcg.2017.2744938", "10.1145/2089125.2089129", "10.1145/2702123.2702509", "10.1109/vast.2017.8585721", "10.1109/tits.2017.2706978", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2013.65", "10.1109/vl.1996.545307", "10.1007/s11704-016-6028-y", "10.1007/s10994-010-5188-5", "10.1145/2858036.2858529", "10.1016/j.visinf.2017.01.006", "10.1109/5.726791", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1145/3190618", "10.1007/s10462-009-9109-6", "10.1109/tvcg.2018.2865044", "10.1111/cgf.13210", "10.1109/tvcg.2018.2816223", "10.3233/978-1-61499-098-7-870", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2014.2346594", "10.1109/tkde.2013.57", "10.1016/j.patcog.2018.07.023", "10.1109/tvcg.2018.2864812", "10.1007/978-3-030-10925-74", "10.1109/tvcg.2017.2744878", "10.1109/msp.2015.2426728", "10.1145/1562849.1562851", "10.1007/978-3-642-40994-325", "10.1109/tvcg.2014.2346660", "10.1038/nature21056", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1145/3041008.3041012", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2014.2346574", "10.1109/tvcg.2017.2744378", "10.1109/sp.2018.00057", "10.1145/2254556.2254651", "10.1109/tvcg.2017.2754480", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028888", "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes", "year": "2020", "conferenceName": "VAST", "authors": "Yuxin Ma;Arlen Fan;Jingrui He;Arun Reddy Nelakurthi;Ross Maciejewski", "citationCount": "0", "affiliation": "Ma, YX (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Ma, Yuxin; Fan, Arlen; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. He, Jingrui, Univ Illinois, Champaign, IL USA. Nelakurthi, Arun Reddy, Samsung Res Amer, Mountain View, CA USA.", "countries": "USA", "abstract": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.", "keywords": "Transfer learning,deep learning,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028888", "refList": ["10.1109/tvcg.2014.2346578", "10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1109/tpami.2018.2868685", "10.1145/2702123.2702509", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2016.2598828", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2013.65", "10.1145/2976749.2978318", "10.1007/978-3-030-01424-7\\_27", "10.1109/tvcg.2019.2934261", "10.1007/s11704-016-6028-y", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/iccv.2015.279", "10.1109/mci.2018.2840738", "10.1109/tvcg.2019.2892483", "10.1109/vast.2018.8802509", "10.1109/tvcg.2013.124", "10.1186/s40537-016-0043-6", "10.1109/tvcg.2018.2864475", "10.1145/3200489", "10.1109/tvcg.2018.2865044", "10.1111/cgf.13210", "10.1109/tvcg.2018.2816223", "10.23915/distill.00007", "10.1109/tvcg.2017.2744199", "10.1109/tkde.2018.2876857", "10.1109/tvcg.2019.2934631", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2011.188", "10.1007/978-3-642-15561-1\\_16", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1109/tvcg.2017.2744718", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2016.2598541", "10.1109/tkde.2009.191", "10.1145/3065386", "10.1016/j.ins.2016.03.021", "10.1109/tvcg.2019.2903943", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/iccv.2017.74", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2012.207", "10.1111/cgf.13092", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2017.2754480", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13972", "year": "2020", "title": "Boxer: Interactive Comparison of Classifier Results", "conferenceName": "EuroVis", "authors": "Michael Gleicher;Aditya Barve;Xinyi Yu;Florian Heimerl", "citationCount": "0", "affiliation": "Gleicher, M (Corresponding Author), Univ Wisconsin, Madison, WI 53706 USA.\nGleicher, Michael; Barve, Aditya; Yu, Xinyi; Heimerl, Florian, Univ Wisconsin, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Machine learning practitioners often compare the results of different classifiers to help select, diagnose and tune models. We present Boxer, a system to enable such comparison. Our system facilitates interactive exploration of the experimental results obtained by applying multiple classifiers to a common set of model inputs. The approach focuses on allowing the user to identify interesting subsets of training and testing instances and comparing performance of the classifiers on these subsets. The system couples standard visual designs with set algebra interactions and comparative elements. This allows the user to compose and coordinate views to specify subsets and assess classifier performance on them. The flexibility of these compositions allow the user to address a wide range of scenarios in developing and assessing classifiers. We demonstrate Boxer in use cases including model selection, tuning, fairness assessment, and data quality diagnosis.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13972", "refList": ["10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2016.2598838", "10.1007/s10618-014-0368-8", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1145/3287560.3287589", "10.1109/vast.2017.8585721", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2009.128", "10.1109/tvcg.2017.2744018", "10.1080/00994480.2000.10748487", "10.5555/3305890.3306024", "10.1109/iccv.2015.329", "10.1109/tvcg.2013.125", "10.1089/big.2016.0007", "10.1109/memsys.2019.8870817", "10.1145/2939672.2939778", "10.1007/s11104-019-04156-0", "10.1371/journal.pone.0181142", "10.1145/3301275.3302324", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2018.2865044", "10.1023/a:1010933404324", "10.1145/2487575.2487579", "10.1109/tvcg.2013.157", "10.1145/2783258.2788613", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934631", "10.1016/s0304-3800(02)00064-9", "10.1007/s10115-013-0679-x", "10.1109/tvcg.2019.2934267", "10.1007/978-3-319-10590-1\\_53", "10.1109/vast.2017.8585720", "10.1016/0004-3702(80)90021-1", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2009.84", "10.1007/s11263-016-0911-8", "10.1111/cgf.12918", "10.1111/cgf.12373", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1080/00207176808905715", "10.1002/er.3827", "10.1109/tvcg.2014.2346660", "10.1111/cgf.13417", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2017.2744158", "10.1016/b978-0-12-815849-4.00004-9", "10.1097/ede.0b013e3181c30fb2", "10.1111/cgf.13681", "10.1016/j.ejor.2006.04.051", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2016.2598468", "10.9735/2229-3981", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 11}, {"doi": "10.1109/tvcg.2019.2934659", "title": "Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations", "year": "2019", "conferenceName": "VAST", "authors": "Fred Hohman;Haekyu Park;Caleb Robinson;Duen Horng Chau", "citationCount": "8", "affiliation": "Hohman, F (Corresponding Author), Georgia Tech, Atlanta, GA 30332 USA. Hohman, Fred; Park, Haekyu; Robinson, Caleb; Chau, Duen Horng (Polo), Georgia Tech, Atlanta, GA 30332 USA.", "countries": "USA", "abstract": "Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model's outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier's learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.", "keywords": "Deep learning interpretability,visual analytics,scalable summarization,attribution graph", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934659", "refList": ["10.1097/pas.0000000000001151", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2009.108", "10.1109/cvpr.2016.265", "10.23915/distill.00015", "10.1126/science.aaf7894", "10.1145/2702123.2702509", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2016.2598828", "10.1111/j.1467-8659.2011.01957.x", "10.1109/iccv.2017.371", "10.5555/3305890.3306024", "10.1109/cvpr.2015.7298594", "10.1145/3219819.3219910", "10.1371/journal.pone.0130140", "10.1109/cvpr.2018.00391", "10.1145/2939672.2939778", "10.1007/s11263-015-0816-y", "10.1007/978-3-319-27857-5\\_77", "10.1137/s0036144503424786", "10.1111/cgf.13210", "10.23915/distill.00010", "10.23915/distill.00007", "10.1007/978-3-319-10590-1\\_53", "10.1007/978-3-030-01261-8\\_32", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2017.2744718", "10.23915/distill.00005", "10.1109/tkde.2009.191", "10.1109/cvpr.2018.00910", "10.5858/arpa.2018-0147-oa", "10.1109/tvcg.2018.2864500", "10.1109/iccv.2017.74", "10.1109/cvpr.2016.90", "10.1109/ijcnn.2017.7966422", "10.1111/cgf.12642", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028957", "title": "A Visual Analytics Approach for Exploratory Causal Analysis: Exploration, Validation, and Applications", "year": "2020", "conferenceName": "VAST", "authors": "Xiao Xie;Fan Du;Yingcai Wu", "citationCount": "0", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Zhejiang, Peoples R China. Du, F (Corresponding Author), Adobe Res, San Jose, CA 95110 USA. Xie, Xiao; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Zhejiang, Peoples R China. Du, Fan, Adobe Res, San Jose, CA 95110 USA.", "countries": "USA;China", "abstract": "Using causal relations to guide decision making has become an essential analytical task across various domains, from marketing and medicine to education and social science. While powerful statistical models have been developed for inferring causal relations from data, domain practitioners still lack effective visual interface for interpreting the causal relations and applying them in their decision-making process. Through interview studies with domain experts, we characterize their current decision-making workflows, challenges, and needs. Through an iterative design process, we developed a visualization tool that allows analysts to explore, validate, and apply causal relations in real-world decision-making scenarios. The tool provides an uncertainty-aware causal graph visualization for presenting a large set of causal relations inferred from high-dimensional data. On top of the causal graph, it supports a set of intuitive user controls for performing what-if analyses and making action plans. We report on two case studies in marketing and student advising to demonstrate that users can effectively explore causal relations and design action plans for reaching their goals.", "keywords": "Exploratory causal analysis,correlation and causation,causal graph", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028957", "refList": ["10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1145/3336191.3371824", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1145/2939672.2939778", "10.1613/jair.346", "10.1038/s42256-019-0048-x", "10.1109/tvcg.2018.2865044", "10.1145/3287560.3287566", "10.1109/tvcg.2018.2816223", "10.1145/3287560.3287569", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1016/j.artint.2018.07.007", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744718", "10.1145/3359786", "10.1109/tvcg.2019.2934659", "10.1109/ic-cids.2019.8862140", "10.1109/tvcg.2017.2744158", "10.1038/ng.142", "10.1145/3351095.3372850", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3028888", "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes", "year": "2020", "conferenceName": "VAST", "authors": "Yuxin Ma;Arlen Fan;Jingrui He;Arun Reddy Nelakurthi;Ross Maciejewski", "citationCount": "0", "affiliation": "Ma, YX (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Ma, Yuxin; Fan, Arlen; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. He, Jingrui, Univ Illinois, Champaign, IL USA. Nelakurthi, Arun Reddy, Samsung Res Amer, Mountain View, CA USA.", "countries": "USA", "abstract": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.", "keywords": "Transfer learning,deep learning,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028888", "refList": ["10.1109/tvcg.2014.2346578", "10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1109/tpami.2018.2868685", "10.1145/2702123.2702509", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2016.2598828", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2013.65", "10.1145/2976749.2978318", "10.1007/978-3-030-01424-7\\_27", "10.1109/tvcg.2019.2934261", "10.1007/s11704-016-6028-y", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/iccv.2015.279", "10.1109/mci.2018.2840738", "10.1109/tvcg.2019.2892483", "10.1109/vast.2018.8802509", "10.1109/tvcg.2013.124", "10.1186/s40537-016-0043-6", "10.1109/tvcg.2018.2864475", "10.1145/3200489", "10.1109/tvcg.2018.2865044", "10.1111/cgf.13210", "10.1109/tvcg.2018.2816223", "10.23915/distill.00007", "10.1109/tvcg.2017.2744199", "10.1109/tkde.2018.2876857", "10.1109/tvcg.2019.2934631", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2011.188", "10.1007/978-3-642-15561-1\\_16", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1109/tvcg.2017.2744718", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2016.2598541", "10.1109/tkde.2009.191", "10.1145/3065386", "10.1016/j.ins.2016.03.021", "10.1109/tvcg.2019.2903943", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/iccv.2017.74", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2012.207", "10.1111/cgf.13092", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2017.2754480", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030418", "title": "CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization", "year": "2020", "conferenceName": "VAST", "authors": "Zijie J. Wang;Robert Turko;Omar Shaikh;Haekyu Park;Nilaksh Das;Fred Hohman;Minsuk Kahng;Duen Horng Chau", "citationCount": "0", "affiliation": "Wang, ZJ (Corresponding Author), Georgia Tech, Atlanta, GA 30332 USA. Wang, Zijie J.; Turko, Robert; Shaikh, Omar; Park, Haekyu; Das, Nilaksh; Hohman, Fred; Chau, Duen Horng (Polo), Georgia Tech, Atlanta, GA 30332 USA. Kahng, Minsuk, Oregon State Univ, Corvallis, OR 97331 USA.", "countries": "USA", "abstract": "Deep learning's great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN's structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern deep learning techniques.", "keywords": "Deep learning,machine learning,convolutional neural networks,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030418", "refList": ["10.1016/j.cag.2018.09.018", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2017.2744938", "10.1037/0022-0663.83.4.484", "10.1016/j.patcog.2017.10.013", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2011.185", "10.1016/s0360-1315(99)00023-8", "10.1080/07380569.2012.651422", "10.1006/s1045-926x(02)00027-7", "10.1145/1821996.1821997", "10.1109/vast.2018.8802509", "10.1109/vl.2000.874346", "10.1007/978-3-319-27857-5\\_77", "10.1145/1227504.1227384", "10.1038/nature14539", "10.1109/tvcg.2018.2816223", "10.23915/distill.00016", "10.1162/neco.1997.9.8.1735", "10.1109/tvcg.2016.2640960", "10.1006/ijhc.2000.0409", "10.1109/tvcg.2017.2744718", "10.1006/s1045-926x(02)00028-9", "10.1111/cgf.13720", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1145/782941.782998", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030342", "title": "DECE: Decision Explorer with Counterfactual Explanations for Machine Learning Models", "year": "2020", "conferenceName": "VAST", "authors": "Furui Cheng;Yao Ming;Huamin Qu", "citationCount": "0", "affiliation": "Cheng, FR (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Cheng, Furui; Ming, Yao; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Ming, Yao, Bloomberg LP, New York, NY USA.", "countries": "USA;China", "abstract": "With machine learning models being increasingly applied to various decision-making scenarios, people have spent growing efforts to make machine learning models more transparent and explainable. Among various explanation techniques, counterfactual explanations have the advantages of being human-friendly and actionable-a counterfactual explanation tells the user how to gain the desired prediction with minimal changes to the input. Besides, counterfactual explanations can also serve as efficient probes to the models' decisions. In this work, we exploit the potential of counterfactual explanations to understand and explore the behavior of machine learning models. We design DECE, an interactive visualization system that helps understand and explore a model's decisions on individual instances and data subsets, supporting users ranging from decision-subjects to model developers. DECE supports exploratory analysis of model decisions by combining the strengths of counterfactual explanations at instance- and subgroup-levels. We also introduce a set of interactions that enable users to customize the generation of counterfactual explanations to find more actionable ones that can suit their needs. Through three use cases and an expert interview, we demonstrate the effectiveness of DECE in supporting decision exploration tasks and instance explanations.", "keywords": "Tabular Data,Explainable Machine Learning,Counterfactual Explanation,Decision Making", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030342", "refList": ["10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744683", "10.1145/3173574.3173951", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1145/3336191.3371824", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1145/2939672.2939778", "10.1613/jair.346", "10.1038/s42256-019-0048-x", "10.1109/tvcg.2018.2865044", "10.1145/3287560.3287566", "10.1109/tvcg.2018.2816223", "10.1145/3287560.3287569", "10.1109/tvcg.2018.2864504", "10.1016/j.artint.2018.07.007", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744718", "10.1145/3359786", "10.1109/tvcg.2019.2934659", "10.1109/ic-cids.2019.8862140", "10.1109/tvcg.2017.2744158", "10.1038/ng.142", "10.2139/ssrn.3063289", "10.1145/3351095.3372850", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13972", "year": "2020", "title": "Boxer: Interactive Comparison of Classifier Results", "conferenceName": "EuroVis", "authors": "Michael Gleicher;Aditya Barve;Xinyi Yu;Florian Heimerl", "citationCount": "0", "affiliation": "Gleicher, M (Corresponding Author), Univ Wisconsin, Madison, WI 53706 USA.\nGleicher, Michael; Barve, Aditya; Yu, Xinyi; Heimerl, Florian, Univ Wisconsin, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Machine learning practitioners often compare the results of different classifiers to help select, diagnose and tune models. We present Boxer, a system to enable such comparison. Our system facilitates interactive exploration of the experimental results obtained by applying multiple classifiers to a common set of model inputs. The approach focuses on allowing the user to identify interesting subsets of training and testing instances and comparing performance of the classifiers on these subsets. The system couples standard visual designs with set algebra interactions and comparative elements. This allows the user to compose and coordinate views to specify subsets and assess classifier performance on them. The flexibility of these compositions allow the user to address a wide range of scenarios in developing and assessing classifiers. We demonstrate Boxer in use cases including model selection, tuning, fairness assessment, and data quality diagnosis.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13972", "refList": ["10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2016.2598838", "10.1007/s10618-014-0368-8", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1145/3287560.3287589", "10.1109/vast.2017.8585721", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2009.128", "10.1109/tvcg.2017.2744018", "10.1080/00994480.2000.10748487", "10.5555/3305890.3306024", "10.1109/iccv.2015.329", "10.1109/tvcg.2013.125", "10.1089/big.2016.0007", "10.1109/memsys.2019.8870817", "10.1145/2939672.2939778", "10.1007/s11104-019-04156-0", "10.1371/journal.pone.0181142", "10.1145/3301275.3302324", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2018.2865044", "10.1023/a:1010933404324", "10.1145/2487575.2487579", "10.1109/tvcg.2013.157", "10.1145/2783258.2788613", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934631", "10.1016/s0304-3800(02)00064-9", "10.1007/s10115-013-0679-x", "10.1109/tvcg.2019.2934267", "10.1007/978-3-319-10590-1\\_53", "10.1109/vast.2017.8585720", "10.1016/0004-3702(80)90021-1", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2009.84", "10.1007/s11263-016-0911-8", "10.1111/cgf.12918", "10.1111/cgf.12373", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1080/00207176808905715", "10.1002/er.3827", "10.1109/tvcg.2014.2346660", "10.1111/cgf.13417", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2017.2744158", "10.1016/b978-0-12-815849-4.00004-9", "10.1097/ede.0b013e3181c30fb2", "10.1111/cgf.13681", "10.1016/j.ejor.2006.04.051", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2016.2598468", "10.9735/2229-3981", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 17}, {"doi": "10.1109/tvcg.2019.2934630", "title": "Tac-Simur: Tactic-based Simulative Visual Analytics of Table Tennis", "year": "2019", "conferenceName": "VAST", "authors": "Jiachen Wang;Kejian Zhao;Dazhen Deng;Anqi Cao;Xiao Xie;Zheng Zhou;Hui Zhang;Yingcai Wu", "citationCount": "3", "affiliation": "Wang, JC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Zhejiang, Peoples R China. Wang, Jiachen; Zhao, Kejian; Deng, Dazhen; Cao, Anqi; Xie, Xiao; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Zhejiang, Peoples R China. Zhou, Zheng; Zhang, Hui, Zhejiang Univ, Dept Sport Sci, Hangzhou, Zhejiang, Peoples R China.", "countries": "China", "abstract": "Simulative analysis in competitive sports can provide prospective insights, which can help improve the performance of players in future matches. However, adequately simulating the complex competition process and effectively explaining the simulation result to domain experts are typically challenging. This work presents a design study to address these challenges in table tennis. We propose a well-established hybrid second-order Markov chain model to characterize and simulate the competition process in table tennis. Compared with existing methods, our approach is the first to support the effective simulation of tactics, which represent high-level competition strategies in table tennis. Furthermore, we introduce a visual analytics system called Tac-Simur based on the proposed model for simulative visual analytics. Tac-Simur enables users to easily navigate different players and their tactics based on their respective performance in matches to identify the player and the tactics of interest for further analysis. Then, users can utilize the system to interactively explore diverse simulation tasks and visually explain the simulation results. The effectiveness and usefulness of this work are demonstrated by two case studies, in which domain experts utilize Tac-Simur to find interesting and valuable insights. The domain experts also provide positive feedback on the usability of Tac-Simur. Our work can be extended to other similar sports such as tennis and badminton.", "keywords": "Simulative Visual Analytics,Table Tennis,Design Study", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934630", "refList": ["10.1140/epjds29", "10.1002/nav.20017", "10.1016/j.eswa.2015.09.004", "10.1145/3200491", "10.1111/j.1467-8659.2012.03118.x", "10.1145/2939672.2939764", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1515/ijcss-2016-0002", "10.1109/icdm.2015.26", "10.1145/2362383.2362387", "10.1007/s12650-018-0521-3", "10.1109/mcg.2016.124", "10.1109/tmm.2016.2614221", "10.1007/s12650-015-0337-3", "10.1080/02640414.2013.805885", "10.1109/mcg.2016.101", "10.1111/cgf.13210", "10.16829/j.slxb.201702001", "10.1155/2013/409539", "10.1080/02640414.2013.792948", "10.1109/tvcg.2012.263", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042478", "10.1515/hukin-2017-0002", "10.1145/2783258.2788598", "10.2312/eurovisshort.20151137", "10.1111/cgf.13447", "10.1145/3097983.3098045", "10.1111/j.2517-6161.1985.tb01383.x", "10.1109/mmul.2012.54", "10.1515/1559-0410.1416", "10.1111/cgf.13436", "10.1260/1747-9541.5.2.205", "10.1080/02640414.2018.1450073", "10.1109/tvcg.2014.2346445", "10.1109/tvcg.2016.2598432"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028958", "title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Tiankai Xie;Yuxin Ma;Hanghang Tong;My T. Thai;Ross Maciejewski", "citationCount": "0", "affiliation": "Xie, TK (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Xie, Tiankai; Ma, Yuxin; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. Tong, Hanghang, Univ Illinois, Urbana, IL USA. Thai, My T., Univ Florida, Gainesville, FL 32611 USA.", "countries": "USA", "abstract": "Graph mining plays a pivotal role across a number of disciplines, and a variety of algorithms have been developed to answer who/what type questions. For example, what items shall we recommend to a given user on an e-commerce platform? The answers to such questions are typically returned in the form of a ranked list, and graph-based ranking methods are widely used in industrial information retrieval settings. However, these ranking algorithms have a variety of sensitivities, and even small changes in rank can lead to vast reductions in product sales and page hits. As such, there is a need for tools and methods that can help model developers and analysts explore the sensitivities of graph ranking algorithms with respect to perturbations within the graph structure. In this paper, we present a visual analytics framework for explaining and exploring the sensitivity of any graph-based ranking algorithm by performing perturbation-based what-if analysis. We demonstrate our framework through three case studies inspecting the sensitivity of two classic graph-based ranking algorithms (PageRank and HITS) as applied to rankings in political news media and social networks.", "keywords": "Graph-based ranking,sensitivity analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028958", "refList": ["10.1109/wsc.2017.8247800", "10.1023/a:1022649401552", "10.1515/1559-0410.11416", "10.1109/tvcg.2016.2598919", "10.1177/1473871611416549", "10.1109/tvcg.2019.2934630", "10.1140/epjds29", "10.1109/tvcg.2019.2934670", "10.1016/j.eswa.2015.09.004", "10.1145/2702123.2702509", "10.1016/j.visinf.2018.12.001", "10.2307/3002000", "10.1109/tvcg.2019.2934399", "10.1007/s41060-016-0032-z", "10.1111/cgf.13198", "10.14778/2350229.2350254", "10.1145/2939672.2939764", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/vast.2017.8585647", "10.1007/bf01187020", "10.1109/icdm.2015.26", "10.1145/2362383.2362387", "10.1177/0049124104268644", "10.1109/vast.2011.6102442", "10.1109/infvis.2003.1249025", "10.1109/tvcg.2018.2864475", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2015.2467691", "10.1111/cgf.13210", "10.1214/aos/1176344136", "10.1109/tvcg.2015.2424872", "10.1016/j.visinf.2018.09.001", "10.1177/089443939100900106", "10.1109/tvcg.2015.2467931", "10.1162/neco.1997.9.8.1735", "10.1007/s11162-011-9241-4", "10.1111/cgf.13680", "10.1145/3065386", "10.1109/tvcg.2018.2864889", "10.1177/003804070808100402", "10.1109/icdm.2010.62", "10.1038/s41598-020-59669-x", "10.1162/153244303321897717", "10.1109/tvcg.2019.2934619", "10.1007/bf00356088", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030358", "title": "Once Upon A Time In Visualization: Understanding the Use of Textual Narratives for Causality", "year": "2020", "conferenceName": "VAST", "authors": "Arjun Choudhry;Mandar Sharma;Pramod Chundury;Thomas Kapler;Derek W. S. Gray;Naren Ramakrishnan;Niklas Elmqvist", "citationCount": "0", "affiliation": "Choudhry, A (Corresponding Author), Virginia Tech, Arlington, VA 24061 USA. Choudhry, Arjun; Sharma, Mandar; Ramakrishnan, Naren, Virginia Tech, Arlington, VA 24061 USA. Chundury, Pramod; Elmqvist, Niklas, Univ Maryland, College Pk, MD 20742 USA. Kapler, Thomas; Gray, Derek W. S., Uncharted Software, Toronto, ON, Canada.", "countries": "Canada;USA", "abstract": "Causality visualization can help people understand temporal chains of events, such as messages sent in a distributed system, cause and effect in a historical conflict, or the interplay between political actors over time. However, as the scale and complexity of these event sequences grows, even these visualizations can become overwhelming to use. In this paper, we propose the use of textual narratives as a data-driven storytelling method to augment causality visualization. We first propose a design space for how textual narratives can be used to describe causal data. We then present results from a crowdsourced user study where participants were asked to recover causality information from two causality visualizations-causal graphs and Hasse diagrams-with and without an associated textual narrative. Finally, we describe Causeworks, a causality visualization system for understanding how specific interventions influence a causal model. The system incorporates an automatic textual narrative mechanism based on our design space. We validate Causeworks through interviews with experts who used the system for understanding complex events.", "keywords": "Causality visualization,natural language generation,data-driven storytelling,temporal data,quantitative studies", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030358", "refList": ["10.1371/journal.pone.0171156", "10.1007/s11424-013-2290-3", "10.1109/tvcg.2019.2934630", "10.1515/hukin-2015-0013", "10.1109/tvcg.2017.2745181", "10.1111/j.1467-8659.2012.03118.x", "10.1007/s10618-017-0513-2", "10.1371/journal.pone.0029638", "10.1016/j.visinf.2019.10.002", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1590/s1980-65742014000300004", "10.1186/1471-2105-16-s13-s8", "10.1109/mcg.2016.124", "10.1109/tmm.2016.2614221", "10.1145/3219819.3219832", "10.1007/s11424-013-2291-2", "10.1198/016214506000000302", "10.1109/tvcg.2019.2934243", "10.1109/iv.2011.57", "10.1111/cgf.13189", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1177/1473871613511959", "10.1109/iv.2010.39", "10.3390/ijgi4042159", "10.1111/cgf.13436", "10.1016/j.visinf.2017.01.005", "10.1109/tvcg.2014.2346445"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13965", "year": "2020", "title": "Bombalytics: Visualization of Competition and Collaboration Strategies of Players in a Bomb Laying Game", "conferenceName": "EuroVis", "authors": "Shivam Agarwal;G{\\\"{u}}nter Wallner;Fabian Beck", "citationCount": "0", "affiliation": "Agarwal, S (Corresponding Author), Univ Duisburg Essen, Duisburg, Germany.\nAgarwal, Shivam; Beck, Fabian, Univ Duisburg Essen, Duisburg, Germany.\nWallner, Gunter, Eindhoven Univ Technol, Eindhoven, Netherlands.\nWallner, Gunter, Univ Appl Arts Vienna, Vienna, Austria.", "countries": "Germany;Austria;Netherlands", "abstract": "Competition and collaboration form complex interaction patterns between the agents and objects involved. Only by understanding these interaction patterns, we can reveal the strategies the participating parties applied. In this paper, we study such competition and collaboration behavior for a computer game. Serving as a testbed for artificial intelligence, the multiplayer bomb laying game Pommerman provides a rich source of advanced behavior of computer agents. We propose a visualization approach that shows an overview of multiple games, with a detailed timeline-based visualization for exploring the specifics of each game. Since an analyst can only fully understand the data when considering the direct and indirect interactions between agents, we suggest various visual encodings of these interactions. Based on feedback from expert users and an application example, we demonstrate that the approach helps identify central competition strategies and provides insights on collaboration.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13965", "refList": ["10.1145/1822348.1822349", "10.1016/j.cag.2013.11.010", "10.1145/2793107.2793112", "10.1109/tvcg.2019.2934630", "10.1145/3343055.3360747", "10.1016/j.intcom.2010.04.004", "10.1109/cig.2019.8848091", "10.1007/s11554-013-0347-0", "10.1007/s12650-019-00566-5", "10.1109/bigdata.2018.8622571", "10.1007/978-3-319-63519-4", "10.1109/tciaig.2014.2365470", "10.1016/j.entcom.2013.02.002", "10.1609/aimag.v34i3.2492", "10.1145/3311350.3357716", "10.1109/tvcg.2018.2864885", "10.1177/1473871617718377", "10.1109/tvcg.2018.2864504", "10.1111/cgf.12919", "10.1109/tvcg.2019.2934243", "10.1109/tvcg.2018.2859969", "10.1145/3235765.3235812", "10.1109/tvcg.2017.2745320", "10.1109/tvcg.2016.2598415", "10.1109/tciaig.2012.2188528", "10.1109/iscas.2019.8702471", "10.2307/350902", "10.1109/mc.2018.2890217", "10.1145/3027063.3053146", "10.1111/cgf.13436", "10.1145/3130859.3131439", "10.1016/j.compedu.2011.11.015"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934595", "title": "Visual Interaction with Deep Learning Models through Collaborative Semantic Inference", "year": "2019", "conferenceName": "VAST", "authors": "Sebastian Gehrmann;Hendrik Strobelt;Robert Kr\u00fcger;Hanspeter Pfister;Alexander M. Rush", "citationCount": "3", "affiliation": "Gehrmann, S (Corresponding Author), Harvard NLP Grp, Cambridge, MA 02138 USA. Gehrmann, Sebastian; Rush, Alexander M., Harvard NLP Grp, Cambridge, MA 02138 USA. Strobelt, Hendrik, IBM Res Cambridge, Cambridge, MA USA. Kruger, Robert, MIT IBM Watson AI Lab, Cambridge, MA USA. Pfister, Hanspeter, Harvard Visual Comp Grp, Cambridge, MA USA.", "countries": "USA", "abstract": "Automation of tasks can have critical consequences when humans lose agency over decision processes. Deep learning models are particularly susceptible since current black-box approaches lack explainable reasoning. We argue that both the visual interface and model structure of deep learning systems need to take into account interaction design. We propose a framework of collaborative semantic inference (CSI) for the co-design of interactions and models to enable visual collaboration between humans and algorithms. The approach exposes the intermediate reasoning process of models which allows semantic interactions with the visual metaphors of a problem, which means that a user can both understand and control parts of the model reasoning process. We demonstrate the feasibility of CSI with a co-designed case study of a document summarization system.", "keywords": "Human-Computer Collaboration,Deep Learning,Neural Networks,Interaction Design,Human-Centered Design", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934595", "refList": ["10.1109/vast.2017.8585721", "10.1109/tvcg.2012.195", "10.1613/jair.295", "10.1162/tacl\\textbackslash{}a\\textbackslash{}00254", "10.1007/978-3-540-70956-5", "10.1145/2678025.2701399", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.5555/645526.657137", "10.1146/annurev.neur0.26.041002.131047.issn", "10.1145/2207676.2207741", "10.1145/2939672.2939778", "10.1109/vlhcc.2010.15", "10.3115/v1/d14-1130", "10.18653/v1/p17-1099", "10.1109/tvcg.2018.2865044", "10.18653/v1/p17-1080", "10.1111/cgf.13210", "10.1162/tacl\\_a\\_00254", "10.1109/tvcg.2018.2816223", "10.1109/72.279181", "10.1145/2783258.2788613", "10.2112/si85-057.1", "10.1145/1866029.1866078", "10.1007/978-3-319-10590-1\\_53", "10.1109/tvcg.2018.2865230", "10.1007/s12650-018-0531-1", "10.1145/2365952.2365964", "10.1017/s026988890200019x", "10.1109/iccv.2015.337", "10.1109/tvcg.2017.2744878", "10.1145/3027063.3053103", "10.1109/tvcg.2017.2744718", "10.1145/302979.303030", "10.1109/cvpr.2018.00917", "10.1007/s40708-016-0042-6", "10.1073/pnas.1807184115", "10.1609/aimag.v35i4.2513", "10.1016/j.ijhcs.2009.03.004", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2018.2864838", "10.1111/cgf.13092", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2014.2346574", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1016/s0167-739x(97)00022-8", "10.1093/bi0inf0rmatics/bth267"], "wos": 1, "children": [{"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 7}, {"doi": "10.1109/tvcg.2020.3028888", "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes", "year": "2020", "conferenceName": "VAST", "authors": "Yuxin Ma;Arlen Fan;Jingrui He;Arun Reddy Nelakurthi;Ross Maciejewski", "citationCount": "0", "affiliation": "Ma, YX (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Ma, Yuxin; Fan, Arlen; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. He, Jingrui, Univ Illinois, Champaign, IL USA. Nelakurthi, Arun Reddy, Samsung Res Amer, Mountain View, CA USA.", "countries": "USA", "abstract": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.", "keywords": "Transfer learning,deep learning,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028888", "refList": ["10.1109/tvcg.2014.2346578", "10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1109/tpami.2018.2868685", "10.1145/2702123.2702509", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2016.2598828", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2013.65", "10.1145/2976749.2978318", "10.1007/978-3-030-01424-7\\_27", "10.1109/tvcg.2019.2934261", "10.1007/s11704-016-6028-y", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/iccv.2015.279", "10.1109/mci.2018.2840738", "10.1109/tvcg.2019.2892483", "10.1109/vast.2018.8802509", "10.1109/tvcg.2013.124", "10.1186/s40537-016-0043-6", "10.1109/tvcg.2018.2864475", "10.1145/3200489", "10.1109/tvcg.2018.2865044", "10.1111/cgf.13210", "10.1109/tvcg.2018.2816223", "10.23915/distill.00007", "10.1109/tvcg.2017.2744199", "10.1109/tkde.2018.2876857", "10.1109/tvcg.2019.2934631", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2011.188", "10.1007/978-3-642-15561-1\\_16", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1109/tvcg.2017.2744718", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2016.2598541", "10.1109/tkde.2009.191", "10.1145/3065386", "10.1016/j.ins.2016.03.021", "10.1109/tvcg.2019.2903943", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/iccv.2017.74", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2012.207", "10.1111/cgf.13092", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2017.2754480", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3028958", "title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Tiankai Xie;Yuxin Ma;Hanghang Tong;My T. Thai;Ross Maciejewski", "citationCount": "0", "affiliation": "Xie, TK (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Xie, Tiankai; Ma, Yuxin; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. Tong, Hanghang, Univ Illinois, Urbana, IL USA. Thai, My T., Univ Florida, Gainesville, FL 32611 USA.", "countries": "USA", "abstract": "Graph mining plays a pivotal role across a number of disciplines, and a variety of algorithms have been developed to answer who/what type questions. For example, what items shall we recommend to a given user on an e-commerce platform? The answers to such questions are typically returned in the form of a ranked list, and graph-based ranking methods are widely used in industrial information retrieval settings. However, these ranking algorithms have a variety of sensitivities, and even small changes in rank can lead to vast reductions in product sales and page hits. As such, there is a need for tools and methods that can help model developers and analysts explore the sensitivities of graph ranking algorithms with respect to perturbations within the graph structure. In this paper, we present a visual analytics framework for explaining and exploring the sensitivity of any graph-based ranking algorithm by performing perturbation-based what-if analysis. We demonstrate our framework through three case studies inspecting the sensitivity of two classic graph-based ranking algorithms (PageRank and HITS) as applied to rankings in political news media and social networks.", "keywords": "Graph-based ranking,sensitivity analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028958", "refList": ["10.1109/wsc.2017.8247800", "10.1023/a:1022649401552", "10.1515/1559-0410.11416", "10.1109/tvcg.2016.2598919", "10.1177/1473871611416549", "10.1109/tvcg.2019.2934630", "10.1140/epjds29", "10.1109/tvcg.2019.2934670", "10.1016/j.eswa.2015.09.004", "10.1145/2702123.2702509", "10.1016/j.visinf.2018.12.001", "10.2307/3002000", "10.1109/tvcg.2019.2934399", "10.1007/s41060-016-0032-z", "10.1111/cgf.13198", "10.14778/2350229.2350254", "10.1145/2939672.2939764", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/vast.2017.8585647", "10.1007/bf01187020", "10.1109/icdm.2015.26", "10.1145/2362383.2362387", "10.1177/0049124104268644", "10.1109/vast.2011.6102442", "10.1109/infvis.2003.1249025", "10.1109/tvcg.2018.2864475", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2015.2467691", "10.1111/cgf.13210", "10.1214/aos/1176344136", "10.1109/tvcg.2015.2424872", "10.1016/j.visinf.2018.09.001", "10.1177/089443939100900106", "10.1109/tvcg.2015.2467931", "10.1162/neco.1997.9.8.1735", "10.1007/s11162-011-9241-4", "10.1111/cgf.13680", "10.1145/3065386", "10.1109/tvcg.2018.2864889", "10.1177/003804070808100402", "10.1109/icdm.2010.62", "10.1038/s41598-020-59669-x", "10.1162/153244303321897717", "10.1109/tvcg.2019.2934619", "10.1007/bf00356088", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13731", "year": "2019", "title": "The State of the Art in Visual Analysis Approaches for Ocean and Atmospheric Datasets", "conferenceName": "EuroVis", "authors": "Shehzad Afzal;Mohamad Mazen Hittawe;Sohaib Ghani;Tahira Jamil;Omar M. Knio;Markus Hadwiger;Kevin I.{-}J. Ho", "citationCount": "0", "affiliation": "Afzal, S (Corresponding Author), King Abdullah Univ Sci \\& Technol, Thuwal, Saudi Arabia.\nAfzal, S.; Hittawe, M. M.; Ghani, S.; Jamil, T.; Knio, O.; Hadwiger, M.; Hoteit, I, King Abdullah Univ Sci \\& Technol, Thuwal, Saudi Arabia.", "countries": "Arabia", "abstract": "The analysis of ocean and atmospheric datasets offers a unique set of challenges to scientists working in different application areas. These challenges include dealing with extremely large volumes of multidimensional data, supporting interactive visual analysis, ensembles exploration and visualization, exploring model sensitivities to inputs, mesoscale ocean features analysis, predictive analytics, heterogeneity and complexity of observational data, representing uncertainty, and many more. Researchers across disciplines collaborate to address such challenges, which led to significant research and development advances in ocean and atmospheric sciences, and also in several relevant areas such as visualization and visual analytics, big data analytics, machine learning and statistics. In this report, we perform an extensive survey of research advances in the visual analysis of ocean and atmospheric datasets. First, we survey the task requirements by conducting interviews with researchers, domain experts, and end users working with these datasets on a spectrum of analytics problems in the domain of ocean and atmospheric sciences. We then discuss existing models and frameworks related to data analysis, sense-making, and knowledge discovery for visual analytics applications. We categorize the techniques, systems, and tools presented in the literature based on the taxonomies of task requirements, interaction methods, visualization techniques, machine learning and statistical methods, evaluation methods, data types, data dimensions and size, spatial scale and application areas. We then evaluate the task requirements identified based on our interviews with domain experts in the context of categorized research based on our taxonomies, and existing models and frameworks of visual analytics to determine the extent to which they fulfill these task requirements, and identify the gaps in current research. In the last part of this report, we summarize the trends, challenges, and opportunities for future research in this area. (see http://www.acm.org/about/class/class/2012) )", "keywords": "", "link": "https://doi.org/10.1111/cgf.13731", "refList": ["10.1111/cgf.12898", "10.1109/icdmw.2009.55", "10.1109/tvcg.2013.144", "10.1109/tvcg.2015.2467754", "10.1109/tvcg.2015.2507569", "10.1109/tvcg.2014.2346455", "10.1111/cgf.12901", "10.1109/tvcg.2008.184", "10.1109/mc.2013.119", "10.1111/j.1467-8659.2009.01697.x", "10.1109/tvcg.2009.200", "10.1111/cgf.12649", "10.1109/2945.981847", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2012.190", "10.1109/mis.2006.75", "10.1109/iv.2015.13", "10.1111/cgf.12135", "10.1109/vast.2009.5332586", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2008.131", "10.1109/tvcg.2013.131", "10.1109/tvcg.2010.82", "10.1109/pacificvis.2015.7156374", "10.1111/cgf.12886", "10.1109/tvcg.2016.2598869", "10.1109/vast.2012.6400553", "10.1109/pacificvis.2013.6596144", "10.1109/tvcg.2017.2745178", "10.1109/tvcg.2015.2410278", "10.1111/cgf.12931", "10.1109/tvcg.2009.155", "10.1109/pacificvis.2015.7156366", "10.1109/mcg.2015.121", "10.1007/978-1-4471-2804-5\\_6", "10.1109/ldav.2015.7348068", "10.1007/978-1-4471-6497-5\\_1", "10.1109/pacificvis.2009.4906852", "10.1111/cgf.12646", "10.1109/tvcg.2016.2607204", "10.1109/tvcg.2011.162", "10.1177/1473871612465214", "10.1109/mcg.2017.3621228", "10.1109/tvcg.2012.110", "10.1109/sc.2014.40", "10.5194/gmd-8-2329-2015", "10.1109/tvcg.2012.80", "10.1109/tvcg.2014.2346448", "10.1109/ldav.2012.6378978", "10.1007/s10915-011-9501-7", "10.1109/tvcg.2016.2598830", "10.1109/tvcg.2007.70515", "10.1109/vast.2011.6102460", "10.1109/pacificvis.2017.8031584", "10.1109/tvcg.2016.2637904", "10.1175/2009jtecha1374.1", "10.1109/tvcg.2015.2467591", "10.1109/tvcg.2015.2467204", "10.1109/tvcg.2008.59", "10.1109/tvcg.2013.143", "10.1109/tvcg.2008.119", "10.1109/tvcg.2015.2467411", "10.1109/icdmw.2009.91", "10.1109/tvcg.2016.2598868", "10.1109/tvcg.2017.2661309", "10.1109/vl.1996.545307", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2008.140", "10.1109/icra.2012.6224689", "10.1109/tvcg.2010.80", "10.1167/tvst.7.1.16", "10.1109/pacificvis.2018.00037", "10.1111/cgf.13210", "10.1109/tvcg.2017.2698041", "10.1109/iv.2010.51", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2010.170", "10.1103/physrevd.71.077102", "10.5194/npg-22-545-2015", "10.1109/tvcg.2014.2307892", "10.1111/cgf.12650", "10.1109/iv.2009.38", "10.2312/pe.envirvis.envirvis13.053-057", "10.1145/3122948.3122952", "10.1109/ldav.2014.7013208", "10.1038/nature14956", "10.1109/vast.2015.7347671", "10.1109/vast.2015.7347634", "10.1109/tvcg.2014.2346755", "10.1177/1473871613481692", "10.1109/ldav.2017.8231849", "10.1109/tvcg.2017.2773071", "10.1109/iv.2011.79", "10.1109/tvcg.2013.10", "10.1109/tvcg.2008.157", "10.1111/j.1467-8659.2009.01664.x", "10.1109/pacificvis.2016.7465251", "10.1109/vast.2014.7042489", "10.1109/mis.2006.100", "10.1109/tvcg.2010.247", "10.1109/tvcg.2016.2534560", "10.1109/tvcg.2010.181", "10.1109/tvcg.2018.2864817", "10.1109/vast.2015.7347635", "10.1109/tvcg.2018.2864901", "10.1109/hicss.2016.183", "10.1109/iv.2010.32", "10.1111/j.1467-8659.2011.01948.x", "10.1109/tvcg.2017.2743989", "10.1109/pacificvis.2016.7465272", "10.1109/tvcg.2008.69", "10.1109/tvcg.2008.139", "10.1109/iv.2011.12", "10.1111/cgf.12520"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030466", "title": "Uncertainty in Continuous Scatterplots, Continuous Parallel Coordinates, and Fibers", "year": "2020", "conferenceName": "SciVis", "authors": "Boyan Zheng;Filip Sadlo", "citationCount": "0", "affiliation": "Zheng, BY (Corresponding Author), Heidelberg Univ, Heidelberg, Germany. Zheng, Boyan; Sadlo, Filip, Heidelberg Univ, Heidelberg, Germany.", "countries": "Germany", "abstract": "In this paper, we introduce uncertainty to continuous scatterplots and continuous parallel coordinates. We derive respective models, validate them with sampling-based brute-force schemes, and present acceleration strategies for their computation. At the same time, we show that our approach lends itself as well for introducing uncertainty into the definition of fibers in bivariate data. Finally, we demonstrate the properties and the utility of our approach using specifically designed synthetic cases and simulated data.", "keywords": "Multivariate data,uncertainty visualization,uncertain continuous scatterplots,uncertain continuous parallel coordinates,uncertain fibers", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030466", "refList": ["10.1109/tvcg.2018.2865193", "10.1109/tvcg.2013.143", "10.1109/tvcg.2015.2467204", "10.1109/pacificvis.2013.6596144", "10.1109/tvcg.2017.2745178", "10.1109/tmm.2016.2614227", "10.1109/scivis.2015.7429488", "10.1111/cgf.12100", "10.1109/pacificvis.2016.7465251", "10.1111/cgf.12898", "10.1109/tvcg.2015.2410278", "10.1109/icdmw.2009.55", "10.1109/tvcg.2015.2467754", "10.1109/tvcg.2010.247", "10.1109/tvcg.2019.2934312", "10.1111/cgf.13397", "10.1109/tvcg.2016.2598868", "10.1111/j.1467-8659.2011.01944.x", "10.1109/tvcg.2015.2507569", "10.1109/tvcg.2013.92", "10.1145/1268517.1268563", "10.1109/tvcg.2014.2346455", "10.1109/tvcg.2010.181", "10.1109/tvcg.2018.2853721", "10.1109/tvcg.2008.140", "10.1007/s12650-015-0341-7", "10.1109/mcg.2014.52", "10.1109/tvcg.2018.2864815", "10.1111/j.1467-8659.2012.03095.x", "10.1109/tvcg.2013.138", "10.1109/tvcg.2019.2934242", "10.3390/e20070540", "10.1016/j.jcp.2007.02.014", "10.1111/cgf.13999", "10.1109/tvcg.2017.2779501", "10.1111/cgf.12390", "10.1109/tvcg.2014.2307892", "10.1111/cgf.13531", "10.1109/tvcg.2013.152", "10.1038/nature14956", "10.1007/978-3-540-88606-8\\_4", "10.1109/mcg.2005.71", "10.1111/j.1467-8659.2011.01942.x", "10.1109/tvcg.2019.2934800", "10.1109/tvcg.2016.2598830", "10.1109/cvpr.2005.188", "10.1109/tvcg.2011.261", "10.1111/cgf.13731", "10.1109/tvcg.2017.2754480"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.13677", "year": "2019", "title": "An Ontological Framework for Supporting the Design and Evaluation of Visual Analytics Systems", "conferenceName": "EuroVis", "authors": "Min Chen;David S. Ebert", "citationCount": "5", "affiliation": "Chen, M (Corresponding Author), Univ Oxford, Oxford, England.\nChen, Min, Univ Oxford, Oxford, England.\nEbert, David S., Purdue Univ, W Lafayette, IN 47907 USA.", "countries": "USA;England", "abstract": "Designing, evaluating, and improving visual analytics (VA) systems is a primary area of activities in our discipline. In this paper, we present an ontological framework for recording and categorizing technical shortcomings to be addressed in a VA workflow, reasoning about the causes of such problems, identifying technical solutions, and anticipating secondary effects of the solutions. The methodology is built on the theoretical premise that designing a VA workflow is an optimization of the cost-benefit ratio of the processes in the workflow. It makes uses three fundamental measures to group and connect symptoms, causes, remedies, and side-effects, and guide the search for potential solutions to the problems. In terms of requirement analysis and system design, the proposed methodology can enable system designers to explore the decision space in a structured manner. In terms of evaluation, the proposed methodology is time-efficient and complementary to various forms of empirical studies, such as user surveys, controlled experiments, observational studies, focus group discussions, and so on. In general, it reduces the amount of trial-and-error in the lifecycle of VA system development.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13677", "refList": ["10.1109/tvcg.2006.178", "10.1109/infvis.2000.885092", "10.1111/cgf.12920", "10.1057/ivs.2009.26", "10.1109/mcg.2017.3271463", "10.1007/978-3-319-10578-9\\_1", "10.1109/icdmw.2008.62", "10.1109/mcg.2017.51", "10.1145/3011141.3011207", "10.1109/tvcg.2013.134", "10.1080/10618600.1996.10474696", "10.1007/978-3-540-70956-5", "10.1109/visual.1990.146375", "10.1109/tvcg.2012.219", "10.1109/vl.1996.545307", "10.1057/ivs.2009.23", "10.1002/j.1538-7305.1948.tb00917.x", "10.1109/tvcg.2010.79", "10.1109/tvcg.2013.124", "10.1111/cgf.13211", "10.1007/978-3-642-40897-7\\_9", "10.1103/physrev.108.171", "10.1109/infvis.2004.59", "10.1109/iv.2008.36", "10.1111/cgf.13210", "10.1111/j.1467-8659.2008.01230.x", "10.1001/jama.293.10.1223", "10.1177/1473871611407399", "10.1109/visual.1995.480821", "10.1117/12.539227", "10.1109/tvcg.2015.2513410", "10.1109/vast.2011.6102463", "10.7749/citiescommunitiesterritories.dec2014.029.art01", "10.1109/mcg.2005.55", "10.1109/tvcg.2012.234", "10.1109/pacificvis.2012.6183556", "10.1103/physrev.106.620", "10.1109/infvis.1997.636792", "10.2307/2104491", "10.1145/2468356.2468677", "10.1145/3173574.3173611", "10.1109/tvcg.2018.2864838", "10.1111/cgf.13092", "10.1109/visual.2004.10", "10.1109/tvcg.2017.2744319", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2603178"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934264", "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;David S. Ebert;Arif Ghafoor", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Karimzadeh, Morteza; Ebert, David S.; Ghafoor, Arif, Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.", "keywords": "Summative evaluation,usefulness,evaluation process,taxonomy,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934264", "refList": ["10.1109/tvcg.2017.2744478", "10.1109/tvcg.2018.2865025", "10.1109/tvcg.2006.85", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2017.2745181", "10.1111/cgf.13677", "10.1109/tvcg.2018.2864844", "10.1109/tvcg.2013.126", "10.1109/tvcg.2018.2864811", "10.1109/infvis.2005.1532147", "10.1177/0956797613504966", "10.1145/2669557.2669579", "10.1109/mcg.2005.102", "10.1109/visual.2003.1250426", "10.1136/bmj.39489.470347.ad", "10.1109/tvcg.2017.2744080", "10.1109/mcg.2009.53", "10.1111/j.1467-8527.2005.00307.x", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2864886", "10.1109/tvcg.2018.2864843", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2018.2865051", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2018.2865026", "10.1007/978-3-540-71080-6\\_6", "10.1109/tvcg.2018.2865020", "10.1177/1473871611407399", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2018.2864526", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2864905", "10.1049/sej.1991.0040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2711030", "10.1109/tvcg.2011.279", "10.1109/vast.2017.8585505", "10.1147/jrd.2010.2042914", "10.1016/s0378-7206(98)00044-5", "10.1145/2993901.2993913", "10.1109/tvcg.2018.2865041", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744758", "10.1145/1168149.1168158", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2012.213", "10.1109/tvcg.2017.2744738", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2018.2864826", "10.1145/1377966.1377974", "10.1109/apec.2009.4802646", "10.1145/1168149.1168152", "10.1016/j.jss.2008.03.059", "10.1109/vast.2017.8585484", "10.1109/tvcg.2017.2744818", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2865042", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030388", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "year": "2020", "conferenceName": "SciVis", "authors": "Pepe Eulzer;Sabine Bauer;Francis Kilian;Kai Lawonn", "citationCount": "0", "affiliation": "Eulzer, P (Corresponding Author), Univ Jena, Jena, Germany. Eulzer, Pepe; Lawonn, Kai, Univ Jena, Jena, Germany. Bauer, Sabine, Univ Koblenz Landau, Koblenz, Germany. Kilian, Francis, Cath Clin Koblenz Montabaur, Dept Spine Surg, Koblenz, Germany.", "countries": "Germany", "abstract": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. $\\mathrm{By}$ linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. $\\mathrm{in}$ a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.", "keywords": "Medical visualization,bioinformatics,coordinated views,focus and context,biomechanical simulation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030388", "refList": ["10.1109/tvcg.2018.2864903", "10.1177/1473871613510429", "10.1093/ehjqcco/qcz052", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2018.2865076", "10.1055/s-0039-1687862", "10.1109/visual.1990.146375", "10.1109/tvcg.2017.2744198", "10.1016/j.ijmedinf.2014.10.001", "10.1109/tvcg.2013.124", "10.1016/j.jacc", "10.1111/cgf.13167", "10.17705/1thci.00055", "10.1136/bmjqs.2009.037895", "10.1109/tvcg.2013.238", "10.1109/tvcg.2018.2865240", "10.1186/1471-2261-6-34", "10.1109/tvcg.2019.2934264", "10.1109/tvcg.2013.200", "10.1109/tvcg.2011.209", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467091", "10.1136/bmjopen-2019-033208", "10.1109/beliv.2018.8634027", "10.1109/tvcg.2012.213", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2015.2467325", "10.1145/2133806.2133821", "10.1145/1806799.1806866", "10.1108/02635570610688869", "10.1002/hbm.20701", "10.1561/1100000039", "10.1145/3025453.3025645", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 5}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 71}], "len": 237}, {"doi": "10.1109/tvcg.2018.2865139", "title": "Structure-Based Suggestive Exploration: A New Approach for Effective Exploration of Large Networks", "year": "2018", "conferenceName": "InfoVis", "authors": "Wei Chen;Fangzhou Guo;Dongming Han;Jacheng Pan;Xiaotao Nie;Jiazhi Xia;Xiaolong Zhang", "citationCount": "5", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Labo CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Chen, Wei; Guo, Fangzhou; Han, Dongming; Pan, Jacheng; Nie, Xiaotao, Zhejiang Univ, State Key Labo CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Xia, Jiazhi, Cent S Univ, Changsha, Hunan, Peoples R China. Zhang, Xiaolong, Penn State Univ, University Pk, PA 16802 USA.", "countries": "USA;China", "abstract": "When analyzing a visualized network, users need to explore different sections of the network to gain insight. However, effective exploration of large networks is often a challenge. While various tools are available for users to explore the global and local features of a network, these tools usually require significant interaction activities, such as repetitive navigation actions to follow network nodes and edges. In this paper, we propose a structure-based suggestive exploration approach to support effective exploration of large networks by suggesting appropriate structures upon user request. Encoding nodes with vectorized representations by transforming information of surrounding structures of nodes into a high dimensional space, our approach can identify similar structures within a large network, enable user interaction with multiple similar structures simultaneously, and guide the exploration of unexplored structures. We develop a web-based visual exploration system to incorporate this suggestive exploration approach and compare performances of our approach under different vectorizing methods and networks. We also present the usability and effectiveness of our approach through a controlled user study with two datasets.", "keywords": "Large Network Exploration,Structure-Based Exploration,Suggestive Exploration", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865139", "refList": ["10.1109/tvcg.2007.70582", "10.1109/tvcg.2006.120", "10.1109/mc.2013.242", "10.1109/tvcg.2009.108", "10.1145/2939672.2939754", "10.1016/j.comnet.2011.08.019", "10.1145/2702123.2702476", "10.1109/tvcg.2017.2744938", "10.1007/s00371-013-0892-3", "10.1137/1.9781611974973.67", "10.1109/vast.2009.5333893", "10.1109/tst.2013.6509098", "10.1109/35021bigcomp.2015.7072812", "10.1145/2909132.2909246", "10.1111/j.1467-8659.2011.01957.x", "10.1111/j.1467-8659.2011.01898.x", "10.1177/1473871612455749", "10.1109/tvcg.2013.167", "10.1145/3097983.3098061", "10.1007/978-1-4613-0303-9\\_28", "10.1109/vast.2014.7042485", "10.1109/tmm.2016.2614220", "10.1145/1376616.1376675", "10.1145/2623330.2623732", "10.14778/1920841.1920887", "10.1109/tvcg.2017.2745219", "10.1109/tvcg.2017.2743858", "10.1109/tvcg.2008.151", "10.1145/1150402.1150479", "10.1093/bioinformatics/bth436", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2016.2598958", "10.1111/cgf.12883", "10.1145/1556262.1556300", "10.1109/icdm.2012.159", "10.1145/2470654.2466444", "10.1109/tvcg.2013.109", "10.1109/infvis.2004.1", "10.1109/icdmw.2008.99", "10.1002/aris.1440370106", "10.1007/978-3-319-05813-9\\_11", "10.1371/journal.pone.0098679", "10.1109/tvcg.2006.106", "10.1111/j.1467-8659.2011.01935.x", "10.1111/cgf.12397", "10.1111/cgf.13184", "10.1111/cgf.12642", "10.1109/tvcg.2016.2598831", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934396", "title": "A Deep Generative Model for Graph Layout", "year": "2019", "conferenceName": "InfoVis", "authors": "Oh-Hyun Kwon;Kwan-Liu Ma", "citationCount": "4", "affiliation": "Kwon, OH (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Kwon, Oh-Hyun; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "Different layouts can characterize different aspects of the same graph. Finding a \u201cgood\u201d layout of a graph is thus an important task for graph visualization. In practice, users often visualize a graph in multiple layouts by using different methods and varying parameter settings until they find a layout that best suits the purpose of the visualization. However, this trial-and-error process is often haphazard and time-consuming. To provide users with an intuitive way to navigate the layout design space, we present a technique to systematically visualize a graph in diverse layouts using deep generative models. We design an encoder-decoder architecture to learn a model from a collection of example layouts, where the encoder represents training examples in a latent space and the decoder produces layouts from the latent space. In particular, we train the model to construct a two-dimensional latent space for users to easily explore and generate various layouts. We demonstrate our approach through quantitative and qualitative evaluations of the generated layouts. The results of our evaluations show that our model is capable of learning and generalizing abstract concepts of graph layouts, not just memorizing the training examples. In summary, this paper presents a fundamentally new approach to graph visualization where a machine learning model learns to visualize a graph from examples without manually-defined heuristics.", "keywords": "Graph,network,visualization,layout,machine learning,deep learning,neural network,generative model,autoencoder", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934396", "refList": ["10.1103/physreve.74.036104", "10.1145/234535.234538", "10.2307/2412323", "10.1109/tvcg.2007.70580", "10.7155/jgaa.00405", "10.1177/1473871612455749", "10.1109/tvcg.2011.185", "10.1073/pnas.122653799", "10.1007/3-540-58950-3", "10.1145/2897824.2925974", "10.1007/3-540-44541-2\\_17", "10.1007/bf00410640", "10.1021/acscentsci.7b00572", "10.1007/s10208-011-9093-5", "10.1109/tvcg.2015.2467451", "10.1016/0020-0190(89)90102-6", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13187", "10.1214/aoms/1177729586", "10.1109/tvcg.2014.2346277", "10.1109/tvcg.2017.2743858", "10.1007/978-3-662-44043-8\\_3", "10.1038/30918", "10.1109/mcg.2018.2881501", "10.1016/j.camwa.2004.08.015", "10.1109/tvcg.2010.269", "10.1006/s1045-926x(02)00016-2", "10.1016/0925-7721(94)00014-x", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1103/physrevx.4.011047", "10.1371/journal.pone.0098679", "10.1145/2487788.2488173", "10.1007/978-3-030-01418-6\\_41", "10.1007/978-3-030-04414-5\\_12", "10.1109/tvcg.2018.2865139", "10.1142/s0219525903001067", "10.7155/jgaa.00051"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030367", "title": "Lyra 2: Designing Interactive Visualizations by Demonstration", "year": "2020", "conferenceName": "InfoVis", "authors": "Jonathan Zong;Dhiraj Barnwal;Rupayan Neogy;Arvind Satyanarayan", "citationCount": "0", "affiliation": "Zong, J (Corresponding Author), MIT, Cambridge, MA 02139 USA. Zong, Jonathan; Neogy, Rupayan; Satyanarayan, Arvind, MIT, Cambridge, MA 02139 USA. Barnwal, Dhiraj, Indian Inst Technol Kharagpur, Kharagpur, W Bengal, India.", "countries": "India;USA", "abstract": "Recent graphical interfaces offer direct manipulation mechanisms for authoring visualizations, but are largely restricted to static output. To author interactive visualizations, users must instead turn to textual specification, but such approaches impose a higher technical burden. To bridge this gap, we introduce Lyra 2, a system that extends a prior visualization design environment with novel methods for authoring interaction techniques by demonstration. Users perform an interaction (e.g., button clicks, drags, or key presses) directly on the visualization they are editing. The system interprets this performance using a set of heuristics and enumerates suggestions of possible interaction designs. These heuristics account for the properties of the interaction (e.g., target and event type) as well as the visualization (e.g., mark and scale types, and multiple views). Interaction design suggestions are displayed as thumbnails; users can preview and test these suggestions, iteratively refine them through additional demonstrations, and finally apply and customize them via property inspectors. We evaluate our approach through a gallery of diverse examples, and evaluate its usability through a first-use study and via an analysis of its cognitive dimensions. We find that, in Lyra 2, interaction design by demonstration enables users to rapidly express a wide range of interactive visualizations.", "keywords": "Direct manipulation,interactive visualization,interaction design by demonstration", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030367", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030467", "title": "PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch;Lingyun Yu;Peiran Ren;Thomas Ertl;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Knittel, Johannes; Koch, Steffen; Ertl, Thomas, Univ Stuttgart, VIS VISUS, Stuttgart, Germany. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Dept Comp Sci \\& Software Engn, Suzhou, Peoples R China. Ren, Peiran, Alibaba Grp, Hangzhou, Peoples R China.", "countries": "Germany;China", "abstract": "Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.", "keywords": "Storyline visualization,reinforcement learning,mixed-initiative design", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030467", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030459", "title": "Scalability of Network Visualisation from a Cognitive Load Perspective", "year": "2020", "conferenceName": "InfoVis", "authors": "Vahan Yoghourdjian;Yalong Yang;Tim Dwyer;Lawrence Lee;Michael Wybrow;Kim Marriott", "citationCount": "0", "affiliation": "Yoghourdjian, V (Corresponding Author), Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Yoghourdjian, Vahan; Yang, Yalong; Dwyer, Tim; Wybrow, Michael; Marriott, Kim, Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Lawrence, Lee, Monash Univ, Fac Business \\& Econ, Melbourne, Vic, Australia. Yang, Yalong, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA.", "countries": "USA;Australia", "abstract": "Node-link diagrams are widely used to visualise networks. However, even the best network layout algorithms ultimately result in \u2018hairball\u2019 visualisations when the graph reaches a certain degree of complexity, requiring simplification through aggregation or interaction (such as filtering) to remain usable. Until now, there has been little data to indicate at what level of complexity node-link diagrams become ineffective or how visual complexity affects cognitive load. To this end, we conducted a controlled study to understand workload limits for a task that requires a detailed understanding of the network topology-finding the shortest path between two nodes. We tested performance on graphs with 25 to 175 nodes with varying density. We collected performance measures (accuracy and response time), subjective feedback, and physiological measures (EEG, pupil dilation, and heart rate variability). To the best of our knowledge this is the first network visualisation study to include physiological measures. Our results show that people have significant difficulty finding the shortest path in high density node-link diagrams with more than 50 nodes and even low density graphs with more than 100 nodes. From our collected EEG data we observe functional differences in brain activity between hard and easy tasks. We found that cognitive load increased up to certain level of difficulty after which it decreased, likely because participants had given up. We also explored the effects of global network layout features such as size or number of crossings, and features of the shortest path such as length or straightness on task difficulty. We found that global features generally had a greater impact than those of the shortest path.", "keywords": "Data Visualisation,Network Visualisation,Cognitive Load,EEG", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030459", "refList": ["10.1109/tvcg.2019.2934396", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2016.2570755", "10.1007/s00371-013-0892-3", "10.1007/b98835", "10.1109/isda.2014.7066252", "10.1109/tvcg.2015.2467251", "10.1177/1473871612455749", "10.1109/tvcg.2012.299", "10.1007/3-540-58950-3", "10.1111/cgf.12878", "10.1109/mcse.2007.55", "10.1109/tvcg.2012.238", "10.1145/264645.264657", "10.1109/tvcg.2015.2467451", "10.1109/tvcg.2013.151", "10.1016/0020-0190(89)90102-6", "10.1109/tvcg.2019.2934307", "10.1109/tvcg.2015.2468151", "10.1109/tvcg.2017.2745919", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13440", "10.1109/tvcg.2011.220", "10.1111/cgf.13187", "10.1109/t-c.1969.222678", "10.1109/tvcg.2017.2743858", "10.1126/science.290.5500.2319", "10.1109/cahpc.2018.8645912", "10.1109/tvcg.2015.2465151", "10.1109/tvcg.2016.2598958", "10.1109/tvcg.2017.2751473", "10.1002/spe.4380211102", "10.1109/tpds.2018.2869805", "10.1016/j.jpdc.2019.04.008", "10.1109/pacificvis.2017.8031574", "10.1006/s1045-926x(02)00016-2", "10.1109/tvcg.2017.2674999", "10.1145/2872427.2883041", "10.1145/3292500.3330989", "10.1109/tvcg.2017.2744878", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1109/sbac-pad.2018.00060", "10.1007/978-3-662-45803-7\\_27", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1111/j.1469-1809.1936.tb02137.x", "10.1007/bf02289565", "10.1109/tvcg.2017.2689016", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.4722", "year": "2020", "title": "A Study of Mental Maps in Immersive Network Visualization", "conferenceName": "PacificVis", "authors": "Joseph Kotlarek;Oh{-}Hyun Kwon;Kwan{-}Liu Ma;Peter Eades;Andreas Kerren;Karsten Klein;Falk Schreiber", "citationCount": "0", "affiliation": "Kotlarek, J (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA.\nKotlarek, Joseph; Kwon, Oh-Hyun; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.\nEades, Peter, Univ Sydney, Sydney, NSW, Australia.\nKerren, Andreas, Linnaeus Univ, Vaxjo, Sweden.\nKlein, Karsten; Schreiber, Falk, Univ Konstanz, Constance, Germany.", "countries": "Sweden;Germany;USA;Australia", "abstract": "The visualization of a network influences the quality of the mental map that the viewer develops to understand the network. In this study, we investigate the effects of a 3D immersive visualization environment compared to a traditional 2D desktop environment on the comprehension of a network's structure. We compare the two visualization environments using three tasks-interpreting network structure, memorizing a set of nodes, and identifying the structural changes-commonly used for evaluating the quality of a mental map in network visualization. The results show that participants were able to interpret network structure more accurately when viewing the network in an immersive environment, particularly for larger networks. However, we found that 2D visualizations performed better than immersive visualization for tasks that required spatial memory.", "keywords": "Human-centered computing; Visualization; Visualization techniques; Graph drawings; Human-centered computing; Visualization; Empirical studies in visualization", "link": "https://doi.org/10.1109/PacificVis48177.2020.4722", "refList": ["10.1103/physreve.74.036104", "10.1109/tvcg.2019.2934396", "10.1007/978-3-540-87730-1\\_9", "10.1117/12.2005484", "10.1177/1473871612455749", "10.1109/pacificvis.2017.8031577", "10.1007/978-3-319-73207-7", "10.1109/38.888006", "10.1109/2945.841119", "10.1109/mc.2005.297", "10.1007/s10055-018-0346-3", "10.1109/tvcg.2016.2599107", "10.1109/icsmc.1992.271688", "10.1038/30918", "10.1006/jvlc.1995.1010", "10.1089/109493101300117938", "10.1109/vrais.1998.658488", "10.1109/pacificvis.2015.7156357", "10.1109/tvcg.2010.78", "10.1109/tvcg.2016.2520921", "10.1007/978-3-030-01388-22", "10.1145/229459.229467", "10.1145/1056808.1056875", "10.1109/tvcg.2017.2744079", "10.1109/bdva.2015.7314293", "10.1086/jar.33.4.3629752", "10.1016/j.ijhcs.2013.08.004"], "wos": 1, "children": [], "len": 1}], "len": 11}, {"doi": "10.1109/tvcg.2020.3030440", "title": "Context-aware Sampling of Large Networks via Graph Representation Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Zhiguang Zhou;Chen Shi;Xilong Shen;Lihong Cai;Haoxuan Wang;Yuhua Liu;Ying Zhao;Wei Chen", "citationCount": "0", "affiliation": "Zhao, Y (Corresponding Author), Cent South Univ, Changsha, Peoples R China. Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Zhou, Zhiguang; Shi, Chen; Shen, Xilong; Cai, Lihong; Wang, Haoxuan; Liu, Yuhua, Zhejiang Univ Finance \\& Econ, Sch Informat, Hangzhou, Peoples R China. Zhao, Ying, Cent South Univ, Changsha, Peoples R China. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China.", "countries": "China", "abstract": "Numerous sampling strategies have been proposed to simplify large-scale networks for highly readable visualizations. It is of great challenge to preserve contextual structures formed by nodes and edges with tight relationships in a sampled graph, because they are easily overlooked during the process of sampling due to their irregular distribution and immunity to scale. In this paper, a new graph sampling method is proposed oriented to the preservation of contextual structures. We first utilize a graph representation learning (GRL) model to transform nodes into vectors so that the contextual structures in a network can be effectively extracted and organized. Then, we propose a multi-objective blue noise sampling model to select a subset of nodes in the vectorized space to preserve contextual structures with the retention of relative data and cluster densities in addition to those features of significance, such as bridging nodes and graph connections. We also design a set of visual interfaces enabling users to interactively conduct context-aware sampling, visually compare results with various sampling strategies, and deeply explore large networks. Case studies and quantitative comparisons based on real-world datasets have demonstrated the effectiveness of our method in the abstraction and exploration of large networks.", "keywords": "Graph sampling,Graph representation learning,Blue noise sampling,Graph evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030440", "refList": ["10.1145/2491159.2491168", "10.1016/j.physa.2015.04.035", "10.1145/1830252.1830274", "10.1109/icdmw.2007.91", "10.1002/net.21834", "10.1109/tvcg.2018.2864503", "10.1016/j.cag.2018.01.010", "10.1109/icc.2016.7511156", "10.1111/cgf.13444", "10.1145/956750.956831", "10.1145/364099.364331", "10.1007/s00180-016-0663-5", "10.1109/tvcg.2013.223", "10.1007/s12650-018-0530-2", "10.1103/physreve.73.016102", "10.1109/access.2018.2870684", "10.1007/978-3-319-06793-3\\_1", "10.2312/vissym/eurovis05/239-246", "10.1016/j.ins.2015.02.014", "10.1145/2339530.2339723", "10.1109/icde.2015.7113345", "10.1109/tvcg.2011.233", "10.14778/2809974.2809980", "10.1109/glocom.2015.7417471", "10.1145/2578153.2578175", "10.1214/aoms/1177705148", "10.1109/tvcg.2008.130", "10.14232/actacyb.20.1.2011.6", "10.1504/ijitm.2019.099809", "10.1109/tvcg.2018.2865020", "10.1145/956750", "10.1002/cpe.4330060203", "10.1145/1150402.1150479", "10.1103/physreve.72.036118", "10.1109/tvcg.2017.2744098", "10.1145/2020408.2020512", "10.1142/s0129183114400075", "10.1109/jsac.2011.111005", "10.1016/j.camwa.2011.11.057", "10.1145/2470654.2466444", "10.1109/tvcg.2017.2674999", "10.1214/aos/1013203451", "10.1109/icdcsw.2011.34", "10.1016/j.physa.2013.11.015", "10.1145/1081870.1081893", "10.1109/tnet.2008.2001730", "10.1109/access.2016.2633485", "10.1145/1879141.1879192", "10.1371/journal.pone.0098679", "10.1126/science.220.4598.671", "10.1109/pacificvis.2015.7156355", "10.1088/1475-7516/2011/08/011", "10.1007/978-3-319-27261-0\\_41", "10.1111/cgf.13410", "10.1109/tvcg.2018.2865139", "10.1109/tvcg.2016.2598831", "10.1016/j.physa.2014.06.065"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030447", "title": "DRGraph: An Efficient Graph Layout Algorithm for Large-scale Graphs by Dimensionality Reduction", "year": "2020", "conferenceName": "InfoVis", "authors": "Minfeng Zhu;Wei Chen;Yuanzhe Hu;Yuxuan Hou;Liangjun Liu;Kaiyuan Zhang", "citationCount": "0", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Zhu, Minfeng; Chen, Wei; Hu, Yuanzhe; Hou, Yuxuan; Liu, Liangjun; Zhang, Kaiyuan, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China.", "countries": "China", "abstract": "Efficient layout of large-scale graphs remains a challenging problem: the force-directed and dimensionality reduction-based methods suffer from high overhead for graph distance and gradient computation. In this paper, we present a new graph layout algorithm, called DRGraph, that enhances the nonlinear dimensionality reduction process with three schemes: approximating graph distances by means of a sparse distance matrix, estimating the gradient by using the negative sampling technique, and accelerating the optimization process through a multi-level layout scheme. DRGraph achieves a linear complexity for the computation and memory consumption, and scales up to large-scale graphs with millions of nodes. Experimental results and comparisons with state-of-the-art graph layout methods demonstrate that DRGraph can generate visually comparable layouts with a faster running time and a lower memory requirement.", "keywords": "graph visualization,graph layout,dimensionality reduction,force-directed layout", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030447", "refList": ["10.1109/tvcg.2007.70582", "10.1371/journal.pone.0136497", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/3018661.3018731", "10.1109/34.491619", "10.1145/2939672.2939754", "10.1145/3269206.3271788", "10.1016/j.comnet.2011.08.019", "10.1007/s11263-011-0442-2", "10.1016/s0020-0190(98)00108-2", "10.1109/tvcg.2018.2865151", "10.1007/978-3-319-61188-4\\_2", "10.1177/1473871612455749", "10.1177/1473871616666394", "10.1109/tvcg.2015.2467035", "10.1186/1471-2105-10-375", "10.1145/3097983.3098061", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1109/tvcg.2015.2467451", "10.1109/2945.841119", "10.1016/j.swevo.2015.10.002", "10.1016/0020-0190(89)90102-6", "10.1109/infvis.2003.1249009", "10.1109/tvcg.2017.2745919", "10.1111/cgf.13187", "10.1111/cgf.13440", "10.1109/tvcg.2012.245", "10.1109/tvcg.2017.2743858", "10.1177/1473871618821740", "10.1186/s12859-015-0585-1", "10.1002/nav.3800020109", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1006/s1045-926x(02)00016-2", "10.1109/cvpr.2012.6247667", "10.1023/b:jogo.0000042115.44455.f3", "10.1109/pacificvis.2017.8031607", "10.1002/nav.3800030404", "10.1109/cvpr.2008.4587500", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1090/s0002-9904-1920-03322-7", "10.1109/iv.2013.3", "10.1145/568522.568523", "10.1109/tvcg.2006.156", "10.1109/tvcg.2012.236", "10.1109/tvcg.2018.2865139", "10.1145/3219819.3220025", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030393", "title": "Exemplar-based Layout Fine-tuning for Node-link Diagrams", "year": "2020", "conferenceName": "InfoVis", "authors": "Jiacheng Pan;Wei Chen;Xiaodong Zhao;Shuyue Zhou;Wei Zeng;Minfeng Zhu;Jian Chen;Siwei Fu;Yingcai Wu", "citationCount": "1", "affiliation": "Chen, W; Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Lab, Hangzhou, Peoples R China. Pan, Jiacheng; Chen, Wei; Zhao, Xiaodong; Zhou, Shuyue; Zhu, Minfeng; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Fu, Siwei; Wu, Yingcai, Zhejiang Lab, Hangzhou, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Jian, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA;China", "abstract": "We design and evaluate a novel layout fine-tuning technique for node-link diagrams that facilitates exemplar-based adjustment of a group of substructures in batching mode. The key idea is to transfer user modifications on a local substructure to other substructures in the entire graph that are topologically similar to the exemplar. We first precompute a canonical representation for each substructure with node embedding techniques and then use it for on-the-fly substructure retrieval. We design and develop a light-weight interactive system to enable intuitive adjustment, modification transfer, and visual graph exploration. We also report some results of quantitative comparisons, three case studies, and a within-participant user study.", "keywords": "Node-link diagram,graph layout,graph visualization,user interactions", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030393", "refList": ["10.1109/tvcg.2007.70582", "10.1371/journal.pone.0136497", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/3018661.3018731", "10.1109/34.491619", "10.1145/2939672.2939754", "10.1145/3269206.3271788", "10.1016/j.comnet.2011.08.019", "10.1007/s11263-011-0442-2", "10.1016/s0020-0190(98)00108-2", "10.1109/tvcg.2018.2865151", "10.1177/1473871612455749", "10.1177/1473871616666394", "10.1109/tvcg.2015.2467035", "10.1186/1471-2105-10-375", "10.1145/3097983.3098061", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1109/tvcg.2015.2467451", "10.1016/j.swevo.2015.10.002", "10.1016/0020-0190(89)90102-6", "10.1109/infvis.2003.1249009", "10.1109/tvcg.2017.2745919", "10.1111/cgf.13187", "10.1111/cgf.13440", "10.1109/tvcg.2012.245", "10.1109/tvcg.2017.2743858", "10.1177/1473871618821740", "10.1186/s12859-015-0585-1", "10.1002/nav.3800020109", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1006/s1045-926x(02)00016-2", "10.1109/cvpr.2012.6247667", "10.1023/b:jogo.0000042115.44455.f3", "10.1109/pacificvis.2017.8031607", "10.1002/nav.3800030404", "10.1109/infvis.2004.1", "10.1109/cvpr.2008.4587500", "10.1109/pacificvis.2011.5742389", "10.1140/epjb/e2011-10979-2", "10.1371/journal.pone.0098679", "10.1090/s0002-9904-1920-03322-7", "10.1109/iv.2013.3", "10.1145/568522.568523", "10.1109/tvcg.2006.156", "10.1109/tvcg.2012.236", "10.1109/tvcg.2018.2865139", "10.1145/3219819.3220025", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}], "len": 19}, {"doi": "10.1109/tvcg.2020.3030398", "title": "Multiscale Snapshots: Visual Analysis of Temporal Summaries in Dynamic Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Eren Cakmak;Udo Schlegel;Dominik J\u00e4ckle;Daniel A. Keim;Tobias Schreck", "citationCount": "0", "affiliation": "Cakmak, E (Corresponding Author), Univ Konstanz, Constance, Germany. Cakmak, Eren; Schlegel, Udo; Keim, Daniel, Univ Konstanz, Constance, Germany. Schreck, Tobias, Graz Univ Technol, Graz, Austria.", "countries": "Germany;Austria", "abstract": "The overview-driven visual analysis of large-scale dynamic graphs poses a major challenge. We propose Multiscale Snapshots, a visual analytics approach to analyze temporal summaries of dynamic graphs at multiple temporal scales. First, we recursively generate temporal summaries to abstract overlapping sequences of graphs into compact snapshots. Second, we apply graph embeddings to the snapshots to learn low-dimensional representations of each sequence of graphs to speed up specific analytical tasks (e.g., similarity search). Third, we visualize the evolving data from a coarse to fine-granular snapshots to semi-automatically analyze temporal states, trends, and outliers. The approach enables us to discover similar temporal summaries (e.g., reoccurring states), reduces the temporal data to speed up automatic analysis, and to explore both structural and temporal properties of a dynamic graph. We demonstrate the usefulness of our approach by a quantitative evaluation and the application to a real-world dataset.", "keywords": "Dynamic Graph,Dynamic Network,Unsupervised Graph Learning,Graph Embedding,Multiscale Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030398", "refList": ["10.1007/s00442-002-1137-8", "10.1111/cgf.13668", "10.1086/282697", "10.1109/tvcg.2002.1044518", "10.1890/0012-9658(2000)081{[}0008:ppbatp]2.0.co", "2", "10.1101/2020.04.08.032524", "10.1111/j.1461-0248.2009.01391.x", "10.1016/0040-5809(80)90016-7", "10.1109/tvcg.2009.181", "10.1145/2669557.2669559", "10.1109/vl.1996.545307", "10.1109/tvcg.2013.254", "10.1109/tvcg.2019.2934251", "10.1073/pnas.93.6.2608", "10.2312/cgvc", "10.1080/14786440109462720", "10.1111/1440-1703.12057", "10.2307/1940591", "10.1037/h0071325", "10.1073/pnas.1215506110", "10.1126/science.283.5407.1528", "10.1109/tvcg.2013.198", "10.7717/peerj.824", "10.1126/science.290.5500.2319", "10.1038/nature06512", "10.1109/2945.981847", "10.1109/tvcg.2006.192", "10.1098/rsta.1994.0106", "10.1126/science.1227079", "10.1109/tvcg.2015.2468078", "10.1098/rspb.2015.2258", "10.2312/cgvc.20181210", "10.1177/1473871617692841", "10.1007/s11284-017-1469-9", "10.1109/wi.2006.118", "10.1038/nature25504", "10.1109/tvcg.2017.2745258", "10.1109/tvcg.2013.109", "10.1126/science.290.5500.2323", "10.1109/iv.2013.8", "10.1038/344734a0", "10.1890/0012-9658(1998)079{[}0201:acoams]2.0.co", "2", "10.1109/tvcg.2018.2846735", "10.1038/srep14750", "10.1111/cgf.12396", "10.1007/bf02289565", "10.1890/07-1246.1", "10.1046/j.1461-0248.2002.00312.x", "10.1145/3139295.3139303", "10.1177/1473871613487087"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.12883", "year": "2016", "title": "Pathfinder: Visual Analysis of Paths in Graphs", "conferenceName": "EuroVis", "authors": "Christian Partl;Samuel Gratzl;Marc Streit;Anne Mai Wassermann;Hanspeter Pfister;Dieter Schmalstieg;Alexander Lex", "citationCount": "14", "affiliation": "Partl, C (Corresponding Author), Graz Univ Technol, A-8010 Graz, Austria.\nPartl, C.; Schmalstieg, D., Graz Univ Technol, A-8010 Graz, Austria.\nGratzl, S.; Streit, M., Johannes Kepler Univ Linz, Linz, Austria.\nWassermann, A. M., Pfizer, New York, NY USA.\nPfister, H., Harvard Univ, Cambridge, MA 02138 USA.\nLex, A., Univ Utah, Salt Lake City, UT 84112 USA.", "countries": "USA;Austria", "abstract": "The analysis of paths in graphs is highly relevant in many domains. Typically, path-related tasks are performed in node-link layouts. Unfortunately, graph layouts often do not scale to the size of many real world networks. Also, many networks are multivariate, i.e., contain rich attribute sets associated with the nodes and edges. These attributes are often critical in judging paths, but directly visualizing attributes in a graph layout exacerbates the scalability problem. In this paper, we present visual analysis solutions dedicated to path-related tasks in large and highly multivariate graphs. We show that by focusing on paths, we can address the scalability problem of multivariate graph visualization, equipping analysts with a powerful tool to explore large graphs. We introduce Pathfinder, a technique that provides visual methods to query paths, while considering various constraints. The resulting set of paths is visualized in both a ranked list and as a node-link diagram. For the paths in the list, we display rich attribute data associated with nodes and edges, and the node-link diagram provides topological context. The paths can be ranked based on topological properties, such as path length or average node degree, and scores derived from attribute data. Pathfinder is designed to scale to graphs with tens of thousands of nodes and edges by employing strategies such as incremental query results. We demonstrate Pathfinder's fitness for use in scenarios with data from a coauthor network and biological pathways.", "keywords": "", "link": "https://doi.org/10.1111/cgf.12883", "refList": ["10.2312/eurovisstar.20151110", "10.1007/978-3-642-10543-2\\_21", "10.1109/tvcg.2009.108", "10.1145/2207676.2208294", "10.1109/tvcg.2013.154", "10.1111/j.1467-8659.2009.01451.x", "10.1111/j.1467-8659.2011.01898.x", "10.1038/sj.onc.1210422", "10.1145/1060745.1060766", "10.1093/bioinformatics/btq675", "10.2312/vissym/eurovis07/083-090", "10.1186/1471-2105-14-s19-s3", "10.1038/nature11003", "10.1007/978-3-319-06826-8\\textbackslash{}\\_21", "10.1109/tvcg.2014.2346248", "10.1117/12.761555", "10.1109/mic.2005.63", "10.1145/1168149.1168168", "10.1007/978-3-319-11915-1\\_27", "10.1109/tvcg.2013.109", "10.1177/1473871612462152", "10.1109/tvcg.2006.106", "10.1016/j.cell.2012.03.017", "10.1109/tvcg.2013.173"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2017.2744738", "title": "SkyLens: Visual Analysis of Skyline on Multi-Dimensional Data", "year": "2017", "conferenceName": "VAST", "authors": "Xun Zhao;Yanhong Wu;Weiwei Cui;Xinnan Du;Yuan Chen;Yong Wang;Dik Lun Lee;Huamin Qu", "citationCount": "5", "affiliation": "Wu, YH (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Hong Kong, Peoples R China. Zhao, Xun; Wu, Yanhong; Du, Xinnan; Chen, Yuan; Wang, Yong; Lee, Dik Lun; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Hong Kong, Peoples R China. Cui, Weiwei, Microsoft Res Asia, Beijing, Peoples R China.", "countries": "China", "abstract": "Skyline queries have wide-ranging applications in fields that involve multi-criteria decision making, including tourism, retail industry, and human resources. By automatically removing incompetent candidates, skyline queries allow users to focus on a subset of superior data items (i.e., the skyline), thus reducing the decision-making overhead. However, users are still required to interpret and compare these superior items manually before making a successful choice. This task is challenging because of two issues. First, people usually have fuzzy, unstable, and inconsistent preferences when presented with multiple candidates. Second, skyline queries do not reveal the reasons for the superiority of certain skyline points in a multi-dimensional space. To address these issues, we propose SkyLens, a visual analytic system aiming at revealing the superiority of skyline points from different perspectives and at different scales to aid users in their decision making. Two scenarios demonstrate the usefulness of SkyLens on two datasets with a dozen of attributes. A qualitative study is also conducted to show that users can efficiently accomplish skyline understanding and comparison tasks with SkyLens.", "keywords": "Skyline query,skyline visualization,multi-dimensional data,visual analytics,multi-criteria decision making", "link": "http://dx.doi.org/10.1109/TVCG.2017.2744738", "refList": ["10.1016/j.ins.2012.04.007", "10.1109/tvcg.2015.2468011", "10.1016/j.is.2008.04.004", "10.3166/jds.12.193-208", "10.1057/ivs.2009.4", "10.1021/ie020865g", "10.1109/vl.1996.545307", "10.1145/2505515.2505739", "10.1080/14786440109462720", "10.1109/icde.2007.367855", "10.1145/1061318.1061320", "10.1109/icde.2001.914855", "10.14778/1920841.1920980", "10.1111/cgf.12883", "10.1016/s0925-2312(98)00030-7", "10.1145/1189769.1189774", "10.1109/icde.2009.84", "10.1109/tvcg.2016.2598589", "10.1109/tvcg.2013.173", "10.1145/989863.989885", "10.1109/tvcg.2016.2598432", "10.1109/pacificvis.2013.6596140"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934603", "title": "MetricsVis: A Visual Analytics System for Evaluating Employee Performance in Public Safety Agencies", "year": "2019", "conferenceName": "VAST", "authors": "Jieqiong Zhao;Morteza Karimzadeh;Luke S. Snyder;Chittayong Surakitbanharn;Cheryl Z. Qian;David S. Ebert", "citationCount": "0", "affiliation": "Zhao, JQ (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Zhao, Jieqiong; Snyder, Luke S.; Qian, Zhenyu Cheryl; Ebert, David S., Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Purdue Univ, Boulder, CO 80309 USA. Surakitbanharn, Chittayong, Hivemapper, Burlingame, CA USA.", "countries": "USA", "abstract": "Evaluating employee performance in organizations with varying workloads and tasks is challenging. Specifically, it is important to understand how quantitative measurements of employee achievements relate to supervisor expectations, what the main drivers of good performance are, and how to combine these complex and flexible performance evaluation metrics into an accurate portrayal of organizational performance in order to identify shortcomings and improve overall productivity. To facilitate this process, we summarize common organizational performance analyses into four visual exploration task categories. Additionally, we develop MetricsVis, a visual analytics system composed of multiple coordinated views to support the dynamic evaluation and comparison of individual, team, and organizational performance in public safety organizations. MetricsVis provides four primary visual components to expedite performance evaluation: (1) a priority adjustment view to support direct manipulation on evaluation metrics; (2) a reorderable performance matrix to demonstrate the details of individual employees; (3) a group performance view that highlights aggregate performance and individual contributions for each group; and (4) a projection view illustrating employees with similar specialties to facilitate shift assignments and training. We demonstrate the usability of our framework with two case studies from medium-sized law enforcement agencies and highlight its broader applicability to other domains.", "keywords": "Organizational performance analysis,multi-dimensional data,hierarchical relationships,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934603", "refList": ["10.1111/j.1744-6570.2010.01207.x", "10.1002/aenm.201601266", "10.1146/annurev.psych.49.1.141", "10.2307/256718", "10.1109/tvcg.2007.70515", "10.1109/mcg.2017.21", "10.1109/tvcg.2012.64", "10.1016/s0377-2217(97)00147-1", "10.1108/13639510410519921", "10.1146/annurev.so.07.080181.001541", "10.1207/s15327043hup0601\\_1", "10.1111/cgf.12935", "10.1109/tvcg.2013.173", "10.1097/jom.0b013e318226a763", "10.1145/2470654.2466443", "10.1109/tvcg.2007.70589", "10.1145/3105971.3105979", "10.1109/tvcg.2017.2744738", "10.1109/tvcg.2015.2467325", "10.4135/9781848608320.n9", "10.2307/2346830", "10.1146/annurev-psych-120710-100401", "10.2307/256712", "10.1145/989863.989885", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2010.209", "10.1109/tvcg.2006.76"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934264", "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;David S. Ebert;Arif Ghafoor", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Karimzadeh, Morteza; Ebert, David S.; Ghafoor, Arif, Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.", "keywords": "Summative evaluation,usefulness,evaluation process,taxonomy,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934264", "refList": ["10.1109/tvcg.2017.2744478", "10.1109/tvcg.2018.2865025", "10.1109/tvcg.2006.85", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2017.2745181", "10.1111/cgf.13677", "10.1109/tvcg.2018.2864844", "10.1109/tvcg.2013.126", "10.1109/tvcg.2018.2864811", "10.1109/infvis.2005.1532147", "10.1177/0956797613504966", "10.1145/2669557.2669579", "10.1109/mcg.2005.102", "10.1109/visual.2003.1250426", "10.1136/bmj.39489.470347.ad", "10.1109/tvcg.2017.2744080", "10.1109/mcg.2009.53", "10.1111/j.1467-8527.2005.00307.x", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2864886", "10.1109/tvcg.2018.2864843", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2018.2865051", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2018.2865026", "10.1007/978-3-540-71080-6\\_6", "10.1109/tvcg.2018.2865020", "10.1177/1473871611407399", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2018.2864526", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2864905", "10.1049/sej.1991.0040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2711030", "10.1109/tvcg.2011.279", "10.1109/vast.2017.8585505", "10.1147/jrd.2010.2042914", "10.1016/s0378-7206(98)00044-5", "10.1145/2993901.2993913", "10.1109/tvcg.2018.2865041", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744758", "10.1145/1168149.1168158", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2012.213", "10.1109/tvcg.2017.2744738", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2018.2864826", "10.1145/1377966.1377974", "10.1109/apec.2009.4802646", "10.1145/1168149.1168152", "10.1016/j.jss.2008.03.059", "10.1109/vast.2017.8585484", "10.1109/tvcg.2017.2744818", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2865042", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030388", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "year": "2020", "conferenceName": "SciVis", "authors": "Pepe Eulzer;Sabine Bauer;Francis Kilian;Kai Lawonn", "citationCount": "0", "affiliation": "Eulzer, P (Corresponding Author), Univ Jena, Jena, Germany. Eulzer, Pepe; Lawonn, Kai, Univ Jena, Jena, Germany. Bauer, Sabine, Univ Koblenz Landau, Koblenz, Germany. Kilian, Francis, Cath Clin Koblenz Montabaur, Dept Spine Surg, Koblenz, Germany.", "countries": "Germany", "abstract": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. $\\mathrm{By}$ linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. $\\mathrm{in}$ a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.", "keywords": "Medical visualization,bioinformatics,coordinated views,focus and context,biomechanical simulation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030388", "refList": ["10.1109/tvcg.2018.2864903", "10.1177/1473871613510429", "10.1093/ehjqcco/qcz052", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2018.2865076", "10.1055/s-0039-1687862", "10.1109/visual.1990.146375", "10.1109/tvcg.2017.2744198", "10.1016/j.ijmedinf.2014.10.001", "10.1109/tvcg.2013.124", "10.1016/j.jacc", "10.1111/cgf.13167", "10.17705/1thci.00055", "10.1136/bmjqs.2009.037895", "10.1109/tvcg.2013.238", "10.1109/tvcg.2018.2865240", "10.1186/1471-2261-6-34", "10.1109/tvcg.2019.2934264", "10.1109/tvcg.2013.200", "10.1109/tvcg.2011.209", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467091", "10.1136/bmjopen-2019-033208", "10.1109/beliv.2018.8634027", "10.1109/tvcg.2012.213", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2015.2467325", "10.1145/2133806.2133821", "10.1145/1806799.1806866", "10.1108/02635570610688869", "10.1002/hbm.20701", "10.1561/1100000039", "10.1145/3025453.3025645", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.13401", "year": "2018", "title": "Towards Easy Comparison of Local Businesses Using Online Reviews", "conferenceName": "EuroVis", "authors": "Yong Wang;Hammad Haleem;Conglei Shi;Yanhong Wu;Xun Zhao;Siwei Fu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, Y (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.\nWang, Yong; Haleem, Hammad; Zhao, Xun; Fu, Siwei; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.\nShi, Conglei, Airbnb Inc, San Francisco, CA USA.\nWu, Yanhong, Vis Res, Palo Alto, CA USA.", "countries": "USA;China", "abstract": "With the rapid development of e-commerce, there is an increasing number of online review websites, such as Yelp, to help customers make better purchase decisions. Viewing online reviews, including the rating score and text comments by other customers, and conducting a comparison between different businesses are the key to making an optimal decision. However, due to the massive amount of online reviews, the potential difference of user rating standards, and the significant variance of review time, length, details and quality, it is difficult for customers to achieve a quick and comprehensive comparison. In this paper, we present E-Comp, a carefully-designed visual analytics system based on online reviews, to help customers compare local businesses at different levels of details. More specifically, intuitive glyphs overlaid on maps are designed for quick candidate selection. Grouped Sankey diagram visualizing the rating difference by common customers is chosen for more reliable comparison of two businesses. Augmented word cloud showing adjective-noun word pairs, combined with a temporal view, is proposed to facilitate in-depth comparison of businesses in terms of different time periods, rating scores and features. The effectiveness and usability of E-Comp are demonstrated through a case study and in-depth user interviews.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13401", "refList": ["10.1007/978-3-211-77280-5\\_4", "10.1177/1473871611416549", "10.1145/2702123.2702476", "10.1145/1014052.1014073", "10.1109/tvcg.2014.2346249", "10.1007/978-3-540-33037-08", "10.1109/tvcg.2007.70570", "10.1016/j.ijhm.2008.06.011", "10.1109/mic.2003.1167344", "10.1177/0047287513481274", "10.1109/iv.2013.5", "10.1111/j.1467-8659.2008.01205.x", "10.1561/1500000001", "10.1007/978-3-319-30319-2\\_13", "10.1109/tvcg.2013.254", "10.1007/978-3-540-33037-0\\_8", "10.1109/tvcg.2013.122", "10.1002/acp.2350050106", "10.1109/tvcg.2016.2598590", "10.2312/eurovisshort.20161167", "10.1007/s11002-013-9278-6", "10.1111/cgf.12888", "10.1145/1134707.1134743", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2010.183", "10.1109/vast.2009.5333919", "10.1109/tvcg.2017.2744199", "10.1111/cgf.13217", "10.1145/1060745.1060797", "10.1162/jmlr.2003.3.4-5.951", "10.1109/tvcg.2017.2723397", "10.2753/jec1086-4415170204", "10.1287/mksc.1110.0653", "10.1109/tvcg.2012.110", "10.1086/268567", "10.1109/tvcg.2017.2744738", "10.1007/978-3-540", "10.1109/tvcg.2014.2346912", "10.1109/mis.2013.36", "10.1109/tvcg.2013.173", "10.1109/tvcg.2006.76", "10.1109/tvcg.2017.2745298", "10.1559/152304009788188808"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030419", "title": "Comparative Layouts Revisited: Design Space, Guidelines, and Future Directions", "year": "2020", "conferenceName": "InfoVis", "authors": "Sehi L'Yi;Jaemin Jo;Jinwook Seo", "citationCount": "0", "affiliation": "L'Yi, S (Corresponding Author), Harvard Med Sch, Boston, MA 02115 USA. L'Yi, Sehi, Harvard Med Sch, Boston, MA 02115 USA. Jo, Jaemin, Sungkyunkwan Univ, Seoul, South Korea. Seo, Jinwook, Seoul Natl Univ, Seoul, South Korea.", "countries": "USA;Korea", "abstract": "We present a systematic review on three comparative layouts-juxtaposition, superposition, and explicit-encoding-which are information visualization (InfoVis) layouts designed to support comparison tasks. For the last decade, these layouts have served as fundamental idioms in designing many visualization systems. However, we found that the layouts have been used with inconsistent terms and confusion, and the lessons from previous studies are fragmented. The goal of our research is to distill the results from previous studies into a consistent and reusable framework. We review 127 research papers, including 15 papers with quantitative user studies, which employed comparative layouts. We first alleviate the ambiguous boundaries in the design space of comparative layouts by suggesting lucid terminology (e.g., chart-wise and item-wise juxtaposition). We then identify the diverse aspects of comparative layouts, such as the advantages and concerns of using each layout in the real-world scenarios and researchers' approaches to overcome the concerns. Building our knowledge on top of the initial insights gained from the Gleicher et al.'s survey [19], we elaborate on relevant empirical evidence that we distilled from our survey (e.g., the actual effectiveness of the layouts in different study settings) and identify novel facets that the original work did not cover (e.g., the familiarity of the layouts to people). Finally, we show the consistent and contradictory results on the performance of comparative layouts and offer practical implications for using the layouts by suggesting trade-offs and seven actionable guidelines.", "keywords": "Comparative layout,visual comparison,literature review,juxtaposition,superposition,explicit-encoding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030419", "refList": ["10.1109/tvcg.2013.233", "10.1111/cgf.12380", "10.1177/1473871611416549", "10.1145/2702123.2702419", "10.1109/tvcg.2014.2322363", "10.1111/cgf.12791", "10.1145/2702123.2702130", "10.1145/2702123.2702217", "10.1109/tvcg.2012.237", "10.1177/1473871613480062", "10.1109/mcg.2017.377152546", "10.1109/tvcg.2013.213", "10.1111/cgf.12369", "10.1109/tvcg.2017.2744198", "10.1145/3139295.3139309", "10.1109/tvcg.2019.2934801", "10.1109/tvcg.2013.122", "10.1109/tvcg.2017.2747545", "10.1109/tvcg.2015.2413774", "10.1037/0096-1523.24.3.719", "10.1109/tvcg.2014.2346320", "10.1109/tvcg.2007.70535", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1145/2556288.2557141", "10.1109/tvcg.2013.149", "10.1145/1165734.1165736", "10.5220/0006127502170224", "10.1109/tvcg.2017.2745298", "10.1177/1473871617692841", "10.1190/int-2017-0083.1", "10.1190/int-2014-0283.1", "10.1109/tvcg.2016.2598796", "10.1111/cgf.13401", "10.1016/j.cag.2017.05.005", "10.1177/1473871616667632", "10.1145/3025453.3025882", "10.1145/2470654.2470724", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2010.164", "10.1145/3103010.3103013", "10.1109/pacificvis.2016.7465266", "10.1109/pacificvis.2012.6183556", "10.1109/tvcg.2015.2467751", "10.1109/tvcg.2018.2796557", "10.1111/cgf.13531", "10.1109/tvcg.2013.161", "10.1109/iv.2018.00051", "10.1109/tvcg.2010.162", "10.1109/tvcg.2018.2864510", "10.1109/iv.2017.30", "10.1109/tvcg.2007.70623"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 19}, {"doi": "10.1109/tvcg.2018.2865139", "title": "Structure-Based Suggestive Exploration: A New Approach for Effective Exploration of Large Networks", "year": "2018", "conferenceName": "InfoVis", "authors": "Wei Chen;Fangzhou Guo;Dongming Han;Jacheng Pan;Xiaotao Nie;Jiazhi Xia;Xiaolong Zhang", "citationCount": "5", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Labo CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Chen, Wei; Guo, Fangzhou; Han, Dongming; Pan, Jacheng; Nie, Xiaotao, Zhejiang Univ, State Key Labo CAD \\& CG, Hangzhou, Zhejiang, Peoples R China. Xia, Jiazhi, Cent S Univ, Changsha, Hunan, Peoples R China. Zhang, Xiaolong, Penn State Univ, University Pk, PA 16802 USA.", "countries": "USA;China", "abstract": "When analyzing a visualized network, users need to explore different sections of the network to gain insight. However, effective exploration of large networks is often a challenge. While various tools are available for users to explore the global and local features of a network, these tools usually require significant interaction activities, such as repetitive navigation actions to follow network nodes and edges. In this paper, we propose a structure-based suggestive exploration approach to support effective exploration of large networks by suggesting appropriate structures upon user request. Encoding nodes with vectorized representations by transforming information of surrounding structures of nodes into a high dimensional space, our approach can identify similar structures within a large network, enable user interaction with multiple similar structures simultaneously, and guide the exploration of unexplored structures. We develop a web-based visual exploration system to incorporate this suggestive exploration approach and compare performances of our approach under different vectorizing methods and networks. We also present the usability and effectiveness of our approach through a controlled user study with two datasets.", "keywords": "Large Network Exploration,Structure-Based Exploration,Suggestive Exploration", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865139", "refList": ["10.1109/tvcg.2007.70582", "10.1109/tvcg.2006.120", "10.1109/mc.2013.242", "10.1109/tvcg.2009.108", "10.1145/2939672.2939754", "10.1016/j.comnet.2011.08.019", "10.1145/2702123.2702476", "10.1109/tvcg.2017.2744938", "10.1007/s00371-013-0892-3", "10.1137/1.9781611974973.67", "10.1109/vast.2009.5333893", "10.1109/tst.2013.6509098", "10.1109/35021bigcomp.2015.7072812", "10.1145/2909132.2909246", "10.1111/j.1467-8659.2011.01957.x", "10.1111/j.1467-8659.2011.01898.x", "10.1177/1473871612455749", "10.1109/tvcg.2013.167", "10.1145/3097983.3098061", "10.1007/978-1-4613-0303-9\\_28", "10.1109/vast.2014.7042485", "10.1109/tmm.2016.2614220", "10.1145/1376616.1376675", "10.1145/2623330.2623732", "10.14778/1920841.1920887", "10.1109/tvcg.2017.2745219", "10.1109/tvcg.2017.2743858", "10.1109/tvcg.2008.151", "10.1145/1150402.1150479", "10.1093/bioinformatics/bth436", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2016.2598958", "10.1111/cgf.12883", "10.1145/1556262.1556300", "10.1109/icdm.2012.159", "10.1145/2470654.2466444", "10.1109/tvcg.2013.109", "10.1109/infvis.2004.1", "10.1109/icdmw.2008.99", "10.1002/aris.1440370106", "10.1007/978-3-319-05813-9\\_11", "10.1371/journal.pone.0098679", "10.1109/tvcg.2006.106", "10.1111/j.1467-8659.2011.01935.x", "10.1111/cgf.12397", "10.1111/cgf.13184", "10.1111/cgf.12642", "10.1109/tvcg.2016.2598831", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934396", "title": "A Deep Generative Model for Graph Layout", "year": "2019", "conferenceName": "InfoVis", "authors": "Oh-Hyun Kwon;Kwan-Liu Ma", "citationCount": "4", "affiliation": "Kwon, OH (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Kwon, Oh-Hyun; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "Different layouts can characterize different aspects of the same graph. Finding a \u201cgood\u201d layout of a graph is thus an important task for graph visualization. In practice, users often visualize a graph in multiple layouts by using different methods and varying parameter settings until they find a layout that best suits the purpose of the visualization. However, this trial-and-error process is often haphazard and time-consuming. To provide users with an intuitive way to navigate the layout design space, we present a technique to systematically visualize a graph in diverse layouts using deep generative models. We design an encoder-decoder architecture to learn a model from a collection of example layouts, where the encoder represents training examples in a latent space and the decoder produces layouts from the latent space. In particular, we train the model to construct a two-dimensional latent space for users to easily explore and generate various layouts. We demonstrate our approach through quantitative and qualitative evaluations of the generated layouts. The results of our evaluations show that our model is capable of learning and generalizing abstract concepts of graph layouts, not just memorizing the training examples. In summary, this paper presents a fundamentally new approach to graph visualization where a machine learning model learns to visualize a graph from examples without manually-defined heuristics.", "keywords": "Graph,network,visualization,layout,machine learning,deep learning,neural network,generative model,autoencoder", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934396", "refList": ["10.1103/physreve.74.036104", "10.1145/234535.234538", "10.2307/2412323", "10.1109/tvcg.2007.70580", "10.7155/jgaa.00405", "10.1177/1473871612455749", "10.1109/tvcg.2011.185", "10.1073/pnas.122653799", "10.1007/3-540-58950-3", "10.1145/2897824.2925974", "10.1007/3-540-44541-2\\_17", "10.1007/bf00410640", "10.1021/acscentsci.7b00572", "10.1007/s10208-011-9093-5", "10.1109/tvcg.2015.2467451", "10.1016/0020-0190(89)90102-6", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13187", "10.1214/aoms/1177729586", "10.1109/tvcg.2014.2346277", "10.1109/tvcg.2017.2743858", "10.1007/978-3-662-44043-8\\_3", "10.1038/30918", "10.1109/mcg.2018.2881501", "10.1016/j.camwa.2004.08.015", "10.1109/tvcg.2010.269", "10.1006/s1045-926x(02)00016-2", "10.1016/0925-7721(94)00014-x", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1103/physrevx.4.011047", "10.1371/journal.pone.0098679", "10.1145/2487788.2488173", "10.1007/978-3-030-01418-6\\_41", "10.1007/978-3-030-04414-5\\_12", "10.1109/tvcg.2018.2865139", "10.1142/s0219525903001067", "10.7155/jgaa.00051"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030367", "title": "Lyra 2: Designing Interactive Visualizations by Demonstration", "year": "2020", "conferenceName": "InfoVis", "authors": "Jonathan Zong;Dhiraj Barnwal;Rupayan Neogy;Arvind Satyanarayan", "citationCount": "0", "affiliation": "Zong, J (Corresponding Author), MIT, Cambridge, MA 02139 USA. Zong, Jonathan; Neogy, Rupayan; Satyanarayan, Arvind, MIT, Cambridge, MA 02139 USA. Barnwal, Dhiraj, Indian Inst Technol Kharagpur, Kharagpur, W Bengal, India.", "countries": "India;USA", "abstract": "Recent graphical interfaces offer direct manipulation mechanisms for authoring visualizations, but are largely restricted to static output. To author interactive visualizations, users must instead turn to textual specification, but such approaches impose a higher technical burden. To bridge this gap, we introduce Lyra 2, a system that extends a prior visualization design environment with novel methods for authoring interaction techniques by demonstration. Users perform an interaction (e.g., button clicks, drags, or key presses) directly on the visualization they are editing. The system interprets this performance using a set of heuristics and enumerates suggestions of possible interaction designs. These heuristics account for the properties of the interaction (e.g., target and event type) as well as the visualization (e.g., mark and scale types, and multiple views). Interaction design suggestions are displayed as thumbnails; users can preview and test these suggestions, iteratively refine them through additional demonstrations, and finally apply and customize them via property inspectors. We evaluate our approach through a gallery of diverse examples, and evaluate its usability through a first-use study and via an analysis of its cognitive dimensions. We find that, in Lyra 2, interaction design by demonstration enables users to rapidly express a wide range of interactive visualizations.", "keywords": "Direct manipulation,interactive visualization,interaction design by demonstration", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030367", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030467", "title": "PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch;Lingyun Yu;Peiran Ren;Thomas Ertl;Yingcai Wu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Zhejiang Lab, Hangzhou, Peoples R China. Tang, Tan; Li, Renzhong; Wu, Xinke; Liu, Shuhan; Wu, Yingcai, Zhejiang Univ, Stare Key Lab CAD\\&CG, Hangzhou, Peoples R China. Knittel, Johannes; Koch, Steffen; Ertl, Thomas, Univ Stuttgart, VIS VISUS, Stuttgart, Germany. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Dept Comp Sci \\& Software Engn, Suzhou, Peoples R China. Ren, Peiran, Alibaba Grp, Hangzhou, Peoples R China.", "countries": "Germany;China", "abstract": "Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.", "keywords": "Storyline visualization,reinforcement learning,mixed-initiative design", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030467", "refList": ["10.1109/tvcg.2019.2934396", "10.1613/jair.301", "10.1016/j.automatica.2009.07.008", "10.1016/j.visinf.2018.12.001", "10.1016/j.neucom.2007.11.026", "10.1109/tvcg.2015.2392771", "10.1109/tvcg.2019.2934798", "10.1613/jair.3912", "10.1109/tvcg.2012.212", "10.1109/tvcg.2018.2816203", "10.1111/cgf.13193", "10.1109/21.87055", "10.1109/tvcg.2018.2864899", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2013.196", "10.1145/302979.303030", "10.1109/tvcg.2013.191", "10.1007/978-3-642-36955-1\\_16", "10.1109/vast.2017.8585487", "10.1109/cvpr.2016.90", "10.1038/nature14236", "10.1145/568522.568523", "10.1016/j.neunet.2014.09.003", "10.1016/j.visinf.2018.04.011", "10.1109/iccv.2019.00880", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030459", "title": "Scalability of Network Visualisation from a Cognitive Load Perspective", "year": "2020", "conferenceName": "InfoVis", "authors": "Vahan Yoghourdjian;Yalong Yang;Tim Dwyer;Lawrence Lee;Michael Wybrow;Kim Marriott", "citationCount": "0", "affiliation": "Yoghourdjian, V (Corresponding Author), Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Yoghourdjian, Vahan; Yang, Yalong; Dwyer, Tim; Wybrow, Michael; Marriott, Kim, Monash Univ, Fac Informat Technol, Dept Human Ctr Comp, Melbourne, Vic, Australia. Lawrence, Lee, Monash Univ, Fac Business \\& Econ, Melbourne, Vic, Australia. Yang, Yalong, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA.", "countries": "USA;Australia", "abstract": "Node-link diagrams are widely used to visualise networks. However, even the best network layout algorithms ultimately result in \u2018hairball\u2019 visualisations when the graph reaches a certain degree of complexity, requiring simplification through aggregation or interaction (such as filtering) to remain usable. Until now, there has been little data to indicate at what level of complexity node-link diagrams become ineffective or how visual complexity affects cognitive load. To this end, we conducted a controlled study to understand workload limits for a task that requires a detailed understanding of the network topology-finding the shortest path between two nodes. We tested performance on graphs with 25 to 175 nodes with varying density. We collected performance measures (accuracy and response time), subjective feedback, and physiological measures (EEG, pupil dilation, and heart rate variability). To the best of our knowledge this is the first network visualisation study to include physiological measures. Our results show that people have significant difficulty finding the shortest path in high density node-link diagrams with more than 50 nodes and even low density graphs with more than 100 nodes. From our collected EEG data we observe functional differences in brain activity between hard and easy tasks. We found that cognitive load increased up to certain level of difficulty after which it decreased, likely because participants had given up. We also explored the effects of global network layout features such as size or number of crossings, and features of the shortest path such as length or straightness on task difficulty. We found that global features generally had a greater impact than those of the shortest path.", "keywords": "Data Visualisation,Network Visualisation,Cognitive Load,EEG", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030459", "refList": ["10.1109/tvcg.2019.2934396", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2016.2570755", "10.1007/s00371-013-0892-3", "10.1007/b98835", "10.1109/isda.2014.7066252", "10.1109/tvcg.2015.2467251", "10.1177/1473871612455749", "10.1109/tvcg.2012.299", "10.1007/3-540-58950-3", "10.1111/cgf.12878", "10.1109/mcse.2007.55", "10.1109/tvcg.2012.238", "10.1145/264645.264657", "10.1109/tvcg.2015.2467451", "10.1109/tvcg.2013.151", "10.1016/0020-0190(89)90102-6", "10.1109/tvcg.2019.2934307", "10.1109/tvcg.2015.2468151", "10.1109/tvcg.2017.2745919", "10.3402/qhw.v6i2.5918", "10.1111/cgf.13440", "10.1109/tvcg.2011.220", "10.1111/cgf.13187", "10.1109/t-c.1969.222678", "10.1109/tvcg.2017.2743858", "10.1126/science.290.5500.2319", "10.1109/cahpc.2018.8645912", "10.1109/tvcg.2015.2465151", "10.1109/tvcg.2016.2598958", "10.1109/tvcg.2017.2751473", "10.1002/spe.4380211102", "10.1109/tpds.2018.2869805", "10.1016/j.jpdc.2019.04.008", "10.1109/pacificvis.2017.8031574", "10.1006/s1045-926x(02)00016-2", "10.1109/tvcg.2017.2674999", "10.1145/2872427.2883041", "10.1145/3292500.3330989", "10.1109/tvcg.2017.2744878", "10.1145/2049662.2049670", "10.1145/2049662.2049663", "10.1109/sbac-pad.2018.00060", "10.1007/978-3-662-45803-7\\_27", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1111/j.1469-1809.1936.tb02137.x", "10.1007/bf02289565", "10.1109/tvcg.2017.2689016", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.4722", "year": "2020", "title": "A Study of Mental Maps in Immersive Network Visualization", "conferenceName": "PacificVis", "authors": "Joseph Kotlarek;Oh{-}Hyun Kwon;Kwan{-}Liu Ma;Peter Eades;Andreas Kerren;Karsten Klein;Falk Schreiber", "citationCount": "0", "affiliation": "Kotlarek, J (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA.\nKotlarek, Joseph; Kwon, Oh-Hyun; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.\nEades, Peter, Univ Sydney, Sydney, NSW, Australia.\nKerren, Andreas, Linnaeus Univ, Vaxjo, Sweden.\nKlein, Karsten; Schreiber, Falk, Univ Konstanz, Constance, Germany.", "countries": "Sweden;Germany;USA;Australia", "abstract": "The visualization of a network influences the quality of the mental map that the viewer develops to understand the network. In this study, we investigate the effects of a 3D immersive visualization environment compared to a traditional 2D desktop environment on the comprehension of a network's structure. We compare the two visualization environments using three tasks-interpreting network structure, memorizing a set of nodes, and identifying the structural changes-commonly used for evaluating the quality of a mental map in network visualization. The results show that participants were able to interpret network structure more accurately when viewing the network in an immersive environment, particularly for larger networks. However, we found that 2D visualizations performed better than immersive visualization for tasks that required spatial memory.", "keywords": "Human-centered computing; Visualization; Visualization techniques; Graph drawings; Human-centered computing; Visualization; Empirical studies in visualization", "link": "https://doi.org/10.1109/PacificVis48177.2020.4722", "refList": ["10.1103/physreve.74.036104", "10.1109/tvcg.2019.2934396", "10.1007/978-3-540-87730-1\\_9", "10.1117/12.2005484", "10.1177/1473871612455749", "10.1109/pacificvis.2017.8031577", "10.1007/978-3-319-73207-7", "10.1109/38.888006", "10.1109/2945.841119", "10.1109/mc.2005.297", "10.1007/s10055-018-0346-3", "10.1109/tvcg.2016.2599107", "10.1109/icsmc.1992.271688", "10.1038/30918", "10.1006/jvlc.1995.1010", "10.1089/109493101300117938", "10.1109/vrais.1998.658488", "10.1109/pacificvis.2015.7156357", "10.1109/tvcg.2010.78", "10.1109/tvcg.2016.2520921", "10.1007/978-3-030-01388-22", "10.1145/229459.229467", "10.1145/1056808.1056875", "10.1109/tvcg.2017.2744079", "10.1109/bdva.2015.7314293", "10.1086/jar.33.4.3629752", "10.1016/j.ijhcs.2013.08.004"], "wos": 1, "children": [], "len": 1}], "len": 11}, {"doi": "10.1109/tvcg.2020.3030440", "title": "Context-aware Sampling of Large Networks via Graph Representation Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Zhiguang Zhou;Chen Shi;Xilong Shen;Lihong Cai;Haoxuan Wang;Yuhua Liu;Ying Zhao;Wei Chen", "citationCount": "0", "affiliation": "Zhao, Y (Corresponding Author), Cent South Univ, Changsha, Peoples R China. Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Zhou, Zhiguang; Shi, Chen; Shen, Xilong; Cai, Lihong; Wang, Haoxuan; Liu, Yuhua, Zhejiang Univ Finance \\& Econ, Sch Informat, Hangzhou, Peoples R China. Zhao, Ying, Cent South Univ, Changsha, Peoples R China. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China.", "countries": "China", "abstract": "Numerous sampling strategies have been proposed to simplify large-scale networks for highly readable visualizations. It is of great challenge to preserve contextual structures formed by nodes and edges with tight relationships in a sampled graph, because they are easily overlooked during the process of sampling due to their irregular distribution and immunity to scale. In this paper, a new graph sampling method is proposed oriented to the preservation of contextual structures. We first utilize a graph representation learning (GRL) model to transform nodes into vectors so that the contextual structures in a network can be effectively extracted and organized. Then, we propose a multi-objective blue noise sampling model to select a subset of nodes in the vectorized space to preserve contextual structures with the retention of relative data and cluster densities in addition to those features of significance, such as bridging nodes and graph connections. We also design a set of visual interfaces enabling users to interactively conduct context-aware sampling, visually compare results with various sampling strategies, and deeply explore large networks. Case studies and quantitative comparisons based on real-world datasets have demonstrated the effectiveness of our method in the abstraction and exploration of large networks.", "keywords": "Graph sampling,Graph representation learning,Blue noise sampling,Graph evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030440", "refList": ["10.1145/2491159.2491168", "10.1016/j.physa.2015.04.035", "10.1145/1830252.1830274", "10.1109/icdmw.2007.91", "10.1002/net.21834", "10.1109/tvcg.2018.2864503", "10.1016/j.cag.2018.01.010", "10.1109/icc.2016.7511156", "10.1111/cgf.13444", "10.1145/956750.956831", "10.1145/364099.364331", "10.1007/s00180-016-0663-5", "10.1109/tvcg.2013.223", "10.1007/s12650-018-0530-2", "10.1103/physreve.73.016102", "10.1109/access.2018.2870684", "10.1007/978-3-319-06793-3\\_1", "10.2312/vissym/eurovis05/239-246", "10.1016/j.ins.2015.02.014", "10.1145/2339530.2339723", "10.1109/icde.2015.7113345", "10.1109/tvcg.2011.233", "10.14778/2809974.2809980", "10.1109/glocom.2015.7417471", "10.1145/2578153.2578175", "10.1214/aoms/1177705148", "10.1109/tvcg.2008.130", "10.14232/actacyb.20.1.2011.6", "10.1504/ijitm.2019.099809", "10.1109/tvcg.2018.2865020", "10.1145/956750", "10.1002/cpe.4330060203", "10.1145/1150402.1150479", "10.1103/physreve.72.036118", "10.1109/tvcg.2017.2744098", "10.1145/2020408.2020512", "10.1142/s0129183114400075", "10.1109/jsac.2011.111005", "10.1016/j.camwa.2011.11.057", "10.1145/2470654.2466444", "10.1109/tvcg.2017.2674999", "10.1214/aos/1013203451", "10.1109/icdcsw.2011.34", "10.1016/j.physa.2013.11.015", "10.1145/1081870.1081893", "10.1109/tnet.2008.2001730", "10.1109/access.2016.2633485", "10.1145/1879141.1879192", "10.1371/journal.pone.0098679", "10.1126/science.220.4598.671", "10.1109/pacificvis.2015.7156355", "10.1088/1475-7516/2011/08/011", "10.1007/978-3-319-27261-0\\_41", "10.1111/cgf.13410", "10.1109/tvcg.2018.2865139", "10.1109/tvcg.2016.2598831", "10.1016/j.physa.2014.06.065"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030447", "title": "DRGraph: An Efficient Graph Layout Algorithm for Large-scale Graphs by Dimensionality Reduction", "year": "2020", "conferenceName": "InfoVis", "authors": "Minfeng Zhu;Wei Chen;Yuanzhe Hu;Yuxuan Hou;Liangjun Liu;Kaiyuan Zhang", "citationCount": "0", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Zhu, Minfeng; Chen, Wei; Hu, Yuanzhe; Hou, Yuxuan; Liu, Liangjun; Zhang, Kaiyuan, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China.", "countries": "China", "abstract": "Efficient layout of large-scale graphs remains a challenging problem: the force-directed and dimensionality reduction-based methods suffer from high overhead for graph distance and gradient computation. In this paper, we present a new graph layout algorithm, called DRGraph, that enhances the nonlinear dimensionality reduction process with three schemes: approximating graph distances by means of a sparse distance matrix, estimating the gradient by using the negative sampling technique, and accelerating the optimization process through a multi-level layout scheme. DRGraph achieves a linear complexity for the computation and memory consumption, and scales up to large-scale graphs with millions of nodes. Experimental results and comparisons with state-of-the-art graph layout methods demonstrate that DRGraph can generate visually comparable layouts with a faster running time and a lower memory requirement.", "keywords": "graph visualization,graph layout,dimensionality reduction,force-directed layout", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030447", "refList": ["10.1109/tvcg.2007.70582", "10.1371/journal.pone.0136497", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/3018661.3018731", "10.1109/34.491619", "10.1145/2939672.2939754", "10.1145/3269206.3271788", "10.1016/j.comnet.2011.08.019", "10.1007/s11263-011-0442-2", "10.1016/s0020-0190(98)00108-2", "10.1109/tvcg.2018.2865151", "10.1007/978-3-319-61188-4\\_2", "10.1177/1473871612455749", "10.1177/1473871616666394", "10.1109/tvcg.2015.2467035", "10.1186/1471-2105-10-375", "10.1145/3097983.3098061", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1109/tvcg.2015.2467451", "10.1109/2945.841119", "10.1016/j.swevo.2015.10.002", "10.1016/0020-0190(89)90102-6", "10.1109/infvis.2003.1249009", "10.1109/tvcg.2017.2745919", "10.1111/cgf.13187", "10.1111/cgf.13440", "10.1109/tvcg.2012.245", "10.1109/tvcg.2017.2743858", "10.1177/1473871618821740", "10.1186/s12859-015-0585-1", "10.1002/nav.3800020109", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1006/s1045-926x(02)00016-2", "10.1109/cvpr.2012.6247667", "10.1023/b:jogo.0000042115.44455.f3", "10.1109/pacificvis.2017.8031607", "10.1002/nav.3800030404", "10.1109/cvpr.2008.4587500", "10.1109/pacificvis.2011.5742389", "10.1371/journal.pone.0098679", "10.1090/s0002-9904-1920-03322-7", "10.1109/iv.2013.3", "10.1145/568522.568523", "10.1109/tvcg.2006.156", "10.1109/tvcg.2012.236", "10.1109/tvcg.2018.2865139", "10.1145/3219819.3220025", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030393", "title": "Exemplar-based Layout Fine-tuning for Node-link Diagrams", "year": "2020", "conferenceName": "InfoVis", "authors": "Jiacheng Pan;Wei Chen;Xiaodong Zhao;Shuyue Zhou;Wei Zeng;Minfeng Zhu;Jian Chen;Siwei Fu;Yingcai Wu", "citationCount": "1", "affiliation": "Chen, W; Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Wu, YC (Corresponding Author), Zhejiang Lab, Hangzhou, Peoples R China. Pan, Jiacheng; Chen, Wei; Zhao, Xiaodong; Zhou, Shuyue; Zhu, Minfeng; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Fu, Siwei; Wu, Yingcai, Zhejiang Lab, Hangzhou, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Jian, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA;China", "abstract": "We design and evaluate a novel layout fine-tuning technique for node-link diagrams that facilitates exemplar-based adjustment of a group of substructures in batching mode. The key idea is to transfer user modifications on a local substructure to other substructures in the entire graph that are topologically similar to the exemplar. We first precompute a canonical representation for each substructure with node embedding techniques and then use it for on-the-fly substructure retrieval. We design and develop a light-weight interactive system to enable intuitive adjustment, modification transfer, and visual graph exploration. We also report some results of quantitative comparisons, three case studies, and a within-participant user study.", "keywords": "Node-link diagram,graph layout,graph visualization,user interactions", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030393", "refList": ["10.1109/tvcg.2007.70582", "10.1371/journal.pone.0136497", "10.1109/tvcg.2016.2598867", "10.1145/234535.234538", "10.1145/3018661.3018731", "10.1109/34.491619", "10.1145/2939672.2939754", "10.1145/3269206.3271788", "10.1016/j.comnet.2011.08.019", "10.1007/s11263-011-0442-2", "10.1016/s0020-0190(98)00108-2", "10.1109/tvcg.2018.2865151", "10.1177/1473871612455749", "10.1177/1473871616666394", "10.1109/tvcg.2015.2467035", "10.1186/1471-2105-10-375", "10.1145/3097983.3098061", "10.1109/tvcg.2018.2864911", "10.1145/3025453.3025628", "10.1109/tvcg.2015.2467451", "10.1016/j.swevo.2015.10.002", "10.1016/0020-0190(89)90102-6", "10.1109/infvis.2003.1249009", "10.1109/tvcg.2017.2745919", "10.1111/cgf.13187", "10.1111/cgf.13440", "10.1109/tvcg.2012.245", "10.1109/tvcg.2017.2743858", "10.1177/1473871618821740", "10.1186/s12859-015-0585-1", "10.1002/nav.3800020109", "10.1145/263407.263521", "10.1002/spe.4380211102", "10.1006/s1045-926x(02)00016-2", "10.1109/cvpr.2012.6247667", "10.1023/b:jogo.0000042115.44455.f3", "10.1109/pacificvis.2017.8031607", "10.1002/nav.3800030404", "10.1109/infvis.2004.1", "10.1109/cvpr.2008.4587500", "10.1109/pacificvis.2011.5742389", "10.1140/epjb/e2011-10979-2", "10.1371/journal.pone.0098679", "10.1090/s0002-9904-1920-03322-7", "10.1109/iv.2013.3", "10.1145/568522.568523", "10.1109/tvcg.2006.156", "10.1109/tvcg.2012.236", "10.1109/tvcg.2018.2865139", "10.1145/3219819.3220025", "10.1007/3-540-63938-1\\_"], "wos": 1, "children": [], "len": 1}], "len": 19}, {"doi": "10.1109/tvcg.2018.2865149", "title": "Juniper: A Tree+Table Approach to Multivariate Graph Visualization", "year": "2018", "conferenceName": "InfoVis", "authors": "Carolina Nobre;Marc Streit;Alexander Lex", "citationCount": "7", "affiliation": "Nobre, C (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Nobre, Carolina; Lex, Alexander, Univ Utah, Salt Lake City, UT 84112 USA. Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.", "countries": "USA;Austria", "abstract": "Analyzing large, multivariate graphs is an important problem in many domains, yet such graphs are challenging to visualize. In this paper, we introduce a novel, scalable, tree-table multivariate graph visualization technique, which makes many tasks related to multivariate graph analysis easier to achieve. The core principle we follow is to selectively query for nodes or subgraphs of interest and visualize these subgraphs as a spanning tree of the graph. The tree is laid out linearly, which enables us to juxtapose the nodes with a table visualization where diverse attributes can be shown. We also use this table as an adjacency matrix, so that the resulting technique is a hybrid node-link/adjacency matrix technique. We implement this concept in Juniper and complement it with a set of interaction techniques that enable analysts to dynamically grow, restructure, and aggregate the tree, as well as change the layout or show paths between nodes. We demonstrate the utility of our tool in usage scenarios for different multivariate networks: a bipartite network of scholars, papers, and citation metrics and a multitype network of story characters, places, books, etc.", "keywords": "Multivariate graphs,networks,tree-based graph visualization,adjacency matrix,spanning trees,visualization", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865149", "refList": ["10.1109/tvcg.2007.70582", "10.1109/infvis.2000.885091", "10.1109/tvcg.2016.2615308", "10.1109/tvcg.2008.117", "10.1109/tvcg.2009.108", "10.1109/pacificvis.2013.6596127", "10.1111/j.1467-8659.2011.01898.x", "10.1109/visual.1991.175815", "10.1093/bioinformatics/btp454", "10.1093/bioinformatics/btq675", "10.1109/tvcg.2011.247", "10.1111/j.1467-8659.2012.03110.x", "10.1145/2207676.2208293", "10.1056/nejmsa066082", "10.1109/infvis.2003.1249009", "10.1038/nmeth.1436", "10.1073/pnas.95.25.14863", "10.1109/cw.2002.1180907", "10.1109/tvcg.2014.2346248", "10.1111/j.1467-8659.2009.01710.x", "10.1109/tvcg.2006.147", "10.1109/tvcg.2015.2468078", "10.1093/bioinformatics/btx324", "10.1111/cgf.12883", "10.1145/1168149.1168168", "10.1109/pacificvis.2010.5429609", "10.1093/bioinformatics/btn068", "10.1109/tvcg.2006.106", "10.2307/2685881", "10.1057/palgrave.ivs.9500092", "10.1109/tvcg.2016.2598885", "10.1111/j.1467-8659.2009.01687.x", "10.1109/tvcg.2018.2811488", "10.1145/22339.22342", "10.1111/cgf.13184", "10.1111/cgf.12642", "10.1109/biovis.2012.6378600", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934670", "title": "AirVis: Visual Analytics of Air Pollution Propagation", "year": "2019", "conferenceName": "VAST", "authors": "Zikun Deng;Di Weng;Jiahui Chen;Ren Liu;Zhibin Wang;Jie Bao 0003;Yu Zheng 0004;Yingcai Wu", "citationCount": "6", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Deng, Zikun; Weng, Di; Chen, Jiahui; Liu, Ren; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Wang, Zhibin, Zhejiang Univ, Res Ctr Air Pollut \\& Hlth, Hangzhou, Peoples R China. Bao, Jie; Zheng, Yu, JD Intelligent City Res, Beijing, Peoples R China.", "countries": "China", "abstract": "Air pollution has become a serious public health problem for many cities around the world. To find the causes of air pollution, the propagation processes of air pollutants must be studied at a large spatial scale. However, the complex and dynamic wind fields lead to highly uncertain pollutant transportation. The state-of-the-art data mining approaches cannot fully support the extensive analysis of such uncertain spatiotemporal propagation processes across multiple districts without the integration of domain knowledge. The limitation of these automated approaches motivates us to design and develop AirVis, a novel visual analytics system that assists domain experts in efficiently capturing and interpreting the uncertain propagation patterns of air pollution based on graph visualizations. Designing such a system poses three challenges: a) the extraction of propagation patterns; b) the scalability of pattern presentations; and c) the analysis of propagation processes. To address these challenges, we develop a novel pattern mining framework to model pollutant transportation and extract frequent propagation patterns efficiently from large-scale atmospheric data. Furthermore, we organize the extracted patterns hierarchically based on the minimum description length (MDL) principle and empower expert users to explore and analyze these patterns effectively on the basis of pattern topologies. We demonstrated the effectiveness of our approach through two case studies conducted with a real-world dataset and positive feedback from domain experts.", "keywords": "Air pollution propagation,pattern mining,graph visualization", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934670", "refList": ["10.1109/tvcg.2016.2598919", "10.1093/bib/bbr069", "10.1145/2487575.2488188", "10.1109/tvcg.2013.193", "10.1016/j.atmosenv.2014.12.011", "10.1109/tvcg.2013.226", "10.1109/tvcg.2018.2864503", "10.1109/tvcg.2015.2468111", "10.1109/icicta.2015.183", "10.1016/j.atmosenv.2014.05.039", "10.1111/cgf.12791", "10.1111/j.1467-8659.2009.01451.x", "10.1111/j.1467-8659.2011.01898.x", "10.5194/acp-12-5031-2012", "10.1016/j.atmosenv.2008.05.053", "10.1109/tvcg.2016.2535234", "10.1016/j.atmosres.2014.12.003", "10.1109/tvcg.2015.2467194", "10.1109/tvcg.2013.263", "10.1109/icdm.2002.1184038", "10.1145/2783258.2788573", "10.1109/tvcg.2018.2865149", "10.1109/tbdata.2017.2723899", "10.1109/tvcg.2012.311", "10.1109/vl.1996.545307", "10.1007/s12650-018-0481-7", "10.1016/j.envpol.2007.06.012", "10.3155/1047-3289.61.6.660", "10.1080/13658810701349037", "10.1145/3097983.3098090", "10.1109/tvcg.2007.70523", "10.1115/1.2128636", "10.1109/tvcg.2015.2467619", "10.3978/j.issn.2072-1439.2016.01.19", "10.1109/tkde.2005.127", "10.1017/s0269888912000331", "10.2312/conf/eg2013/stars/039-063", "10.1109/tvcg.2014.2346271", "10.1175/bams-d-14-00110.1", "10.1016/0005-1098(78)90005-5", "10.1007/s10618-006-0044-8", "10.1109/tvcg.2018.2865126", "10.2307/1912791", "10.3390/su6085322", "10.1007/s12650-018-0489-z", "10.2312/eurovisstar.20151109", "10.1109/tvcg.2011.181", "10.1126/science.298.5594.824", "10.1162/jmlr.2003.3.4-5.951", "10.1109/asonam.2014.6921638", "10.1111/j.1467-8659.2008.01213.x", "10.1038/s41598-017-18107-1", "10.1109/tvcg.2018.2865041", "10.1109/tits.2019.2901117", "10.1038/srep20668", "10.1109/tvcg.2012.265", "10.1109/tpami.2016.2608884", "10.1109/tvcg.2012.213", "10.1145/1376616.1376661", "10.1007/s00521-019-04567-1", "10.1109/tvcg.2017.2745083", "10.1126/science.243.4892.745", "10.1109/tvcg.2018.2864826", "10.1109/tnn.2003.820440", "10.1109/tvcg.2016.2598885", "10.1145/3219819.3219822", "10.1073/pnas.1502596112", "10.1016/j.envsoft.2009.01.004", "10.1002/pmic.200700095", "10.1145/2254556.2254651", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2011.202"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028958", "title": "Auditing the Sensitivity of Graph-based Ranking with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Tiankai Xie;Yuxin Ma;Hanghang Tong;My T. Thai;Ross Maciejewski", "citationCount": "0", "affiliation": "Xie, TK (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Xie, Tiankai; Ma, Yuxin; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. Tong, Hanghang, Univ Illinois, Urbana, IL USA. Thai, My T., Univ Florida, Gainesville, FL 32611 USA.", "countries": "USA", "abstract": "Graph mining plays a pivotal role across a number of disciplines, and a variety of algorithms have been developed to answer who/what type questions. For example, what items shall we recommend to a given user on an e-commerce platform? The answers to such questions are typically returned in the form of a ranked list, and graph-based ranking methods are widely used in industrial information retrieval settings. However, these ranking algorithms have a variety of sensitivities, and even small changes in rank can lead to vast reductions in product sales and page hits. As such, there is a need for tools and methods that can help model developers and analysts explore the sensitivities of graph ranking algorithms with respect to perturbations within the graph structure. In this paper, we present a visual analytics framework for explaining and exploring the sensitivity of any graph-based ranking algorithm by performing perturbation-based what-if analysis. We demonstrate our framework through three case studies inspecting the sensitivity of two classic graph-based ranking algorithms (PageRank and HITS) as applied to rankings in political news media and social networks.", "keywords": "Graph-based ranking,sensitivity analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028958", "refList": ["10.1109/wsc.2017.8247800", "10.1023/a:1022649401552", "10.1515/1559-0410.11416", "10.1109/tvcg.2016.2598919", "10.1177/1473871611416549", "10.1109/tvcg.2019.2934630", "10.1140/epjds29", "10.1109/tvcg.2019.2934670", "10.1016/j.eswa.2015.09.004", "10.1145/2702123.2702509", "10.1016/j.visinf.2018.12.001", "10.2307/3002000", "10.1109/tvcg.2019.2934399", "10.1007/s41060-016-0032-z", "10.1111/cgf.13198", "10.14778/2350229.2350254", "10.1145/2939672.2939764", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/vast.2017.8585647", "10.1007/bf01187020", "10.1109/icdm.2015.26", "10.1145/2362383.2362387", "10.1177/0049124104268644", "10.1109/vast.2011.6102442", "10.1109/infvis.2003.1249025", "10.1109/tvcg.2018.2864475", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2015.2467691", "10.1111/cgf.13210", "10.1214/aos/1176344136", "10.1109/tvcg.2015.2424872", "10.1016/j.visinf.2018.09.001", "10.1177/089443939100900106", "10.1109/tvcg.2015.2467931", "10.1162/neco.1997.9.8.1735", "10.1007/s11162-011-9241-4", "10.1111/cgf.13680", "10.1145/3065386", "10.1109/tvcg.2018.2864889", "10.1177/003804070808100402", "10.1109/icdm.2010.62", "10.1038/s41598-020-59669-x", "10.1162/153244303321897717", "10.1109/tvcg.2019.2934619", "10.1007/bf00356088", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030410", "title": "Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction with Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Wei Zeng 0002;Chengqiao Lin;Juncong Lin;Jincheng Jiang;Jiazhi Xia;Cagatay Turkay;Wei Chen", "citationCount": "0", "affiliation": "Lin, JC (Corresponding Author), Xiamen Univ, Xiamen, Peoples R China. Zeng, Wei; Jiang, Jincheng, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Lin, Chengqiao; Lin, Juncong, Xiamen Univ, Xiamen, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Turkay, Cagatay, Univ Warwick, Coventry, W Midlands, England. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Zhejiang, Peoples R China.", "countries": "China;England", "abstract": "Deep learning methods are being increasingly used for urban traffic prediction where spatiotemporal traffic data is aggregated into sequentially organized matrices that are then fed into convolution-based residual neural networks. However, the widely known modifiable areal unit problem within such aggregation processes can lead to perturbations in the network inputs. This issue can significantly destabilize the feature embeddings and the predictions - rendering deep networks much less useful for the experts. This paper approaches this challenge by leveraging unit visualization techniques that enable the investigation of many-to-many relationships between dynamically varied multi-scalar aggregations of urban traffic data and neural network predictions. Through regular exchanges with a domain expert, we design and develop a visual analytics solution that integrates 1) a Bivariate Map equipped with an advanced bivariate colormap to simultaneously depict input traffic and prediction errors across space, 2) a Moran's I Scatterplot that provides local indicators of spatial association analysis, and 3) a Multi-scale Attribution View that arranges non-linear dot plots in a tree layout to promote model analysis and comparison across scales. We evaluate our approach through a series of case studies involving a real-world dataset of Shenzhen taxi trips, and through interviews with domain experts. We observe that geographical scale variations have important impact on prediction performances, and interactive visual exploration of dynamically varying inputs and outputs benefit experts in the development of deep traffic prediction models.", "keywords": "MAUP,traffic prediction,deep learning,model diagnostic,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030410", "refList": ["10.1038/srep26377", "10.1109/mcg.2011.88", "10.1080/13658816.2015.1119279", "10.1109/tvcg.2013.226", "10.1109/pacificvis.2011.5742387", "10.1038/s41467-017-01882-w", "10.1109/tvcg.2019.2934670", "10.1111/j.1467-8659.2009.01440.x", "10.1111/cgf.13712", "10.1016/j.compenvurbsys.2008.09.006", "10.1109/pacificvis.2014.50", "10.1109/tvcg.2018.2816219", "10.1109/tvcg.2016.2535234", "10.1109/tvcg.2014.2346893", "10.3390/ijgi8080344", "10.1109/tvcg.2013.246", "10.1007/s10940-005-9003-6", "10.1016/j.compenvurbsys.2008.05.001", "10.1007/s10661-019-7831-3", "10.1111/j.1538-4632.2007.00699.x", "10.1016/j.aap.2016.08.015", "10.1080/13658816.2018.1541177", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2011.181", "10.1137/090759069", "10.1109/pacificvis.2011.5742390", "10.1214/10-aos799", "10.1109/tits.2017.2683539", "10.1109/tits.2015.2436897", "10.3390/ijerph16071150", "10.1109/tvcg.2009.145", "10.1109/tvcg.2012.265", "10.1080/10106049.2017.1404140", "10.3390/ijgi8020063", "10.3390/info6020134", "10.1080/13658816.2014.955027", "10.1109/tits.2016.2639320", "10.2307/143141", "10.1109/tvcg.2016.2598432"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030370", "title": "TaxThemis: Interactive Mining and Exploration of Suspicious Tax Evasion Groups", "year": "2020", "conferenceName": "VAST", "authors": "Yating Lin;Kamkwai Wong;Yong Wang;Rong Zhang;Bo Dong;Huamin Qu;Qinghua Zheng", "citationCount": "0", "affiliation": "Lin, YT (Corresponding Author), Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian, Shaanxi, Peoples R China. Lin, Yating; Zheng, Qinghua, Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian, Shaanxi, Peoples R China. Wong, Kamkwai; Wang, Yong; Zhang, Rong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Dong, Bo, Xi An Jiao Tong Univ, Natl Engn Lab Big Data Analyt, Xian, Shaanxi, Peoples R China.", "countries": "China", "abstract": "Tax evasion is a serious economic problem for many countries, as it can undermine the government's tax system and lead to an unfair business competition environment. Recent research has applied data analytics techniques to analyze and detect tax evasion behaviors of individual taxpayers. However, they have failed to support the analysis and exploration of the related party transaction tax evasion (RPTTE) behaviors (e.g., transfer pricing), where a group of taxpayers is involved. In this paper, we present TaxThemis, an interactive visual analytics system to help tax officers mine and explore suspicious tax evasion groups through analyzing heterogeneous tax-related data. A taxpayer network is constructed and fused with the respective trade network to detect suspicious RPTTE groups. Rich visualizations are designed to facilitate the exploration and investigation of suspicious transactions between related taxpayers with profit and topological data analysis. Specifically, we propose a calendar heatmap with a carefully-designed encoding scheme to intuitively show the evidence of transferring revenue through related party transactions. We demonstrate the usefulness and effectiveness of TaxThemis through two case studies on real-world tax-related data and interviews with domain experts.", "keywords": "Visual Analytics,Tax Network,Tax Evasion Detection,Anomaly detection,Multidimensional data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030370", "refList": ["10.1111/cgf.12886", "10.2307/2277827", "10.1109/tvcg.2010.44", "10.1109/tits.2014.2315794", "10.1109/tvcg.2019.2934670", "10.1038/s41467-019-08987-4", "10.1111/cgf.12920", "10.1109/vast.2017.8585721", "10.1080/15230406.2015.1093431", "10.1109/tvcg.2018.2843369", "10.1038/srep01001", "10.1109/tvcg.2017.2744018", "10.1109/tvcg.2017.2744159", "10.1068/b130199p", "10.1109/tvcg.2009.143", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2019.2892483", "10.1109/pacificvis.2017.8031583", "10.1109/pacificvis48177.2020.2785", "10.2307/2686111", "10.1109/tvcg.2015.2467199", "10.1111/cgf.12114", "10.1109/tvcg.2018.2865126", "10.1111/j.1538-4632.1996.tb00936.x", "10.1109/tvcg.2019.2934619", "10.2307/2332142", "10.1007/978-3-319-10590-1\\_53", "10.1109/cvpr.2016.485", "10.1109/cvpr.2017.17", "10.2307/2986645", "10.1109/tvcg.2014.2346321", "10.1017/s0140525x16001837", "10.1109/tvcg.2016.2598541", "10.1371/journal.pone.0207377", "10.1109/tvcg.2014.2346265", "10.1007/s4095-020-0191-7", "10.3141/1644-14", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2016.90", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2785807", "10.1109/tvcg.2017.2744358", "10.1111/j.1538-4632.1995.tb00338.x", "10.1080/03081068808717359", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030469", "title": "Topology Density Map for Urban Data Visualization and Analysis", "year": "2020", "conferenceName": "VAST", "authors": "Zezheng Feng;Haotian Li;Wei Zeng 0004;Shuang-Hua Yang;Huamin Qu", "citationCount": "0", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Feng, Zezheng; Li, Haotian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yang, Shuang-Hua, Southern Univ Sci \\& Technol, Shenzhen, Peoples R China.", "countries": "China", "abstract": "Density map is an effective visualization technique for depicting the scalar field distribution in 2D space. Conventional methods for constructing density maps are mainly based on Euclidean distance, limiting their applicability in urban analysis that shall consider road network and urban traffic. In this work, we propose a new method named Topology Density Map, targeting for accurate and intuitive density maps in the context of urban environment. Based on the various constraints of road connections and traffic conditions, the method first constructs a directed acyclic graph (DAG) that propagates nonlinear scalar fields along 1D road networks. Next, the method extends the scalar fields to a 2D space by identifying key intersecting points in the DAG and calculating the scalar fields for every point, yielding a weighted Voronoi diagram like effect of space division. Two case studies demonstrate that the Topology Density Map supplies accurate information to users and provides an intuitive visualization for decision making. An interview with domain experts demonstrates the feasibility, usability, and effectiveness of our method.", "keywords": "Density map,network topology,urban data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030469", "refList": ["10.1109/vast.2009.5332584", "10.1109/tvcg.2013.193", "10.1080/03081060.2013.844903", "10.1109/tvcg.2018.2864503", "10.1145/2702123.2702419", "10.1109/tvcg.2019.2934670", "10.1109/tits.2015.2496783", "10.1177/1473871615581216", "10.3141/1617-02", "10.1145/2024156.2024169", "10.1111/cgf.13712", "10.1016/j.ejor.2007.02.005", "10.1109/tvcg.2014.2346893", "10.1007/11871842\\_29", "10.1109/vast.2010.5652478", "10.1016/j.visinf.2019.10.002", "10.1109/tvcg.2016.2616404", "10.1109/vl.1996.545307", "10.1145/2629592", "10.1155/2018/2696037", "10.1061/(asce)0733-947x(1998)124:4(368", "10.3141/1899-21", "10.1023/a:1026123329433", "10.1109/mcg.2010.79", "10.1057/palgrave.ivs.9500174", "10.1109/tcyb.2019.2963681", "10.1109/tvcg.2015.2467554", "10.1111/cgf.12114", "10.1145/2814575", "10.1016/j.jcps.2014.08.002", "10.1109/2945.981847", "10.1080/03052150210909", "10.1109/tciaig.2012.2186810", "10.1109/tits.2017.2683539", "10.1109/iv.2004.1320137", "10.1016/0377-2217(80)90126-5", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2015.2467196", "10.1145/3097983.3098056", "10.1007/s11432-018-9801-4", "10.1109/vast.2014.7042490", "10.1061/(asce)0733-947x(2006)132:2(122", "10.1016/j.tra.2008.03.011", "10.1109/tits.2014.2298892", "10.1016/j.trb.2005.12.003", "10.1007/bf01840357", "10.1109/vast.2011.6102454", "10.1109/tvcg.2013.145", "10.1007/bf02289588", "10.1109/pacificvis.2014.56", "10.1109/mcg.2018.053491730", "10.1109/tvcg.2009.111", "10.1057/palgrave.ivs.9500184", "10.1109/tvcg.2013.173", "10.1109/tvcg.2016.2598432", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/vast47406.2019.8986909", "title": "Origraph: Interactive Network Wrangling", "year": "2019", "conferenceName": "VAST", "authors": "Alex Bigelow;Carolina Nobre;Miriah D. Meyer;Alexander Lex", "citationCount": "2", "affiliation": "Bigelow, A (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bigelow, Alex; Nobre, Carolina; Meyer, Miriah; Lex, Alexander, Univ Utah, Salt Lake City, UT 84112 USA.", "countries": "USA", "abstract": "Networks are a natural way of thinking about many datasets. The data on which a network is based, however, is rarely collected in a form that suits the analysis process, making it necessary to create and reshape networks. Data wrangling is widely acknowledged to be a critical part of the data analysis pipeline, yet interactive network wrangling has received little attention in the visualization research community. In this paper, we discuss a set of operations that are important for wrangling network datasets and introduce a visual data wrangling tool, Origraph, that enables analysts to apply these operations to their datasets. Key operations include creating a network from source data such as tables, reshaping a network by introducing new node or edge classes, filtering nodes or edges, and deriving new node or edge attributes. Our tool, Origraph, enables analysts to execute these operations with little to no programming, and to immediately visualize the results. Origraph provides views to investigate the network model, a sample of the network, and node and edge attributes. In addition, we introduce interfaces designed to aid analysts in specifying arguments for sensible network wrangling operations. We demonstrate the usefulness of Origraph in two Use Cases: first, we investigate gender bias in the film industry, and then the influence of money on the political support for the war in Yemen.", "keywords": "Graph visualization,Data abstraction,Data wrangling,Human-centered computing [Information visualization],[Human-centered computing]: Visualization systems and tools,Information systems [Graph-based database models]", "link": "http://dx.doi.org/10.1109/VAST47406.2019.8986909", "refList": ["10.1101/gr.1239303", "10.1145/1054972.1055032", "10.1016/b978-0-12-382229-1.00002-3", "10.1109/tvcg.2017.2744843", "10.1109/tvcg.2013.154", "10.1007/978-1-4614-7163-9315-1", "10.1073/pnas.1607151113", "10.18637/jss.v040.i01", "10.1145/1124772.1124891", "10.1177/1473871611415994", "10.1109/tvcg.2018.2865149", "10.1145/2598153.2598175", "10.1177/1473871613488591", "10.1109/tvcg.2018.2859973", "10.1186/1471-2105-14-s19-s3", "10.1056/nejmsa066082", "10.1007/978-3-642-36763-2\\_48", "10.1016/j.socscimed.2016.01.049", "10.1111/j.1467-8659.2008.01231.x", "10.1002/cne.24084", "10.2138/am-2017-6104ccbyncnd", "10.1109/tvcg.2014.2346248", "10.1111/j.1467-8659.2009.01710.x", "10.1007/978-3-642-03658-3\\_47", "10.1007/978-3-319-06793-3\\_5", "10.3390/genes9110519", "10.1145/3290605.3300356", "10.1111/cgf.12883", "10.1177/1473871612462152", "10.1016/j.jelectrocard.2010.09.003", "10.1111/cgf.13610", "10.1109/vast.2010.5652520", "10.1111/evo.13318", "10.1109/tvcg.2018.2811488", "10.1109/tvcg.2009.111", "10.1111/cgf.13184", "10.1109/tvcg.2009.116", "10.1109/biovis.2012.6378600"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030462", "title": "Table Scraps: An Actionable Framework for Multi-Table Data Wrangling From An Artifact Study of Computational Journalism", "year": "2020", "conferenceName": "InfoVis", "authors": "Stephen Kasica;Charles Berret;Tamara Munzner", "citationCount": "0", "affiliation": "Kasica, S (Corresponding Author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada. Kasica, Stephen; Munzner, Tamara, Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada. Berret, Charles, Univ British Columbia, Sch Journalism Writing \\& Media, Vancouver, BC, Canada.", "countries": "Canada", "abstract": "For the many journalists who use data and computation to report the news, data wrangling is an integral part of their work. Despite an abundance of literature on data wrangling in the context of enterprise data analysis, little is known about the specific operations, processes, and pain points journalists encounter while performing this tedious, time-consuming task. To better understand the needs of this user group, we conduct a technical observation study of 50 public repositories of data and analysis code authored by 33 professional journalists at 26 news organizations. We develop two detailed and cross-cutting taxonomies of data wrangling in computational journalism, for actions and for processes. We observe the extensive use of multiple tables, a notable gap in previous wrangling analyses. We develop a concise, actionable framework for general multi-table data wrangling that includes wrangling operations documented in our taxonomy that are without clear parallels in other work. This framework, the first to incorporate tables as first-class objects, will support future interactive wrangling tools for both computational journalism and general-purpose use. We assess the generative and descriptive power of our framework through discussion of its relationship to our set of taxonomies.", "keywords": "Computational journalism,Data journalism,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030462", "refList": ["10.1145/1378773.1378792", "10.1109/tvcg.2012.219", "10.1109/vast47406.2019.8986909", "10.1145/1084805.1084812", "10.1007/s00778-008-0098-x", "10.1016/j.websem.2008.09.005", "10.18637/jss.v040.i01", "10.1145/989863.989865", "10.1109/tvcg.2015.2467551", "10.5281/zenodo.3509134", "10.1109/tvcg.2019.2934539", "10.1109/tvcg.2019.2934593", "10.1109/tse.2018.2796554", "10.17349/jmc117309", "10.1109/2945.981851", "10.1109/vast.2011.6102440", "10.1177/1473871611415994", "10.1145/2001269.2001288"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934285", "title": "Visualizing a Moving Target: A Design Study on Task Parallel Programs in the Presence of Evolving Data and Concerns", "year": "2019", "conferenceName": "InfoVis", "authors": "Katy Williams;Alex Bigelow;Katherine E. Isaacs", "citationCount": "5", "affiliation": "Williams, K (Corresponding Author), Univ Arizona, Tucson, AZ 85721 USA. Williams, Katy; Bigelow, Alex; Isaacs, Kate, Univ Arizona, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "Common pitfalls in visualization projects include lack of data availability and the domain users' needs and focus changing too rapidly for the design process to complete. While it is often prudent to avoid such projects, we argue it can be beneficial to engage them in some cases as the visualization process can help refine data collection, solving a \u201cchicken and egg\u201d problem of having the data and tools to analyze it. We found this to be the case in the domain of task parallel computing where such data and tooling is an open area of research. Despite these hurdles, we conducted a design study. Through a tightly-coupled iterative design process, we built Atria, a multi-view execution graph visualization to support performance analysis. Atria simplifies the initial representation of the execution graph by aggregating nodes as related to their line of code. We deployed Atria on multiple platforms, some requiring design alteration. We describe how we adapted the design study methodology to the \u201cmoving target\u201d of both the data and the domain experts' concerns and how this movement kept both the visualization and programming project healthy. We reflect on our process and discuss what factors allow the project to be successful in the presence of changing data and user needs.", "keywords": "design studies,software visualization,parallel computing,graph visualization", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934285", "refList": ["10.1007/978-3-642-31476-6\\_7", "10.1109/tvcg.2018.2859974", "10.1145/3337821.3337915", "10.1109/tse.1981.234519", "10.1109/32.221135", "10.1111/cgf.13433", "10.1109/tvcg.2011.185", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2018.2865149", "10.1109/tvcg.2014.2346323", "10.3233/978-1-61499-649-1-87", "10.1145/2993901.2993911", "10.1109/mcg.2011.103", "10.1002/1097-024x(200009)30:11", "10.1177/1094342006064482", "10.1057/palgrave.ivs.9500116", "10.1177/1473871615621602", "10.1109/tvcg.2013.124", "10.1109/infvis.2003.1249009", "10.1145/642611.642616", "10.1109/tvcg.2015.2467452", "10.1109/cw.2002.1180907", "10.1145/3125571.3125585", "10.1145/2807591.2807634", "10.1109/infvis.2002.1173148", "10.1145/2676870.2676883", "10.1145/1168149.1168168", "10.1007/978-3-030-17872-7\\_14", "10.1145/2993901.2993916", "10.1109/iotdi.2015.41", "10.1109/infvis.2004.1", "10.1080/14639220903165169", "10.1109/hpdc.2000.868632", "10.1145/882262.882291", "10.1109/mcse.2013.98", "10.1109/tvcg.2012.213", "10.1109/tvcg.2018.2811488", "10.1109/tvcg.2017.2744319", "10.1007/978-3-642-31476-67", "10.1109/mcg.2018.2874523", "10.1109/sc.2012.71"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030355", "title": "Guidelines For Pursuing and Revealing Data Abstractions", "year": "2020", "conferenceName": "InfoVis", "authors": "Alex Bigelow;Katy Williams;Katherine E. Isaacs", "citationCount": "0", "affiliation": "Bigelow, A (Corresponding Author), Univ Arizona, Tucson, AZ 85721 USA. Bigelow, Alex; Williams, Katy; Isaacs, Katherine E., Univ Arizona, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "Many data abstraction types, such as networks or set relationships, remain unfamiliar to data workers beyond the visualization research community. We conduct a survey and series of interviews about how people describe their data, either directly or indirectly. We refer to the latter as latent data abstractions. We conduct a Grounded Theory analysis that (1) interprets the extent to which latent data abstractions exist, (2) reveals the far-reaching effects that the interventionist pursuit of such abstractions can have on data workers, (3) describes why and when data workers may resist such explorations, and (4) suggests how to take advantage of opportunities and mitigate risks through transparency about visualization research perspectives and agendas. We then use the themes and codes discovered in the Grounded Theory analysis to develop guidelines for data abstraction in visualization projects. To continue the discussion, we make our dataset open along with a visual interface for further exploration.", "keywords": "Data abstraction,Grounded theory,Survey design,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030355", "refList": ["10.1080/2159676x.2016.1251701", "10.1109/infvis.2000.885092", "10.1145/2702123.2702298", "10.4135/9781848607941.n14", "10.1007/978-1-4939", "10.1109/tvcg.2014.2346331", "10.1109/tvcg.2017.2744843", "10.1177/1473871613510429", "10.1007/978-1-4939-0378-8\\_2", "10.1145/2598153.2598175", "10.1109/tvcg.2019.2934285", "10.1177/1473871613488591", "10.1145/2501105.2501106", "10.1109/tvcg.2019.2934538", "10.1109/tvcg.2019.2934539", "10.1017/s1049096510990781", "10.1145/3025453.3025837", "10.1145/3290605.3300474", "10.1145/3290605.3300356", "10.1002/nur.1025", "10.1145/2993901.2993916", "10.1145/3392826", "10.1086/269268", "10.1109/tvcg.2018.2865241", "10.1145/2998181.2998331", "10.1145/291224.291229", "10.1057/ivs.2009.13", "10.1145/2047196.2047205", "10.1109/tvcg.2012.213", "10.1145/3274405", "10.1109/tvcg.2013.145", "10.1016/0040-6031(92)85160-w", "10.1109/iv.2013.45", "10.1109/tvcg.2009.111", "10.1109/mcg.2019.2914844", "10.1109/tvcg.2009.116"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2020.3029413", "title": "A Design Space of Vision Science Methods for Visualization Research", "year": "2020", "conferenceName": "InfoVis", "authors": "Madison A. Elliott;Christine Nothelfer;Cindy Xiong;Danielle Albers Szafir", "citationCount": "0", "affiliation": "Elliott, MA (Corresponding Author), Univ British Columbia, Vancouver, BC, Canada. Elliott, Madison A., Univ British Columbia, Vancouver, BC, Canada. Nothelfer, Christine, Northwestern Univ, Evanston, IL 60208 USA. Xiong, Cindy, Univ Massachusetts, Amherst, MA 01003 USA. Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "Canada;USA", "abstract": "A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.", "keywords": "Perception,human vision,empirical research,evaluation,HCI", "link": "http://dx.doi.org/10.1109/TVCG.2020.3029413", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1177/0886109909354981", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1093/bioinformatics/btq110", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1177/1744987107081254", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1177/174498710501000305", "10.1017/s1049096513001789", "10.1109/tvcg.2012.213", "10.1093/nar/gkz239", "10.1093/sysbio/sys062", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030405", "title": "Insights From Experiments With Rigor in an EvoBio Design Study", "year": "2020", "conferenceName": "InfoVis", "authors": "Jennifer Rogers;Austin H. Patton;Luke Harmon;Alexander Lex;Miriah D. Meyer", "citationCount": "0", "affiliation": "Rogers, J (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Rogers, Jen; Lex, Alexander; Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Patton, Austin H., Washington State Univ, Pullman, WA 99164 USA. Harmon, Luke, Univ Idaho, Moscow, ID 83843 USA.", "countries": "USA", "abstract": "Design study is an established approach of conducting problem-driven visualization research. The academic visualization community has produced a large body of work for reporting on design studies, informed by a handful of theoretical frameworks, and applied to a broad range of application areas. The result is an abundance of reported insights into visualization design, with an emphasis on novel visualization techniques and systems as the primary contribution of these studies. In recent work we proposed a new, interpretivist perspective on design study and six companion criteria for rigor that highlight the opportunities for researchers to contribute knowledge that extends beyond visualization idioms and software. In this work we conducted a year-long collaboration with evolutionary biologists to develop an interactive tool for visual exploration of multivariate datasets and phylogenetic trees. During this design study we experimented with methods to support three of the rigor criteria: ABUNDANT, REFLEXIVE, and TRANSPARENT. As a result we contribute two novel visualization techniques for the analysis of multivariate phylogenetic datasets, three methodological recommendations for conducting design studies drawn from reflections over our process of experimentation, and two writing devices for reporting interpretivist design study. We offer this work as an example for implementing the rigor criteria to produce a diverse range of knowledge contributions.", "keywords": "Methodologies,Application Motivated Visualization,Guidelines,Life Sciences Visualization,Health,Medicine,Biology,Bioinformatics,Genomics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030405", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1080/17493460802276893", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1109/tvcg.2012.213", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13728", "year": "2019", "title": "The State of the Art in Visualizing Multivariate Networks", "conferenceName": "EuroVis", "authors": "Carolina Nobre;Miriah D. Meyer;Marc Streit;Alexander Lex", "citationCount": "5", "affiliation": "Nobre, C (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA.\nNobre, C.; Meyer, M.; Lex, A., Univ Utah, Salt Lake City, UT 84112 USA.\nStreit, M., Johannes Kepler Univ Linz, Linz, Austria.", "countries": "USA;Austria", "abstract": "Multivariate networks are made up of nodes and their relationships (links), but also data about those nodes and links as attributes. Most real-world networks are associated with several attributes, and many analysis tasks depend on analyzing both, relationships and attributes. Visualization of multivariate networks, however, is challenging, especially when both the topology of the network and the attributes need to be considered concurrently. In this state-of-the-art report, we analyze current practices and classify techniques along four axes: layouts, view operations, layout operations, and data operations. We also provide an analysis of tasks specific to multivariate networks and give recommendations for which technique to use in which scenario. Finally, we survey application areas and evaluation methodologies.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13728", "refList": ["10.2312/eurovisstar.20151110", "10.1111/cgf.12106", "10.1109/iv.2016.19", "10.1111/j.1467-8659.2011.02087.x", "10.1111/j.1467-8659.2008.01214.x", "10.1109/tvcg.2014.2346893", "10.1145/1124772.1124891", "10.1109/vast.2014.7042484", "10.1109/tvcg.2018.2865149", "10.1080/10447318.2010.516722", "10.1109/icsmc.2011.6084125", "10.1109/mcg.2011.103", "10.1117/12.378894", "10.1145/2207676.2208293", "10.1109/tvcg.2009.122", "10.1111/cgf.12935", "10.1016/s0020-0255(02)00191-3", "10.1038/nmeth.1436", "10.1109/tvcg.2011.187", "10.1109/32.177365", "10.1007/978-3-642-03658-3\\_47", "10.1109/tvcg.2006.147", "10.1109/pacificvis.2011.5742390", "10.1145/2470654.2466444", "10.1109/tvcg.2006.166", "10.1145/2470654.2470724", "10.1109/tvcg.2014.2346441", "10.1109/vizsec.2005.1532070", "10.1101/gr.092759.109", "10.1111/cgf.13184", "10.1109/tvcg.2018.2865940", "10.1016/j.scico.2012.05.002", "10.1109/biovis.2012.6378600", "10.1111/cgf.13213", "10.1109/noms.2006.1687547", "10.1109/tvcg.2008.141", "10.1007/978-3-540-78243-8\\_13", "10.1145/1029208.1029217", "10.1145/345513.345271", "10.1109/tvcg.2015.2467811", "10.1109/visual.1991.175815", "10.1186/1471-2105-10-375", "10.1007/978-3-319-06793-3\\_1", "10.1186/1471-2105-13-275", "10.1109/tvcg.2010.79", "10.1109/tvcg.2011.217", "10.1186/1471-2105-14-s19-s3", "10.3389/fmicb.2017.00010", "10.1109/infvis.2003.1249009", "10.1111/cgf.13187", "10.1111/j.1467-8659.2008.01231.x", "10.1109/tvcg.2011.144", "10.1145/2556288.2557010", "10.1111/j.1467-8659.2009.01710.x", "10.1109/infvis.2004.46", "10.1109/iv.2009.97", "10.1109/tvcg.2008.34", "10.1109/infvis.2003.1249011", "10.1109/iv.2016.41", "10.1109/pacificvis.2010.5429609", "10.1109/tvcg.2006.160", "10.1109/tvcg.2010.205", "10.1109/38.486685", "10.1109/tvcg.2006.106", "10.1057/palgrave.ivs.9500092", "10.1145/568522.568523", "10.1111/j.1467-8659.2008.01221.x", "10.1109/iv.2010.15", "10.1145/22339.22342", "10.1109/csmr.2009.17", "10.1109/tvcg.2017.2744898", "10.1109/tvcg.2008.117", "10.1186/1752-0509-3-82", "10.1109/tvcg.2008.61", "10.1109/pacificvis.2013.6596127", "10.1109/tvcg.2007.70529", "10.1186/1471-2105-15-198", "10.1109/tvcg.2009.128", "10.2312/eurovisshort.20151124", "10.1093/bioinformatics/btq675", "10.1109/tvcg.2009.143", "10.1109/tvcg.2014.2346752", "10.1109/tvcg.2013.124", "10.1007/s00450-007-0036-y", "10.1093/bioinformatics/17.suppl\\_1.s22", "10.2312/eurovisstar.20151109", "10.1007/978-3-319-06793-3\\_5", "10.1109/vissof.2007.4290706", "10.1109/tvcg.2009.145", "10.1109/mcas.2003.1228503", "10.1109/infvis.2005.1532128", "10.1145/989863.989941", "10.1109/infvis.1999.801860", "10.1111/j.1467-8659.2009.01687.x", "10.1109/tvcg.2013.120", "10.1109/tvcg.2007.70582", "10.1109/tvcg.2009.167", "10.1109/pacificvis.2010.5429590", "10.1109/infvis.2000.885091", "10.1109/tvcg.2012.189", "10.1109/tvcg.2009.108", "10.1109/tvcg.2013.154", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2010.159", "10.1016/s0306-4573(98)00024-7", "10.1109/tvcg.2013.223", "10.1002/sam.10071", "10.2312/vissym/eurovis07/083-090", "10.1109/infvis.2002.1173156", "10.1186/1471-2105-7-109", "10.1177/1473871612455983", "10.1073/pnas.95.25.14863", "10.1007/978-3-319-06793-3\\_2", "10.1111/cgf.12883", "10.1007/978-3-540-78243-8\\_9", "10.1057/palgrave.ivs.9500180", "10.1109/pacificvis.2012.6183556", "10.1117/12.872578", "10.1145/1168149.1168169", "10.1109/iv.2013.3", "10.1109/tvcg.2011.186", "10.2307/2685881", "10.1109/pacificvis.2015.7156354", "10.1109/tvcg.2016.2598885", "10.1109/tvcg.2018.2811488", "10.1109/infvis.2005.1532129", "10.1111/j.1467-8659.2009.01450.x", "10.1057/palgrave.ivs.9500162", "10.1109/tvcg.2009.116", "10.1109/2945.468391"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3029413", "title": "A Design Space of Vision Science Methods for Visualization Research", "year": "2020", "conferenceName": "InfoVis", "authors": "Madison A. Elliott;Christine Nothelfer;Cindy Xiong;Danielle Albers Szafir", "citationCount": "0", "affiliation": "Elliott, MA (Corresponding Author), Univ British Columbia, Vancouver, BC, Canada. Elliott, Madison A., Univ British Columbia, Vancouver, BC, Canada. Nothelfer, Christine, Northwestern Univ, Evanston, IL 60208 USA. Xiong, Cindy, Univ Massachusetts, Amherst, MA 01003 USA. Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "Canada;USA", "abstract": "A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.", "keywords": "Perception,human vision,empirical research,evaluation,HCI", "link": "http://dx.doi.org/10.1109/TVCG.2020.3029413", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1177/0886109909354981", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1093/bioinformatics/btq110", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1177/1744987107081254", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1177/174498710501000305", "10.1017/s1049096513001789", "10.1109/tvcg.2012.213", "10.1093/nar/gkz239", "10.1093/sysbio/sys062", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030405", "title": "Insights From Experiments With Rigor in an EvoBio Design Study", "year": "2020", "conferenceName": "InfoVis", "authors": "Jennifer Rogers;Austin H. Patton;Luke Harmon;Alexander Lex;Miriah D. Meyer", "citationCount": "0", "affiliation": "Rogers, J (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Rogers, Jen; Lex, Alexander; Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Patton, Austin H., Washington State Univ, Pullman, WA 99164 USA. Harmon, Luke, Univ Idaho, Moscow, ID 83843 USA.", "countries": "USA", "abstract": "Design study is an established approach of conducting problem-driven visualization research. The academic visualization community has produced a large body of work for reporting on design studies, informed by a handful of theoretical frameworks, and applied to a broad range of application areas. The result is an abundance of reported insights into visualization design, with an emphasis on novel visualization techniques and systems as the primary contribution of these studies. In recent work we proposed a new, interpretivist perspective on design study and six companion criteria for rigor that highlight the opportunities for researchers to contribute knowledge that extends beyond visualization idioms and software. In this work we conducted a year-long collaboration with evolutionary biologists to develop an interactive tool for visual exploration of multivariate datasets and phylogenetic trees. During this design study we experimented with methods to support three of the rigor criteria: ABUNDANT, REFLEXIVE, and TRANSPARENT. As a result we contribute two novel visualization techniques for the analysis of multivariate phylogenetic datasets, three methodological recommendations for conducting design studies drawn from reflections over our process of experimentation, and two writing devices for reporting interpretivist design study. We offer this work as an example for implementing the rigor criteria to produce a diverse range of knowledge contributions.", "keywords": "Methodologies,Application Motivated Visualization,Guidelines,Life Sciences Visualization,Health,Medicine,Biology,Bioinformatics,Genomics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030405", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1080/17493460802276893", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1109/tvcg.2012.213", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13963", "year": "2020", "title": "MotionGlyphs: Visual Abstraction of Spatio-Temporal Networks in Collective Animal Behavior", "conferenceName": "EuroVis", "authors": "Eren Cakmak;Hanna Sch{\\\"{a}}fer;Juri Buchm{\\\"{u}}ller;Johannes Fuchs;Tobias Schreck;A. Jordan;Daniel A. Keim", "citationCount": "0", "affiliation": "Cakmak, E (Corresponding Author), Univ Konstanz, Constance, Germany.\nCakmak, E (Corresponding Author), Ctr Adv Study Collect Behav, Constance, Germany.\nCakmak, E.; Schaefer, H.; Buchmueller, J.; Fuchs, J.; Jordan, A.; Keim, D., Univ Konstanz, Constance, Germany.\nCakmak, E.; Jordan, A.; Keim, D., Ctr Adv Study Collect Behav, Constance, Germany.\nSchreck, T., Graz Univ Technol, Graz, Austria.\nJordan, A., Max Planck Inst Anim Behav, Radolfzell am Bodensee, Germany.", "countries": "Germany;Austria", "abstract": "Domain experts for collective animal behavior analyze relationships between single animal movers and groups of animals over time and space to detect emergent group properties. A common way to interpret this type of data is to visualize it as a spatio-temporal network. Collective behavior data sets are often large, and may hence result in dense and highly connected node-link diagrams, resulting in issues of node-overlap and edge clutter. In this design study, in an iterative design process, we developed glyphs as a design for seamlessly encoding relationships and movement characteristics of a single mover or clusters of movers. Based on these glyph designs, we developed a visual exploration prototype, MotionGlyphs, that supports domain experts in interactively filtering, clustering, and animating spatio-temporal networks for collective animal behavior analysis. By means of an expert evaluation, we show how MotionGlyphs supports important tasks and analysis goals of our domain experts, and we give evidence of the usefulness for analyzing spatio-temporal networks of collective animal behavior.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13963", "refList": ["10.1109/tvcg.2007.70582", "10.1111/cgf.13213", "10.1109/pacificvis.2014.13", "10.1145/2093973.2094038", "10.1111/j.1467-8659.2009.01664.x", "10.1109/tvcg.2010.44", "10.1111/cgf.12106", "10.1111/cgf.12791", "10.1080/15230406.2014.890071", "10.1111/tgis.12100", "10.1111/j.1467-8659.2009.01451.x", "10.1179/000870409x12525737905042", "10.1111/cgf.12923", "10.1006/ijhc.2002.1017", "10.1145/1124772.1124891", "10.1109/tvcg.2011.213", "10.1109/vast.2014.7042484", "10.1007/s12650-016-0375-5", "10.1109/vlhcc.2012.6344514", "10.1073/pnas.1420068112", "10.1006/ijhc.1017", "10.1007/s00371-017-1461-y", "10.1109/infvis.2003.1249008", "10.1111/cgf.13728", "10.2312/conf/eg2013/stars/039-063", "10.1109/tvcg.2014.2346271", "10.1109/hicss.2011.339", "10.1068/p3104", "10.1145/2931002.2931012", "10.1109/asonam.2012.39", "10.1179/000870403235002042", "10.1145/2556288.2557010", "10.1145/2470654.2466443", "10.1109/tvcg.2011.209", "10.1016/j.tree.2013.06.002", "10.1111/1365-2656.12418", "10.1111/cgf.12872", "10.1145/2470654.2466444", "10.1109/tvcg.2014.2322594", "10.1109/tvcg.2008.125", "10.1109/tvcg.2014.2346426", "10.1109/tvcg.2006.166", "10.1111/cgf.12615", "10.1057/palgrave.ivs.9500170", "10.1007/3-540-36151-0", "10.1117/12.872578", "10.1016/j.tics.2008.10.002", "10.1006/jtbi.2002.3065", "10.1109/tvcg.2010.78", "10.1109/iv.2013.3", "10.1073/pnas.1001763107", "10.1109/pacificvis.2015.7156354", "10.1371/journal.pbio.1001805", "10.5220/0005303801230130", "10.1111/j.1467-8659.2009.01687.x", "10.1007/s10844-011-0159-2", "10.1007/s12650-018-00543-4", "10.1145/2669557.2669572", "10.1111/cgf.13184", "10.1186/s40462-015-0032-y", "10.1016/j.ins.2016.06.048", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13987", "year": "2020", "title": "Augmenting Node-Link Diagrams with Topographic Attribute Maps", "conferenceName": "EuroVis", "authors": "Reinhold Preiner;Johanna Schmidt;Katharina Kr{\\\"{o}}sl;Tobias Schreck;Gabriel Mistelbauer", "citationCount": "0", "affiliation": "Preiner, R (Corresponding Author), Graz Univ Technol, Inst Comp Graph \\& Knowledge Visualizat, Graz, Austria.\nPreiner, R.; Schreck, T., Graz Univ Technol, Inst Comp Graph \\& Knowledge Visualizat, Graz, Austria.\nSchmidt, J.; Kroesl, K., Virtual Real \\& Visualisierung Forsch GmbH, VRVis Zentrum, Vienna, Austria.\nKroesl, K., TU Wien, Inst Visual Comp \\& Human Ctr Technol, Vienna, Austria.\nMistelbauer, G., Otto von Guericke Univ, Dept Simulat \\& Graph, Magdeburg, Germany.", "countries": "Germany;Austria", "abstract": "We propose a novel visualization technique for graphs that are attributed with scalar data. In many scenarios, these attributes (e.g., birth date in a family network) provide ambient context information for the graph structure, whose consideration is important for different visual graph analysis tasks. Graph attributes are usually conveyed using different visual representations (e.g., color, size, shape) or by reordering the graph structure according to the attribute domain (e.g., timelines). While visual encodings allow graphs to be arranged in a readable layout, assessing contextual information such as the relative similarities of attributes across the graph is often cumbersome. In contrast, attribute-based graph reordering serves the comparison task of attributes, but typically strongly impairs the readability of the structural information given by the graph's topology. In this work, we augment force-directed node-link diagrams with a continuous ambient representation of the attribute context. This way, we provide a consistent overview of the graph's topological structure as well as its attributes, supporting a wide range of graph-related analysis tasks. We resort to an intuitive height field metaphor, illustrated by a topographic map rendering using contour lines and suitable color maps. Contour lines visually connect nodes of similar attribute values, and depict their relative arrangement within the global context. Moreover, our contextual representation supports visualizing attribute value ranges associated with graph nodes (e.g., lifespans in a family network) as trajectories routed through this height field. We discuss how user interaction with both the structural and the contextual information fosters exploratory graph analysis tasks. The effectiveness and versatility of our technique is confirmed in a user study and case studies from various application domains.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13987", "refList": ["10.1109/tvcg.2013.269", "10.1109/pacificvis.2010.5429590", "10.1073/pnas.0307654100", "10.1145/2505515.2505758", "10.1559/152304082783948286", "10.1109/pacificvis.2014.47", "10.1093/bioinformatics/btp432", "10.1111/j.1467-8659.2011.01898.x", "10.1111/cgf.12931", "10.1111/cgf.12880", "10.1109/tvcg.2014.2346422", "10.1111/j.1467-8659.2009.01706.x", "10.1109/tvcg.2016.2598795", "10.1111/cgf.12800", "10.1109/tvcg.2014.2315995", "10.1111/cgf.12656", "10.1111/cgf.13728", "10.1109/tvcg.2009.122", "10.1111/cgf.13211", "10.1109/tvcg.2007.70596", "10.1109/infvis.2002.1173152", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2003.1196007", "10.1109/infvis.2005.1532150", "10.1145/3243250.3243266", "10.1080/02693799008941549", "10.1371/journal.pone.0058779", "10.1109/infvis.1995.528686", "10.1111/cgf.12872", "10.1002/spe.4380211102", "10.1109/38.974518", "10.1145/3097983.3098130", "10.1002/aris.1440370106", "10.1145/1360612.1360691", "10.1109/mc.2016.145", "10.2307/3006914", "10.1111/j.1467-8659.2009.01683.x", "10.1145/1639714.1639784"], "wos": 1, "children": [], "len": 1}], "len": 9}], "len": 33}, {"doi": "10.1109/vast47406.2019.8986909", "title": "Origraph: Interactive Network Wrangling", "year": "2019", "conferenceName": "VAST", "authors": "Alex Bigelow;Carolina Nobre;Miriah D. Meyer;Alexander Lex", "citationCount": "2", "affiliation": "Bigelow, A (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bigelow, Alex; Nobre, Carolina; Meyer, Miriah; Lex, Alexander, Univ Utah, Salt Lake City, UT 84112 USA.", "countries": "USA", "abstract": "Networks are a natural way of thinking about many datasets. The data on which a network is based, however, is rarely collected in a form that suits the analysis process, making it necessary to create and reshape networks. Data wrangling is widely acknowledged to be a critical part of the data analysis pipeline, yet interactive network wrangling has received little attention in the visualization research community. In this paper, we discuss a set of operations that are important for wrangling network datasets and introduce a visual data wrangling tool, Origraph, that enables analysts to apply these operations to their datasets. Key operations include creating a network from source data such as tables, reshaping a network by introducing new node or edge classes, filtering nodes or edges, and deriving new node or edge attributes. Our tool, Origraph, enables analysts to execute these operations with little to no programming, and to immediately visualize the results. Origraph provides views to investigate the network model, a sample of the network, and node and edge attributes. In addition, we introduce interfaces designed to aid analysts in specifying arguments for sensible network wrangling operations. We demonstrate the usefulness of Origraph in two Use Cases: first, we investigate gender bias in the film industry, and then the influence of money on the political support for the war in Yemen.", "keywords": "Graph visualization,Data abstraction,Data wrangling,Human-centered computing [Information visualization],[Human-centered computing]: Visualization systems and tools,Information systems [Graph-based database models]", "link": "http://dx.doi.org/10.1109/VAST47406.2019.8986909", "refList": ["10.1101/gr.1239303", "10.1145/1054972.1055032", "10.1016/b978-0-12-382229-1.00002-3", "10.1109/tvcg.2017.2744843", "10.1109/tvcg.2013.154", "10.1007/978-1-4614-7163-9315-1", "10.1073/pnas.1607151113", "10.18637/jss.v040.i01", "10.1145/1124772.1124891", "10.1177/1473871611415994", "10.1109/tvcg.2018.2865149", "10.1145/2598153.2598175", "10.1177/1473871613488591", "10.1109/tvcg.2018.2859973", "10.1186/1471-2105-14-s19-s3", "10.1056/nejmsa066082", "10.1007/978-3-642-36763-2\\_48", "10.1016/j.socscimed.2016.01.049", "10.1111/j.1467-8659.2008.01231.x", "10.1002/cne.24084", "10.2138/am-2017-6104ccbyncnd", "10.1109/tvcg.2014.2346248", "10.1111/j.1467-8659.2009.01710.x", "10.1007/978-3-642-03658-3\\_47", "10.1007/978-3-319-06793-3\\_5", "10.3390/genes9110519", "10.1145/3290605.3300356", "10.1111/cgf.12883", "10.1177/1473871612462152", "10.1016/j.jelectrocard.2010.09.003", "10.1111/cgf.13610", "10.1109/vast.2010.5652520", "10.1111/evo.13318", "10.1109/tvcg.2018.2811488", "10.1109/tvcg.2009.111", "10.1111/cgf.13184", "10.1109/tvcg.2009.116", "10.1109/biovis.2012.6378600"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030462", "title": "Table Scraps: An Actionable Framework for Multi-Table Data Wrangling From An Artifact Study of Computational Journalism", "year": "2020", "conferenceName": "InfoVis", "authors": "Stephen Kasica;Charles Berret;Tamara Munzner", "citationCount": "0", "affiliation": "Kasica, S (Corresponding Author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada. Kasica, Stephen; Munzner, Tamara, Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada. Berret, Charles, Univ British Columbia, Sch Journalism Writing \\& Media, Vancouver, BC, Canada.", "countries": "Canada", "abstract": "For the many journalists who use data and computation to report the news, data wrangling is an integral part of their work. Despite an abundance of literature on data wrangling in the context of enterprise data analysis, little is known about the specific operations, processes, and pain points journalists encounter while performing this tedious, time-consuming task. To better understand the needs of this user group, we conduct a technical observation study of 50 public repositories of data and analysis code authored by 33 professional journalists at 26 news organizations. We develop two detailed and cross-cutting taxonomies of data wrangling in computational journalism, for actions and for processes. We observe the extensive use of multiple tables, a notable gap in previous wrangling analyses. We develop a concise, actionable framework for general multi-table data wrangling that includes wrangling operations documented in our taxonomy that are without clear parallels in other work. This framework, the first to incorporate tables as first-class objects, will support future interactive wrangling tools for both computational journalism and general-purpose use. We assess the generative and descriptive power of our framework through discussion of its relationship to our set of taxonomies.", "keywords": "Computational journalism,Data journalism,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030462", "refList": ["10.1145/1378773.1378792", "10.1109/tvcg.2012.219", "10.1109/vast47406.2019.8986909", "10.1145/1084805.1084812", "10.1007/s00778-008-0098-x", "10.1016/j.websem.2008.09.005", "10.18637/jss.v040.i01", "10.1145/989863.989865", "10.1109/tvcg.2015.2467551", "10.5281/zenodo.3509134", "10.1109/tvcg.2019.2934539", "10.1109/tvcg.2019.2934593", "10.1109/tse.2018.2796554", "10.17349/jmc117309", "10.1109/2945.981851", "10.1109/vast.2011.6102440", "10.1177/1473871611415994", "10.1145/2001269.2001288"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934661", "title": "Visual Analysis of High-Dimensional Event Sequence Data via Dynamic Hierarchical Aggregation", "year": "2019", "conferenceName": "VAST", "authors": "David Gotz;Jonathan Zhang;Wenyuan Wang;Joshua Shrestha;David Borland", "citationCount": "3", "affiliation": "Gotz, D (Corresponding Author), Univ N Carolina, Sch Informat \\& Lib Sci, Chapel Hill, NC 27515 USA. Gotz, David; Wang, Wenyuan, Univ N Carolina, Sch Informat \\& Lib Sci, Chapel Hill, NC 27515 USA. Zhang, Jonathan, Univ N Carolina, Dept Biostat, Chapel Hill, NC 27515 USA. Shrestha, Joshua, Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27515 USA. Borland, David, Univ N Carolina, RENCI, Chapel Hill, NC 27515 USA.", "countries": "USA", "abstract": "Temporal event data are collected across a broad range of domains, and a variety of visual analytics techniques have been developed to empower analysts working with this form of data. These techniques generally display aggregate statistics computed over sets of event sequences that share common patterns. Such techniques are often hindered, however, by the high-dimensionality of many real-world event sequence datasets which can prevent effective aggregation. A common coping strategy for this challenge is to group event types together prior to visualization, as a pre-process, so that each group can be represented within an analysis as a single event type. However, computing these event groupings as a pre-process also places significant constraints on the analysis. This paper presents a new visual analytics approach for dynamic hierarchical dimension aggregation. The approach leverages a predefined hierarchy of dimensions to computationally quantify the informativeness, with respect to a measure of interest, of alternative levels of grouping within the hierarchy at runtime. This information is then interactively visualized, enabling users to dynamically explore the hierarchy to select the most appropriate level of grouping to use at any individual step within an analysis. Key contributions include an algorithm for interactively determining the most informative set of event groupings for a specific analysis context, and a scented scatter-plus-focus visualization design with an optimization-based layout algorithm that supports interactive hierarchical exploration of alternative event type groupings. We apply these techniques to high-dimensional event sequence data from the medical domain and report findings from domain expert interviews.", "keywords": "Temporal event sequence visualization,visual analytics,hierarchical aggregation,medical informatics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934661", "refList": ["10.1145/937549.937550", "10.1109/infvis.2000.885091", "10.1145/2468356.2468434", "10.1016/j.jbi.2012.01.009", "10.1136/amiajnl-2014-002747", "10.1109/tvcg.2009.108", "10.1145/2702123.2702419", "10.3233/978-1-61499-289-9-1224", "10.13063/2327-9214", "10.1109/vast.2016.7883512", "10.1111/j.1467-8659.2011.01898.x", "10.1109/infvis.2005.1532152", "10.1177/1473871614526077", "10.1016/j.jbi.2014.01.007", "10.1145/2856767.2856779", "10.1038/nrg3208", "10.1001/jama.2014.4228", "10.1109/tvcg.2012.238", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2018.2864886", "10.2307/2983604", "10.1145/3009973", "10.1109/tvcg.2013.200", "10.1109/tvcg.2016.2539960", "10.1200/jco.2010.28.5478", "10.1145/2557500.2557508", "10.1109/tvcg.2014.2346682", "10.1111/cgf.12883", "10.1145/2678025.2701407", "10.1109/vast.2011.6102443", "10.1109/tvcg.2009.84", "10.1109/tvcg.2007.70589", "10.1109/tvcg.2017.2745320", "10.3233/978-1-61499-830-3-1327", "10.1136/jamia.2009.000893", "10.1186/2047-2501-2-3", "10.1145/2890478", "10.2307/2685881", "10.1109/vast.2014.7042487", "10.1561/1100000039", "10.1109/mcg.2016.59", "10.1109/tvcg.2017.2744686", "10.1109/vds.2017.8573439", "10.1016/j.otohns.2010.05.007", "10.1109/tvcg.2014.2346433"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3029413", "title": "A Design Space of Vision Science Methods for Visualization Research", "year": "2020", "conferenceName": "InfoVis", "authors": "Madison A. Elliott;Christine Nothelfer;Cindy Xiong;Danielle Albers Szafir", "citationCount": "0", "affiliation": "Elliott, MA (Corresponding Author), Univ British Columbia, Vancouver, BC, Canada. Elliott, Madison A., Univ British Columbia, Vancouver, BC, Canada. Nothelfer, Christine, Northwestern Univ, Evanston, IL 60208 USA. Xiong, Cindy, Univ Massachusetts, Amherst, MA 01003 USA. Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "Canada;USA", "abstract": "A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.", "keywords": "Perception,human vision,empirical research,evaluation,HCI", "link": "http://dx.doi.org/10.1109/TVCG.2020.3029413", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1177/0886109909354981", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1093/bioinformatics/btq110", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1177/1744987107081254", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1177/174498710501000305", "10.1017/s1049096513001789", "10.1109/tvcg.2012.213", "10.1093/nar/gkz239", "10.1093/sysbio/sys062", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030405", "title": "Insights From Experiments With Rigor in an EvoBio Design Study", "year": "2020", "conferenceName": "InfoVis", "authors": "Jennifer Rogers;Austin H. Patton;Luke Harmon;Alexander Lex;Miriah D. Meyer", "citationCount": "0", "affiliation": "Rogers, J (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Rogers, Jen; Lex, Alexander; Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Patton, Austin H., Washington State Univ, Pullman, WA 99164 USA. Harmon, Luke, Univ Idaho, Moscow, ID 83843 USA.", "countries": "USA", "abstract": "Design study is an established approach of conducting problem-driven visualization research. The academic visualization community has produced a large body of work for reporting on design studies, informed by a handful of theoretical frameworks, and applied to a broad range of application areas. The result is an abundance of reported insights into visualization design, with an emphasis on novel visualization techniques and systems as the primary contribution of these studies. In recent work we proposed a new, interpretivist perspective on design study and six companion criteria for rigor that highlight the opportunities for researchers to contribute knowledge that extends beyond visualization idioms and software. In this work we conducted a year-long collaboration with evolutionary biologists to develop an interactive tool for visual exploration of multivariate datasets and phylogenetic trees. During this design study we experimented with methods to support three of the rigor criteria: ABUNDANT, REFLEXIVE, and TRANSPARENT. As a result we contribute two novel visualization techniques for the analysis of multivariate phylogenetic datasets, three methodological recommendations for conducting design studies drawn from reflections over our process of experimentation, and two writing devices for reporting interpretivist design study. We offer this work as an example for implementing the rigor criteria to produce a diverse range of knowledge contributions.", "keywords": "Methodologies,Application Motivated Visualization,Guidelines,Life Sciences Visualization,Health,Medicine,Biology,Bioinformatics,Genomics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030405", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1080/17493460802276893", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1109/tvcg.2012.213", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2017.8031572", "year": "2017", "title": "HoNVis: Visualizing and exploring higher-order networks", "conferenceName": "PacificVis", "authors": "Jun Tao;Jian Xu;Chaoli Wang;Nitesh V. Chawla", "citationCount": "16", "affiliation": "Tao, J (Corresponding Author), Univ Notre Dame, Notre Dame, IN 46556 USA.\nTao, Jun; Xu, Jian; Wang, Chaoli; Chawla, Nitesh V., Univ Notre Dame, Notre Dame, IN 46556 USA.", "countries": "USA", "abstract": "Unlike the conventional first-order network (FoN), the higher-order network (HoN) provides a more accurate description of transitions by creating additional nodes to encode higher-order dependencies. However, there exists no visualization and exploration tool for the HoN. For applications such as the development of strategies to control species invasion through global shipping which is known to exhibit higher-order dependencies, the existing FoN visualization tools are limited. In this paper, we present HoNVis, a novel visual analytics framework for exploring higher-order dependencies of the global ocean shipping network. Our framework leverages coordinated multiple views to reveal the network structure at three levels of detail (i.e., the global, local, and individual port levels). Users can quickly identify ports of interest at the global level and specify a port to investigate its higher-order nodes at the individual port level. Investigating a larger-scale impact is enabled through the exploration of HoN at the local level. Using the global ocean shipping network data, we demonstrate the effectiveness of our approach with a real-world use case conducted by domain experts specializing in species invasion. Finally, we discuss the generalizability of this framework to other real-world applications such as information diffusion in social networks and epidemic spreading through air transportation.", "keywords": "", "link": "https://doi.org/10.1109/PACIFICVIS.2017.8031572", "refList": ["10.1016/j.ecolecon.2004.10.002", "10.2312/eurovisstar.20141170", "10.1109/tvcg.2015.2468111", "10.1111/j.1472-4642.2010.00696.x", "10.1111/j.1467-8659.2011.01898.x", "10.1126/sciadv.1600028", "10.1109/tvcg.2009.181", "10.1109/tvcg.2009.143", "10.1214/aoms/1177729694", "10.1098/rsif.2009.0495", "10.1890/070064", "10.1109/iv.2009.108", "10.1038/ncomms5630", "10.1641/b580507", "10.1111/cgf.12883", "10.1088/1742-5468/2008/10/p10008", "10.1371/journal.pone.0098679", "10.1641/b570707", "10.1109/pacificvis.2016.7465254", "10.1111/j.1467-8659.2009.01450.x", "10.1098/rspb.2003.2629"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13433", "year": "2018", "title": "CFGExplorer: Designing a Visual Control Flow Analytics System around Basic Program Analysis Operations", "conferenceName": "EuroVis", "authors": "Sabin Devkota;Katherine E. Isaacs", "citationCount": "1", "affiliation": "Devkota, S (Corresponding Author), Univ Arizona, Comp Sci, Tucson, AZ 85721 USA.\nDevkota, Sabin; Isaacs, Katherine E., Univ Arizona, Comp Sci, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "To develop new compilation and optimization techniques, computer scientists frequently Consult program analysis artifacts such as Control flow graphs (CFGs) and traces of executed instructions. A CFG is a directed graph representing possible execution paths in a program. CFGs are commonly visualized as node-link diagrams while traces are commonly viewed in raw text format. Visualizing and exploring CFGs and traces is challenging because of the complexity and specificity of the operations researchers perform. We present a design study where we collaborate with computer scientists researching dynamic binary analysis and compilation techniques. The research group primarily employs CFGs and traces to reason about and develop new algorithms for program optimization and parallelization. Through questionnaires, interviews, and a year-long observation, we analyzed their use of visualization, noting that the tasks they perform match common subroutines they employ in their techniques. Based on this task analysis, we designed CFGExplorer, a visual analytics system that supports computer scientists with interactions that are integrated with the program structure. We developed a domain-specific graph modification to generate graph layouts that reflect program structure. CFGExplorer incorporates structures such as functions and loops, and uses the correspondence between CFGs and traces to support navigation. We further augment the system to highlight the output of program analysis techniques, facilitating exploration at a higher level. We evaluate the tool through guided sessions and semi-structured interviews as well as deployment. Our collaborators have integrated CFGExplorer into their workflow and use it to reason about programs, develop and debug new algorithms, and share their findings.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13433", "refList": ["10.1109/tvcg.2006.120", "10.1016/j.ijhcs.2012.09.004", "10.1145/24039.24041", "10.1109/32.221135", "10.1109/tvcg.2011.185", "10.1109/icsm.2004.1357801", "10.1111/cgf.12394", "10.1002/1097-024x(200009)30:11", "10.1057/palgrave.ivs.9500116", "10.1109/sc.2014.70", "10.1016/j.cag.2009.06.002", "10.1111/cgf.12883", "10.1109/vissoft.2014.25", "10.1109/tvcg.2008.34", "10.1145/1168149.1168168", "10.1109/infvis.2004.1", "10.1145/291224.291229", "10.1109/tsmc.1981.4308636", "10.1002/smr.291", "10.1109/tvcg.2017.2744319"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934285", "title": "Visualizing a Moving Target: A Design Study on Task Parallel Programs in the Presence of Evolving Data and Concerns", "year": "2019", "conferenceName": "InfoVis", "authors": "Katy Williams;Alex Bigelow;Katherine E. Isaacs", "citationCount": "5", "affiliation": "Williams, K (Corresponding Author), Univ Arizona, Tucson, AZ 85721 USA. Williams, Katy; Bigelow, Alex; Isaacs, Kate, Univ Arizona, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "Common pitfalls in visualization projects include lack of data availability and the domain users' needs and focus changing too rapidly for the design process to complete. While it is often prudent to avoid such projects, we argue it can be beneficial to engage them in some cases as the visualization process can help refine data collection, solving a \u201cchicken and egg\u201d problem of having the data and tools to analyze it. We found this to be the case in the domain of task parallel computing where such data and tooling is an open area of research. Despite these hurdles, we conducted a design study. Through a tightly-coupled iterative design process, we built Atria, a multi-view execution graph visualization to support performance analysis. Atria simplifies the initial representation of the execution graph by aggregating nodes as related to their line of code. We deployed Atria on multiple platforms, some requiring design alteration. We describe how we adapted the design study methodology to the \u201cmoving target\u201d of both the data and the domain experts' concerns and how this movement kept both the visualization and programming project healthy. We reflect on our process and discuss what factors allow the project to be successful in the presence of changing data and user needs.", "keywords": "design studies,software visualization,parallel computing,graph visualization", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934285", "refList": ["10.1007/978-3-642-31476-6\\_7", "10.1109/tvcg.2018.2859974", "10.1145/3337821.3337915", "10.1109/tse.1981.234519", "10.1109/32.221135", "10.1111/cgf.13433", "10.1109/tvcg.2011.185", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2018.2865149", "10.1109/tvcg.2014.2346323", "10.3233/978-1-61499-649-1-87", "10.1145/2993901.2993911", "10.1109/mcg.2011.103", "10.1002/1097-024x(200009)30:11", "10.1177/1094342006064482", "10.1057/palgrave.ivs.9500116", "10.1177/1473871615621602", "10.1109/tvcg.2013.124", "10.1109/infvis.2003.1249009", "10.1145/642611.642616", "10.1109/tvcg.2015.2467452", "10.1109/cw.2002.1180907", "10.1145/3125571.3125585", "10.1145/2807591.2807634", "10.1109/infvis.2002.1173148", "10.1145/2676870.2676883", "10.1145/1168149.1168168", "10.1007/978-3-030-17872-7\\_14", "10.1145/2993901.2993916", "10.1109/iotdi.2015.41", "10.1109/infvis.2004.1", "10.1080/14639220903165169", "10.1109/hpdc.2000.868632", "10.1145/882262.882291", "10.1109/mcse.2013.98", "10.1109/tvcg.2012.213", "10.1109/tvcg.2018.2811488", "10.1109/tvcg.2017.2744319", "10.1007/978-3-642-31476-67", "10.1109/mcg.2018.2874523", "10.1109/sc.2012.71"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030355", "title": "Guidelines For Pursuing and Revealing Data Abstractions", "year": "2020", "conferenceName": "InfoVis", "authors": "Alex Bigelow;Katy Williams;Katherine E. Isaacs", "citationCount": "0", "affiliation": "Bigelow, A (Corresponding Author), Univ Arizona, Tucson, AZ 85721 USA. Bigelow, Alex; Williams, Katy; Isaacs, Katherine E., Univ Arizona, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "Many data abstraction types, such as networks or set relationships, remain unfamiliar to data workers beyond the visualization research community. We conduct a survey and series of interviews about how people describe their data, either directly or indirectly. We refer to the latter as latent data abstractions. We conduct a Grounded Theory analysis that (1) interprets the extent to which latent data abstractions exist, (2) reveals the far-reaching effects that the interventionist pursuit of such abstractions can have on data workers, (3) describes why and when data workers may resist such explorations, and (4) suggests how to take advantage of opportunities and mitigate risks through transparency about visualization research perspectives and agendas. We then use the themes and codes discovered in the Grounded Theory analysis to develop guidelines for data abstraction in visualization projects. To continue the discussion, we make our dataset open along with a visual interface for further exploration.", "keywords": "Data abstraction,Grounded theory,Survey design,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030355", "refList": ["10.1080/2159676x.2016.1251701", "10.1109/infvis.2000.885092", "10.1145/2702123.2702298", "10.4135/9781848607941.n14", "10.1007/978-1-4939", "10.1109/tvcg.2014.2346331", "10.1109/tvcg.2017.2744843", "10.1177/1473871613510429", "10.1007/978-1-4939-0378-8\\_2", "10.1145/2598153.2598175", "10.1109/tvcg.2019.2934285", "10.1177/1473871613488591", "10.1145/2501105.2501106", "10.1109/tvcg.2019.2934538", "10.1109/tvcg.2019.2934539", "10.1017/s1049096510990781", "10.1145/3025453.3025837", "10.1145/3290605.3300474", "10.1145/3290605.3300356", "10.1002/nur.1025", "10.1145/2993901.2993916", "10.1145/3392826", "10.1086/269268", "10.1109/tvcg.2018.2865241", "10.1145/2998181.2998331", "10.1145/291224.291229", "10.1057/ivs.2009.13", "10.1145/2047196.2047205", "10.1109/tvcg.2012.213", "10.1145/3274405", "10.1109/tvcg.2013.145", "10.1016/0040-6031(92)85160-w", "10.1109/iv.2013.45", "10.1109/tvcg.2009.111", "10.1109/mcg.2019.2914844", "10.1109/tvcg.2009.116"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 5}, {"doi": "10.1111/cgf.13728", "year": "2019", "title": "The State of the Art in Visualizing Multivariate Networks", "conferenceName": "EuroVis", "authors": "Carolina Nobre;Miriah D. Meyer;Marc Streit;Alexander Lex", "citationCount": "5", "affiliation": "Nobre, C (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA.\nNobre, C.; Meyer, M.; Lex, A., Univ Utah, Salt Lake City, UT 84112 USA.\nStreit, M., Johannes Kepler Univ Linz, Linz, Austria.", "countries": "USA;Austria", "abstract": "Multivariate networks are made up of nodes and their relationships (links), but also data about those nodes and links as attributes. Most real-world networks are associated with several attributes, and many analysis tasks depend on analyzing both, relationships and attributes. Visualization of multivariate networks, however, is challenging, especially when both the topology of the network and the attributes need to be considered concurrently. In this state-of-the-art report, we analyze current practices and classify techniques along four axes: layouts, view operations, layout operations, and data operations. We also provide an analysis of tasks specific to multivariate networks and give recommendations for which technique to use in which scenario. Finally, we survey application areas and evaluation methodologies.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13728", "refList": ["10.2312/eurovisstar.20151110", "10.1111/cgf.12106", "10.1109/iv.2016.19", "10.1111/j.1467-8659.2011.02087.x", "10.1111/j.1467-8659.2008.01214.x", "10.1109/tvcg.2014.2346893", "10.1145/1124772.1124891", "10.1109/vast.2014.7042484", "10.1109/tvcg.2018.2865149", "10.1080/10447318.2010.516722", "10.1109/icsmc.2011.6084125", "10.1109/mcg.2011.103", "10.1117/12.378894", "10.1145/2207676.2208293", "10.1109/tvcg.2009.122", "10.1111/cgf.12935", "10.1016/s0020-0255(02)00191-3", "10.1038/nmeth.1436", "10.1109/tvcg.2011.187", "10.1109/32.177365", "10.1007/978-3-642-03658-3\\_47", "10.1109/tvcg.2006.147", "10.1109/pacificvis.2011.5742390", "10.1145/2470654.2466444", "10.1109/tvcg.2006.166", "10.1145/2470654.2470724", "10.1109/tvcg.2014.2346441", "10.1109/vizsec.2005.1532070", "10.1101/gr.092759.109", "10.1111/cgf.13184", "10.1109/tvcg.2018.2865940", "10.1016/j.scico.2012.05.002", "10.1109/biovis.2012.6378600", "10.1111/cgf.13213", "10.1109/noms.2006.1687547", "10.1109/tvcg.2008.141", "10.1007/978-3-540-78243-8\\_13", "10.1145/1029208.1029217", "10.1145/345513.345271", "10.1109/tvcg.2015.2467811", "10.1109/visual.1991.175815", "10.1186/1471-2105-10-375", "10.1007/978-3-319-06793-3\\_1", "10.1186/1471-2105-13-275", "10.1109/tvcg.2010.79", "10.1109/tvcg.2011.217", "10.1186/1471-2105-14-s19-s3", "10.3389/fmicb.2017.00010", "10.1109/infvis.2003.1249009", "10.1111/cgf.13187", "10.1111/j.1467-8659.2008.01231.x", "10.1109/tvcg.2011.144", "10.1145/2556288.2557010", "10.1111/j.1467-8659.2009.01710.x", "10.1109/infvis.2004.46", "10.1109/iv.2009.97", "10.1109/tvcg.2008.34", "10.1109/infvis.2003.1249011", "10.1109/iv.2016.41", "10.1109/pacificvis.2010.5429609", "10.1109/tvcg.2006.160", "10.1109/tvcg.2010.205", "10.1109/38.486685", "10.1109/tvcg.2006.106", "10.1057/palgrave.ivs.9500092", "10.1145/568522.568523", "10.1111/j.1467-8659.2008.01221.x", "10.1109/iv.2010.15", "10.1145/22339.22342", "10.1109/csmr.2009.17", "10.1109/tvcg.2017.2744898", "10.1109/tvcg.2008.117", "10.1186/1752-0509-3-82", "10.1109/tvcg.2008.61", "10.1109/pacificvis.2013.6596127", "10.1109/tvcg.2007.70529", "10.1186/1471-2105-15-198", "10.1109/tvcg.2009.128", "10.2312/eurovisshort.20151124", "10.1093/bioinformatics/btq675", "10.1109/tvcg.2009.143", "10.1109/tvcg.2014.2346752", "10.1109/tvcg.2013.124", "10.1007/s00450-007-0036-y", "10.1093/bioinformatics/17.suppl\\_1.s22", "10.2312/eurovisstar.20151109", "10.1007/978-3-319-06793-3\\_5", "10.1109/vissof.2007.4290706", "10.1109/tvcg.2009.145", "10.1109/mcas.2003.1228503", "10.1109/infvis.2005.1532128", "10.1145/989863.989941", "10.1109/infvis.1999.801860", "10.1111/j.1467-8659.2009.01687.x", "10.1109/tvcg.2013.120", "10.1109/tvcg.2007.70582", "10.1109/tvcg.2009.167", "10.1109/pacificvis.2010.5429590", "10.1109/infvis.2000.885091", "10.1109/tvcg.2012.189", "10.1109/tvcg.2009.108", "10.1109/tvcg.2013.154", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2010.159", "10.1016/s0306-4573(98)00024-7", "10.1109/tvcg.2013.223", "10.1002/sam.10071", "10.2312/vissym/eurovis07/083-090", "10.1109/infvis.2002.1173156", "10.1186/1471-2105-7-109", "10.1177/1473871612455983", "10.1073/pnas.95.25.14863", "10.1007/978-3-319-06793-3\\_2", "10.1111/cgf.12883", "10.1007/978-3-540-78243-8\\_9", "10.1057/palgrave.ivs.9500180", "10.1109/pacificvis.2012.6183556", "10.1117/12.872578", "10.1145/1168149.1168169", "10.1109/iv.2013.3", "10.1109/tvcg.2011.186", "10.2307/2685881", "10.1109/pacificvis.2015.7156354", "10.1109/tvcg.2016.2598885", "10.1109/tvcg.2018.2811488", "10.1109/infvis.2005.1532129", "10.1111/j.1467-8659.2009.01450.x", "10.1057/palgrave.ivs.9500162", "10.1109/tvcg.2009.116", "10.1109/2945.468391"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3029413", "title": "A Design Space of Vision Science Methods for Visualization Research", "year": "2020", "conferenceName": "InfoVis", "authors": "Madison A. Elliott;Christine Nothelfer;Cindy Xiong;Danielle Albers Szafir", "citationCount": "0", "affiliation": "Elliott, MA (Corresponding Author), Univ British Columbia, Vancouver, BC, Canada. Elliott, Madison A., Univ British Columbia, Vancouver, BC, Canada. Nothelfer, Christine, Northwestern Univ, Evanston, IL 60208 USA. Xiong, Cindy, Univ Massachusetts, Amherst, MA 01003 USA. Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "Canada;USA", "abstract": "A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.", "keywords": "Perception,human vision,empirical research,evaluation,HCI", "link": "http://dx.doi.org/10.1109/TVCG.2020.3029413", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1177/0886109909354981", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1093/bioinformatics/btq110", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1177/1744987107081254", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1177/174498710501000305", "10.1017/s1049096513001789", "10.1109/tvcg.2012.213", "10.1093/nar/gkz239", "10.1093/sysbio/sys062", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030405", "title": "Insights From Experiments With Rigor in an EvoBio Design Study", "year": "2020", "conferenceName": "InfoVis", "authors": "Jennifer Rogers;Austin H. Patton;Luke Harmon;Alexander Lex;Miriah D. Meyer", "citationCount": "0", "affiliation": "Rogers, J (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Rogers, Jen; Lex, Alexander; Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Patton, Austin H., Washington State Univ, Pullman, WA 99164 USA. Harmon, Luke, Univ Idaho, Moscow, ID 83843 USA.", "countries": "USA", "abstract": "Design study is an established approach of conducting problem-driven visualization research. The academic visualization community has produced a large body of work for reporting on design studies, informed by a handful of theoretical frameworks, and applied to a broad range of application areas. The result is an abundance of reported insights into visualization design, with an emphasis on novel visualization techniques and systems as the primary contribution of these studies. In recent work we proposed a new, interpretivist perspective on design study and six companion criteria for rigor that highlight the opportunities for researchers to contribute knowledge that extends beyond visualization idioms and software. In this work we conducted a year-long collaboration with evolutionary biologists to develop an interactive tool for visual exploration of multivariate datasets and phylogenetic trees. During this design study we experimented with methods to support three of the rigor criteria: ABUNDANT, REFLEXIVE, and TRANSPARENT. As a result we contribute two novel visualization techniques for the analysis of multivariate phylogenetic datasets, three methodological recommendations for conducting design studies drawn from reflections over our process of experimentation, and two writing devices for reporting interpretivist design study. We offer this work as an example for implementing the rigor criteria to produce a diverse range of knowledge contributions.", "keywords": "Methodologies,Application Motivated Visualization,Guidelines,Life Sciences Visualization,Health,Medicine,Biology,Bioinformatics,Genomics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030405", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1080/17493460802276893", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1109/tvcg.2012.213", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13963", "year": "2020", "title": "MotionGlyphs: Visual Abstraction of Spatio-Temporal Networks in Collective Animal Behavior", "conferenceName": "EuroVis", "authors": "Eren Cakmak;Hanna Sch{\\\"{a}}fer;Juri Buchm{\\\"{u}}ller;Johannes Fuchs;Tobias Schreck;A. Jordan;Daniel A. Keim", "citationCount": "0", "affiliation": "Cakmak, E (Corresponding Author), Univ Konstanz, Constance, Germany.\nCakmak, E (Corresponding Author), Ctr Adv Study Collect Behav, Constance, Germany.\nCakmak, E.; Schaefer, H.; Buchmueller, J.; Fuchs, J.; Jordan, A.; Keim, D., Univ Konstanz, Constance, Germany.\nCakmak, E.; Jordan, A.; Keim, D., Ctr Adv Study Collect Behav, Constance, Germany.\nSchreck, T., Graz Univ Technol, Graz, Austria.\nJordan, A., Max Planck Inst Anim Behav, Radolfzell am Bodensee, Germany.", "countries": "Germany;Austria", "abstract": "Domain experts for collective animal behavior analyze relationships between single animal movers and groups of animals over time and space to detect emergent group properties. A common way to interpret this type of data is to visualize it as a spatio-temporal network. Collective behavior data sets are often large, and may hence result in dense and highly connected node-link diagrams, resulting in issues of node-overlap and edge clutter. In this design study, in an iterative design process, we developed glyphs as a design for seamlessly encoding relationships and movement characteristics of a single mover or clusters of movers. Based on these glyph designs, we developed a visual exploration prototype, MotionGlyphs, that supports domain experts in interactively filtering, clustering, and animating spatio-temporal networks for collective animal behavior analysis. By means of an expert evaluation, we show how MotionGlyphs supports important tasks and analysis goals of our domain experts, and we give evidence of the usefulness for analyzing spatio-temporal networks of collective animal behavior.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13963", "refList": ["10.1109/tvcg.2007.70582", "10.1111/cgf.13213", "10.1109/pacificvis.2014.13", "10.1145/2093973.2094038", "10.1111/j.1467-8659.2009.01664.x", "10.1109/tvcg.2010.44", "10.1111/cgf.12106", "10.1111/cgf.12791", "10.1080/15230406.2014.890071", "10.1111/tgis.12100", "10.1111/j.1467-8659.2009.01451.x", "10.1179/000870409x12525737905042", "10.1111/cgf.12923", "10.1006/ijhc.2002.1017", "10.1145/1124772.1124891", "10.1109/tvcg.2011.213", "10.1109/vast.2014.7042484", "10.1007/s12650-016-0375-5", "10.1109/vlhcc.2012.6344514", "10.1073/pnas.1420068112", "10.1006/ijhc.1017", "10.1007/s00371-017-1461-y", "10.1109/infvis.2003.1249008", "10.1111/cgf.13728", "10.2312/conf/eg2013/stars/039-063", "10.1109/tvcg.2014.2346271", "10.1109/hicss.2011.339", "10.1068/p3104", "10.1145/2931002.2931012", "10.1109/asonam.2012.39", "10.1179/000870403235002042", "10.1145/2556288.2557010", "10.1145/2470654.2466443", "10.1109/tvcg.2011.209", "10.1016/j.tree.2013.06.002", "10.1111/1365-2656.12418", "10.1111/cgf.12872", "10.1145/2470654.2466444", "10.1109/tvcg.2014.2322594", "10.1109/tvcg.2008.125", "10.1109/tvcg.2014.2346426", "10.1109/tvcg.2006.166", "10.1111/cgf.12615", "10.1057/palgrave.ivs.9500170", "10.1007/3-540-36151-0", "10.1117/12.872578", "10.1016/j.tics.2008.10.002", "10.1006/jtbi.2002.3065", "10.1109/tvcg.2010.78", "10.1109/iv.2013.3", "10.1073/pnas.1001763107", "10.1109/pacificvis.2015.7156354", "10.1371/journal.pbio.1001805", "10.5220/0005303801230130", "10.1111/j.1467-8659.2009.01687.x", "10.1007/s10844-011-0159-2", "10.1007/s12650-018-00543-4", "10.1145/2669557.2669572", "10.1111/cgf.13184", "10.1186/s40462-015-0032-y", "10.1016/j.ins.2016.06.048", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13987", "year": "2020", "title": "Augmenting Node-Link Diagrams with Topographic Attribute Maps", "conferenceName": "EuroVis", "authors": "Reinhold Preiner;Johanna Schmidt;Katharina Kr{\\\"{o}}sl;Tobias Schreck;Gabriel Mistelbauer", "citationCount": "0", "affiliation": "Preiner, R (Corresponding Author), Graz Univ Technol, Inst Comp Graph \\& Knowledge Visualizat, Graz, Austria.\nPreiner, R.; Schreck, T., Graz Univ Technol, Inst Comp Graph \\& Knowledge Visualizat, Graz, Austria.\nSchmidt, J.; Kroesl, K., Virtual Real \\& Visualisierung Forsch GmbH, VRVis Zentrum, Vienna, Austria.\nKroesl, K., TU Wien, Inst Visual Comp \\& Human Ctr Technol, Vienna, Austria.\nMistelbauer, G., Otto von Guericke Univ, Dept Simulat \\& Graph, Magdeburg, Germany.", "countries": "Germany;Austria", "abstract": "We propose a novel visualization technique for graphs that are attributed with scalar data. In many scenarios, these attributes (e.g., birth date in a family network) provide ambient context information for the graph structure, whose consideration is important for different visual graph analysis tasks. Graph attributes are usually conveyed using different visual representations (e.g., color, size, shape) or by reordering the graph structure according to the attribute domain (e.g., timelines). While visual encodings allow graphs to be arranged in a readable layout, assessing contextual information such as the relative similarities of attributes across the graph is often cumbersome. In contrast, attribute-based graph reordering serves the comparison task of attributes, but typically strongly impairs the readability of the structural information given by the graph's topology. In this work, we augment force-directed node-link diagrams with a continuous ambient representation of the attribute context. This way, we provide a consistent overview of the graph's topological structure as well as its attributes, supporting a wide range of graph-related analysis tasks. We resort to an intuitive height field metaphor, illustrated by a topographic map rendering using contour lines and suitable color maps. Contour lines visually connect nodes of similar attribute values, and depict their relative arrangement within the global context. Moreover, our contextual representation supports visualizing attribute value ranges associated with graph nodes (e.g., lifespans in a family network) as trajectories routed through this height field. We discuss how user interaction with both the structural and the contextual information fosters exploratory graph analysis tasks. The effectiveness and versatility of our technique is confirmed in a user study and case studies from various application domains.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13987", "refList": ["10.1109/tvcg.2013.269", "10.1109/pacificvis.2010.5429590", "10.1073/pnas.0307654100", "10.1145/2505515.2505758", "10.1559/152304082783948286", "10.1109/pacificvis.2014.47", "10.1093/bioinformatics/btp432", "10.1111/j.1467-8659.2011.01898.x", "10.1111/cgf.12931", "10.1111/cgf.12880", "10.1109/tvcg.2014.2346422", "10.1111/j.1467-8659.2009.01706.x", "10.1109/tvcg.2016.2598795", "10.1111/cgf.12800", "10.1109/tvcg.2014.2315995", "10.1111/cgf.12656", "10.1111/cgf.13728", "10.1109/tvcg.2009.122", "10.1111/cgf.13211", "10.1109/tvcg.2007.70596", "10.1109/infvis.2002.1173152", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2003.1196007", "10.1109/infvis.2005.1532150", "10.1145/3243250.3243266", "10.1080/02693799008941549", "10.1371/journal.pone.0058779", "10.1109/infvis.1995.528686", "10.1111/cgf.12872", "10.1002/spe.4380211102", "10.1109/38.974518", "10.1145/3097983.3098130", "10.1002/aris.1440370106", "10.1145/1360612.1360691", "10.1109/mc.2016.145", "10.2307/3006914", "10.1111/j.1467-8659.2009.01683.x", "10.1145/1639714.1639784"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1111/cgf.13968", "year": "2020", "title": "Ocupado: Visualizing Location-Based Counts Over Time Across Buildings", "conferenceName": "EuroVis", "authors": "Michael Oppermann;Tamara Munzner", "citationCount": "0", "affiliation": "Oppermann, M (Corresponding Author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada.\nOppermann, Michael; Munzner, Tamara, Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada.", "countries": "Canada", "abstract": "Understanding how spaces in buildings are being used is vital for optimizing space utilization, for improving resource allocation, and for the design of new facilities. We present a multi-year design study that resulted in Ocupado, a set of visual decision-support tools centered around occupancy data for stakeholders in facilities management and planning. Ocupado uses WiFi devices as a proxy for human presence, capturing location-based counts that preserve privacy without trajectories. We contribute data and task abstractions for studying space utilization for combinations of data granularities in both space and time. In addition, we contribute generalizable design choices for visualizing location-based counts relating to indoor environments. We provide evidence of Ocupado's utility through multiple analysis scenarios with real-world data refined through extensive stakeholder feedback, and discussion of its take-up by our industry partner.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13968", "refList": ["10.1111/j.1467-8659.2011.01946.x", "10.1109/vast.2012.6400553", "10.1007/978-3-642-37583-5", "10.14257/ijt.2018.6.1.01", "10.1145/882082.882086", "10.1109/bdva.2018.8534026", "10.1109/tvcg.2014.2346312", "10.1016/j.pmcj.2016.08.011", "10.1057/palgrave.ivs.9500165", "10.1109/tvcg.2014.2346323", "10.1109/infvis.1999.801851", "10.1109/igcc.2011.6008560", "10.1016/j.energy.2015.09.002", "10.1080/13658816.2010.511223", "10.1109/tvcg.2007.70621", "10.1109/tvcg.2016.2598585", "10.1109/tvcg.2011.195", "10.1109/tvcg.2009.100", "10.1109/rev.2014.6784260", "10.1145/642611.642616", "10.1007/3-540-31190-4", "10.1109/vast.2015.7347672", "10.1109/vast.2012.6400557", "10.1109/infvis.2005.1532150", "10.1111/j.1467-8659.2009.01710.x", "10.1109/tvcg.2011.209", "10.1109/tvcg.2007.70583", "10.1111/cgf.12883", "10.1007/3-4.540-31190-4", "10.1145/2517351.2517370", "10.1007/s12273-017-0355-2", "10.1145/985692.985706", "10.3138/carto.46.4.239", "10.1016/j.autcon.2014.03.023", "10.1109/tvcg.2012.213", "10.1051/sbuild/2017005", "10.1109/tvcg.2010.162", "10.1145/1360612.1360700", "10.1038/srep01376", "10.1109/tvcg.2018.2811488", "10.1145/3025453.3026055", "10.1109/tvcg.2009.111", "10.1109/tvcg.2015.2466971", "10.1109/tvcg.2013.173", "10.1109/tvcg.2017.2666146", "10.1109/tvcg.2018.2864914", "10.1007/978-0-85729-079-3", "10.1109/tvcg.2016.2614803"], "wos": 1, "children": [], "len": 1}], "len": 105}, {"doi": "10.1111/cgf.12924", "year": "2016", "title": "AVOCADO: Visualization of Workflow-Derived Data Provenance for Reproducible Biomedical Research", "conferenceName": "EuroVis", "authors": "Holger Stitz;S. Luger;Marc Streit;Nils Gehlenborg", "citationCount": "14", "affiliation": "Stitz, H (Corresponding Author), Johannes Kepler Univ Linz, Linz, Austria.\nStitz, H.; Luger, S.; Streit, M., Johannes Kepler Univ Linz, Linz, Austria.\nGehlenborg, N., Harvard Med Sch, Boston, MA USA.", "countries": "USA;Austria", "abstract": "A major challenge in data-driven biomedical research lies in the collection and representation of data provenance information to ensure that findings are reproducibile. In order to communicate and reproduce multi-step analysis workflows executed on datasets that contain data for dozens or hundreds of samples, it is crucial to be able to visualize the provenance graph at different levels of aggregation. Most existing approaches are based on node-link diagrams, which do not scale to the complexity of typical data provenance graphs. In our proposed approach, we reduce the complexity of the graph using hierarchical and motif-based aggregation. Based on user action and graph attributes, a modular degree-of-interest (DoI) function is applied to expand parts of the graph that are relevant to the user. This interest-driven adaptive approach to provenance visualization allows users to review and communicate complex multi-step analyses, which can be based on hundreds of files that are processed by numerous workflows. We have integrated our approach into an analysis platform that captures extensive data provenance information, and demonstrate its effectiveness by means of a biomedical usage scenario.", "keywords": "", "link": "https://doi.org/10.1111/cgf.12924", "refList": ["10.1186/s12859-015-0550-z", "10.1109/tvcg.2009.108", "10.1109/tvcg.2013.225", "10.1126/science.aac8041", "10.1161/circresaha.114.303819", "10.1109/visual.2005.1532788", "10.1038/483531a", "10.1109/tvcg.2011.185", "10.7155/jgaa.00001", "10.1186/gb-2010-11-8-r86", "10.1093/bib/bbl022", "10.1093/bib/bbs078", "10.1093/bioinformatics/btq415", "10.1371/journal.pone.0127612", "10.1126/science.348.6242.1411", "10.1093/nar/gkt328", "10.1126/science.298.5594.824", "10.1145/1168149.1168168", "10.1109/tvcg.2009.84", "10.1038/nrg2641", "10.1109/tvcg.2013.109", "10.1145/1029208.1029225", "10.1038/ng.2761", "10.1145/989863.989941", "10.1109/pacificvis.2015.7156375", "10.1145/22339.22342", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2017.2744805", "title": "SOMFlow: Guided Exploratory Cluster Analysis with Self-Organizing Maps and Analytic Provenance", "year": "2017", "conferenceName": "VAST", "authors": "Dominik Sacha;Matthias Kraus;J\u00fcrgen Bernard;Michael Behrisch;Tobias Schreck;Yuki Asano;Daniel A. Keim", "citationCount": "10", "affiliation": "Sacha, D (Corresponding Author), Univ Konstanz, Constance, Germany. Sacha, Dominik; Kraus, Matthias; Behrisch, Michael; Keim, Daniel A., Univ Konstanz, Constance, Germany. Bernard, Juergen, Tech Univ Darmstadt, Darmstadt, Germany. Schreck, Tobias, Graz Univ Technol, Graz, Austria. Asano, Yuki, Univ Tubingen, Tubingen, Germany.", "countries": "Germany;Austria", "abstract": "Clustering is a core building block for data analysis, aiming to extract otherwise hidden structures and relations from raw datasets, such as particular groups that can be effectively related, compared, and interpreted. A plethora of visual-interactive cluster analysis techniques has been proposed to date, however, arriving at useful clusterings often requires several rounds of user interactions to fine-tune the data preprocessing and algorithms. We present a multi-stage Visual Analytics (VA) approach for iterative cluster refinement together with an implementation (SOMFlow) that uses Self-Organizing Maps (SOM) to analyze time series data. It supports exploration by offering the analyst a visual platform to analyze intermediate results, adapt the underlying computations, iteratively partition the data, and to reflect previous analytical activities. The history of previous decisions is explicitly visualized within a flow graph, allowing to compare earlier cluster refinements and to explore relations. We further leverage quality and interestingness measures to guide the analyst in the discovery of useful patterns, relations, and data partitions. We conducted two pair analytics experiments together with a subject matter expert in speech intonation research to demonstrate that the approach is effective for interactive data analysis, supporting enhanced understanding of clustering results as well as the interactive process itself.", "keywords": "Visual Analytics,Interaction,Visual Cluster Analysis,Quality Metrics,Guidance,Self-Organizing Maps,Time Series", "link": "http://dx.doi.org/10.1109/TVCG.2017.2744805", "refList": ["10.1109/tvcg.2007.70582", "10.1109/tvcg.2015.2467591", "10.1109/tnn.2002.804221", "10.1111/j.1467-8659.2009.01467.x", "10.1111/cgf.12106", "10.1109/tvcg.2014.2346260", "10.1109/tvcg.2011.229", "10.1109/vast.2010.5653042", "10.1111/cgf.12924", "10.1145/2232817.2232844", "10.1109/icdew.2006.75", "10.1109/vast.2014.7042480", "10.1109/tvcg.2013.178", "10.1007/978-3-642-21602-2\\_67", "10.1109/pacificvis.2016.7465244", "10.1145/2207676.2208293", "10.1117/12.872545", "10.1109/hicss.2011.339", "10.1109/vast.2015.7347625", "10.1007/s00799-014-0134-y", "10.1109/tvcg.2014.2346481", "10.1145/1963192.1963283", "10.1007/978-3-642-00304-2\\_1", "10.1016/0377-0427(87)90125-7", "10.1145/2362456.2362485", "10.1016/j.neunet.2012.09.018", "10.1109/tvcg.2011.188", "10.1117/12.2079841", "10.1057/palgrave.ivs.9500170", "10.1057/ivs.2008.29", "10.1109/tvcg.2010.242", "10.1002/(sici)1097-4571(199307)44:6", "10.1007/978-0-387-09823-4\\_56", "10.2200/s00730ed1v01y201608vis007", "10.1016/s1088-467x(99)00013-x", "10.1109/vast.2010.5652443", "10.1109/mcg.2015.50", "10.1016/b978-044450270-4/50003-6", "10.1016/j.neucom.2017.01.105", "10.1109/tvcg.2016.2598468", "10.1007/978-0-85729-079-3", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2864477", "title": "Clustrophile 2: Guided Visual Clustering Analysis", "year": "2018", "conferenceName": "VAST", "authors": "Marco Cavallo;\u00c7agatay Demiralp", "citationCount": "8", "affiliation": "Cavallo, M (Corresponding Author), IBM Res, Yorktown Hts, NY 10598 USA. Cavallo, Marco, IBM Res, Yorktown Hts, NY 10598 USA. Demiralp, Cagatay, MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA. Demiralp, Cagatay, Fitnescity Labs, Cambridge, MA USA.", "countries": "USA", "abstract": "Data clustering is a common unsupervised learning method frequently used in exploratory data analysis. However, identifying relevant structures in unlabeled, high-dimensional data is nontrivial, requiring iterative experimentation with clustering parameters as well as data features and instances. The number of possible clusterings for a typical dataset is vast, and navigating in this vast space is also challenging. The absence of ground-truth labels makes it impossible to define an optimal solution, thus requiring user judgment to establish what can be considered a satisfiable clustering result. Data scientists need adequate interactive tools to effectively explore and navigate the large clustering space so as to improve the effectiveness of exploratory clustering analysis. We introduce Clustrophile 2, a new interactive tool for guided clustering analysis. Clustrophile 2 guides users in clustering-based exploratory analysis, adapts user feedback to improve user guidance, facilitates the interpretation of clusters, and helps quickly reason about differences between clusterings. To this end, Clustrophile 2 contributes a novel feature, the Clustering Tour, to help users choose clustering parameters and assess the quality of different clustering results in relation to current analysis goals and user expectations. We evaluate Clustrophile 2 through a user study with 12 data scientists, who used our tool to explore and interpret sub-cohorts in a dataset of Parkinson's disease patients. Results suggest that Clustrophile 2 improves the speed and effectiveness of exploratory clustering analysis for both experts and non-experts.", "keywords": "Clustering tour,Guided data analysis,Unsupervised learning,Exploratory data analysis,Interactive clustering analysis,Interpretability,Explainability,Visual data exploration recommendation,Dimensionality reduction,What-if analysis,Clustrophile", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864477", "refList": ["10.1002/mds.22340", "10.1186/1471-2105-15-s6-s4", "10.1101/cshperspect.a009282", "10.14778/2831360.2831371", "10.1186/1471-2105-16-s11-s5", "10.1109/tvcg.2012.219", "10.1007/978-0-387-84858-7", "10.1109/icdm.2010.35", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2012.03110.x", "10.1111/1467-9868.00196", "10.1137/0906011", "10.1109/tvcg.2017.2744805", "10.1016/s0306-4379(01)00008-4", "10.1016/j.neucom.2014.09.062", "10.1023/a:1012487302797", "10.1007/bf02985802", "10.1109/tvcg.2013.119", "10.1093/nar/gkv468", "10.1126/science.290.5500.2319", "10.1016/0377-0427(87)90125-7", "10.1109/tvcg.2010.138", "10.1109/tvcg.2011.188", "10.1057/ivs.2008.29", "10.1126/science.290.5500.2323", "10.1109/tvcg.2012.207", "10.1109/34.868688", "10.1007/bf02289565", "10.1109/t-c.1974.224051"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934261", "title": "Ablate, Variate, and Contemplate: Visual Analytics for Discovering Neural Architectures", "year": "2019", "conferenceName": "VAST", "authors": "Dylan Cashman;Adam Perer;Remco Chang;Hendrik Strobelt", "citationCount": "1", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Perer, Adam, Carnegie Mellon Univ, Pittsburgh, PA 15213 USA. Strobelt, Hendrik, MIT IBM Watson AI Lab, Cambridge, MA USA.", "countries": "USA", "abstract": "The performance of deep learning models is dependent on the precise configuration of many layers and parameters. However, there are currently few systematic guidelines for how to configure a successful model. This means model builders often have to experiment with different configurations by manually programming different architectures (which is tedious and time consuming) or rely on purely automated approaches to generate and train the architectures (which is expensive). In this paper, we present Rapid Exploration of Model Architectures and Parameters, or REMAP, a visual analytics tool that allows a model builder to discover a deep learning model quickly via exploration and rapid experimentation of neural network architectures. In REMAP, the user explores the large and complex parameter space for neural network architectures using a combination of global inspection and local experimentation. Through a visual overview of a set of models, the user identifies interesting clusters of architectures. Based on their findings, the user can run ablation and variation experiments to identify the effects of adding, removing, or replacing layers in a given architecture and generate new models accordingly. They can also handcraft new models using a simple graphical interface. As a result, a model builder can build deep learning models quickly, efficiently, and without manual programming. We inform the design of REMAP through a design study with four deep learning model builders. Through a use case, we demonstrate that REMAP allows users to discover performant neural network architectures efficiently using visual exploration and user-defined semi-automated searches through the model space.", "keywords": "visual analytics,neural networks,parameter space exploration", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934261", "refList": ["10.1109/mcg.2018.2878902", "10.1111/cgf.12639", "10.1109/tvcg.2017.2744938", "10.1117/12.2007316", "10.1109/cvpr.2014.81", "10.1016/j.csda.2008.02.031", "10.1080/00994480.2000.10748487", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2013.125", "10.1109/cvpr.2015.7298594", "10.1111/j.1467-8659.2009.01475.x", "10.1109/tvcg.2012.65", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2017.2744805", "10.1145/2487575.2487629", "10.1109/tvcg.2018.2865044", "10.23915/distill.00010", "10.1109/72.279181", "10.1109/tvcg.2017.2744199", "10.1007/s13398-014-0173-7.2", "10.1109/tvcg.2018.2864504", "10.1109/vast.2012.6400490", "10.1007/978-3-319-10590-1\\_53", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2014.2346321", "10.1094/pdis-11-11-0999-pdn", "10.1109/ijcnn.2015.7280767", "10.1109/tvcg.2017.2744878", "10.5555/3326943.3327130", "10.1109/tvcg.2017.2744718", "10.1109/iccv.2015.169", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2014.223", "10.1109/cvpr.2016.90", "10.1109/vast.2010.5652443", "10.1111/cgf.13681", "10.1109/tvcg.2016.2598831", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028888", "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes", "year": "2020", "conferenceName": "VAST", "authors": "Yuxin Ma;Arlen Fan;Jingrui He;Arun Reddy Nelakurthi;Ross Maciejewski", "citationCount": "0", "affiliation": "Ma, YX (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Ma, Yuxin; Fan, Arlen; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. He, Jingrui, Univ Illinois, Champaign, IL USA. Nelakurthi, Arun Reddy, Samsung Res Amer, Mountain View, CA USA.", "countries": "USA", "abstract": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.", "keywords": "Transfer learning,deep learning,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028888", "refList": ["10.1109/tvcg.2014.2346578", "10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1109/tpami.2018.2868685", "10.1145/2702123.2702509", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2016.2598828", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2013.65", "10.1145/2976749.2978318", "10.1007/978-3-030-01424-7\\_27", "10.1109/tvcg.2019.2934261", "10.1007/s11704-016-6028-y", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/iccv.2015.279", "10.1109/mci.2018.2840738", "10.1109/tvcg.2019.2892483", "10.1109/vast.2018.8802509", "10.1109/tvcg.2013.124", "10.1186/s40537-016-0043-6", "10.1109/tvcg.2018.2864475", "10.1145/3200489", "10.1109/tvcg.2018.2865044", "10.1111/cgf.13210", "10.1109/tvcg.2018.2816223", "10.23915/distill.00007", "10.1109/tvcg.2017.2744199", "10.1109/tkde.2018.2876857", "10.1109/tvcg.2019.2934631", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2011.188", "10.1007/978-3-642-15561-1\\_16", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1109/tvcg.2017.2744718", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2016.2598541", "10.1109/tkde.2009.191", "10.1145/3065386", "10.1016/j.ins.2016.03.021", "10.1109/tvcg.2019.2903943", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/iccv.2017.74", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2012.207", "10.1111/cgf.13092", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2017.2754480", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934395", "title": "The Impact of Immersion on Cluster Identification Tasks", "year": "2019", "conferenceName": "InfoVis", "authors": "Matthias Kraus;Niklas Weiler;Daniela Oelke;Johannes Kehrer;Daniel A. Keim;Johannes Fuchs", "citationCount": "4", "affiliation": "Kraus, M (Corresponding Author), Univ Konstanz, Constance, Germany. Kraus, M.; Weiler, N.; Keim, D. A.; Fuchs, J., Univ Konstanz, Constance, Germany. Oelke, D.; Kehrer, J., Siemens Corp Technol, Munich, Germany.", "countries": "Germany", "abstract": "Recent developments in technology encourage the use of head-mounted displays (HMDs) as a medium to explore visualizations in virtual realities (VRs). VR environments (VREs) enable new, more immersive visualization design spaces compared to traditional computer screens. Previous studies in different domains, such as medicine, psychology, and geology, report a positive effect of immersion, e.g., on learning performance or phobia treatment effectiveness. Our work presented in this paper assesses the applicability of those findings to a common task from the information visualization (InfoVis) domain. We conducted a quantitative user study to investigate the impact of immersion on cluster identification tasks in scatterplot visualizations. The main experiment was carried out with 18 participants in a within-subjects setting using four different visualizations, (1) a 2D scatterplot matrix on a screen, (2) a 3D scatterplot on a screen, (3) a 3D scatterplot miniature in a VRE and (4) a fully immersive 3D scatterplot in a VRE. The four visualization design spaces vary in their level of immersion, as shown in a supplementary study. The results of our main study indicate that task performance differs between the investigated visualization design spaces in terms of accuracy, efficiency, memorability, sense of orientation, and user preference. In particular, the 2D visualization on the screen performed worse compared to the 3D visualizations with regard to the measured variables. The study shows that an increased level of immersion can be a substantial benefit in the context of 3D data and cluster detection.", "keywords": "Virtual reality,evaluation,visual analytics,clustering", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934395", "refList": ["10.1111/j.1467-9671.2010.01194.x", "10.1177/1473871614556393", "10.1109/tvcg.2008.153", "10.1111/cgf.13430", "10.1109/tvcg.2004.17", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2007.70433", "10.1089/109493103322011641", "10.2307/2290001", "10.2307/2289444", "10.1016/j.ijhcs.2008.04.004", "10.1109/tvcg.2012.42", "10.1109/vr.2018.8447558", "10.1162/105474602760204318", "10.2307/2986199", "10.3758/bf03200735", "10.1097/opx.0b013e31825da430", "10.1109/infvis.1999.801851", "10.1109/vast.2008.4677350", "10.1162/105474698565686", "10.1109/iv.2013.51", "10.1109/tvcg.2017.2745941", "10.1109/infvis.1998.729555", "10.1111/j.1467-8659.2012.03125.x", "10.1109/2945.506223", "10.1016/s1045-926x(03)00046-6", "10.1097/00042871-200701010-00099", "10.1109/iv.2004.1320137", "10.1109/visual.2002.1183816", "10.1109/tvcg.2018.2864477", "10.1109/vast.2007.4389000", "10.1016/j.ijms.2006.06.015", "10.1007/pl00022704", "10.1111/cgf.13072", "10.1145/3290605.3300555", "10.1109/bdva.2016.7787042", "10.1109/hicss.2013.197", "10.2312/vissym/vissym04/255-260", "10.1115/imece2007-43781", "10.1007/978-4-431-68057-43", "10.1109/tvcg.2016.2520921", "10.1109/mcg.2004.1255801", "10.1109/tvcg.2013.153", "10.1162/pres.1996.5.3.274", "10.1198/106186004x12425", "10.1162/105474601300343612", "10.1162/pres.1997.6.6.603", "10.1111/cgf.12804", "10.1111/j.1467-8659.2009.01666.x", "10.1109/vr.2004.1310069", "10.1109/vr.1999.756938", "10.1007/978-3-658-02897-8\\_16", "10.1162/pres\\_a\\_00016", "10.2307/2288711"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030450", "title": "Shared Surfaces and Spaces: Collaborative Data Visualisation in a Co-located Immersive Environment", "year": "2020", "conferenceName": "InfoVis", "authors": "Benjamin Lee;Xiaoyun Hu;Maxime Cordeil;Arnaud Prouzeau;Bernhard Jenny;Tim Dwyer", "citationCount": "0", "affiliation": "Lee, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Lee, Benjamin; Hu, Xiaoyun; Cordeil, Maxime; Prouzeau, Arnaud; Jenny, Bernhard; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia.", "countries": "Australia", "abstract": "Immersive technologies offer new opportunities to support collaborative visual data analysis by providing each collaborator a personal, high-resolution view of a flexible shared visualisation space through a head mounted display. However, most prior studies of collaborative immersive analytics have focused on how groups interact with surface interfaces such as tabletops and wall displays. This paper reports on a study in which teams of three co-located participants are given flexible visualisation authoring tools to allow a great deal of control in how they structure their shared workspace. They do so using a prototype system we call FIESTA: the Free-roaming Immersive Environment to Support Team-based Analysis. Unlike traditional visualisation tools, FIESTA allows users to freely position authoring interfaces and visualisation artefacts anywhere in the virtual environment, either on virtual surfaces or suspended within the interaction space. Our participants solved visual analytics tasks on a multivariate data set, doing so individually and collaboratively by creating a large number of 2D and 3D visualisations. Their behaviours suggest that the usage of surfaces is coupled with the type of visualisation used, often using walls to organise 2D visualisations, but positioning 3D visualisations in the space around them. Outside of tightly-coupled collaboration, participants followed social protocols and did not interact with visualisations that did not belong to them even if outside of its owner's personal workspace.", "keywords": "Immersive analytics,collaboration,virtual reality,qualitative study,multivariate data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030450", "refList": ["10.1109/tvcg.2008.153", "10.1007/s10606-004-5062-8", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2019.2914677", "10.1016/0020-7373(91)90039-a", "10.1109/tvcg.2011.287", "10.1117/12.2005484", "10.1109/mmul.2009.35", "10.1109/tvcg.2019.2934803", "10.1145/3359996.3364242", "10.1109/immersive.2016.7932384", "10.1016/j.future.2008.07.015", "10.1057/palgrave.ivs.9500167", "10.1109/mcg.2019.2898941", "10.1145/3343055.3360746", "10.1007/978-3-319-45853-3\\_8", "10.1145/2576099", "10.1145/2858036.2858039", "10.1109/tvcg.2016.2599107", "10.1145/3173574.3173664", "10.1007/978-3-030-01388-2\\_2", "10.1109/vr.2019.8797978", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1007/978-3-030-01388-2\\_8", "10.1023/a:1021271517844", "10.1109/bigdata.2014.7004282", "10.1109/tvcg.2019.2934395", "10.1109/ismar.2010.5643530", "10.1007/978-3-030-01388-22", "10.1145/2133806.2133821", "10.1109/3dvis.2014.7160093", "10.1145/2556288.2557058", "10.1109/vr.2019.8797845"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/tvcg.2020.3030347", "title": "Integrating Prior Knowledge in Mixed-Initiative Social Network Clustering", "year": "2020", "conferenceName": "VAST", "authors": "Alexis Pister;Paolo Buono;Jean-Daniel Fekete;Catherine Plaisant;Paola Valdivia", "citationCount": "0", "affiliation": "Pister, A (Corresponding Author), Univ Paris Saclay, CNRS, Inria, LRI, Gif Sur Yvette, France. Pister, Alexis; Fekete, Jean-Daniel; Plaisant, Catherine; Valdivia, Paola, Univ Paris Saclay, CNRS, Inria, LRI, Gif Sur Yvette, France. Buono, Paolo, Univ Bari, Bari, Italy. Plaisant, Catherine, Univ Maryland, College Pk, MD USA.", "countries": "Italy;USA;France", "abstract": "We propose a new approach-called PK-clustering-to help social scientists create meaningful clusters in social networks. Many clustering algorithms exist but most social scientists find them difficult to understand, and tools do not provide any guidance to choose algorithms, or to evaluate results taking into account the prior knowledge of the scientists. Our work introduces a new clustering approach and a visual analytics user interface that address this issue. It is based on a process that 1) captures the prior knowledge of the scientists as a set of incomplete clusters, 2) runs multiple clustering algorithms (similarly to clustering ensemble methods), 3) visualizes the results of all the algorithms ranked and summarized by how well each algorithm matches the prior knowledge, 4) evaluates the consensus between user-selected algorithms and 5) allows users to review details and iteratively update the acquired knowledge. We describe our approach using an initial functional prototype, then provide two examples of use and early feedback from social scientists. We believe our clustering approach offers a novel constructive method to iteratively build knowledge while avoiding being overly influenced by the results of often randomly selected black-box clustering algorithms.", "keywords": "Social network analysis,network visualization,clustering,mixed-initiative,prior knowledge,user interface", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030347", "refList": ["10.1109/tvcg.2016.2615308", "10.2312/eurovisstar.20151110", "10.1145/1353343.1353398", "10.1109/tvcg.2017.2745178", "10.1162/153244303321897735", "10.4230/dagrep.8.10.1", "10.1007/978-3-662-47824-0\\_2", "10.1145/1124772.1124830", "10.1109/tvcg.2014.2346260", "10.1109/2945.841121", "10.1109/hicss.2016.181", "10.1073/pnas.122653799", "10.1103/physreve.76.036102", "10.2312/eur0vissh0rt.20141162", "10.1186/1471-2105-16-s11-s5", "10.1109/tvcg.2014.2315995", "10.1007/s41109-019-0165-9", "10.1109/icdm.2013.167", "10.1559/1523040054738936", "10.1109/tpami.2002.1017616", "10.1007/978-1-4471-6497-5\\_1", "10.1057/palgrave/ivs/9500006", "10.1007/s13042-015-0338-5", "10.1145/3172867", "10.1109/vast.2015.7347625", "10.1103/physreve.76.046115", "10.1109/ijcnn.2008.4634108", "10.1109/tvcg.2014.2346248", "10.1145/3340960", "10.1109/icdm.2009.143", "10.1109/tvcg.2006.147", "10.2312/eurovisshort.20141162", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2014.2346321", "10.1145/2493102.2499786", "10.1002/wics.1270", "10.1016/j.physrep.2009.11.002", "10.1145/302979.303030", "10.1145/1124772.1124890", "10.2312/eur0visstar.20151110", "10.1145/3110025.3110030", "10.1016/0031-3203(95)00120-4", "10.1093/bioinformatics/bti373", "10.1109/tkde.2007.190689", "10.1016/j.ejrs.2018.11.001", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2019.2933196", "10.1109/tvcg.2006.76"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030369", "title": "Investigating Visual Analysis of Differentially Private Data", "year": "2020", "conferenceName": "InfoVis", "authors": "Dan Zhang;Ali Sarvghad;Gerome Miklau", "citationCount": "0", "affiliation": "Zhang, D (Corresponding Author), Univ Massachusetts, Amherst, MA 01003 USA. Zhang, Dan; Sarvghad, Ali; Miklau, Gerome, Univ Massachusetts, Amherst, MA 01003 USA.", "countries": "USA", "abstract": "Differential Privacy is an emerging privacy model with increasing popularity in many domains. It functions by adding carefully calibrated noise to data that blurs information about individuals while preserving overall statistics about the population. Theoretically, it is possible to produce robust privacy-preserving visualizations by plotting differentially private data. However, noise-induced data perturbations can alter visual patterns and impact the utility of a private visualization. We still know little about the challenges and opportunities for visual data exploration and analysis using private visualizations. As a first step towards filling this gap, we conducted a crowdsourced experiment, measuring participants' performance under three levels of privacy (high, low, non-private) for combinations of eight analysis tasks and four visualization types (bar chart, pie chart, line chart, scatter plot). Our findings show that for participants' accuracy for summary tasks (e.g., find clusters in data) was higher that value tasks (e.g., retrieve a certain value). We also found that under DP, pie chart and line chart offer similar or better accuracy than bar chart. In this work, we contribute the results of our empirical study, investigating the task-based effectiveness of basic private visualizations, a dichotomous model for defining and measuring user success in performing visual analysis tasks under DP, and a set of distribution metrics for tuning the injection to improve the utility of private visualizations.", "keywords": "Differential privacy,information visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030369", "refList": ["10.1109/tvcg.2016.2615308", "10.2312/eurovisstar.20151110", "10.1145/1353343.1353398", "10.1109/tvcg.2017.2745178", "10.1162/153244303321897735", "10.4230/dagrep.8.10.1", "10.1145/1124772.1124830", "10.1007/978-3-662-47824-0\\_2", "10.1109/2945.841121", "10.1109/hicss.2016.181", "10.1073/pnas.122653799", "10.1103/physreve.76.036102", "10.1186/1471-2105-16-s11-s5", "10.1109/tvcg.2014.2315995", "10.1007/s41109-019-0165-9", "10.1109/icdm.2013.167", "10.1559/1523040054738936", "10.1109/tpami.2002.1017616", "10.1007/978-1-4471-6497-5\\_1", "10.1057/palgrave/ivs/9500006", "10.1007/s13042-015-0338-5", "10.1145/3172867", "10.1109/vast.2015.7347625", "10.1103/physreve.76.046115", "10.1109/ijcnn.2008.4634108", "10.1109/tvcg.2014.2346248", "10.1145/3340960", "10.1109/tvcg.2006.147", "10.2312/eurovisshort.20141162", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2014.2346321", "10.1145/2493102.2499786", "10.1002/wics.1270", "10.1016/j.physrep.2009.11.002", "10.1145/1124772.1124890", "10.1145/3110025.3110030", "10.1016/0031-3203(95)00120-4", "10.1093/bioinformatics/bti373", "10.1109/tkde.2007.190689", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2019.2933196"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030471", "title": "Visual Analysis of Discrimination in Machine Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Qianwen Wang;Zhenhua Xu;Zhutian Chen;Yong Wang;Shixia Liu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Xu, Zhenhua; Chen, Zhutian; Wang, Yong; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Shixia, Tsinghua Univ, Beijing, Peoples R China.", "countries": "China", "abstract": "The growing use of automated decision-making in critical applications, such as crime prediction and college admission, has raised questions about fairness in machine learning. How can we decide whether different treatments are reasonable or discriminatory? In this paper, we investigate discrimination in machine learning from a visual analytics perspective and propose an interactive visualization tool, DiscriLens, to support a more comprehensive analysis. To reveal detailed information on algorithmic discrimination, DiscriLens identifies a collection of potentially discriminatory itemsets based on causal modeling and classification rules mining. By combining an extended Euler diagram with a matrix-based visualization, we develop a novel set visualization to facilitate the exploration and interpretation of discriminatory itemsets. A user study shows that users can interpret the visually encoded information in DiscriLens quickly and accurately. Use cases demonstrate that DiscriLens provides informative guidance in understanding and reducing algorithmic discrimination.", "keywords": "Machine Learning,Discrimination,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030471", "refList": ["10.1109/tvcg.2019.2934396", "10.2312/eurovisstar.20141170", "10.1145/3357384.3357910", "10.1111/cgf.12791", "10.1109/tvcg.2018.2861397", "10.1111/j.1467-8659.2011.01898.x", "10.1145/2702123.2702237", "10.1109/tvcg.2019.2934798", "10.1109/mcg.2017.21", "10.1109/tvcg.2019.2934300", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2018.2859997", "10.1145/3173574.3174237", "10.1109/tvcg.2018.2865126", "10.1145/1718487.1718520", "10.1109/tvcg.2017.2743858", "10.1109/pacificvis.2015.7156392", "10.1109/tvcg.2018.2864477", "10.1145/324133.324140", "10.1137/140976649", "10.1145/3219819.3220088", "10.1109/tvcg.2019.2934805", "10.1145/1134271.1134277", "10.1137/090772745", "10.1016/j.jelectrocard.2010.09.003", "10.1109/tvcg.2012.253", "10.1145/2556612", "10.1109/tvcg.2013.173", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13681", "year": "2019", "title": "A User-based Visual Analytics Workflow for Exploratory Model Analysis", "conferenceName": "EuroVis", "authors": "Dylan Cashman;Shah Rukh Humayoun;Florian Heimerl;Kendall Park;Subhajit Das;John Thompson;Bahador Saket;Abigail Mosca;John T. Stasko;Alex Endert;Michael Gleicher;Remco Chang", "citationCount": "6", "affiliation": "Cashman, D; Humayoun, SR (Corresponding Author), Tufts Univ, Medford, MA 02155 USA.\nCashman, Dylan; Humayoun, Shah Rukh; Mosca, Abigail; Chang, Remco, Tufts Univ, Medford, MA 02155 USA.\nHeimerl, Florian; Park, Kendall; Gleicher, Michael, Georgia Tech, Atlanta, GA USA.\nDas, Subhajit; Thompson, John; Saket, Bahador; Stasko, John; Endert, Alex, Univ Wisconsin, Madison, WI USA.", "countries": "USA", "abstract": "Many visual analytics systems allow users to interact with machine learning models towards the goals of data exploration and insight generation on a given dataset. However, in some situations, insights may be less important than the production of an accurate predictive model for future use. In that case, users are more interested in generating of diverse and robust predictive models, verifying their performance on holdout data, and selecting the most suitable model for their usage scenario. In this paper, we consider the concept of Exploratory Model Analysis (EMA), which is defined as the process of discovering and selecting relevant models that can be used to make predictions on a data source. We delineate the differences between EMA and the well-known term exploratory data analysis in terms of the desired outcome of the analytic process: insights into the data or a set of deployable models. The contributions of this work are a visual analytics system workflow for EMA, a user study, and two use cases validating the effectiveness of the workflow. We found that our system workflow enabled users to generate complex models, to assess them for various qualities, and to select the most relevant model for their task.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13681", "refList": ["10.1109/tvcg.2017.2744683", "10.1111/cgf.12639", "10.1007/s11390-016-1663-1", "10.1109/tvcg.2017.2744938", "10.1117/12.2007316", "10.1109/tvcg.2017.2745178", "10.1145/2702123.2702509", "10.1080/02701367.1992.10608764", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2011.185", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2013.125", "10.1109/mcse.2007.55", "10.1007/978-3-540-70956-5", "10.1109/vl.1996.545307", "10.1901/jeab.1979.31-433", "10.1111/j.1467-8659.2009.01475.x", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2012.65", "10.1145/1835804.1835827", "10.1109/tvcg.2017.2745085", "10.1145/2939672.2939778", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2017.2744805", "10.1145/2487575.2487629", "10.1109/tvcg.2013.157", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2011.209", "10.1109/vast.2012.6400490", "10.1109/tvcg.2015.2513410", "10.1007/978-3-540-79347-2\\_3", "10.1109/tvcg.2014.2346431", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2012.277", "10.1109/infvis.1998.729560", "10.1109/infvis.2004.64", "10.1109/mcg.2006.70", "10.1109/tvcg.2012.260", "10.1109/tvcg.2014.2346660", "10.1145/1743546.1743567", "10.1111/cgf.13417", "10.1109/tvcg.2014.2346325", "10.1109/tvcg.2018.2864838", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2003.1207445", "10.1109/vast.2010.5652443", "10.1145/2641190.2641198", "10.1109/tvcg.2016.2598830", "10.1109/tvcg.2017.2744378", "10.1111/cgf.13324", "10.1109/mcg.2009.22", "10.1109/tvcg.2016.2599030", "10.1109/vast.2012.6400486", "10.1109/tvcg.2016.2598831", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934261", "title": "Ablate, Variate, and Contemplate: Visual Analytics for Discovering Neural Architectures", "year": "2019", "conferenceName": "VAST", "authors": "Dylan Cashman;Adam Perer;Remco Chang;Hendrik Strobelt", "citationCount": "1", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Perer, Adam, Carnegie Mellon Univ, Pittsburgh, PA 15213 USA. Strobelt, Hendrik, MIT IBM Watson AI Lab, Cambridge, MA USA.", "countries": "USA", "abstract": "The performance of deep learning models is dependent on the precise configuration of many layers and parameters. However, there are currently few systematic guidelines for how to configure a successful model. This means model builders often have to experiment with different configurations by manually programming different architectures (which is tedious and time consuming) or rely on purely automated approaches to generate and train the architectures (which is expensive). In this paper, we present Rapid Exploration of Model Architectures and Parameters, or REMAP, a visual analytics tool that allows a model builder to discover a deep learning model quickly via exploration and rapid experimentation of neural network architectures. In REMAP, the user explores the large and complex parameter space for neural network architectures using a combination of global inspection and local experimentation. Through a visual overview of a set of models, the user identifies interesting clusters of architectures. Based on their findings, the user can run ablation and variation experiments to identify the effects of adding, removing, or replacing layers in a given architecture and generate new models accordingly. They can also handcraft new models using a simple graphical interface. As a result, a model builder can build deep learning models quickly, efficiently, and without manual programming. We inform the design of REMAP through a design study with four deep learning model builders. Through a use case, we demonstrate that REMAP allows users to discover performant neural network architectures efficiently using visual exploration and user-defined semi-automated searches through the model space.", "keywords": "visual analytics,neural networks,parameter space exploration", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934261", "refList": ["10.1109/mcg.2018.2878902", "10.1111/cgf.12639", "10.1109/tvcg.2017.2744938", "10.1117/12.2007316", "10.1109/cvpr.2014.81", "10.1016/j.csda.2008.02.031", "10.1080/00994480.2000.10748487", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2013.125", "10.1109/cvpr.2015.7298594", "10.1111/j.1467-8659.2009.01475.x", "10.1109/tvcg.2012.65", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2017.2744805", "10.1145/2487575.2487629", "10.1109/tvcg.2018.2865044", "10.23915/distill.00010", "10.1109/72.279181", "10.1109/tvcg.2017.2744199", "10.1007/s13398-014-0173-7.2", "10.1109/tvcg.2018.2864504", "10.1109/vast.2012.6400490", "10.1007/978-3-319-10590-1\\_53", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2014.2346321", "10.1094/pdis-11-11-0999-pdn", "10.1109/ijcnn.2015.7280767", "10.1109/tvcg.2017.2744878", "10.5555/3326943.3327130", "10.1109/tvcg.2017.2744718", "10.1109/iccv.2015.169", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2014.223", "10.1109/cvpr.2016.90", "10.1109/vast.2010.5652443", "10.1111/cgf.13681", "10.1109/tvcg.2016.2598831", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028888", "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes", "year": "2020", "conferenceName": "VAST", "authors": "Yuxin Ma;Arlen Fan;Jingrui He;Arun Reddy Nelakurthi;Ross Maciejewski", "citationCount": "0", "affiliation": "Ma, YX (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Ma, Yuxin; Fan, Arlen; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. He, Jingrui, Univ Illinois, Champaign, IL USA. Nelakurthi, Arun Reddy, Samsung Res Amer, Mountain View, CA USA.", "countries": "USA", "abstract": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.", "keywords": "Transfer learning,deep learning,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028888", "refList": ["10.1109/tvcg.2014.2346578", "10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1109/tpami.2018.2868685", "10.1145/2702123.2702509", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2016.2598828", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2013.65", "10.1145/2976749.2978318", "10.1007/978-3-030-01424-7\\_27", "10.1109/tvcg.2019.2934261", "10.1007/s11704-016-6028-y", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/iccv.2015.279", "10.1109/mci.2018.2840738", "10.1109/tvcg.2019.2892483", "10.1109/vast.2018.8802509", "10.1109/tvcg.2013.124", "10.1186/s40537-016-0043-6", "10.1109/tvcg.2018.2864475", "10.1145/3200489", "10.1109/tvcg.2018.2865044", "10.1111/cgf.13210", "10.1109/tvcg.2018.2816223", "10.23915/distill.00007", "10.1109/tvcg.2017.2744199", "10.1109/tkde.2018.2876857", "10.1109/tvcg.2019.2934631", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2011.188", "10.1007/978-3-642-15561-1\\_16", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1109/tvcg.2017.2744718", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2016.2598541", "10.1109/tkde.2009.191", "10.1145/3065386", "10.1016/j.ins.2016.03.021", "10.1109/tvcg.2019.2903943", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/iccv.2017.74", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2012.207", "10.1111/cgf.13092", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2017.2754480", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 9}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13970", "year": "2020", "title": "QUESTO: Interactive Construction of Objective Functions for Classification Tasks", "conferenceName": "EuroVis", "authors": "Subhajit Das;Shenyu Xu;Michael Gleicher;Remco Chang;Alex Endert", "citationCount": "0", "affiliation": "Das, S (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA.\nDas, Subhajit; Xu, Shenyu; Endert, Alex, Georgia Inst Technol, Atlanta, GA 30332 USA.\nGleicher, Michael, Univ Wisconsin, Madison, WI USA.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.", "countries": "USA", "abstract": "Building effective classifiers requires providing the modeling algorithms with information about the training data and modeling goals in order to create a model that makes proper tradeoffs. Machine learning algorithms allow for flexible specification of such meta-information through the design of the objective functions that they solve. However, such objective functions are hard for users to specify as they are a specific mathematical formulation of their intents. In this paper, we present an approach that allows users to generate objective functions for classification problems through an interactive visual interface. Our approach adopts a semantic interaction design in that user interactions over data elements in the visualization are translated into objective function terms. The generated objective functions are solved by a machine learning solver that provides candidate models, which can be inspected by the user, and used to suggest refinements to the specifications. We demonstrate a visual analytics system QUESTO for users to manipulate objective functions to define domain-specific constraints. Through a user study we show that QUESTO helps users create various objective functions that satisfy their goals.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13970", "refList": ["10.1016/j.neucom.2017.01.105", "10.1371/journal.pone.0050474", "10.1109/tvcg.2016.2598839", "10.1007/978-3-642-21530-8\\_14", "10.1109/tvcg.2016.2598828", "10.5555/2969442.2969547", "10.1007/s00371-015-1132-9", "10.1145/2851581.2856492", "10.1126/scirobotics.aao6760", "10.1109/vast.2011.6102453", "10.1109/vast.2011.6102449", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2016.2598446", "10.1145/359784.360332", "10.1111/j.1467-8659.2009.01475.x", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2016.2598460", "10.1109/tevc.2015.2472283", "10.1145/2207676.2207741", "10.1109/tvcg.2017.2745085", "10.1145/2939672.2939778", "10.1145/1866029.1866038", "10.1145/2487575.2487629", "10.1145/3077257.3077259", "10.2312/eurova.20171123", "10.1145/2983924", "10.1109/mcg.2013.53", "10.1109/tvcg.2015.2467615", "10.1109/vast.2014.7042492", "10.1145/2675133.2675214", "10.1109/tvcg.2014.2346482", "10.1145/3180308.3180362", "10.1109/tvcg.2014.2346321", "10.24963/ijcai.2017/202", "10.1109/tvcg.2014.2346291", "10.1109/tbme.2012.2212278", "10.1007/s40708-016-0042-6", "10.1609/aimag.v35i4.2513", "10.1016/j.ijhcs.2009.03.004", "10.1109/tevc.2012.2225064", "10.1016/s0890-6955(02)00074-3", "10.1109/icmlde.2018.00014", "10.1111/cgf.13681", "10.1109/tvcg.2013.173", "10.1145/3025171.3025208", "10.1145/3025453.3026044", "10.1109/cec.2017.7969334", "10.1109/vast.2012.6400486", "10.1109/tvcg.2016.2598831", "10.1109/tcyb.2014.2310651"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13972", "year": "2020", "title": "Boxer: Interactive Comparison of Classifier Results", "conferenceName": "EuroVis", "authors": "Michael Gleicher;Aditya Barve;Xinyi Yu;Florian Heimerl", "citationCount": "0", "affiliation": "Gleicher, M (Corresponding Author), Univ Wisconsin, Madison, WI 53706 USA.\nGleicher, Michael; Barve, Aditya; Yu, Xinyi; Heimerl, Florian, Univ Wisconsin, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Machine learning practitioners often compare the results of different classifiers to help select, diagnose and tune models. We present Boxer, a system to enable such comparison. Our system facilitates interactive exploration of the experimental results obtained by applying multiple classifiers to a common set of model inputs. The approach focuses on allowing the user to identify interesting subsets of training and testing instances and comparing performance of the classifiers on these subsets. The system couples standard visual designs with set algebra interactions and comparative elements. This allows the user to compose and coordinate views to specify subsets and assess classifier performance on them. The flexibility of these compositions allow the user to address a wide range of scenarios in developing and assessing classifiers. We demonstrate Boxer in use cases including model selection, tuning, fairness assessment, and data quality diagnosis.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13972", "refList": ["10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2016.2598838", "10.1007/s10618-014-0368-8", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1145/3287560.3287589", "10.1109/vast.2017.8585721", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2009.128", "10.1109/tvcg.2017.2744018", "10.1080/00994480.2000.10748487", "10.5555/3305890.3306024", "10.1109/iccv.2015.329", "10.1109/tvcg.2013.125", "10.1089/big.2016.0007", "10.1109/memsys.2019.8870817", "10.1145/2939672.2939778", "10.1007/s11104-019-04156-0", "10.1371/journal.pone.0181142", "10.1145/3301275.3302324", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2018.2865044", "10.1023/a:1010933404324", "10.1145/2487575.2487579", "10.1109/tvcg.2013.157", "10.1145/2783258.2788613", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934631", "10.1016/s0304-3800(02)00064-9", "10.1007/s10115-013-0679-x", "10.1109/tvcg.2019.2934267", "10.1007/978-3-319-10590-1\\_53", "10.1109/vast.2017.8585720", "10.1016/0004-3702(80)90021-1", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2009.84", "10.1007/s11263-016-0911-8", "10.1111/cgf.12918", "10.1111/cgf.12373", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1080/00207176808905715", "10.1002/er.3827", "10.1109/tvcg.2014.2346660", "10.1111/cgf.13417", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2017.2744158", "10.1016/b978-0-12-815849-4.00004-9", "10.1097/ede.0b013e3181c30fb2", "10.1111/cgf.13681", "10.1016/j.ejor.2006.04.051", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2016.2598468", "10.9735/2229-3981", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 21}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13972", "year": "2020", "title": "Boxer: Interactive Comparison of Classifier Results", "conferenceName": "EuroVis", "authors": "Michael Gleicher;Aditya Barve;Xinyi Yu;Florian Heimerl", "citationCount": "0", "affiliation": "Gleicher, M (Corresponding Author), Univ Wisconsin, Madison, WI 53706 USA.\nGleicher, Michael; Barve, Aditya; Yu, Xinyi; Heimerl, Florian, Univ Wisconsin, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Machine learning practitioners often compare the results of different classifiers to help select, diagnose and tune models. We present Boxer, a system to enable such comparison. Our system facilitates interactive exploration of the experimental results obtained by applying multiple classifiers to a common set of model inputs. The approach focuses on allowing the user to identify interesting subsets of training and testing instances and comparing performance of the classifiers on these subsets. The system couples standard visual designs with set algebra interactions and comparative elements. This allows the user to compose and coordinate views to specify subsets and assess classifier performance on them. The flexibility of these compositions allow the user to address a wide range of scenarios in developing and assessing classifiers. We demonstrate Boxer in use cases including model selection, tuning, fairness assessment, and data quality diagnosis.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13972", "refList": ["10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2016.2598838", "10.1007/s10618-014-0368-8", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1145/3287560.3287589", "10.1109/vast.2017.8585721", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2009.128", "10.1109/tvcg.2017.2744018", "10.1080/00994480.2000.10748487", "10.5555/3305890.3306024", "10.1109/iccv.2015.329", "10.1109/tvcg.2013.125", "10.1089/big.2016.0007", "10.1109/memsys.2019.8870817", "10.1145/2939672.2939778", "10.1007/s11104-019-04156-0", "10.1371/journal.pone.0181142", "10.1145/3301275.3302324", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2018.2865044", "10.1023/a:1010933404324", "10.1145/2487575.2487579", "10.1109/tvcg.2013.157", "10.1145/2783258.2788613", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934631", "10.1016/s0304-3800(02)00064-9", "10.1007/s10115-013-0679-x", "10.1109/tvcg.2019.2934267", "10.1007/978-3-319-10590-1\\_53", "10.1109/vast.2017.8585720", "10.1016/0004-3702(80)90021-1", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2009.84", "10.1007/s11263-016-0911-8", "10.1111/cgf.12918", "10.1111/cgf.12373", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1080/00207176808905715", "10.1002/er.3827", "10.1109/tvcg.2014.2346660", "10.1111/cgf.13417", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2017.2744158", "10.1016/b978-0-12-815849-4.00004-9", "10.1097/ede.0b013e3181c30fb2", "10.1111/cgf.13681", "10.1016/j.ejor.2006.04.051", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2016.2598468", "10.9735/2229-3981", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 53}, {"doi": "10.1109/tvcg.2018.2864838", "title": "VIS4ML: An Ontology for Visual Analytics Assisted Machine Learning", "year": "2018", "conferenceName": "VAST", "authors": "Dominik Sacha;Matthias Kraus;Daniel A. Keim;Min Chen", "citationCount": "11", "affiliation": "Sacha, D (Corresponding Author), Univ Konstanz, Constance, Germany. Sacha, Dominik; Kraus, Matthias; Keim, Daniel A., Univ Konstanz, Constance, Germany. Chen, Min, Univ Oxford, Oxford, England.", "countries": "Germany;England", "abstract": "While many VA workflows make use of machine-learned models to support analytical tasks, VA workflows have become increasingly important in understanding and improving Machine Learning (ML) processes. In this paper, we propose an ontology (VIS4ML) for a subarea of VA, namely \u201cVA-assisted ML\u201d. The purpose of VIS4ML is to describe and understand existing VA workflows used in ML as well as to detect gaps in ML processes and the potential of introducing advanced VA techniques to such processes. Ontologies have been widely used to map out the scope of a topic in biology, medicine, and many other disciplines. We adopt the scholarly methodologies for constructing VIS4ML, including the specification, conceptualization, formalization, implementation, and validation of ontologies. In particular, we reinterpret the traditional VA pipeline to encompass model-development workflows. We introduce necessary definitions, rules, syntaxes, and visual notations for formulating VIS4ML and make use of semantic web technologies for implementing it in the Web Ontology Language (OWL). VIS4ML captures the high-level knowledge about previous workflows where VA is used to assist in ML. It is consistent with the established VA concepts and will continue to evolve along with the future developments in VA and ML. While this ontology is an effort for building the theoretical foundation of VA, it can be used by practitioners in real-world applications to optimize model-development workflows by systematically examining the potential benefits that can be brought about by either machine or human capabilities. Meanwhile, VIS4ML is intended to be extensible and will continue to be updated to reflect future advancements in using VA for building high-quality data-analytical models or for building such models rapidly.", "keywords": "Visual Analytics,Visualization,Machine Learning,Human-Computer Interaction,Ontology,VIS4ML", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864838", "refList": ["10.1109/tvcg.2015.2467591", "10.1109/38.31462", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2017.2745178", "10.1145/1287620.1287621", "10.1109/mcg.2017.3271463", "10.1109/tvcg.2016.2598828", "10.1109/icdmw.2008.62", "10.1145/3011141.3011207", "10.1007/978-3-540-70956-5", "10.1145/2512208", "10.1016/j.aei.2016.04.003", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2017.2745158", "10.1007/978-3-642-40897-7\\_9", "10.1109/tvcg.2017.2744805", "10.23915/distill.00010", "10.1111/j.1467-8659.2008.01230.x", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2016.2598829", "10.1109/tvcg.2017.2744718", "10.1057/ivs.2008.29", "10.1109/mcg.2005.55", "10.1109/mcg.2018.042731661", "10.2200/s00429ed1v01y201207aim018", "10.1145/2468356.2468677", "10.1609/aimag.v35i4.2513", "10.1057/ivs.2008.28", "10.1201/9781315139470", "10.1109/tvcg.2017.2744158", "10.1109/vast.2008.4677361", "10.1007/s10844-014-0304-9", "10.1109/mcg.2014.33", "10.1016/j.ineucom.2017.01.105", "10.1111/cgf.13092", "10.1109/tvcg.2016.2598830", "10.1109/visual.2004.10", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1111/cgf.13324", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934614", "title": "Interactive Learning for Identifying Relevant Tweets to Support Real-time Situational Awareness", "year": "2019", "conferenceName": "VAST", "authors": "Luke S. Snyder;Yi-Shan Lin;Morteza Karimzadeh;Dan Goldwasser;David S. Ebert", "citationCount": "1", "affiliation": "Snyder, LS (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Snyder, Luke S.; Lin, Yi-Shan; Karimzadeh, Morteza; Goldwasser, Dan; Ebert, David S., Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Various domain users are increasingly leveraging real-time social media data to gain rapid situational awareness. However, due to the high noise in the deluge of data, effectively determining semantically relevant information can be difficult, further complicated by the changing definition of relevancy by each end user for different events. The majority of existing methods for short text relevance classification fail to incorporate users' knowledge into the classification process. Existing methods that incorporate interactive user feedback focus on historical datasets. Therefore, classifiers cannot be interactively retrained for specific events or user-dependent needs in real-time. This limits real-time situational awareness, as streaming data that is incorrectly classified cannot be corrected immediately, permitting the possibility for important incoming data to be incorrectly classified as well. We present a novel interactive learning framework to improve the classification process in which the user iteratively corrects the relevancy of tweets in real-time to train the classification model on-the-fly for immediate predictive improvements. We computationally evaluate our classification model adapted to learn at interactive rates. Our results show that our approach outperforms state-of-the-art machine learning models. In addition, we integrate our framework with the extended Social Media Analytics and Reporting Toolkit (SMART) 2.0 system, allowing the use of our interactive learning framework within a visual analytics system tailored for real-time situational awareness. To demonstrate our framework's effectiveness, we provide domain expert feedback from first responders who used the extended SMART 2.0 system.", "keywords": "Interactive machine learning,human-computer interaction,social media analytics,emergency/disaster management,situational awareness", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934614", "refList": ["10.1145/1835449.1835643", "10.1109/icip.2014.7025078", "10.1162/coli.08-012-r1-06-90", "10.1016/j.ijinfomgt.2018.07.008", "10.1207/s15516709cog1402\\_1", "10.1145/2806416.2806485", "10.1109/vast.2010.5652922", "10.1145/1645953.1646071", "10.1145/3155133.3155206", "10.3115/v1/n15-1142", "10.1109/tvcg.2013.186", "10.1145/1367497.1367510", "10.18653/v1/d15-1167", "10.1109/5.726791", "10.1145/2207676.2207741", "10.1109/icci-cc.2015.7259377", "10.1109/tcbb.2017.2701379", "10.1080/15230406.2017.1370391", "10.1109/infvis.2004.37", "10.1162/jmlr.2003.3.4-5.951", "10.18653/v1/s16-1024", "10.1109/tvcg.2012.277", "10.1162/neco.1997.9.8.1735", "10.1145/347090.347168", "10.1109/tvcg.2017.2744718", "10.1145/2675133.2675242", "10.1002/smr.1874", "10.1109/icassp.2018.8461907", "10.1109/bigmm.2017.82", "10.1109/tvcg.2018.2864838", "10.1109/ssci.2015.33", "10.1109/tvcg.2017.2744818", "10.1145/2872518.2889365", "10.1109/asonam.2016.7752432", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934267", "title": "ProtoSteer: Steering Deep Sequence Model with Prototypes", "year": "2019", "conferenceName": "VAST", "authors": "Yao Ming;Panpan Xu;Furui Cheng;Huamin Qu;Ren Liu", "citationCount": "2", "affiliation": "Ming, Y (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Ming, Yao; Cheng, Furui; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xu, Panpan; Ren, Liu, Bosch Res North Amer, Palo Alto, CA USA.", "countries": "USA;China", "abstract": "Recently we have witnessed growing adoption of deep sequence models (e.g. LSTMs) in many application domains, including predictive health care, natural language processing, and log analysis. However, the intricate working mechanism of these models confines their accessibility to the domain experts. Their black-box nature also makes it a challenging task to incorporate domain-specific knowledge of the experts into the model. In ProtoSteer (Prototype Steering), we tackle the challenge of directly involving the domain experts to steer a deep sequence model without relying on model developers as intermediaries. Our approach originates in case-based reasoning, which imitates the common human problem-solving process of consulting past experiences to solve new problems. We utilize ProSeNet (Prototype Sequence Network), which learns a small set of exemplar cases (i.e., prototypes) from historical data. In ProtoSteer they serve both as an efficient visual summary of the original data and explanations of model decisions. With ProtoSteer the domain experts can inspect, critique, and revise the prototypes interactively. The system then incorporates user-specified prototypes and incrementally updates the model. We conduct extensive case studies and expert interviews in application domains including sentiment analysis on texts and predictive diagnostics based on vehicle fault logs. The results demonstrate that involvements of domain users can help obtain more interpretable models with concise prototypes while retaining similar accuracy.", "keywords": "Sequence Data,Explainable Artificial Intelligence (XAI),Recurrent Neural Networks (RNNs),Prototype Learning", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934267", "refList": ["10.1109/infvis.2000.885091", "10.1145/2468356.2468434", "10.1109/mcg.2018.2878902", "10.1145/2858036.2858107", "10.1145/2702123.2702419", "10.1109/vast.2015.7347682", "10.1109/tvcg.2016.2598797", "10.1145/2939672.2939778", "10.1109/tvcg.2018.2864885", "10.1109/tvcg.2018.2865044", "10.1073/pnas.95.25.14863", "10.1109/tvcg.2016.2539960", "10.1145/3025453.3025456", "10.1145/2557500.2557508", "10.1109/tvcg.2012.225", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2018.2865230", "10.1109/tvcg.2017.2745320", "10.1109/tvcg.2017.2744718", "10.18653/v1/n16-1082", "10.1016/j.neucom.2013.11.045", "10.1109/tvcg.2017.2745083", "10.1609/aimag.v35i4.2513", "10.1007/bf00155578", "10.1007/978-3-319-90403-0\\_17", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2018.2864838", "10.1109/tvcg.2018.2865027", "10.1145/312129.312298", "10.1145/3185517", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13972", "year": "2020", "title": "Boxer: Interactive Comparison of Classifier Results", "conferenceName": "EuroVis", "authors": "Michael Gleicher;Aditya Barve;Xinyi Yu;Florian Heimerl", "citationCount": "0", "affiliation": "Gleicher, M (Corresponding Author), Univ Wisconsin, Madison, WI 53706 USA.\nGleicher, Michael; Barve, Aditya; Yu, Xinyi; Heimerl, Florian, Univ Wisconsin, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Machine learning practitioners often compare the results of different classifiers to help select, diagnose and tune models. We present Boxer, a system to enable such comparison. Our system facilitates interactive exploration of the experimental results obtained by applying multiple classifiers to a common set of model inputs. The approach focuses on allowing the user to identify interesting subsets of training and testing instances and comparing performance of the classifiers on these subsets. The system couples standard visual designs with set algebra interactions and comparative elements. This allows the user to compose and coordinate views to specify subsets and assess classifier performance on them. The flexibility of these compositions allow the user to address a wide range of scenarios in developing and assessing classifiers. We demonstrate Boxer in use cases including model selection, tuning, fairness assessment, and data quality diagnosis.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13972", "refList": ["10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2016.2598838", "10.1007/s10618-014-0368-8", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1145/3287560.3287589", "10.1109/vast.2017.8585721", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2009.128", "10.1109/tvcg.2017.2744018", "10.1080/00994480.2000.10748487", "10.5555/3305890.3306024", "10.1109/iccv.2015.329", "10.1109/tvcg.2013.125", "10.1089/big.2016.0007", "10.1109/memsys.2019.8870817", "10.1145/2939672.2939778", "10.1007/s11104-019-04156-0", "10.1371/journal.pone.0181142", "10.1145/3301275.3302324", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2018.2865044", "10.1023/a:1010933404324", "10.1145/2487575.2487579", "10.1109/tvcg.2013.157", "10.1145/2783258.2788613", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934631", "10.1016/s0304-3800(02)00064-9", "10.1007/s10115-013-0679-x", "10.1109/tvcg.2019.2934267", "10.1007/978-3-319-10590-1\\_53", "10.1109/vast.2017.8585720", "10.1016/0004-3702(80)90021-1", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2009.84", "10.1007/s11263-016-0911-8", "10.1111/cgf.12918", "10.1111/cgf.12373", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1080/00207176808905715", "10.1002/er.3827", "10.1109/tvcg.2014.2346660", "10.1111/cgf.13417", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2017.2744158", "10.1016/b978-0-12-815849-4.00004-9", "10.1097/ede.0b013e3181c30fb2", "10.1111/cgf.13681", "10.1016/j.ejor.2006.04.051", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2016.2598468", "10.9735/2229-3981", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 11}, {"doi": "10.1109/tvcg.2019.2934595", "title": "Visual Interaction with Deep Learning Models through Collaborative Semantic Inference", "year": "2019", "conferenceName": "VAST", "authors": "Sebastian Gehrmann;Hendrik Strobelt;Robert Kr\u00fcger;Hanspeter Pfister;Alexander M. Rush", "citationCount": "3", "affiliation": "Gehrmann, S (Corresponding Author), Harvard NLP Grp, Cambridge, MA 02138 USA. Gehrmann, Sebastian; Rush, Alexander M., Harvard NLP Grp, Cambridge, MA 02138 USA. Strobelt, Hendrik, IBM Res Cambridge, Cambridge, MA USA. Kruger, Robert, MIT IBM Watson AI Lab, Cambridge, MA USA. Pfister, Hanspeter, Harvard Visual Comp Grp, Cambridge, MA USA.", "countries": "USA", "abstract": "Automation of tasks can have critical consequences when humans lose agency over decision processes. Deep learning models are particularly susceptible since current black-box approaches lack explainable reasoning. We argue that both the visual interface and model structure of deep learning systems need to take into account interaction design. We propose a framework of collaborative semantic inference (CSI) for the co-design of interactions and models to enable visual collaboration between humans and algorithms. The approach exposes the intermediate reasoning process of models which allows semantic interactions with the visual metaphors of a problem, which means that a user can both understand and control parts of the model reasoning process. We demonstrate the feasibility of CSI with a co-designed case study of a document summarization system.", "keywords": "Human-Computer Collaboration,Deep Learning,Neural Networks,Interaction Design,Human-Centered Design", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934595", "refList": ["10.1109/vast.2017.8585721", "10.1109/tvcg.2012.195", "10.1613/jair.295", "10.1162/tacl\\textbackslash{}a\\textbackslash{}00254", "10.1007/978-3-540-70956-5", "10.1145/2678025.2701399", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.5555/645526.657137", "10.1146/annurev.neur0.26.041002.131047.issn", "10.1145/2207676.2207741", "10.1145/2939672.2939778", "10.1109/vlhcc.2010.15", "10.3115/v1/d14-1130", "10.18653/v1/p17-1099", "10.1109/tvcg.2018.2865044", "10.18653/v1/p17-1080", "10.1111/cgf.13210", "10.1162/tacl\\_a\\_00254", "10.1109/tvcg.2018.2816223", "10.1109/72.279181", "10.1145/2783258.2788613", "10.2112/si85-057.1", "10.1145/1866029.1866078", "10.1007/978-3-319-10590-1\\_53", "10.1109/tvcg.2018.2865230", "10.1007/s12650-018-0531-1", "10.1145/2365952.2365964", "10.1017/s026988890200019x", "10.1109/iccv.2015.337", "10.1109/tvcg.2017.2744878", "10.1145/3027063.3053103", "10.1109/tvcg.2017.2744718", "10.1145/302979.303030", "10.1109/cvpr.2018.00917", "10.1007/s40708-016-0042-6", "10.1073/pnas.1807184115", "10.1609/aimag.v35i4.2513", "10.1016/j.ijhcs.2009.03.004", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2018.2864838", "10.1111/cgf.13092", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2014.2346574", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1016/s0167-739x(97)00022-8", "10.1093/bi0inf0rmatics/bth267"], "wos": 1, "children": [{"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 7}, {"doi": "10.1109/tvcg.2020.3030380", "title": "HyperTendril: Visual Analytics for User-Driven Hyperparameter Optimization of Deep Neural Networks", "year": "2020", "conferenceName": "VAST", "authors": "Heungseok Park;Yoonsoo Nam;Jihoon Kim;Jaegul Choo", "citationCount": "0", "affiliation": "Kim, JH (Corresponding Author), NAVER Corp, Clova AI Res, Seongnam Si, South Korea. Choo, J (Corresponding Author), Korea Adv Inst Sci \\& Technol, Daejeon, South Korea. Park, Heungseok; Nam, Yoonsoo; Kim, Ji-Hoon, NAVER Corp, Clova AI Res, Seongnam Si, South Korea. Choo, Jaegul, Korea Adv Inst Sci \\& Technol, Daejeon, South Korea.", "countries": "Korea", "abstract": "To mitigate the pain of manually tuning hyperparameters of deep neural networks, automated machine learning (AutoML) methods have been developed to search for an optimal set of hyperparameters in large combinatorial search spaces. However, the search results of AutoML methods significantly depend on initial configurations, making it a non-trivial task to find a proper configuration. Therefore, human intervention via a visual analytic approach bears huge potential in this task. In response, we propose HyperTendril, a web-based visual analytics system that supports user-driven hyperparameter tuning processes in a model-agnostic environment. HyperTendril takes a novel approach to effectively steering hyperparameter optimization through an iterative, interactive tuning procedure that allows users to refine the search spaces and the configuration of the AutoML method based on their own insights from given results. Using HyperTendril, users can obtain insights into the complex behaviors of various hyperparameter search algorithms and diagnose their configurations. In addition, HyperTendril supports variable importance analysis to help the users refine their search spaces based on the analysis of relative importance of different hyperparameters and their interaction effects. We present the evaluation demonstrating how HyperTendril helps users steer their tuning processes via a longitudinal user study based on the analysis of interaction logs and in-depth interviews while we deploy our system in a professional industrial environment.", "keywords": "Visual analytics,deep learning,machine learning,automated machine learning,human-centered computing", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030380", "refList": ["10.1109/tvcg.2019.2934629", "10.1145/2834892.2834896", "10.1007/s10732-014-9275-9", "10.1109/icdis.2019.00018", "10.1145/3147.3165", "10.1109/bigdata.2017.8257923", "10.1145/3379336.3381474", "10.1007/978-3-319-47099-3\\_15", "10.1109/tvcg.2014.2346248", "10.1109/jproc.2015.2494218", "10.1145/3292500.3330701", "10.1162/neco.1997.9.8.1735", "10.1109/tvcg.2016.2598829", "10.1145/3097983.3098043", "10.1109/cvpr.2016.90", "10.1109/tvcg.2018.2864838", "10.1007/978-4-431-68057-4\\_3", "10.1111/cgf.13092", "10.1145/3219819.3220058", "10.1109/tvcg.2017.2744358"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030449", "title": "HypoML: Visual Analysis for Hypothesis-based Evaluation of Machine Learning Models", "year": "2020", "conferenceName": "VAST", "authors": "Qianwen Wang;William Alexander;Jack Pegg;Huamin Qu;Min Chen", "citationCount": "0", "affiliation": "Wang, QW (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wang, Qianwen; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Alexander, William; Pegg, Jack; Chen, Min, Univ Oxford, Oxford, England.", "countries": "China;England", "abstract": "In this paper, we present a visual analytics tool for enabling hypothesis-based evaluation of machine learning (ML) models. We describe a novel ML-testing framework that combines the traditional statistical hypothesis testing (commonly used in empirical research) with logical reasoning about the conclusions of multiple hypotheses. The framework defines a controlled configuration for testing a number of hypotheses as to whether and how some extra information about a \u201cconcept\u201d or \u201cfeature\u201d may benefit or hinder an ML model. Because reasoning multiple hypotheses is not always straightforward, we provide HypoML as a visual analysis tool, with which, the multi-thread testing results are first transformed to analytical results using statistical and logical inferences, and then to a visual representation for rapid observation of the conclusions and the logical flow between the testing results and hypotheses. We have applied HypoML to a number of hypothesized concepts, demonstrating the intuitive and explainable nature of the visual analysis.", "keywords": "Visual analytics,model-developmental visualization,machine learning,neural network,hypothesis test,HypoML", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030449", "refList": ["10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1145/2702123.2702509", "10.1109/tvcg.2016.2598828", "10.1016/j.inffus.2018.07.007", "10.1109/tvcg.2012.197", "10.1631/fitee.1700808", "10.1145/2858036.2858529", "10.1038/nature14539", "10.1109/tvcg.2018.2816223", "10.2307/2288400", "10.3390/s19092212", "10.1109/tvcg.2016.2598829", "10.1109/tvcg.2017.2744718", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/iccv.2017.74", "10.1109/tvcg.2018.2864838", "10.1111/cgf.13092", "10.1016/j.inffus.2018.09.014", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2019.2934619", "10.24963/ijcai.2018/430", "10.1109/tvcg.2018.2864499", "10.1109/cvpr.2016.319", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13681", "year": "2019", "title": "A User-based Visual Analytics Workflow for Exploratory Model Analysis", "conferenceName": "EuroVis", "authors": "Dylan Cashman;Shah Rukh Humayoun;Florian Heimerl;Kendall Park;Subhajit Das;John Thompson;Bahador Saket;Abigail Mosca;John T. Stasko;Alex Endert;Michael Gleicher;Remco Chang", "citationCount": "6", "affiliation": "Cashman, D; Humayoun, SR (Corresponding Author), Tufts Univ, Medford, MA 02155 USA.\nCashman, Dylan; Humayoun, Shah Rukh; Mosca, Abigail; Chang, Remco, Tufts Univ, Medford, MA 02155 USA.\nHeimerl, Florian; Park, Kendall; Gleicher, Michael, Georgia Tech, Atlanta, GA USA.\nDas, Subhajit; Thompson, John; Saket, Bahador; Stasko, John; Endert, Alex, Univ Wisconsin, Madison, WI USA.", "countries": "USA", "abstract": "Many visual analytics systems allow users to interact with machine learning models towards the goals of data exploration and insight generation on a given dataset. However, in some situations, insights may be less important than the production of an accurate predictive model for future use. In that case, users are more interested in generating of diverse and robust predictive models, verifying their performance on holdout data, and selecting the most suitable model for their usage scenario. In this paper, we consider the concept of Exploratory Model Analysis (EMA), which is defined as the process of discovering and selecting relevant models that can be used to make predictions on a data source. We delineate the differences between EMA and the well-known term exploratory data analysis in terms of the desired outcome of the analytic process: insights into the data or a set of deployable models. The contributions of this work are a visual analytics system workflow for EMA, a user study, and two use cases validating the effectiveness of the workflow. We found that our system workflow enabled users to generate complex models, to assess them for various qualities, and to select the most relevant model for their task.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13681", "refList": ["10.1109/tvcg.2017.2744683", "10.1111/cgf.12639", "10.1007/s11390-016-1663-1", "10.1109/tvcg.2017.2744938", "10.1117/12.2007316", "10.1109/tvcg.2017.2745178", "10.1145/2702123.2702509", "10.1080/02701367.1992.10608764", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2011.185", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2013.125", "10.1109/mcse.2007.55", "10.1007/978-3-540-70956-5", "10.1109/vl.1996.545307", "10.1901/jeab.1979.31-433", "10.1111/j.1467-8659.2009.01475.x", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2012.65", "10.1145/1835804.1835827", "10.1109/tvcg.2017.2745085", "10.1145/2939672.2939778", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2017.2744805", "10.1145/2487575.2487629", "10.1109/tvcg.2013.157", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2011.209", "10.1109/vast.2012.6400490", "10.1109/tvcg.2015.2513410", "10.1007/978-3-540-79347-2\\_3", "10.1109/tvcg.2014.2346431", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2012.277", "10.1109/infvis.1998.729560", "10.1109/infvis.2004.64", "10.1109/mcg.2006.70", "10.1109/tvcg.2012.260", "10.1109/tvcg.2014.2346660", "10.1145/1743546.1743567", "10.1111/cgf.13417", "10.1109/tvcg.2014.2346325", "10.1109/tvcg.2018.2864838", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2003.1207445", "10.1109/vast.2010.5652443", "10.1145/2641190.2641198", "10.1109/tvcg.2016.2598830", "10.1109/tvcg.2017.2744378", "10.1111/cgf.13324", "10.1109/mcg.2009.22", "10.1109/tvcg.2016.2599030", "10.1109/vast.2012.6400486", "10.1109/tvcg.2016.2598831", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934261", "title": "Ablate, Variate, and Contemplate: Visual Analytics for Discovering Neural Architectures", "year": "2019", "conferenceName": "VAST", "authors": "Dylan Cashman;Adam Perer;Remco Chang;Hendrik Strobelt", "citationCount": "1", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Perer, Adam, Carnegie Mellon Univ, Pittsburgh, PA 15213 USA. Strobelt, Hendrik, MIT IBM Watson AI Lab, Cambridge, MA USA.", "countries": "USA", "abstract": "The performance of deep learning models is dependent on the precise configuration of many layers and parameters. However, there are currently few systematic guidelines for how to configure a successful model. This means model builders often have to experiment with different configurations by manually programming different architectures (which is tedious and time consuming) or rely on purely automated approaches to generate and train the architectures (which is expensive). In this paper, we present Rapid Exploration of Model Architectures and Parameters, or REMAP, a visual analytics tool that allows a model builder to discover a deep learning model quickly via exploration and rapid experimentation of neural network architectures. In REMAP, the user explores the large and complex parameter space for neural network architectures using a combination of global inspection and local experimentation. Through a visual overview of a set of models, the user identifies interesting clusters of architectures. Based on their findings, the user can run ablation and variation experiments to identify the effects of adding, removing, or replacing layers in a given architecture and generate new models accordingly. They can also handcraft new models using a simple graphical interface. As a result, a model builder can build deep learning models quickly, efficiently, and without manual programming. We inform the design of REMAP through a design study with four deep learning model builders. Through a use case, we demonstrate that REMAP allows users to discover performant neural network architectures efficiently using visual exploration and user-defined semi-automated searches through the model space.", "keywords": "visual analytics,neural networks,parameter space exploration", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934261", "refList": ["10.1109/mcg.2018.2878902", "10.1111/cgf.12639", "10.1109/tvcg.2017.2744938", "10.1117/12.2007316", "10.1109/cvpr.2014.81", "10.1016/j.csda.2008.02.031", "10.1080/00994480.2000.10748487", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2013.125", "10.1109/cvpr.2015.7298594", "10.1111/j.1467-8659.2009.01475.x", "10.1109/tvcg.2012.65", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2017.2744805", "10.1145/2487575.2487629", "10.1109/tvcg.2018.2865044", "10.23915/distill.00010", "10.1109/72.279181", "10.1109/tvcg.2017.2744199", "10.1007/s13398-014-0173-7.2", "10.1109/tvcg.2018.2864504", "10.1109/vast.2012.6400490", "10.1007/978-3-319-10590-1\\_53", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2014.2346321", "10.1094/pdis-11-11-0999-pdn", "10.1109/ijcnn.2015.7280767", "10.1109/tvcg.2017.2744878", "10.5555/3326943.3327130", "10.1109/tvcg.2017.2744718", "10.1109/iccv.2015.169", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2014.223", "10.1109/cvpr.2016.90", "10.1109/vast.2010.5652443", "10.1111/cgf.13681", "10.1109/tvcg.2016.2598831", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028888", "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes", "year": "2020", "conferenceName": "VAST", "authors": "Yuxin Ma;Arlen Fan;Jingrui He;Arun Reddy Nelakurthi;Ross Maciejewski", "citationCount": "0", "affiliation": "Ma, YX (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Ma, Yuxin; Fan, Arlen; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. He, Jingrui, Univ Illinois, Champaign, IL USA. Nelakurthi, Arun Reddy, Samsung Res Amer, Mountain View, CA USA.", "countries": "USA", "abstract": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.", "keywords": "Transfer learning,deep learning,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028888", "refList": ["10.1109/tvcg.2014.2346578", "10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1109/tpami.2018.2868685", "10.1145/2702123.2702509", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2016.2598828", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2013.65", "10.1145/2976749.2978318", "10.1007/978-3-030-01424-7\\_27", "10.1109/tvcg.2019.2934261", "10.1007/s11704-016-6028-y", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/iccv.2015.279", "10.1109/mci.2018.2840738", "10.1109/tvcg.2019.2892483", "10.1109/vast.2018.8802509", "10.1109/tvcg.2013.124", "10.1186/s40537-016-0043-6", "10.1109/tvcg.2018.2864475", "10.1145/3200489", "10.1109/tvcg.2018.2865044", "10.1111/cgf.13210", "10.1109/tvcg.2018.2816223", "10.23915/distill.00007", "10.1109/tvcg.2017.2744199", "10.1109/tkde.2018.2876857", "10.1109/tvcg.2019.2934631", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2011.188", "10.1007/978-3-642-15561-1\\_16", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1109/tvcg.2017.2744718", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2016.2598541", "10.1109/tkde.2009.191", "10.1145/3065386", "10.1016/j.ins.2016.03.021", "10.1109/tvcg.2019.2903943", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/iccv.2017.74", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2012.207", "10.1111/cgf.13092", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2017.2754480", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 9}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13970", "year": "2020", "title": "QUESTO: Interactive Construction of Objective Functions for Classification Tasks", "conferenceName": "EuroVis", "authors": "Subhajit Das;Shenyu Xu;Michael Gleicher;Remco Chang;Alex Endert", "citationCount": "0", "affiliation": "Das, S (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA.\nDas, Subhajit; Xu, Shenyu; Endert, Alex, Georgia Inst Technol, Atlanta, GA 30332 USA.\nGleicher, Michael, Univ Wisconsin, Madison, WI USA.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.", "countries": "USA", "abstract": "Building effective classifiers requires providing the modeling algorithms with information about the training data and modeling goals in order to create a model that makes proper tradeoffs. Machine learning algorithms allow for flexible specification of such meta-information through the design of the objective functions that they solve. However, such objective functions are hard for users to specify as they are a specific mathematical formulation of their intents. In this paper, we present an approach that allows users to generate objective functions for classification problems through an interactive visual interface. Our approach adopts a semantic interaction design in that user interactions over data elements in the visualization are translated into objective function terms. The generated objective functions are solved by a machine learning solver that provides candidate models, which can be inspected by the user, and used to suggest refinements to the specifications. We demonstrate a visual analytics system QUESTO for users to manipulate objective functions to define domain-specific constraints. Through a user study we show that QUESTO helps users create various objective functions that satisfy their goals.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13970", "refList": ["10.1016/j.neucom.2017.01.105", "10.1371/journal.pone.0050474", "10.1109/tvcg.2016.2598839", "10.1007/978-3-642-21530-8\\_14", "10.1109/tvcg.2016.2598828", "10.5555/2969442.2969547", "10.1007/s00371-015-1132-9", "10.1145/2851581.2856492", "10.1126/scirobotics.aao6760", "10.1109/vast.2011.6102453", "10.1109/vast.2011.6102449", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2016.2598446", "10.1145/359784.360332", "10.1111/j.1467-8659.2009.01475.x", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2016.2598460", "10.1109/tevc.2015.2472283", "10.1145/2207676.2207741", "10.1109/tvcg.2017.2745085", "10.1145/2939672.2939778", "10.1145/1866029.1866038", "10.1145/2487575.2487629", "10.1145/3077257.3077259", "10.2312/eurova.20171123", "10.1145/2983924", "10.1109/mcg.2013.53", "10.1109/tvcg.2015.2467615", "10.1109/vast.2014.7042492", "10.1145/2675133.2675214", "10.1109/tvcg.2014.2346482", "10.1145/3180308.3180362", "10.1109/tvcg.2014.2346321", "10.24963/ijcai.2017/202", "10.1109/tvcg.2014.2346291", "10.1109/tbme.2012.2212278", "10.1007/s40708-016-0042-6", "10.1609/aimag.v35i4.2513", "10.1016/j.ijhcs.2009.03.004", "10.1109/tevc.2012.2225064", "10.1016/s0890-6955(02)00074-3", "10.1109/icmlde.2018.00014", "10.1111/cgf.13681", "10.1109/tvcg.2013.173", "10.1145/3025171.3025208", "10.1145/3025453.3026044", "10.1109/cec.2017.7969334", "10.1109/vast.2012.6400486", "10.1109/tvcg.2016.2598831", "10.1109/tcyb.2014.2310651"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13972", "year": "2020", "title": "Boxer: Interactive Comparison of Classifier Results", "conferenceName": "EuroVis", "authors": "Michael Gleicher;Aditya Barve;Xinyi Yu;Florian Heimerl", "citationCount": "0", "affiliation": "Gleicher, M (Corresponding Author), Univ Wisconsin, Madison, WI 53706 USA.\nGleicher, Michael; Barve, Aditya; Yu, Xinyi; Heimerl, Florian, Univ Wisconsin, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Machine learning practitioners often compare the results of different classifiers to help select, diagnose and tune models. We present Boxer, a system to enable such comparison. Our system facilitates interactive exploration of the experimental results obtained by applying multiple classifiers to a common set of model inputs. The approach focuses on allowing the user to identify interesting subsets of training and testing instances and comparing performance of the classifiers on these subsets. The system couples standard visual designs with set algebra interactions and comparative elements. This allows the user to compose and coordinate views to specify subsets and assess classifier performance on them. The flexibility of these compositions allow the user to address a wide range of scenarios in developing and assessing classifiers. We demonstrate Boxer in use cases including model selection, tuning, fairness assessment, and data quality diagnosis.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13972", "refList": ["10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2016.2598838", "10.1007/s10618-014-0368-8", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1145/3287560.3287589", "10.1109/vast.2017.8585721", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2009.128", "10.1109/tvcg.2017.2744018", "10.1080/00994480.2000.10748487", "10.5555/3305890.3306024", "10.1109/iccv.2015.329", "10.1109/tvcg.2013.125", "10.1089/big.2016.0007", "10.1109/memsys.2019.8870817", "10.1145/2939672.2939778", "10.1007/s11104-019-04156-0", "10.1371/journal.pone.0181142", "10.1145/3301275.3302324", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2018.2865044", "10.1023/a:1010933404324", "10.1145/2487575.2487579", "10.1109/tvcg.2013.157", "10.1145/2783258.2788613", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934631", "10.1016/s0304-3800(02)00064-9", "10.1007/s10115-013-0679-x", "10.1109/tvcg.2019.2934267", "10.1007/978-3-319-10590-1\\_53", "10.1109/vast.2017.8585720", "10.1016/0004-3702(80)90021-1", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2009.84", "10.1007/s11263-016-0911-8", "10.1111/cgf.12918", "10.1111/cgf.12373", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1080/00207176808905715", "10.1002/er.3827", "10.1109/tvcg.2014.2346660", "10.1111/cgf.13417", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2017.2744158", "10.1016/b978-0-12-815849-4.00004-9", "10.1097/ede.0b013e3181c30fb2", "10.1111/cgf.13681", "10.1016/j.ejor.2006.04.051", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2016.2598468", "10.9735/2229-3981", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 21}, {"doi": "10.1111/cgf.13677", "year": "2019", "title": "An Ontological Framework for Supporting the Design and Evaluation of Visual Analytics Systems", "conferenceName": "EuroVis", "authors": "Min Chen;David S. Ebert", "citationCount": "5", "affiliation": "Chen, M (Corresponding Author), Univ Oxford, Oxford, England.\nChen, Min, Univ Oxford, Oxford, England.\nEbert, David S., Purdue Univ, W Lafayette, IN 47907 USA.", "countries": "USA;England", "abstract": "Designing, evaluating, and improving visual analytics (VA) systems is a primary area of activities in our discipline. In this paper, we present an ontological framework for recording and categorizing technical shortcomings to be addressed in a VA workflow, reasoning about the causes of such problems, identifying technical solutions, and anticipating secondary effects of the solutions. The methodology is built on the theoretical premise that designing a VA workflow is an optimization of the cost-benefit ratio of the processes in the workflow. It makes uses three fundamental measures to group and connect symptoms, causes, remedies, and side-effects, and guide the search for potential solutions to the problems. In terms of requirement analysis and system design, the proposed methodology can enable system designers to explore the decision space in a structured manner. In terms of evaluation, the proposed methodology is time-efficient and complementary to various forms of empirical studies, such as user surveys, controlled experiments, observational studies, focus group discussions, and so on. In general, it reduces the amount of trial-and-error in the lifecycle of VA system development.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13677", "refList": ["10.1109/tvcg.2006.178", "10.1109/infvis.2000.885092", "10.1111/cgf.12920", "10.1057/ivs.2009.26", "10.1109/mcg.2017.3271463", "10.1007/978-3-319-10578-9\\_1", "10.1109/icdmw.2008.62", "10.1109/mcg.2017.51", "10.1145/3011141.3011207", "10.1109/tvcg.2013.134", "10.1080/10618600.1996.10474696", "10.1007/978-3-540-70956-5", "10.1109/visual.1990.146375", "10.1109/tvcg.2012.219", "10.1109/vl.1996.545307", "10.1057/ivs.2009.23", "10.1002/j.1538-7305.1948.tb00917.x", "10.1109/tvcg.2010.79", "10.1109/tvcg.2013.124", "10.1111/cgf.13211", "10.1007/978-3-642-40897-7\\_9", "10.1103/physrev.108.171", "10.1109/infvis.2004.59", "10.1109/iv.2008.36", "10.1111/cgf.13210", "10.1111/j.1467-8659.2008.01230.x", "10.1001/jama.293.10.1223", "10.1177/1473871611407399", "10.1109/visual.1995.480821", "10.1117/12.539227", "10.1109/tvcg.2015.2513410", "10.1109/vast.2011.6102463", "10.7749/citiescommunitiesterritories.dec2014.029.art01", "10.1109/mcg.2005.55", "10.1109/tvcg.2012.234", "10.1109/pacificvis.2012.6183556", "10.1103/physrev.106.620", "10.1109/infvis.1997.636792", "10.2307/2104491", "10.1145/2468356.2468677", "10.1145/3173574.3173611", "10.1109/tvcg.2018.2864838", "10.1111/cgf.13092", "10.1109/visual.2004.10", "10.1109/tvcg.2017.2744319", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2603178"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934264", "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;David S. Ebert;Arif Ghafoor", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Karimzadeh, Morteza; Ebert, David S.; Ghafoor, Arif, Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.", "keywords": "Summative evaluation,usefulness,evaluation process,taxonomy,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934264", "refList": ["10.1109/tvcg.2017.2744478", "10.1109/tvcg.2018.2865025", "10.1109/tvcg.2006.85", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2017.2745181", "10.1111/cgf.13677", "10.1109/tvcg.2018.2864844", "10.1109/tvcg.2013.126", "10.1109/tvcg.2018.2864811", "10.1109/infvis.2005.1532147", "10.1177/0956797613504966", "10.1145/2669557.2669579", "10.1109/mcg.2005.102", "10.1109/visual.2003.1250426", "10.1136/bmj.39489.470347.ad", "10.1109/tvcg.2017.2744080", "10.1109/mcg.2009.53", "10.1111/j.1467-8527.2005.00307.x", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2864886", "10.1109/tvcg.2018.2864843", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2018.2865051", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2018.2865026", "10.1007/978-3-540-71080-6\\_6", "10.1109/tvcg.2018.2865020", "10.1177/1473871611407399", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2018.2864526", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2864905", "10.1049/sej.1991.0040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2711030", "10.1109/tvcg.2011.279", "10.1109/vast.2017.8585505", "10.1147/jrd.2010.2042914", "10.1016/s0378-7206(98)00044-5", "10.1145/2993901.2993913", "10.1109/tvcg.2018.2865041", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744758", "10.1145/1168149.1168158", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2012.213", "10.1109/tvcg.2017.2744738", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2018.2864826", "10.1145/1377966.1377974", "10.1109/apec.2009.4802646", "10.1145/1168149.1168152", "10.1016/j.jss.2008.03.059", "10.1109/vast.2017.8585484", "10.1109/tvcg.2017.2744818", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2865042", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030388", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "year": "2020", "conferenceName": "SciVis", "authors": "Pepe Eulzer;Sabine Bauer;Francis Kilian;Kai Lawonn", "citationCount": "0", "affiliation": "Eulzer, P (Corresponding Author), Univ Jena, Jena, Germany. Eulzer, Pepe; Lawonn, Kai, Univ Jena, Jena, Germany. Bauer, Sabine, Univ Koblenz Landau, Koblenz, Germany. Kilian, Francis, Cath Clin Koblenz Montabaur, Dept Spine Surg, Koblenz, Germany.", "countries": "Germany", "abstract": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. $\\mathrm{By}$ linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. $\\mathrm{in}$ a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.", "keywords": "Medical visualization,bioinformatics,coordinated views,focus and context,biomechanical simulation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030388", "refList": ["10.1109/tvcg.2018.2864903", "10.1177/1473871613510429", "10.1093/ehjqcco/qcz052", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2018.2865076", "10.1055/s-0039-1687862", "10.1109/visual.1990.146375", "10.1109/tvcg.2017.2744198", "10.1016/j.ijmedinf.2014.10.001", "10.1109/tvcg.2013.124", "10.1016/j.jacc", "10.1111/cgf.13167", "10.17705/1thci.00055", "10.1136/bmjqs.2009.037895", "10.1109/tvcg.2013.238", "10.1109/tvcg.2018.2865240", "10.1186/1471-2261-6-34", "10.1109/tvcg.2019.2934264", "10.1109/tvcg.2013.200", "10.1109/tvcg.2011.209", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467091", "10.1136/bmjopen-2019-033208", "10.1109/beliv.2018.8634027", "10.1109/tvcg.2012.213", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2015.2467325", "10.1145/2133806.2133821", "10.1145/1806799.1806866", "10.1108/02635570610688869", "10.1002/hbm.20701", "10.1561/1100000039", "10.1145/3025453.3025645", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 5}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 61}, {"doi": "10.1109/tvcg.2018.2865018", "title": "TPFlow: Progressive Partition and Multidimensional Pattern Extraction for Large-Scale Spatio-Temporal Data Analysis", "year": "2018", "conferenceName": "VAST", "authors": "Dongyu Liu;Panpan Xu;Ren Liu", "citationCount": "6", "affiliation": "Liu, DY (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Dongyu, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Liu, Dongyu; Xu, Panpan; Ren, Liu, Bosch Res North Amer, Sunnyvale, CA USA.", "countries": "USA;China", "abstract": "Consider a multi-dimensional spatio-temporal (ST) dataset where each entry is a numerical measure defined by the corresponding temporal, spatial and other domain-specific dimensions. A typical approach to explore such data utilizes interactive visualizations with multiple coordinated views. Each view displays the aggregated measures along one or two dimensions. By brushing on the views, analysts can obtain detailed information. However, this approach often cannot provide sufficient guidance for analysts to identify patterns hidden within subsets of data. Without a priori hypotheses, analysts need to manually select and iterate through different slices to search for patterns, which can be a tedious and lengthy process. In this work, we model multidimensional ST data as tensors and propose a novel piecewise rank-one tensor decomposition algorithm which supports automatically slicing the data into homogeneous partitions and extracting the latent patterns in each partition for comparison and visual summarization. The algorithm optimizes a quantitative measure about how faithfully the extracted patterns visually represent the original data. Based on the algorithm we further propose a visual analytics framework that supports a top-down, progressive partitioning workflow for level-of-detail multidimensional ST data exploration. We demonstrate the general applicability and effectiveness of our technique on three datasets from different application domains: regional sales trend analysis, customer traffic analysis in department stores, and taxi trip analysis with origin-destination (OD) data. We further interview domain experts to verify the usability of the prototype.", "keywords": "Spatio-temporal data,tensor decomposition,interactive exploration,automatic pattern discoveries", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865018", "refList": ["10.1007/s13177-014-0099-7", "10.1109/tvcg.2013.226", "10.1111/j.1467-8659.2009.01664.x", "10.1007/s00371-013-0892-3", "10.1109/tvcg.2015.2468111", "10.2307/3151680", "10.1111/cgf.12129", "10.1145/2254556.2254659", "10.1109/tvcg.2017.2744419", "10.1145/2632048.2636073", "10.1109/2945.841121", "10.1109/tvcg.2014.2346449", "10.1109/tvcg.2007.70515", "10.1109/tsmc.2018.2871100", "10.1145/2629592", "10.1109/tvcg.2016.2616404", "10.1109/vl.1996.545307", "10.1109/infvis.2003.1249018", "10.1198/jcgs.2010.09051", "10.1007/978-3-319-71249-9\\_35", "10.1109/tvcg.2016.2598624", "10.1109/tvcg.2017.2744805", "10.1109/icde.2014.6816674", "10.1111/cgf.12888", "10.1109/tvcg.2006.161", "10.1109/tvcg.2016.2598862", "10.1016/s1045-926x(03)00046-6", "10.1089/cmb.2018.0139", "10.1109/tits.2017.2683539", "10.1109/tbdata.2016.2586447", "10.1137/07070111x", "10.1007/bf02310791", "10.1109/tits.2015.2436897", "10.1016/j.trc.2017.10.023", "10.1109/tvcg.2013.179", "10.1177/1473871616667632", "10.1137/s0895479899352045", "10.1109/jas.2017.7510538", "10.1111/j.1467-8659.2012.03117.x", "10.1007/bf01908075", "10.1002/widm.1", "10.2307/2669946", "10.1137/s0895479801387413", "10.1109/tvcg.2015.2467112", "10.1109/tvcg.2014.2346574", "10.1057/palgrave.ivs.9500184", "10.1109/infvis.1999.801851", "10.1109/tvcg.2016.2598432", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028889", "title": "A Visual Analytics Framework for Reviewing Multivariate Time-Series Data with Dimensionality Reduction", "year": "2020", "conferenceName": "VAST", "authors": "Takanori Fujiwara;Shilpika;Naohisa Sakamoto;Jorji Nonaka;Keiji Yamamoto;Kwan-Liu Ma", "citationCount": "0", "affiliation": "Fujiwara, T (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Fujiwara, Takanori; Shilpika; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA. Sakamoto, Naohisa, Kobe Univ, Kobe, Hyogo, Japan. Nonaka, Jorji; Yamamoto, Keiji, RIKEN R CCS, Kobe, Hyogo, Japan.", "countries": "Japan;USA", "abstract": "Data-driven problem solving in many real-world applications involves analysis of time-dependent multivariate data, for which dimensionality reduction (DR) methods are often used to uncover the intrinsic structure and features of the data. However, DR is usually applied to a subset of data that is either single-time-point multivariate or univariate time-series, resulting in the need to manually examine and correlate the DR results out of different data subsets. When the number of dimensions is large either in terms of the number of time points or attributes, this manual task becomes too tedious and infeasible. In this paper, we present MulTiDR, a new DR framework that enables processing of time-dependent multivariate data as a whole to provide a comprehensive overview of the data. With the framework, we employ DR in two steps. When treating the instances, time points, and attributes of the data as a 3D array, the first DR step reduces the three axes of the array to two, and the second DR step visualizes the data in a lower-dimensional space. In addition, by coupling with a contrastive learning method and interactive visualizations, our framework enhances analysts' ability to interpret DR results. We demonstrate the effectiveness of our framework with four case studies using real-world datasets.", "keywords": "Multivariate time-series,tensor,data cube,dimensionality reduction,interpretability,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028889", "refList": ["10.1016/j.neucom.2008.12.017", "10.1109/tvcg.2015.2467591", "10.1007/bf02289464", "10.1109/tvcg.2015.2468111", "10.1177/1350650119867242", "10.1109/pacificvis48177.2020.9280", "10.1109/tvcg.2017.2744419", "10.1109/tvcg.2011.185", "10.1109/bigdata.2015.7363807", "10.1145/2669557.2669559", "10.1016/j.patcog.2011.01.004", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1186/1475-925x-14-s2-s6", "10.1007/s12650-018-0530-2", "10.1109/tkde.2018.2878247", "10.1016/j.visinf.2018.04.010", "10.1002/1099-128x(200005/06)14:3", "10.1371/journal.pone.0107878", "10.1109/pacificvis.2017.8031601", "10.1016/j.jbi.2019.103291", "10.1109/daac49578.2019.00008", "10.1109/allerton.2019.8919886", "10.1007/978-3-319-13105-4\\_14", "10.4258/hir.2016.22.3.156", "10.1109/tvcg.2015.2467553", "10.1145/2245276.2245469", "10.1007/bf02294485", "10.1109/tvcg.2015.2468078", "10.1109/tvcg.2016.2598470", "10.1137/07070111x", "10.1109/tvcg.2019.2934433", "10.1109/tvcg.2016.2640960", "10.1007/bf02310791", "10.1109/tvcg.2016.2534558", "10.1109/access.2016.2529723", "10.1109/tvcg.2016.2598664", "10.2312/eurovisshort.20161164", "10.1109/tvcg.2018.2865018", "10.1111/cgf.12804", "10.1109/tvcg.2018.2846735", "10.1016/j.comnet.2017.06.013", "10.1146/annurev-statistics-041715-033624", "10.1109/pacificvis.2018.00026", "10.1109/tvcg.2015.2467851"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030425", "title": "Visual Analysis of Argumentation in Essays", "year": "2020", "conferenceName": "VAST", "authors": "Dora Kiesel;Patrick Riehmann;Henning Wachsmuth;Benno Stein;Bernd Fr\u00f6hlich", "citationCount": "0", "affiliation": "Kiesel, D (Corresponding Author), Bauhaus Univ Weimar, Weimar, Germany. Kiesel, Dora; Riehmann, Patrick; Stein, Benno; Froehlich, Bernd, Bauhaus Univ Weimar, Weimar, Germany. Wachsmuth, Henning, Paderborn Univ, Paderborn, Germany.", "countries": "Germany", "abstract": "This paper presents a visual analytics system for exploring, analyzing and comparing argument structures in essay corpora. We provide an overview of the corpus by a list of ArguLines which represent the argument units of each essay by a sequence of glyphs. Each glyph encodes the stance, the depth and the relative position of an argument unit. The overview can be ordered in various ways to reveal patterns and outliers. Subsets of essays can be selected and analyzed in detail using the Argument Unit Occurrence Tree which aggregates the argument structures using hierarchical histograms. This hierarchical view facilitates the estimation of statistics and trends concerning the progression of the argumentation in the essays. It also provides insights into the commonalities and differences between selected subsets. The text view is the necessary textual basis to verify conclusions from the other views and the annotation process. Linking the views and interaction techniques for visual filtering, studying the evolution of stance within a subset of essays and scrutinizing the order of argumentative units enable a deep analysis of essay corpora. Our expert reviews confirmed the utility of the system and revealed detailed and previously unknown information about the argumentation in our sample corpus.", "keywords": "Information Visualization,Text Analysis,User Interfaces,Visual Analytics,Argumentation Visualization,Glyph-based Techniques,Text and Document Data,Tree-based Visualization,Coordinated and Multiple Views,Close and Distant Reading", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030425", "refList": ["10.1109/tvcg.2014.2346575", "10.14778/2735479.2735485", "10.14778/3192965.3192971", "10.1111/cgf.12129", "10.1145/2206869.2206874", "10.1109/icde.2016.7498300", "10.14778/2831360.2831371", "10.1145/3035918.3056097", "10.1023/a:1009726021843", "10.1111/cgf.13678", "10.1145/3183713.3196905", "10.14778/3115404.3115418", "10.1109/tvcg.2012.180", "10.1109/icde.1999.754950", "10.14778/1453856.1453924", "10.1145/3209900.3209901", "10.1002/spe.2325", "10.1145/42201.42203", "10.1109/tvcg.2013.124", "10.1109/vast.2008.4677357", "10.1109/icde.2016.7498287", "10.1109/icde.2014.6816674", "10.14778/2732951.2732964", "10.1109/tvcg.2015.2467551", "10.1145/1084805.1084812", "10.1109/2.781635", "10.14778/2732279.2732280", "10.1109/icde.2015.7113427", "10.1109/tvcg.2015.2467091", "10.1007/s00778-017-0486-1", "10.1109/tvcg.2003.1196005", "10.1109/icde.2004.1320035", "10.1109/tvcg.2016.2607714", "10.14778/2732951.2732953", "10.1109/tvcg.2008.131", "10.1109/tvcg.2018.2865018", "10.1145/2133806.2133821", "10.1109/icde.2019.00035", "10.1109/tpds.2005.144", "10.14778/3236187.3236212", "10.1109/tvcg.2009.111", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/tvcg.2019.2934261", "title": "Ablate, Variate, and Contemplate: Visual Analytics for Discovering Neural Architectures", "year": "2019", "conferenceName": "VAST", "authors": "Dylan Cashman;Adam Perer;Remco Chang;Hendrik Strobelt", "citationCount": "1", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Perer, Adam, Carnegie Mellon Univ, Pittsburgh, PA 15213 USA. Strobelt, Hendrik, MIT IBM Watson AI Lab, Cambridge, MA USA.", "countries": "USA", "abstract": "The performance of deep learning models is dependent on the precise configuration of many layers and parameters. However, there are currently few systematic guidelines for how to configure a successful model. This means model builders often have to experiment with different configurations by manually programming different architectures (which is tedious and time consuming) or rely on purely automated approaches to generate and train the architectures (which is expensive). In this paper, we present Rapid Exploration of Model Architectures and Parameters, or REMAP, a visual analytics tool that allows a model builder to discover a deep learning model quickly via exploration and rapid experimentation of neural network architectures. In REMAP, the user explores the large and complex parameter space for neural network architectures using a combination of global inspection and local experimentation. Through a visual overview of a set of models, the user identifies interesting clusters of architectures. Based on their findings, the user can run ablation and variation experiments to identify the effects of adding, removing, or replacing layers in a given architecture and generate new models accordingly. They can also handcraft new models using a simple graphical interface. As a result, a model builder can build deep learning models quickly, efficiently, and without manual programming. We inform the design of REMAP through a design study with four deep learning model builders. Through a use case, we demonstrate that REMAP allows users to discover performant neural network architectures efficiently using visual exploration and user-defined semi-automated searches through the model space.", "keywords": "visual analytics,neural networks,parameter space exploration", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934261", "refList": ["10.1109/mcg.2018.2878902", "10.1111/cgf.12639", "10.1109/tvcg.2017.2744938", "10.1117/12.2007316", "10.1109/cvpr.2014.81", "10.1016/j.csda.2008.02.031", "10.1080/00994480.2000.10748487", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2013.125", "10.1109/cvpr.2015.7298594", "10.1111/j.1467-8659.2009.01475.x", "10.1109/tvcg.2012.65", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2017.2744805", "10.1145/2487575.2487629", "10.1109/tvcg.2018.2865044", "10.23915/distill.00010", "10.1109/72.279181", "10.1109/tvcg.2017.2744199", "10.1007/s13398-014-0173-7.2", "10.1109/tvcg.2018.2864504", "10.1109/vast.2012.6400490", "10.1007/978-3-319-10590-1\\_53", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2014.2346321", "10.1094/pdis-11-11-0999-pdn", "10.1109/ijcnn.2015.7280767", "10.1109/tvcg.2017.2744878", "10.5555/3326943.3327130", "10.1109/tvcg.2017.2744718", "10.1109/iccv.2015.169", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2014.223", "10.1109/cvpr.2016.90", "10.1109/vast.2010.5652443", "10.1111/cgf.13681", "10.1109/tvcg.2016.2598831", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028888", "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes", "year": "2020", "conferenceName": "VAST", "authors": "Yuxin Ma;Arlen Fan;Jingrui He;Arun Reddy Nelakurthi;Ross Maciejewski", "citationCount": "0", "affiliation": "Ma, YX (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Ma, Yuxin; Fan, Arlen; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. He, Jingrui, Univ Illinois, Champaign, IL USA. Nelakurthi, Arun Reddy, Samsung Res Amer, Mountain View, CA USA.", "countries": "USA", "abstract": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.", "keywords": "Transfer learning,deep learning,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028888", "refList": ["10.1109/tvcg.2014.2346578", "10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1109/tpami.2018.2868685", "10.1145/2702123.2702509", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2016.2598828", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2013.65", "10.1145/2976749.2978318", "10.1007/978-3-030-01424-7\\_27", "10.1109/tvcg.2019.2934261", "10.1007/s11704-016-6028-y", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/iccv.2015.279", "10.1109/mci.2018.2840738", "10.1109/tvcg.2019.2892483", "10.1109/vast.2018.8802509", "10.1109/tvcg.2013.124", "10.1186/s40537-016-0043-6", "10.1109/tvcg.2018.2864475", "10.1145/3200489", "10.1109/tvcg.2018.2865044", "10.1111/cgf.13210", "10.1109/tvcg.2018.2816223", "10.23915/distill.00007", "10.1109/tvcg.2017.2744199", "10.1109/tkde.2018.2876857", "10.1109/tvcg.2019.2934631", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2011.188", "10.1007/978-3-642-15561-1\\_16", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1109/tvcg.2017.2744718", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2016.2598541", "10.1109/tkde.2009.191", "10.1145/3065386", "10.1016/j.ins.2016.03.021", "10.1109/tvcg.2019.2903943", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/iccv.2017.74", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2012.207", "10.1111/cgf.13092", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2017.2754480", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934547", "title": "Facetto: Combining Unsupervised and Supervised Learning for Hierarchical Phenotype Analysis in Multi-Channel Image Data", "year": "2019", "conferenceName": "VAST", "authors": "Robert Kr\u00fcger;Johanna Beyer;Won-Dong Jang;Nam Wook Kim;Artem Sokolov;Peter K. Sorger;Hanspeter Pfister", "citationCount": "1", "affiliation": "Krueger, R (Corresponding Author), Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Krueger, R (Corresponding Author), Harvard Med Sch, Lab Syst Pharmacol, Boston, MA 02115 USA. Krueger, Robert; Beyer, Johanna; Jang, Won-Dong; Kim, Nam Wook; Pfister, Hanspeter, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Krueger, Robert; Sokolov, Artem; Sorger, Peter K., Harvard Med Sch, Lab Syst Pharmacol, Boston, MA 02115 USA.", "countries": "USA", "abstract": "Facetto is a scalable visual analytics application that is used to discover single-cell phenotypes in high-dimensional multi-channel microscopy images of human tumors and tissues. Such images represent the cutting edge of digital histology and promise to revolutionize how diseases such as cancer are studied, diagnosed, and treated. Highly multiplexed tissue images are complex, comprising 109 or more pixels, 60-plus channels, and millions of individual cells. This makes manual analysis challenging and error-prone. Existing automated approaches are also inadequate, in large part, because they are unable to effectively exploit the deep knowledge of human tissue biology available to anatomic pathologists. To overcome these challenges, Facetto enables a semi-automated analysis of cell types and states. It integrates unsupervised and supervised learning into the image and feature exploration process and offers tools for analytical provenance. Experts can cluster the data to discover new types of cancer and immune cells and use clustering results to train a convolutional neural network that classifies new cells accordingly. Likewise, the output of classifiers can be clustered to discover aggregate patterns and phenotype subsets. We also introduce a new hierarchical approach to keep track of analysis steps and data subsets created by users; this assists in the identification of cell types. Users can build phenotype trees and interact with the resulting hierarchical structures of both high-dimensional feature and image spaces. We report on use-cases in which domain scientists explore various large-scale fluorescence imaging datasets. We demonstrate how Facetto assists users in steering the clustering and classification process, inspecting analysis results, and gaining new scientific insights into cancer biology.", "keywords": "Clustering,Classification,Visual Analysis,Multiplex Tissue Imaging,Digital Pathology,Cancer Systems Biology", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934547", "refList": ["10.1145/1656274.1656280", "10.1111/cgf.12893", "10.1145/1142473.1142574", "10.1109/vahc.2017.8387544", "10.1111/j.2517-6161.1977.tb01600.x", "10.7554/elife.31657", "10.1109/tvcg.2013.125", "10.1186/1471-2105-16-s11-s5", "10.1109/isbi.2011.5872394", "10.1145/1268517.1268563", "10.1109/tvcg.2013.186", "10.1016/j.patrec.2009.09.011", "10.1001/jama.2016.17216", "10.1145/1656274.1656278", "10.1109/vahc.2017.8387545", "10.1145/1390156.1390183", "10.1109/tvcg.2017.2744805", "10.1109/hicss.2011.339", "10.1111/cgf.12613", "10.1007/bf01898350", "10.1109/vast.2014.7042495", "10.1109/tvcg.2015.2467551", "10.1016/j.cels.2016.03.008", "10.1038/nmeth.1896", "10.1109/tvcg.2012.277", "10.1016/s0893-6080(97)00002-6", "10.1057/palgrave.ivs.9500170", "10.1002/(sici)1097-4571(199307)44:6", "10.1109/tvcg.2007.70569", "10.1145/1830483.1830503", "10.1186/gb-2006-7-10-r100", "10.1109/tvcg.2012.213", "10.1109/tvcg.2012.258", "10.1109/tvcg.2017.2744158", "10.1109/sibgra.2002.1167123", "10.1007/978-3-319-24574-4\\_28", "10.1038/nmeth.4391", "10.1109/vast.2010.5652443", "10.1002/cpch.14", "10.1093/bioinformatics/bts288", "10.1109/tvcg.2016.2598468", "10.1109/tvcg.2016.2598587", "10.1080/00207179208934272"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030363", "title": "Improving the Usability of Virtual Reality Neuron Tracing with Topological Elements", "year": "2020", "conferenceName": "SciVis", "authors": "Torin McDonald;William Usher;Nathan Morrical;Attila Gyulassy;Steve Petruzza;Frederick Federer;Alessandra Angelucci;Valerio Pascucci", "citationCount": "0", "affiliation": "McDonald, T (Corresponding Author), Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA. McDonald, Torin; Usher, Will; Morrical, Nate; Gyulassy, Attila; Petruzza, Steve; Pascucci, Valerio, Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA. Petruzza, Steve, Utah State Univ, Logan, UT 84322 USA. Federer, Frederick; Angelucci, Alessandra, Univ Utah, Moran Ese Inst, Salt Lake City, UT 84112 USA.", "countries": "USA", "abstract": "Researchers in the field of connectomics are working to reconstruct a map of neural connections in the brain in order to understand at a fundamental level how the brain processes information. Constructing this wiring diagram is done by tracing neurons through high-resolution image stacks acquired with fluorescence microscopy imaging techniques. While a large number of automatic tracing algorithms have been proposed, these frequently rely on local features in the data and fail on noisy data or ambiguous cases, requiring time-consuming manual correction. As a result, manual and semi-automatic tracing methods remain the state-of-the-art for creating accurate neuron reconstructions. We propose a new semi-automatic method that uses topological features to guide users in tracing neurons and integrate this method within a virtual reality (VR) framework previously used for manual tracing. Our approach augments both visualization and interaction with topological elements, allowing rapid understanding and tracing of complex morphologies. In our pilot study, neuroscientists demonstrated a strong preference for using our tool over prior approaches, reported less fatigue during tracing, and commended the ability to better understand possible paths and alternatives. Quantitative evaluation of the traces reveals that users' tracing speed increased, while retaining similar accuracy compared to a fully manual approach.", "keywords": "Virtual Reality,Morse-Smale Complex,Semi-automatic Neuron Tracing", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030363", "refList": ["10.1038/nmeth.2869", "10.18637/jss.v028.c01", "10.1177/1473871611416549", "10.1016/j.celrep.2020.107523", "10.1602/neurorx.1.2.182", "10.1038/nprot.2014.191", "10.1136/jnnp-2011-300403", "10.1126/sciadv.aax5851", "10.2312/pe.vmv.vmv13.105-112", "10.1038/s43018-020-0031-9", "10.1109/tvcg.2013.213", "10.1093/jnci/92.8.613", "10.1109/52.329404", "10.1109/tvcg.2017.2785271", "10.1007/s11548-013-0820-z", "10.1109/tvcg.2019.2934547", "10.1002/cjp2.113", "10.1080/2162402x.2018.1507600", "10.1109/tvcg.2013.124", "10.1016/s0140-6736(14)60958-2", "10.1038/s41467-017-01689-9", "10.1016/j.cell.2018.07.010", "10.1007/978-3-319-24523-2", "10.1038/nrg3832", "10.2352/j.imagingsci.technol.2017.61.6.060404", "10.1109/tvcg.2018.2864907", "10.1111/cgf.13413", "10.1007/978-3-319-67979-2\\_4", "10.1145/2836034.2836040", "10.1038/nmeth.2563", "10.1016/0377-0427(87)90125-7", "10.1016/j.immuni.2016.04.014", "10.1002/cyto.a.22702", "10.12688/wellcomeopenres.15191.1", "10.1109/2945.981851", "10.1038/s43018-020-0026-6", "10.5281/zenodo", "10.1109/tvcg.2019.2931299", "10.1109/annes.1995.499469", "10.1145/2133806.2133821", "10.1002/hbm.20701", "10.1109/tvcg.2013.161", "10.1007/978-3-319-24523-210", "10.1126/scitranslmed.3004330", "10.1038/nmeth.4391", "10.1559/152304003100010929", "10.1126/science.280.5363.585", "10.1007/978-3-319-24523-2\\_10", "10.1109/tvcg.2016.2598587", "10.1038/s41597-019-0258-4"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030336", "title": "Visual cohort comparison for spatial single-cell omics-data", "year": "2020", "conferenceName": "VAST", "authors": "Antonios Somarakis;Marieke E. Ijsselsteijn;Sietse J. Luk;Boyd Kenkhuis;Noel F. C. C. de Miranda;Boudewijn P. F. Lelieveldt;Thomas H\u00f6llt", "citationCount": "0", "affiliation": "Hollt, T (Corresponding Author), Delft Univ Technol, Comp Graph \\& Visualizat Grp, Delft, Netherlands. Hollt, T (Corresponding Author), Leiden Univ, Med Ctr, Leiden Computat Biol Ctr, Leiden, Netherlands. Somarakis, Antonios; Lelieveldt, Boudewijn P. F., Leiden Univ, Med Ctr, Dept Radiol, Div Image Proc, Leiden, Netherlands. Ijsselsteijn, Marieke E.; de Miranda, Noel F. C. C., Leiden Univ, Med Ctr, Dept Pathol, Immunogen Grp, Leiden, Netherlands. Luk, Sietse J., Leiden Univ, Med Ctr, Hematol Dept, Leiden, Netherlands. Kenkhuis, Boyd, Leiden Univ, Med Ctr, Human Genet Dept, Leiden, Netherlands. Hollt, Thomas, Delft Univ Technol, Comp Graph \\& Visualizat Grp, Delft, Netherlands. Hollt, Thomas, Leiden Univ, Med Ctr, Leiden Computat Biol Ctr, Leiden, Netherlands.", "countries": "Netherlands", "abstract": "Spatially-resolved omics-data enable researchers to precisely distinguish cell types in tissue and explore their spatial interactions, enabling deep understanding of tissue functionality. To understand what causes or deteriorates a disease and identify related biomarkers, clinical researchers regularly perform large-scale cohort studies, requiring the comparison of such data at cellular level. In such studies, with little a-priori knowledge of what to expect in the data, explorative data analysis is a necessity. Here, we present an interactive visual analysis workflow for the comparison of cohorts of spatially-resolved omics-data. Our workflow allows the comparative analysis of two cohorts based on multiple levels-of-detail, from simple abundance of contained cell types over complex co-localization patterns to individual comparison of complete tissue images. As a result, the workflow enables the identification of cohort-differentiating features, as well as outlier samples at any stage of the workflow. During the development of the workflow, we continuously consulted with domain experts. To show the effectiveness of the workflow, we conducted multiple case studies with domain experts from different application areas and with different data modalities.", "keywords": "Visual analytics,Imaging Mass Cytometry,Vectra,spatially-resolved data,single-cell omics-data,Visual comparison", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030336", "refList": ["10.1038/nmeth.2869", "10.18637/jss.v028.c01", "10.1177/1473871611416549", "10.1016/j.celrep.2020.107523", "10.1602/neurorx.1.2.182", "10.1038/nprot.2014.191", "10.1136/jnnp-2011-300403", "10.1126/sciadv.aax5851", "10.2312/pe.vmv.vmv13.105-112", "10.1038/s43018-020-0031-9", "10.1109/tvcg.2013.213", "10.1093/jnci/92.8.613", "10.1109/52.329404", "10.1109/tvcg.2017.2785271", "10.1007/s11548-013-0820-z", "10.1109/tvcg.2019.2934547", "10.1002/cjp2.113", "10.1080/2162402x.2018.1507600", "10.1109/tvcg.2013.124", "10.1016/s0140-6736(14)60958-2", "10.1038/s41467-017-01689-9", "10.1016/j.cell.2018.07.010", "10.1007/978-3-319-24523-2", "10.1038/nrg3832", "10.2352/j.imagingsci.technol.2017.61.6.060404", "10.1109/tvcg.2018.2864907", "10.1111/cgf.13413", "10.1007/978-3-319-67979-2\\_4", "10.1145/2836034.2836040", "10.1038/nmeth.2563", "10.1101/2020.03.27.001834", "10.1016/0377-0427(87)90125-7", "10.1016/j.immuni.2016.04.014", "10.1002/cyto.a.22702", "10.12688/wellcomeopenres.15191.1", "10.1109/2945.981851", "10.1038/s43018-020-0026-6", "10.1038/s41586-019-1876-x", "10.1109/tvcg.2019.2931299", "10.1109/annes.1995.499469", "10.1145/2133806.2133821", "10.1109/tvcg.2013.161", "10.1007/978-3-319-24523-210", "10.1126/scitranslmed.3004330", "10.1038/nmeth.4391", "10.1111/cgf.14002", "10.1559/152304003100010929", "10.1126/science.280.5363.585", "10.1007/978-3-319-24523-2\\_10", "10.1109/tvcg.2016.2598587", "10.1038/s41597-019-0258-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/vast47406.2019.8986940", "title": "FDive: Learning Relevance Models Using Pattern-based Similarity Measures", "year": "2019", "conferenceName": "VAST", "authors": "Frederik L. Dennig;Tom Polk;Zudi Lin;Tobias Schreck;Hanspeter Pfister;Michael Behrisch", "citationCount": "0", "affiliation": "Dennig, FL (Corresponding Author), Univ Konstanz, Constance, Germany. Dennig, Frederik L.; Polk, Tom, Univ Konstanz, Constance, Germany. Lin, Zudi; Pfister, Hanspeter; Behrisch, Michael, Harvard Univ, Cambridge, MA 02138 USA. Schreck, Tobias, Graz Univ Technol, Graz, Austria.", "countries": "Germany;USA;Austria", "abstract": "The detection of interesting patterns in large high-dimensional datasets is difficult because of their dimensionality and pattern complexity. Therefore, analysts require automated support for the extraction of relevant patterns. In this paper, we present FDive, a visual active learning system that helps to create visually explorable relevance models, assisted by learning a pattern-based similarity. We use a small set of user-provided labels to rank similarity measures, consisting of feature descriptor and distance function combinations, by their ability to distinguish relevant from irrelevant data. Based on the best-ranked similarity measure, the system calculates an interactive Self-Organizing Map-based relevance model, which classifies data according to the cluster affiliation. It also automatically prompts further relevance feedback to improve its accuracy. Uncertain areas, especially near the decision boundaries, are highlighted and can be refined by the user. We evaluate our approach by comparison to state-of-the-art feature selection techniques and demonstrate the usefulness of our approach by a case study classifying electron microscopy images of brain cells. The results show that FDive enhances both the quality and understanding of relevance models and can thus lead to new insights for brain research.", "keywords": "Visual analytics,similarity measure selection,relevance feedback,active learning,self-organizing maps", "link": "http://dx.doi.org/10.1109/VAST47406.2019.8986940", "refList": ["10.1016/j.ijhcs.2006.07.005", "10.1109/tsmc.1978.4309999", "10.1109/igarss.2016.7729190", "10.1145/3301275.3302280", "10.1007/bf00337288", "10.1162/153244303322753616", "10.1186/1471-2105-13-s8-s4", "10.1109/vast.2014.7042480", "10.1006/jvci.1999.0413", "10.1109/tpami.2009.154", "10.1109/wiamis.2008.24", "10.1109/tvcg.2017.2744805", "10.1111/j.1467-8659.2011.01938.x", "10.1109/tvcg.2013.157", "10.1080/01969727308546046", "10.1109/tsmc.1973.4309314", "10.1016/j.jchromb.2012.05.020", "10.1109/tvcg.2014.2346481", "10.1007/978-3-319-14364-4\\_70", "10.2307/2287820", "10.1109/infvis.2005.1532142", "10.1109/tip.2002.801585", "10.1007/978-1-4614-7138-7\\_1", "10.1145/1390156.1390302", "10.1109/tvcg.2012.277", "10.3390/jimaging4080097", "10.1109/cbmi.2015.7153629", "10.1145/2836034.2836035", "10.1145/2362456.2362485", "10.1016/j.visinf.2019.03.002", "10.1109/tpami.1979.4766909", "10.1145/1459359.1459577", "10.1109/tkde.2009.191", "10.1109/icip.2001.959135", "10.1109/tvcg.2016.2598467", "10.1007/978-3-540-87481-2\\_21", "10.1109/cvpr.1997.609412", "10.2307/2685881", "10.1109/cvpr.2016.90", "10.1145/347090.347124", "10.1016/b978-044450270-4/50003-6", "10.1109/tvcg.2017.2744818", "10.1145/989863.989880", "10.1145/3185517", "10.1145/1282280.1282340", "10.1109/vast.2012.6400486", "10.1109/tpami.2006.68", "10.1109/vast.2011.6102453"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13681", "year": "2019", "title": "A User-based Visual Analytics Workflow for Exploratory Model Analysis", "conferenceName": "EuroVis", "authors": "Dylan Cashman;Shah Rukh Humayoun;Florian Heimerl;Kendall Park;Subhajit Das;John Thompson;Bahador Saket;Abigail Mosca;John T. Stasko;Alex Endert;Michael Gleicher;Remco Chang", "citationCount": "6", "affiliation": "Cashman, D; Humayoun, SR (Corresponding Author), Tufts Univ, Medford, MA 02155 USA.\nCashman, Dylan; Humayoun, Shah Rukh; Mosca, Abigail; Chang, Remco, Tufts Univ, Medford, MA 02155 USA.\nHeimerl, Florian; Park, Kendall; Gleicher, Michael, Georgia Tech, Atlanta, GA USA.\nDas, Subhajit; Thompson, John; Saket, Bahador; Stasko, John; Endert, Alex, Univ Wisconsin, Madison, WI USA.", "countries": "USA", "abstract": "Many visual analytics systems allow users to interact with machine learning models towards the goals of data exploration and insight generation on a given dataset. However, in some situations, insights may be less important than the production of an accurate predictive model for future use. In that case, users are more interested in generating of diverse and robust predictive models, verifying their performance on holdout data, and selecting the most suitable model for their usage scenario. In this paper, we consider the concept of Exploratory Model Analysis (EMA), which is defined as the process of discovering and selecting relevant models that can be used to make predictions on a data source. We delineate the differences between EMA and the well-known term exploratory data analysis in terms of the desired outcome of the analytic process: insights into the data or a set of deployable models. The contributions of this work are a visual analytics system workflow for EMA, a user study, and two use cases validating the effectiveness of the workflow. We found that our system workflow enabled users to generate complex models, to assess them for various qualities, and to select the most relevant model for their task.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13681", "refList": ["10.1109/tvcg.2017.2744683", "10.1111/cgf.12639", "10.1007/s11390-016-1663-1", "10.1109/tvcg.2017.2744938", "10.1117/12.2007316", "10.1109/tvcg.2017.2745178", "10.1145/2702123.2702509", "10.1080/02701367.1992.10608764", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2011.185", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2013.125", "10.1109/mcse.2007.55", "10.1007/978-3-540-70956-5", "10.1109/vl.1996.545307", "10.1901/jeab.1979.31-433", "10.1111/j.1467-8659.2009.01475.x", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2012.65", "10.1145/1835804.1835827", "10.1109/tvcg.2017.2745085", "10.1145/2939672.2939778", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2017.2744805", "10.1145/2487575.2487629", "10.1109/tvcg.2013.157", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2011.209", "10.1109/vast.2012.6400490", "10.1109/tvcg.2015.2513410", "10.1007/978-3-540-79347-2\\_3", "10.1109/tvcg.2014.2346431", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2012.277", "10.1109/infvis.1998.729560", "10.1109/infvis.2004.64", "10.1109/mcg.2006.70", "10.1109/tvcg.2012.260", "10.1109/tvcg.2014.2346660", "10.1145/1743546.1743567", "10.1111/cgf.13417", "10.1109/tvcg.2014.2346325", "10.1109/tvcg.2018.2864838", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2003.1207445", "10.1109/vast.2010.5652443", "10.1145/2641190.2641198", "10.1109/tvcg.2016.2598830", "10.1109/tvcg.2017.2744378", "10.1111/cgf.13324", "10.1109/mcg.2009.22", "10.1109/tvcg.2016.2599030", "10.1109/vast.2012.6400486", "10.1109/tvcg.2016.2598831", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934261", "title": "Ablate, Variate, and Contemplate: Visual Analytics for Discovering Neural Architectures", "year": "2019", "conferenceName": "VAST", "authors": "Dylan Cashman;Adam Perer;Remco Chang;Hendrik Strobelt", "citationCount": "1", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Perer, Adam, Carnegie Mellon Univ, Pittsburgh, PA 15213 USA. Strobelt, Hendrik, MIT IBM Watson AI Lab, Cambridge, MA USA.", "countries": "USA", "abstract": "The performance of deep learning models is dependent on the precise configuration of many layers and parameters. However, there are currently few systematic guidelines for how to configure a successful model. This means model builders often have to experiment with different configurations by manually programming different architectures (which is tedious and time consuming) or rely on purely automated approaches to generate and train the architectures (which is expensive). In this paper, we present Rapid Exploration of Model Architectures and Parameters, or REMAP, a visual analytics tool that allows a model builder to discover a deep learning model quickly via exploration and rapid experimentation of neural network architectures. In REMAP, the user explores the large and complex parameter space for neural network architectures using a combination of global inspection and local experimentation. Through a visual overview of a set of models, the user identifies interesting clusters of architectures. Based on their findings, the user can run ablation and variation experiments to identify the effects of adding, removing, or replacing layers in a given architecture and generate new models accordingly. They can also handcraft new models using a simple graphical interface. As a result, a model builder can build deep learning models quickly, efficiently, and without manual programming. We inform the design of REMAP through a design study with four deep learning model builders. Through a use case, we demonstrate that REMAP allows users to discover performant neural network architectures efficiently using visual exploration and user-defined semi-automated searches through the model space.", "keywords": "visual analytics,neural networks,parameter space exploration", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934261", "refList": ["10.1109/mcg.2018.2878902", "10.1111/cgf.12639", "10.1109/tvcg.2017.2744938", "10.1117/12.2007316", "10.1109/cvpr.2014.81", "10.1016/j.csda.2008.02.031", "10.1080/00994480.2000.10748487", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2013.125", "10.1109/cvpr.2015.7298594", "10.1111/j.1467-8659.2009.01475.x", "10.1109/tvcg.2012.65", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2017.2744805", "10.1145/2487575.2487629", "10.1109/tvcg.2018.2865044", "10.23915/distill.00010", "10.1109/72.279181", "10.1109/tvcg.2017.2744199", "10.1007/s13398-014-0173-7.2", "10.1109/tvcg.2018.2864504", "10.1109/vast.2012.6400490", "10.1007/978-3-319-10590-1\\_53", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2014.2346321", "10.1094/pdis-11-11-0999-pdn", "10.1109/ijcnn.2015.7280767", "10.1109/tvcg.2017.2744878", "10.5555/3326943.3327130", "10.1109/tvcg.2017.2744718", "10.1109/iccv.2015.169", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2014.223", "10.1109/cvpr.2016.90", "10.1109/vast.2010.5652443", "10.1111/cgf.13681", "10.1109/tvcg.2016.2598831", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028888", "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes", "year": "2020", "conferenceName": "VAST", "authors": "Yuxin Ma;Arlen Fan;Jingrui He;Arun Reddy Nelakurthi;Ross Maciejewski", "citationCount": "0", "affiliation": "Ma, YX (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Ma, Yuxin; Fan, Arlen; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. He, Jingrui, Univ Illinois, Champaign, IL USA. Nelakurthi, Arun Reddy, Samsung Res Amer, Mountain View, CA USA.", "countries": "USA", "abstract": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.", "keywords": "Transfer learning,deep learning,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028888", "refList": ["10.1109/tvcg.2014.2346578", "10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1109/tpami.2018.2868685", "10.1145/2702123.2702509", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2016.2598828", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2013.65", "10.1145/2976749.2978318", "10.1007/978-3-030-01424-7\\_27", "10.1109/tvcg.2019.2934261", "10.1007/s11704-016-6028-y", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/iccv.2015.279", "10.1109/mci.2018.2840738", "10.1109/tvcg.2019.2892483", "10.1109/vast.2018.8802509", "10.1109/tvcg.2013.124", "10.1186/s40537-016-0043-6", "10.1109/tvcg.2018.2864475", "10.1145/3200489", "10.1109/tvcg.2018.2865044", "10.1111/cgf.13210", "10.1109/tvcg.2018.2816223", "10.23915/distill.00007", "10.1109/tvcg.2017.2744199", "10.1109/tkde.2018.2876857", "10.1109/tvcg.2019.2934631", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2011.188", "10.1007/978-3-642-15561-1\\_16", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1109/tvcg.2017.2744718", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2016.2598541", "10.1109/tkde.2009.191", "10.1145/3065386", "10.1016/j.ins.2016.03.021", "10.1109/tvcg.2019.2903943", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/iccv.2017.74", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2012.207", "10.1111/cgf.13092", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2017.2754480", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 9}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13970", "year": "2020", "title": "QUESTO: Interactive Construction of Objective Functions for Classification Tasks", "conferenceName": "EuroVis", "authors": "Subhajit Das;Shenyu Xu;Michael Gleicher;Remco Chang;Alex Endert", "citationCount": "0", "affiliation": "Das, S (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA.\nDas, Subhajit; Xu, Shenyu; Endert, Alex, Georgia Inst Technol, Atlanta, GA 30332 USA.\nGleicher, Michael, Univ Wisconsin, Madison, WI USA.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.", "countries": "USA", "abstract": "Building effective classifiers requires providing the modeling algorithms with information about the training data and modeling goals in order to create a model that makes proper tradeoffs. Machine learning algorithms allow for flexible specification of such meta-information through the design of the objective functions that they solve. However, such objective functions are hard for users to specify as they are a specific mathematical formulation of their intents. In this paper, we present an approach that allows users to generate objective functions for classification problems through an interactive visual interface. Our approach adopts a semantic interaction design in that user interactions over data elements in the visualization are translated into objective function terms. The generated objective functions are solved by a machine learning solver that provides candidate models, which can be inspected by the user, and used to suggest refinements to the specifications. We demonstrate a visual analytics system QUESTO for users to manipulate objective functions to define domain-specific constraints. Through a user study we show that QUESTO helps users create various objective functions that satisfy their goals.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13970", "refList": ["10.1016/j.neucom.2017.01.105", "10.1371/journal.pone.0050474", "10.1109/tvcg.2016.2598839", "10.1007/978-3-642-21530-8\\_14", "10.1109/tvcg.2016.2598828", "10.5555/2969442.2969547", "10.1007/s00371-015-1132-9", "10.1145/2851581.2856492", "10.1126/scirobotics.aao6760", "10.1109/vast.2011.6102453", "10.1109/vast.2011.6102449", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2016.2598446", "10.1145/359784.360332", "10.1111/j.1467-8659.2009.01475.x", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2016.2598460", "10.1109/tevc.2015.2472283", "10.1145/2207676.2207741", "10.1109/tvcg.2017.2745085", "10.1145/2939672.2939778", "10.1145/1866029.1866038", "10.1145/2487575.2487629", "10.1145/3077257.3077259", "10.2312/eurova.20171123", "10.1145/2983924", "10.1109/mcg.2013.53", "10.1109/tvcg.2015.2467615", "10.1109/vast.2014.7042492", "10.1145/2675133.2675214", "10.1109/tvcg.2014.2346482", "10.1145/3180308.3180362", "10.1109/tvcg.2014.2346321", "10.24963/ijcai.2017/202", "10.1109/tvcg.2014.2346291", "10.1109/tbme.2012.2212278", "10.1007/s40708-016-0042-6", "10.1609/aimag.v35i4.2513", "10.1016/j.ijhcs.2009.03.004", "10.1109/tevc.2012.2225064", "10.1016/s0890-6955(02)00074-3", "10.1109/icmlde.2018.00014", "10.1111/cgf.13681", "10.1109/tvcg.2013.173", "10.1145/3025171.3025208", "10.1145/3025453.3026044", "10.1109/cec.2017.7969334", "10.1109/vast.2012.6400486", "10.1109/tvcg.2016.2598831", "10.1109/tcyb.2014.2310651"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13972", "year": "2020", "title": "Boxer: Interactive Comparison of Classifier Results", "conferenceName": "EuroVis", "authors": "Michael Gleicher;Aditya Barve;Xinyi Yu;Florian Heimerl", "citationCount": "0", "affiliation": "Gleicher, M (Corresponding Author), Univ Wisconsin, Madison, WI 53706 USA.\nGleicher, Michael; Barve, Aditya; Yu, Xinyi; Heimerl, Florian, Univ Wisconsin, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Machine learning practitioners often compare the results of different classifiers to help select, diagnose and tune models. We present Boxer, a system to enable such comparison. Our system facilitates interactive exploration of the experimental results obtained by applying multiple classifiers to a common set of model inputs. The approach focuses on allowing the user to identify interesting subsets of training and testing instances and comparing performance of the classifiers on these subsets. The system couples standard visual designs with set algebra interactions and comparative elements. This allows the user to compose and coordinate views to specify subsets and assess classifier performance on them. The flexibility of these compositions allow the user to address a wide range of scenarios in developing and assessing classifiers. We demonstrate Boxer in use cases including model selection, tuning, fairness assessment, and data quality diagnosis.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13972", "refList": ["10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2016.2598838", "10.1007/s10618-014-0368-8", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1145/3287560.3287589", "10.1109/vast.2017.8585721", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2009.128", "10.1109/tvcg.2017.2744018", "10.1080/00994480.2000.10748487", "10.5555/3305890.3306024", "10.1109/iccv.2015.329", "10.1109/tvcg.2013.125", "10.1089/big.2016.0007", "10.1109/memsys.2019.8870817", "10.1145/2939672.2939778", "10.1007/s11104-019-04156-0", "10.1371/journal.pone.0181142", "10.1145/3301275.3302324", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2018.2865044", "10.1023/a:1010933404324", "10.1145/2487575.2487579", "10.1109/tvcg.2013.157", "10.1145/2783258.2788613", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934631", "10.1016/s0304-3800(02)00064-9", "10.1007/s10115-013-0679-x", "10.1109/tvcg.2019.2934267", "10.1007/978-3-319-10590-1\\_53", "10.1109/vast.2017.8585720", "10.1016/0004-3702(80)90021-1", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2009.84", "10.1007/s11263-016-0911-8", "10.1111/cgf.12918", "10.1111/cgf.12373", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1080/00207176808905715", "10.1002/er.3827", "10.1109/tvcg.2014.2346660", "10.1111/cgf.13417", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2017.2744158", "10.1016/b978-0-12-815849-4.00004-9", "10.1097/ede.0b013e3181c30fb2", "10.1111/cgf.13681", "10.1016/j.ejor.2006.04.051", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2016.2598468", "10.9735/2229-3981", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 21}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 171}, {"doi": "10.1109/tvcg.2017.2745298", "title": "TACO: Visualizing Changes in Tables Over Time", "year": "2017", "conferenceName": "InfoVis", "authors": "Christina Stoiber;Holger Stitz;Reem Hourieh;Florian Grassinger;Wolfgang Aigner;Marc Streit", "citationCount": "5", "affiliation": "Niederer, C (Corresponding Author), St Polten Univ Appl Sci, St Polten, Austria. Niederer, Christina; Grassinger, Florian; Aigner, Wolfgang, St Polten Univ Appl Sci, St Polten, Austria. Stitz, Holger; Hourieh, Reem; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria. Aigner, Wolfgang, TU Wien, Vienna, Austria.", "countries": "Austria", "abstract": "Multivariate, tabular data is one of the most common data structures used in many different domains. Over time, tables can undergo changes in both structure and content, which results in multiple versions of the same table. A challenging task when working with such derived tables is to understand what exactly has changed between versions in terms of additions/deletions, reorder, merge/split, and content changes. For textual data, a variety of commonplace \u201cdiff\u201d tools exist that support the task of investigating changes between revisions of a text. Although there are some comparison tools which assist users in inspecting differences between multiple table instances, the resulting visualizations are often difficult to interpret or do not scale to large tables with thousands of rows and columns. To address these challenges, we developed TACO, an interactive comparison tool that visualizes the differences between multiple tables at various levels of detail. With TACO we show (1) the aggregated differences between multiple table versions over time, (2) the aggregated changes between two selected table versions, and (3) detailed changes between the selected tables. To demonstrate the effectiveness of our approach, we show its application by means of two usage scenarios.", "keywords": "Table comparison,matrix,difference visualization", "link": "http://dx.doi.org/10.1109/TVCG.2017.2745298", "refList": ["10.1109/tvcg.2013.213", "10.1109/tvcg.2012.233", "10.1109/tvcg.2012.237", "10.1002/sam.10071", "10.1109/vl.1996.545307", "10.1177/1473871611416549", "10.1007/bfb0024363", "10.1145/3025453.3026041", "10.1111/cgf.12397", "10.1186/1471-2105-13-s8-s2", "10.7908/c1m32v4k", "10.1109/iv.2005.28", "10.1111/j.1467-8659.2012.03110.x", "10.1109/tvcg.2011.250", "10.1109/tvcg.2010.138", "10.1145/2470654.2470724", "10.1111/cgf.12924", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030419", "title": "Comparative Layouts Revisited: Design Space, Guidelines, and Future Directions", "year": "2020", "conferenceName": "InfoVis", "authors": "Sehi L'Yi;Jaemin Jo;Jinwook Seo", "citationCount": "0", "affiliation": "L'Yi, S (Corresponding Author), Harvard Med Sch, Boston, MA 02115 USA. L'Yi, Sehi, Harvard Med Sch, Boston, MA 02115 USA. Jo, Jaemin, Sungkyunkwan Univ, Seoul, South Korea. Seo, Jinwook, Seoul Natl Univ, Seoul, South Korea.", "countries": "USA;Korea", "abstract": "We present a systematic review on three comparative layouts-juxtaposition, superposition, and explicit-encoding-which are information visualization (InfoVis) layouts designed to support comparison tasks. For the last decade, these layouts have served as fundamental idioms in designing many visualization systems. However, we found that the layouts have been used with inconsistent terms and confusion, and the lessons from previous studies are fragmented. The goal of our research is to distill the results from previous studies into a consistent and reusable framework. We review 127 research papers, including 15 papers with quantitative user studies, which employed comparative layouts. We first alleviate the ambiguous boundaries in the design space of comparative layouts by suggesting lucid terminology (e.g., chart-wise and item-wise juxtaposition). We then identify the diverse aspects of comparative layouts, such as the advantages and concerns of using each layout in the real-world scenarios and researchers' approaches to overcome the concerns. Building our knowledge on top of the initial insights gained from the Gleicher et al.'s survey [19], we elaborate on relevant empirical evidence that we distilled from our survey (e.g., the actual effectiveness of the layouts in different study settings) and identify novel facets that the original work did not cover (e.g., the familiarity of the layouts to people). Finally, we show the consistent and contradictory results on the performance of comparative layouts and offer practical implications for using the layouts by suggesting trade-offs and seven actionable guidelines.", "keywords": "Comparative layout,visual comparison,literature review,juxtaposition,superposition,explicit-encoding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030419", "refList": ["10.1109/tvcg.2013.233", "10.1111/cgf.12380", "10.1177/1473871611416549", "10.1145/2702123.2702419", "10.1109/tvcg.2014.2322363", "10.1111/cgf.12791", "10.1145/2702123.2702130", "10.1145/2702123.2702217", "10.1109/tvcg.2012.237", "10.1177/1473871613480062", "10.1109/mcg.2017.377152546", "10.1109/tvcg.2013.213", "10.1111/cgf.12369", "10.1109/tvcg.2017.2744198", "10.1145/3139295.3139309", "10.1109/tvcg.2019.2934801", "10.1109/tvcg.2013.122", "10.1109/tvcg.2017.2747545", "10.1109/tvcg.2015.2413774", "10.1037/0096-1523.24.3.719", "10.1109/tvcg.2014.2346320", "10.1109/tvcg.2007.70535", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1145/2556288.2557141", "10.1109/tvcg.2013.149", "10.1145/1165734.1165736", "10.5220/0006127502170224", "10.1109/tvcg.2017.2745298", "10.1177/1473871617692841", "10.1190/int-2017-0083.1", "10.1190/int-2014-0283.1", "10.1109/tvcg.2016.2598796", "10.1111/cgf.13401", "10.1016/j.cag.2017.05.005", "10.1177/1473871616667632", "10.1145/3025453.3025882", "10.1145/2470654.2470724", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2010.164", "10.1145/3103010.3103013", "10.1109/pacificvis.2016.7465266", "10.1109/pacificvis.2012.6183556", "10.1109/tvcg.2015.2467751", "10.1109/tvcg.2018.2796557", "10.1111/cgf.13531", "10.1109/tvcg.2013.161", "10.1109/iv.2018.00051", "10.1109/tvcg.2010.162", "10.1109/tvcg.2018.2864510", "10.1109/iv.2017.30", "10.1109/tvcg.2007.70623"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13401", "year": "2018", "title": "Towards Easy Comparison of Local Businesses Using Online Reviews", "conferenceName": "EuroVis", "authors": "Yong Wang;Hammad Haleem;Conglei Shi;Yanhong Wu;Xun Zhao;Siwei Fu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, Y (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.\nWang, Yong; Haleem, Hammad; Zhao, Xun; Fu, Siwei; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.\nShi, Conglei, Airbnb Inc, San Francisco, CA USA.\nWu, Yanhong, Vis Res, Palo Alto, CA USA.", "countries": "USA;China", "abstract": "With the rapid development of e-commerce, there is an increasing number of online review websites, such as Yelp, to help customers make better purchase decisions. Viewing online reviews, including the rating score and text comments by other customers, and conducting a comparison between different businesses are the key to making an optimal decision. However, due to the massive amount of online reviews, the potential difference of user rating standards, and the significant variance of review time, length, details and quality, it is difficult for customers to achieve a quick and comprehensive comparison. In this paper, we present E-Comp, a carefully-designed visual analytics system based on online reviews, to help customers compare local businesses at different levels of details. More specifically, intuitive glyphs overlaid on maps are designed for quick candidate selection. Grouped Sankey diagram visualizing the rating difference by common customers is chosen for more reliable comparison of two businesses. Augmented word cloud showing adjective-noun word pairs, combined with a temporal view, is proposed to facilitate in-depth comparison of businesses in terms of different time periods, rating scores and features. The effectiveness and usability of E-Comp are demonstrated through a case study and in-depth user interviews.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13401", "refList": ["10.1007/978-3-211-77280-5\\_4", "10.1177/1473871611416549", "10.1145/2702123.2702476", "10.1145/1014052.1014073", "10.1109/tvcg.2014.2346249", "10.1007/978-3-540-33037-08", "10.1109/tvcg.2007.70570", "10.1016/j.ijhm.2008.06.011", "10.1109/mic.2003.1167344", "10.1177/0047287513481274", "10.1109/iv.2013.5", "10.1111/j.1467-8659.2008.01205.x", "10.1561/1500000001", "10.1007/978-3-319-30319-2\\_13", "10.1109/tvcg.2013.254", "10.1007/978-3-540-33037-0\\_8", "10.1109/tvcg.2013.122", "10.1002/acp.2350050106", "10.1109/tvcg.2016.2598590", "10.2312/eurovisshort.20161167", "10.1007/s11002-013-9278-6", "10.1111/cgf.12888", "10.1145/1134707.1134743", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2010.183", "10.1109/vast.2009.5333919", "10.1109/tvcg.2017.2744199", "10.1111/cgf.13217", "10.1145/1060745.1060797", "10.1162/jmlr.2003.3.4-5.951", "10.1109/tvcg.2017.2723397", "10.2753/jec1086-4415170204", "10.1287/mksc.1110.0653", "10.1109/tvcg.2012.110", "10.1086/268567", "10.1109/tvcg.2017.2744738", "10.1007/978-3-540", "10.1109/tvcg.2014.2346912", "10.1109/mis.2013.36", "10.1109/tvcg.2013.173", "10.1109/tvcg.2006.76", "10.1109/tvcg.2017.2745298", "10.1559/152304009788188808"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030419", "title": "Comparative Layouts Revisited: Design Space, Guidelines, and Future Directions", "year": "2020", "conferenceName": "InfoVis", "authors": "Sehi L'Yi;Jaemin Jo;Jinwook Seo", "citationCount": "0", "affiliation": "L'Yi, S (Corresponding Author), Harvard Med Sch, Boston, MA 02115 USA. L'Yi, Sehi, Harvard Med Sch, Boston, MA 02115 USA. Jo, Jaemin, Sungkyunkwan Univ, Seoul, South Korea. Seo, Jinwook, Seoul Natl Univ, Seoul, South Korea.", "countries": "USA;Korea", "abstract": "We present a systematic review on three comparative layouts-juxtaposition, superposition, and explicit-encoding-which are information visualization (InfoVis) layouts designed to support comparison tasks. For the last decade, these layouts have served as fundamental idioms in designing many visualization systems. However, we found that the layouts have been used with inconsistent terms and confusion, and the lessons from previous studies are fragmented. The goal of our research is to distill the results from previous studies into a consistent and reusable framework. We review 127 research papers, including 15 papers with quantitative user studies, which employed comparative layouts. We first alleviate the ambiguous boundaries in the design space of comparative layouts by suggesting lucid terminology (e.g., chart-wise and item-wise juxtaposition). We then identify the diverse aspects of comparative layouts, such as the advantages and concerns of using each layout in the real-world scenarios and researchers' approaches to overcome the concerns. Building our knowledge on top of the initial insights gained from the Gleicher et al.'s survey [19], we elaborate on relevant empirical evidence that we distilled from our survey (e.g., the actual effectiveness of the layouts in different study settings) and identify novel facets that the original work did not cover (e.g., the familiarity of the layouts to people). Finally, we show the consistent and contradictory results on the performance of comparative layouts and offer practical implications for using the layouts by suggesting trade-offs and seven actionable guidelines.", "keywords": "Comparative layout,visual comparison,literature review,juxtaposition,superposition,explicit-encoding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030419", "refList": ["10.1109/tvcg.2013.233", "10.1111/cgf.12380", "10.1177/1473871611416549", "10.1145/2702123.2702419", "10.1109/tvcg.2014.2322363", "10.1111/cgf.12791", "10.1145/2702123.2702130", "10.1145/2702123.2702217", "10.1109/tvcg.2012.237", "10.1177/1473871613480062", "10.1109/mcg.2017.377152546", "10.1109/tvcg.2013.213", "10.1111/cgf.12369", "10.1109/tvcg.2017.2744198", "10.1145/3139295.3139309", "10.1109/tvcg.2019.2934801", "10.1109/tvcg.2013.122", "10.1109/tvcg.2017.2747545", "10.1109/tvcg.2015.2413774", "10.1037/0096-1523.24.3.719", "10.1109/tvcg.2014.2346320", "10.1109/tvcg.2007.70535", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1145/2556288.2557141", "10.1109/tvcg.2013.149", "10.1145/1165734.1165736", "10.5220/0006127502170224", "10.1109/tvcg.2017.2745298", "10.1177/1473871617692841", "10.1190/int-2017-0083.1", "10.1190/int-2014-0283.1", "10.1109/tvcg.2016.2598796", "10.1111/cgf.13401", "10.1016/j.cag.2017.05.005", "10.1177/1473871616667632", "10.1145/3025453.3025882", "10.1145/2470654.2470724", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2010.164", "10.1145/3103010.3103013", "10.1109/pacificvis.2016.7465266", "10.1109/pacificvis.2012.6183556", "10.1109/tvcg.2015.2467751", "10.1109/tvcg.2018.2796557", "10.1111/cgf.13531", "10.1109/tvcg.2013.161", "10.1109/iv.2018.00051", "10.1109/tvcg.2010.162", "10.1109/tvcg.2018.2864510", "10.1109/iv.2017.30", "10.1109/tvcg.2007.70623"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 9}, {"doi": "10.1109/tvcg.2018.2864769", "title": "Visual Analytics for Topic Model Optimization based on User-Steerable Speculative Execution", "year": "2018", "conferenceName": "VAST", "authors": "Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel A. Keim;Christopher Collins", "citationCount": "7", "affiliation": "El-Assady, M (Corresponding Author), Univ Konstanz, Constance, Germany. El-Assady, M (Corresponding Author), Univ Ontario Inst Technol, Oshawa, ON, Canada. El-Assady, Mennatallah; Sperrle, Fabian; Deussen, Oliver; Keim, Daniel, Univ Konstanz, Constance, Germany. El-Assady, Mennatallah; Collins, Christopher, Univ Ontario Inst Technol, Oshawa, ON, Canada.", "countries": "Canada;Germany", "abstract": "To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision-making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-Ioop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.", "keywords": "User-Steerable Topic Modeling,Speculative Execution,Mixed-Initiative Visual Analytics,Explainable Machine Learning", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864769", "refList": ["10.1109/vast.2014.7042493", "10.1145/2133806.2133826", "10.1007/bf00114265", "10.1109/tvcg.2014.2346578", "10.5555/2145432.2145462", "10.1145/775047.775110", "10.1016/j.csda.2008.01.011", "10.1109/tvcg.2017.2743959", "10.1109/tkde.2004.58", "10.1109/tvcg.2016.2598445", "10.1007/978-1-4614-3223-4\\_4", "10.1109/tvcg.2013.231", "10.1016/j.ijhcs.2017.03.007", "10.1111/cgf.12924", "10.1007/978-3-540-70956-5", "10.1007/s10994-013-5413-0", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2013.232", "10.1145/219717.219748", "10.1109/tvcg.2013.212", "10.1145/2207676.2207738", "10.1145/1835804.1835827", "10.1145/1189769.1189779", "10.1186/s12859-015-0564-6", "10.1145/1143844.1143859", "10.1109/tvcg.2013.162", "10.1109/tvcg.2017.2745080", "10.1109/tvcg.2017.2744199", "10.1198/016214506000000302", "10.1007/978-3-319-09259-1\\_7", "10.1145/860435.860485", "10.1109/caia.1988.196114", "10.1145/2212776.2223772", "10.1145/1667053.1667056", "10.3115/1690219.1690287", "10.1145/882262.882291", "10.1162/jmlr.2003.3.4-5.993", "10.1145/2678025.2701370", "10.1057/palgrave.ivs.9500157", "10.1109/vast.2011.6102461", "10.1016/j.visinf.2017.01.005", "10.1177/0165551515617393", "10.1145/1273496.1273576", "10.1007/978-1-4614-3223-4", "10.1016/0004-3702(89)90046-5", "10.2312/eurovisstar.20151113", "10.1016/j.nima.2010.11.062"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934654", "title": "Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections", "year": "2019", "conferenceName": "VAST", "authors": "Mennatallah El-Assady;Rebecca Kehlbeck;Christopher Collins;Daniel A. Keim;Oliver Deussen", "citationCount": "3", "affiliation": "El-Assady, M (Corresponding Author), Univ Konstanz, Constance, Germany. El-Assady, M (Corresponding Author), Ontario Tech Univ, Oshawa, ON, Canada. El-Assady, Mennatallah; Kehlbeck, Rebecca; Keim, Daniel; Deussen, Oliver, Univ Konstanz, Constance, Germany. El-Assady, Mennatallah; Collins, Christopher, Ontario Tech Univ, Oshawa, ON, Canada.", "countries": "Canada;Germany", "abstract": "We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decision-making process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.", "keywords": "Topic Model Optimization,Word Embedding,Mixed-Initiative Refinement,Guided Visual Analytics,Semantic Mapping", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934654", "refList": ["10.1109/vast.2014.7042493", "10.1145/2133806.2133826", "10.1016/j.visinf.2018.09.003", "10.1007/978-3-319-67008-9\\_26", "10.1109/tvcg.2013.126", "10.1162/tacl\\_a\\_00140", "10.1007/s13202-018-0509-5", "10.1109/bdva.2018.8534018", "10.1108/eb026526", "10.1007/s10994-013-5413-0", "10.1145/564376.564421", "10.1016/j.visinf.2017.01.006", "10.3115/v1/p14-2050", "10.1007/bf00288933", "10.1109/tvcg.2013.212", "10.1145/2207676.2207741", "10.1109/tvcg.2013.162", "10.1109/tvcg.2016.2515592", "10.1109/tvcg.2017.2745080", "10.1109/mcg.2013.53", "10.1109/tvcg.2017.2744199", "10.1145/3091108", "10.18653/v1/p17-4009", "10.1162/jmlr.2003.3.4-5.951", "10.1109/vast.2017.8585498", "10.1109/tvcg.2017.2723397", "10.1109/tvcg.2018.2864769", "10.1007/s10618-005-0361-3", "10.3115/v1/d14-1167", "10.1007/bf01840357", "10.1162/jmlr.2003.3.4-5.993", "10.1145/2678025.2701370", "10.1016/j.ins.2016.06.040", "10.1109/tvcg.2017.2746018", "10.1109/vast.2011.6102461", "10.1111/cgf.13092", "10.3115/1117729.1117730", "10.1109/mcg.2015.91", "10.1145/2669557.2669572"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/vast47406.2019.8986922", "title": "TopicSifter: Interactive Search Space Reduction through Targeted Topic Modeling", "year": "2019", "conferenceName": "VAST", "authors": "Hannah Kim;Dongjin Choi;Barry L. Drake;Alex Endert;Haesun Park", "citationCount": "0", "affiliation": "Kim, H (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA. Kim, Hannah; Choi, Dongjin; Endert, Alex; Park, Haesun, Georgia Inst Technol, Atlanta, GA 30332 USA. Drake, Barry, Georgia Tech Res Inst, Atlanta, GA 30332 USA.", "countries": "USA", "abstract": "Topic modeling is commonly used to analyze and understand large document collections. However, in practice, users want to focus on specific aspects or \u201ctargets\u201d rather than the entire corpus. For example, given a large collection of documents, users may want only a smaller subset which more closely aligns with their interests, tasks, and domains. In particular, our paper focuses on large-scale document retrieval with high recall where any missed relevant documents can be critical. A simple keyword matching search is generally not effective nor efficient as 1) it is difficult to find a list of keyword queries that can cover the documents of interest before exploring the dataset, 2) some documents may not contain the exact keywords of interest but may still be highly relevant, and 3) some words have multiple meanings, which would result in irrelevant documents included in the retrieved subset. In this paper, we present TopicSifter, a visual analytics system for interactive search space reduction. Our system utilizes targeted topic modeling based on nonnegative matrix factorization and allows users to give relevance feedback in order to refine their target and guide the topic modeling to the most relevant results.", "keywords": "Human-centered computing,Visualization,Visualization application domains,Visual analytics,Information systems,Information retrieval,Users and interactive retrieval,Search interfaces", "link": "http://dx.doi.org/10.1109/VAST47406.2019.8986922", "refList": ["10.1145/2505515.2505644", "10.1007/s10898-014-0247-2", "10.1017/s1351324909005129", "10.1145/3070616", "10.1145/1871437.1871535", "10.1109/wi.2005.158", "10.1109/tvcg.2016.2598445", "10.1109/mcg.2004.22", "10.1111/j.1467-8659.2012.03108.x", "10.1145/3132847.3132968", "10.1017/s0269888903000638", "10.1109/tvcg.2009.176", "10.1109/wi.2006.6", "10.1109/tvcg.2006.142", "10.1109/infvis.2001.963287", "10.1109/tvcg.2013.212", "10.1109/tvcg.2013.242", "10.1145/2939672.2939743", "10.1177/1473871618757228", "10.1007/s10898-017-0515-z", "10.1007/978-3-319-60585-2\\_16", "10.1145/2656334", "10.1145/3178876.3186069", "10.1162/153244303322533223", "10.1007/s00799-004-0111-y", "10.1109/tvcg.2018.2864769", "10.1057/palgrave.ivs.9500023", "10.1145/2600428.2609618", "10.1016/j.cag.2016.12.004", "10.1162/jmlr.2003.3.4-5.993", "10.1145/2678025.2701370", "10.1109/tvcg.2012.260", "10.3115/v1/d14-1162", "10.1109/tvcg.2010.154", "10.1108/eb046814", "10.1109/tvcg.2014.2346433"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/vast47406.2019.8986917", "title": "VIANA: Visual Interactive Annotation of Argumentation", "year": "2019", "conferenceName": "VAST", "authors": "Fabian Sperrle;Rita Sevastjanova;Rebecca Kehlbeck;Mennatallah El-Assady", "citationCount": "0", "affiliation": "Sperrle, F (Corresponding Author), Univ Konstanz, Constance, Germany. Sperrle, Fabian; Sevastjanova, Rita; Kehlbeck, Rebecca; El-Assady, Mennatallah, Univ Konstanz, Constance, Germany.", "countries": "Germany", "abstract": "Argumentation Mining addresses the challenging tasks of identifying boundaries of argumentative text fragments and extracting their relationships. Fully automated solutions do not reach satisfactory accuracy due to their insufficient incorporation of semantics and domain knowledge. Therefore, experts currently rely on time-consuming manual annotations. In this paper, we present a visual analytics system that augments the manual annotation process by automatically suggesting which text fragments to annotate next. The accuracy of those suggestions is improved over time by incorporating linguistic knowledge and language modeling to learn a measure of argument similarity from user interactions. Based on a long-term collaboration with domain experts, we identify and model five high-level analysis tasks. We enable close reading and note-taking, annotation of arguments, argument reconstruction, extraction of argument relations, and exploration of argument graphs. To avoid context switches, we transition between all views through seamless morphing, visually anchoring all text- and graph-based layers. We evaluate our system with a two-stage expert user study based on a corpus of presidential debates. The results show that experts prefer our system over existing solutions due to the speedup provided by the automatic suggestions and the tight integration between text and graph views.", "keywords": "Argumentation annotation,machine learning,user interaction,layered interfaces,semantic transitions", "link": "http://dx.doi.org/10.1109/VAST47406.2019.8986917", "refList": ["10.1007/978-3-642-40624-9\\_1", "10.3233/978-1-61499-436-7-185", "10.1145/371920.372071", "10.3233/978-1-61499-906-5-4", "10.1109/tvcg.2015.2467759", "10.18653/v1/d15-1050", "10.2307/2529310", "10.1109/mic.2003.1167344", "10.1145/312624.312682", "10.1145/2850417", "10.18653/v1/p17-2039", "10.1016/j.eswa.2016.02.013", "10.1007/978-0-387-85820-3\\_3", "10.1007/s11412-009-9080-x", "10.1109/tvcg.2008.127", "10.1145/2207676.2207741", "10.1109/cit.2012.217", "10.3233/aac-170022", "10.1109/tvcg.2017.2745080", "10.1007/978-3-319-44039-2\\_6", "10.1145/3290605.3300233", "10.1007/978-94-017-0431-1", "10.3233/978-1-61499-906-5-313", "10.1142/s0218213004001922", "10.1109/tvcg.2012.262", "10.1177/001316446002000104", "10.1109/tvcg.2018.2834341", "10.3233/978-1-61499-436-7-463", "10.1145/1772690.1772773", "10.1109/tvcg.2014.2346677", "10.1145/2523813", "10.1162/153244303322533223", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2015.2467531", "10.1109/tvcg.2018.2864769", "10.1007/978-3-319-90092-6\\_14", "10.1137/1.9781611972801.19", "10.1109/vast.2012.6400485", "10.1007/s10579-013-9215-6", "10.3115/v1/d14-1162", "10.1111/cgf.13446", "10.1109/tvcg.2006.156", "10.1111/cgf.13092", "10.1145/2645710.2645759", "10.1109/bigdata.2017.8258140", "10.1145/2669557.2669572", "10.2312/eurovisstar.20151113"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2018.2865024", "title": "KnowledgePearls: Provenance-Based Visualization Retrieval", "year": "2018", "conferenceName": "VAST", "authors": "Holger Stitz;Samuel Gratzl;Harald Piringer;Thomas Zichner;Marc Streit", "citationCount": "4", "affiliation": "Stitz, H (Corresponding Author), Johannes Kepler Univ Linz, Linz, Austria. Stitz, Holger; Gratzl, Samuel; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria. Gratzl, Samuel, Datavisyn GmbH, Linz, Austria. Piringer, Harald, VRVis Res Ctr, Vienna, Austria. Zichner, Thomas, Boehringer Ingelheim RCV GmbH \\& Co KG, Vienna, Austria.", "countries": "Austria", "abstract": "Storing analytical provenance generates a knowledge base with a large potential for recalling previous results and guiding users in future analyses. However, without extensive manual creation of meta information and annotations by the users, search and retrieval of analysis states can become tedious. We present KnowledgePearls, a solution for efficient retrieval of analysis states that are structured as provenance graphs containing automatically recorded user interactions and visualizations. As a core component, we describe a visual interface for querying and exploring analysis states based on their similarity to a partial definition of a requested analysis state. Depending on the use case, this definition may be provided explicitly by the user by formulating a search query or inferred from given reference states. We explain our approach using the example of efficient retrieval of demographic analyses by Hans Rosling and discuss our implementation for a fast look-up of previous states. Our approach is independent of the underlying visualization framework. We discuss the applicability for visualizations which are based on the declarative grammar Vega and we use a Vega-based implementation of Gapminder as guiding example. We additionally present a biomedical case study to illustrate how KnowledgePearls facilitates the exploration process by recalling states from earlier analyses.", "keywords": "Visualization provenance,interaction provenance,retrieval", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865024", "refList": ["10.1109/tvcg.2008.137", "10.1007/s10115-018-1164-3", "10.1145/2642918.2647360", "10.1145/3025453.3025957", "10.1145/2047196.2047247", "10.1109/visual.2005.1532788", "10.1145/142750.143082", "10.1109/tvcg.2011.229", "10.1111/cgf.12924", "10.1016/j.aei.2016.04.003", "10.1109/tvcg.2009.176", "10.1109/tvcg.2010.184", "10.1109/tvcg.2006.142", "10.1093/bib/bbl022", "10.1002/cpe.1247", "10.1145/775047.775064", "10.1109/tvcg.2012.271", "10.1111/cgf.12925", "10.1057/ivs.2008.31", "10.1109/tvcg.2017.2745219", "10.1109/tvcg.2015.2467551", "10.1109/2.211893", "10.1111/cgf.13193", "10.1109/infvis.2005.1532142", "10.1109/tvcg.2015.2513389", "10.1109/icde.2008.4497516", "10.1126/science.298.5594.824", "10.1109/tvcg.2015.2467091", "10.1007/s00778-017-0486-1", "10.1109/tvcg.2009.84", "10.1101/277848", "10.1109/2945.981851", "10.1109/tvcg.2013.155", "10.1147/sj.164.0324", "10.1145/3035918.3056418", "10.1109/tvcg.2016.2598589", "10.1145/361219.361220", "10.1109/tvcg.2017.2744320", "10.1109/infvis.2004.2", "10.1109/tvcg.2013.173", "10.1145/1376616.1376747", "10.1109/tvcg.2016.2599030", "10.1109/mcse.2008.79"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934658", "title": "LightGuider: Guiding Interactive Lighting Design using Suggestions, Provenance, and Quality Visualization", "year": "2019", "conferenceName": "VAST", "authors": "Andreas Walch;Michael Schw\u00e4rzler;Christian Luksch;Elmar Eisemann;Theresia Gschwandtner", "citationCount": "0", "affiliation": "Walch, A (Corresponding Author), VRVis Forschungs GmbH, Graz, Austria. Walch, Andreas; Luksch, Christian, VRVis Forschungs GmbH, Graz, Austria. Schwarzler, Michael; Eisemann, Elmar, Delft Univ Technol, Delft, Netherlands. Gschwandtner, Theresia, TU Wien, Vienna, Austria.", "countries": "Netherlands;Austria", "abstract": "LightGuider is a novel guidance-based approach to interactive lighting design, which typically consists of interleaved 3D modeling operations and light transport simulations. Rather than having designers use a trial-and-error approach to match their illumination constraints and aesthetic goals, LightGuider supports the process by simulating potential next modeling steps that can deliver the most significant improvements. LightGuider takes predefined quality criteria and the current focus of the designer into account to visualize suggestions for lighting-design improvements via a specialized provenance tree. This provenance tree integrates snapshot visualizations of how well a design meets the given quality criteria weighted by the designer's preferences. This integration facilitates the analysis of quality improvements over the course of a modeling workflow as well as the comparison of alternative design solutions. We evaluate our approach with three lighting designers to illustrate its usefulness.", "keywords": "guidance,3D modeling,lighting design,provenance,global illumination", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934658", "refList": ["10.1109/tvcg.2015.2468011", "10.1145/1243980.1243983", "10.1145/2629573", "10.1109/tvcg.2008.59", "10.1109/tvcg.2010.223", "10.1145/1345448.1345453", "10.1109/38.920623", "10.1145/1276377.1276444", "10.1109/tvcg.2013.126", "10.1007/s00371-015-1132-9", "10.1109/tvcg.2011.185", "10.1111/cgf.12924", "10.1145/1239451.1239505", "10.1109/tvcg.2013.147", "10.1145/985692.985765", "10.1287/opre.15.3.537", "10.1016/j.cag.2013.11.002", "10.1145/2702123.2702149", "10.1111/cgf.13101", "10.1109/tvcg.2015.2467551", "10.21037/jmai.2018.07.01", "10.1145/258734.258887", "10.1145/1084805.1084812", "10.1111/cgf.13452", "10.1109/tvcg.2014.2346321", "10.1006/ijhc.2000.0420", "10.1109/pg.2007.9", "10.1109/tvcg.2018.2865024", "10.1007/s00371-016-1257-5", "10.1111/cgf.12159", "10.1109/tvcg.2016.2598468", "10.1109/tvcg.2012.175"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030438", "title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening", "year": "2020", "conferenceName": "SciVis", "authors": "Mar\u00eda Virginia Sabando;Pavol Ulbrich;Mat\u00edas N. Selzer;Jan Byska;Jan Mican;Ignacio Ponzoni;Axel J. Soto;Maria Luj\u00e1n Ganuza;Barbora Kozl\u00edkov\u00e1", "citationCount": "0", "affiliation": "Sabando, MV (Corresponding Author), Univ Nacl Sur, Inst Comp Sci \\& Engn UNS CONICET, Bahia Blanca, Buenos Aires, Argentina. Sabando, MV (Corresponding Author), Univ Nacl Sur, Dept Comp Sci \\& Engn, Bahia Blanca, Buenos Aires, Argentina. Sabando, Maria Virginia; Selzer, Matias; Ponzoni, Ignacio; Soto, Axel J.; Ganuza, Maria Lujan, Univ Nacl Sur, Inst Comp Sci \\& Engn UNS CONICET, Bahia Blanca, Buenos Aires, Argentina. Sabando, Maria Virginia; Ponzoni, Ignacio; Soto, Axel J., Univ Nacl Sur, Dept Comp Sci \\& Engn, Bahia Blanca, Buenos Aires, Argentina. Ulbrich, Pavol; Byska, Jan; Kozlikova, Barbora, Masaryk Univ, Fac Informat, Visitlab, Brno, Czech Republic. Selzer, Matias; Ganuza, Maria Lujan, Univ Nacl Sur, VyGLab Res Lab UNS CICPBA, Dept Comp Sci \\& Engn, Bahia Blanca, Buenos Aires, Argentina. Mican, Jan, Masaryk Univ, Dept Expt Biol, Loschmidt Labs, Brno, Czech Republic. Mican, Jan, Masaryk Univ, RECETOX, Brno, Czech Republic. Mican, Jan, Masaryk Univ, Fac Med, Brno, Czech Republic.", "countries": "Argentina;Republic", "abstract": "In the modern drug discovery process, medicinal chemists deal with the complexity of analysis of large ensembles of candidate molecules. Computational tools, such as dimensionality reduction (DR) and classification, are commonly used to efficiently process the multidimensional space of features. These underlying calculations often hinder interpretability of results and prevent experts from assessing the impact of individual molecular features on the resulting representations. To provide a solution for scrutinizing such complex data, we introduce ChemVA, an interactive application for the visual exploration of large molecular ensembles and their features. Our tool consists of multiple coordinated views: Hexagonal view, Detail view, 3D view, Table view, and a newly proposed Difference view designed for the comparison of DR projections. These views display DR projections combined with biological activity, selected molecular features, and confidence scores for each of these projections. This conjunction of views allows the user to drill down through the dataset and to efficiently select candidate compounds. Our approach was evaluated on two case studies of finding structurally similar ligands with similar binding affinity to a target protein, as well as on an external qualitative evaluation. The results suggest that our system allows effective visual inspection and comparison of different high-dimensional molecular representations. Furthermore, ChemVA assists in the identification of candidate compounds while providing information on the certainty behind different molecular representations.", "keywords": "Virtual screening,visual analysis,dimensionality reduction,coordinated views,cheminformatics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030438", "refList": ["10.1109/tvcg.2008.137", "10.1057/ivs.2009.10", "10.2312/eurovisstar.20151110", "10.1109/eisic.2015.35", "10.1109/pacificvis.2014.44", "10.1145/1142473.1142574", "10.1109/tvcg.2013.223", "10.1109/tvcg.2014.2346573", "10.1109/tvcg.2019.2934539", "10.1111/cgf.13717", "10.1109/vizsec.2009.5375536", "10.1111/cgf.12925", "10.1109/tvcg.2015.2467551", "10.1109/mcg.2015.99", "10.1007/978-3-319", "10.1109/tvcg.2012.255", "10.1145/1064830.1064834", "10.1177/1473871611433713", "10.1207/s1532690xci0804\\_2", "10.1145/1168149.1168168", "10.1016/j.chb.2006.10.002", "10.1109/tvcg.2014.2346441", "10.1109/eisic.2017.15", "10.1111/1467-8721.00160", "10.1109/tvcg.2018.2865024", "10.1109/infvis.2004.2"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934658", "title": "LightGuider: Guiding Interactive Lighting Design using Suggestions, Provenance, and Quality Visualization", "year": "2019", "conferenceName": "VAST", "authors": "Andreas Walch;Michael Schw\u00e4rzler;Christian Luksch;Elmar Eisemann;Theresia Gschwandtner", "citationCount": "0", "affiliation": "Walch, A (Corresponding Author), VRVis Forschungs GmbH, Graz, Austria. Walch, Andreas; Luksch, Christian, VRVis Forschungs GmbH, Graz, Austria. Schwarzler, Michael; Eisemann, Elmar, Delft Univ Technol, Delft, Netherlands. Gschwandtner, Theresia, TU Wien, Vienna, Austria.", "countries": "Netherlands;Austria", "abstract": "LightGuider is a novel guidance-based approach to interactive lighting design, which typically consists of interleaved 3D modeling operations and light transport simulations. Rather than having designers use a trial-and-error approach to match their illumination constraints and aesthetic goals, LightGuider supports the process by simulating potential next modeling steps that can deliver the most significant improvements. LightGuider takes predefined quality criteria and the current focus of the designer into account to visualize suggestions for lighting-design improvements via a specialized provenance tree. This provenance tree integrates snapshot visualizations of how well a design meets the given quality criteria weighted by the designer's preferences. This integration facilitates the analysis of quality improvements over the course of a modeling workflow as well as the comparison of alternative design solutions. We evaluate our approach with three lighting designers to illustrate its usefulness.", "keywords": "guidance,3D modeling,lighting design,provenance,global illumination", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934658", "refList": ["10.1109/tvcg.2015.2468011", "10.1145/1243980.1243983", "10.1145/2629573", "10.1109/tvcg.2008.59", "10.1109/tvcg.2010.223", "10.1145/1345448.1345453", "10.1109/38.920623", "10.1145/1276377.1276444", "10.1109/tvcg.2013.126", "10.1007/s00371-015-1132-9", "10.1109/tvcg.2011.185", "10.1111/cgf.12924", "10.1145/1239451.1239505", "10.1109/tvcg.2013.147", "10.1145/985692.985765", "10.1287/opre.15.3.537", "10.1016/j.cag.2013.11.002", "10.1145/2702123.2702149", "10.1111/cgf.13101", "10.1109/tvcg.2015.2467551", "10.21037/jmai.2018.07.01", "10.1145/258734.258887", "10.1145/1084805.1084812", "10.1111/cgf.13452", "10.1109/tvcg.2014.2346321", "10.1006/ijhc.2000.0420", "10.1109/pg.2007.9", "10.1109/tvcg.2018.2865024", "10.1007/s00371-016-1257-5", "10.1111/cgf.12159", "10.1109/tvcg.2016.2598468", "10.1109/tvcg.2012.175"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 203}], "len": 771}, "index": 306, "embedding": [-2.125394344329834, 4.7457146644592285, -1.030107021331787, -2.273205041885376, -0.21688701212406158, 0.16650904715061188, -0.7323254346847534, 4.567408561706543, 16.34262466430664, 2.3721418380737305, 6.3057475090026855, 4.586076736450195, 10.820772171020508, 3.24475359916687, 15.344698905944824, -0.5232402086257935, 2.114119052886963, 0.07072390615940094, 7.325723171234131, 7.51485013961792, -0.06630208343267441, 6.20966100692749, 9.532403945922852, 9.923776626586914, -2.405334949493408, 17.857019424438477, -0.5452967882156372, 0.9371762871742249, 2.1417076587677, -2.657630681991577, 8.036273002624512, 4.751397132873535], "projection": [2.3325088024139404, 12.143447875976562], "size": 386, "height": 8, "width": 120}