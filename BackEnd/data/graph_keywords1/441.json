{"data": {"doi": "10.1109/tvcg.2015.2467201", "title": "Suggested Interactivity: Seeking Perceived Affordances for Information Visualization", "year": "2015", "conferenceName": "InfoVis", "authors": "Jeremy Boy;Louis Eveillard;Fran\u00e7oise D\u00e9tienne;Jean-Daniel Fekete", "citationCount": "17", "affiliation": "Boy, J (Corresponding Author), Telecom ParisTech, INRIA, Paris, France. Boy, Jeremy, Telecom ParisTech, INRIA, Paris, France. Detienne, Francoise, Telecom ParisTech, Paris, France.", "countries": "France", "abstract": "In this article, we investigate methods for suggesting the interactivity of online visualizations embedded with text. We first assess the need for such methods by conducting three initial experiments on Amazon's Mechanical Turk. We then present a design space for Suggested Interactivity (i. e., visual cues used as perceived affordances-SI), based on a survey of 382 HTML5 and visualization websites. Finally, we assess the effectiveness of three SI cues we designed for suggesting the interactivity of bar charts embedded with text. Our results show that only one cue (SI3) was successful in inciting participants to interact with the visualizations, and we hypothesize this is because this particular cue provided feedforward.", "keywords": "Suggested interactivity, perceived affordances, information visualization for the people, online visualization", "link": "http://dx.doi.org/10.1109/TVCG.2015.2467201", "refList": ["10.1145/2470654.2466255", "10.1145/2556288.2557379", "10.1145/2559206.2578881", "10.1145/1978942.1979232", "10.1177/0956797613504966", "10.1006/ijhc.2002.1017", "10.1109/tvcg.2013.134", "10.1006/ijhc.1017", "10.1109/infvis.2005.1532122", "10.1016/s0169-8141(01)00064-6", "10.1109/tvcg.2014.2346984", "10.1080/01449290310001592587", "10.1016/s1364-6613(00)01506-0", "10.1109/mcg.1983.262982", "10.1016/s0020-7373(86)80007-4", "10.1037/1196-1961.48.2.260", "10.1145/2133806.2133821", "10.1109/tvcg.2010.179", "10.1080/00140137008931127", "10.1016/s1071-5819(03)00021-1", "10.1038/27661"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2016.2598920", "title": "VLAT: Development of a Visualization Literacy Assessment Test", "year": "2016", "conferenceName": "InfoVis", "authors": "Sukwon Lee;Sung-Hee Kim;Bum Chul Kwon", "citationCount": "27", "affiliation": "Kim, SH (Corresponding Author), Samsung Elect Co Ltd, Seoul, South Korea. Lee, Sukwon, Purdue Univ, W Lafayette, IN 47907 USA. Kim, Sung-Hee, Samsung Elect Co Ltd, Seoul, South Korea. Kwon, Bum Chul, IBM Corp, TJ Watson Res Ctr, Yorktown Hts, NY USA.", "countries": "USA;Korea", "abstract": "The Information Visualization community has begun to pay attention to visualization literacy; however, researchers still lack instruments for measuring the visualization literacy of users. In order to address this gap, we systematically developed a visualization literacy assessment test (VLAT), especially for non-expert users in data visualization, by following the established procedure of test development in Psychological and Educational Measurement: (1) Test Blueprint Construction, (2) Test Item Generation, (3) Content Validity Evaluation, (4) Test Tryout and Item Analysis, (5) Test Item Selection, and (6) Reliability Evaluation. The VLAT consists of 12 data visualizations and 53 multiple-choice test items that cover eight data visualization tasks. The test items in the VLAT were evaluated with respect to their essentialness by five domain experts in Information Visualization and Visual Analytics (average content validity ratio = 0.66). The VLAT was also tried out on a sample of 191 test takers and showed high reliability (reliability coefficient omega = 0.76). In addition, we demonstrated the relationship between users' visualization literacy and aptitude for learning an unfamiliar visualization and showed that they had a fairly high positive relationship (correlation coefficient = 0.64). Finally, we discuss evidence for the validity of the VLAT and potential research areas that are related to the instrument.", "keywords": "Visualization Literacy;Assessment Test;Instrument;Measurement;Aptitude;Education", "link": "http://dx.doi.org/10.1109/TVCG.2016.2598920", "refList": ["10.1111/j.1756-8765.2009.01066.x", "10.3138/cmlr.54.3.413", "10.1177/1473871615594652", "10.1145/2207676.2207709", "10.1177/0272989x10373805", "10.1109/tvcg.2015.2467201", "10.1109/tvcg.2015.2467195", "10.1093/pan/mpr057", "10.1109/visual.1991.175815", "10.1109/tvcg.2014.2346419", "10.2505/4/jcst15\\_", "10.1111/j.1745-3992.1988.tb00434.x", "10.1109/pacificvis.2009.4906837", "10.1109/tvcg.2014.2346984", "10.1111/bjop.12046", "10.1111/j.1744-6570.1975.tb01393.x", "10.1109/tvcg.2013.124", "10.1145/2858036.2858101", "10.1109/infvis.2005.1532136", "10.1016/0010-0277(84)90023-4", "10.1177/2053168015604648", "10.1109/tvcg.2014.2346481", "10.1002/bdm.1753", "10.3102/0013189x021001014", "10.1145/2541288", "10.1111/j.1745-3992.2006.00075.x", "10.1007/s11336-008-9101-0", "10.2307/749671", "10.1109/vast.2011.6102435", "10.1037/1040-3590.8.4.350", "10.1177/0013164498058003001", "10.2307/749086", "10.1177/001872089203400503", "10.2307/1169935", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2015.2413786"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2017.2744118", "title": "Visualizing Nonlinear Narratives with Story Curves", "year": "2017", "conferenceName": "InfoVis", "authors": "Nam Wook Kim;Benjamin Bach;Hyejin Im;Sasha Schriber;Markus H. Gross;Hanspeter Pfister", "citationCount": "2", "affiliation": "Kim, NW (Corresponding Author), Harvard Univ, John A Paulson Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Kim, Nam Wook; Bach, Benjamin; Pfister, Hanspeter, Harvard Univ, John A Paulson Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Schriber, Sasha; Gross, Markus, Disney Res Zurich, Zurich, Switzerland.", "countries": "Switzerland;USA", "abstract": "In this paper, we present story curves, a visualization technique for exploring and communicating nonlinear narratives in movies. A nonlinear narrative is a storytelling device that portrays events of a story out of chronological order, e.g., in reverse order or going back and forth between past and future events. Many acclaimed movies employ unique narrative patterns which in turn have inspired other movies and contributed to the broader analysis of narrative patterns in movies. However, understanding and communicating nonlinear narratives is a difficult task due to complex temporal disruptions in the order of events as well as no explicit records specifying the actual temporal order of the underlying story. Story curves visualize the nonlinear narrative of a movie by showing the order in which events are told in the movie and comparing them to their actual chronological order, resulting in possibly meandering visual patterns in the curve. We also present Story Explorer, an interactive tool that visualizes a story curve together with complementary information such as characters and settings. Story Explorer further provides a script curation interface that allows users to specify the chronological order of events in movies. We used Story Explorer to analyze 10 popular nonlinear movies and describe the spectrum of narrative patterns that we discovered, including some novel patterns not previously described in the literature. Feedback from experts highlights potential use cases in screenplay writing and analysis, education and film production. A controlled user study shows that users with no expertise are able to understand visual patterns of nonlinear narratives using story curves.", "keywords": "Nonlinear narrative,storytelling,visualization", "link": "http://dx.doi.org/10.1109/TVCG.2017.2744118", "refList": ["10.1109/tvcg.2009.167", "10.1109/cvpr.2014.111", "10.1007/978-3-642-24826-9\\_34", "10.1109/tvcg.2015.2467811", "10.1145/2807442.2807502", "10.1109/tvcg.2012.212", "10.1111/j.1745-3992.1988.tb00434.x", "10.1109/pacificvis.2015.7156359", "10.1145/2209249.2209267", "10.1109/tvcg.2015.2468151", "10.3115/1557769.1557820", "10.1109/tvcg.2016.2598920", "10.1109/tvcg.2015.2465151", "10.1109/tvcg.2013.196", "10.1145/2858036.2858488", "10.1145/2282338.2282365", "10.1145/1842993.1843035", "10.1109/axmedis.2008.45", "10.1007/s00371-017-1409-2"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2018.2865159", "title": "Dynamic Composite Data Physicalization Using Wheeled Micro-Robots", "year": "2018", "conferenceName": "InfoVis", "authors": "Mathieu Le Goc;Charles Perin;Sean Follmer;Jean-Daniel Fekete;Pierre Dragicevic", "citationCount": "3", "affiliation": "Le Goc, M (Corresponding Author), Stanford Univ, Stanford, CA 94305 USA. Le Goc, Mathieu; Follmer, Sean, Stanford Univ, Stanford, CA 94305 USA. Perin, Charles, Univ Victoria, Victoria, BC, Canada. Perin, Charles, City Univ London, London, England. Fekete, Jean-Daniel; Dragicevic, Pierre, INRIA, Saclay, France.", "countries": "Canada;USA;England;France", "abstract": "This paper introduces dynamic composite physicalizations, a new class of physical visualizations that use collections of self-propelled objects to represent data. Dynamic composite physicalizations can be used both to give physical form to well-known interactive visualization techniques, and to explore new visualizations and interaction paradigms. We first propose a design space characterizing composite physicalizations based on previous work in the fields of Information Visualization and Human Computer Interaction. We illustrate dynamic composite physicalizations in two scenarios demonstrating potential benefits for collaboration and decision making, as well as new opportunities for physical interaction. We then describe our implementation using wheeled micro-robots capable of locating themselves and sensing user input, before discussing limitations and opportunities for future work.", "keywords": "information visualization,data physicalization,tangible user interfaces", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865159", "refList": ["10.1145/1057237.1057242", "10.1145/2858036.2858058", "10.1109/tvcg.2008.153", "10.1109/tvcg.2014.2346424", "10.1145/2642918.2647377", "10.1145/2556288.2557379", "10.1145/2702123.2702275", "10.1007/978-3-319-67687-6\\_25", "10.1016/j.intcom.2006.03.006", "10.1559/152304086783900068", "10.1109/tvcg.2012.199", "10.1145/571985.572011", "10.1109/tvcg.2013.227", "10.1057/palgrave.ivs.9500099", "10.1109/mcg.2010.101", "10.1145/2702123.2702237", "10.1109/roman.2013.6628441", "10.1145/2501988.2502032", "10.1109/tvcg.2013.134", "10.1145/1413634.1413696", "10.1109/tvcg.2014.2346250", "10.1145/258549.258803", "10.1109/tvcg.2014.2346984", "10.1145/2807442.2807488", "10.1145/2207676.2208691", "10.1145/2556288.2557231", "10.1111/cgf.12935", "10.1109/tvcg.2014.2346279", "10.1109/tvcg.2014.2346292", "10.1109/tvcg.2016.2598920", "10.1145/3173574.3173728", "10.1145/3025453.3025512", "10.1007/s00779-009-0279-7", "10.1145/1226969.1226984", "10.1109/tvcg.2016.2598498", "10.2307/2288400", "10.1145/2702123.2702180", "10.1016/0004-3702(94)00017-u", "10.1109/tvcg.2007.70539", "10.1145/2858036.2858041", "10.1109/tvcg.2017.2743859", "10.1109/sibgrapi.2007.21", "10.1177/014662168000400305", "10.1145/2396636.2396675", "10.1145/1226969", "10.1145/2702123.2702604", "10.1145/2207676.2208350", "10.1007/s10551-008-9665-8", "10.1145/3130931", "10.1109/comcomap.2012.6154871", "10.1145/2470654.2481359", "10.1145/3025453.3025877", "10.1145/2984511.2984547", "10.1109/tvcg.2014.2359887"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934283", "title": "What is Interaction for Data Visualization?", "year": "2019", "conferenceName": "InfoVis", "authors": "Evanthia Dimara;Charles Perin", "citationCount": "1", "affiliation": "Dimara, E (Corresponding Author), Sorbonne Univ, Paris, France. Dimara, Evanthia, Sorbonne Univ, Paris, France. Perin, Charles, Univ Victoria, Victoria, BC, Canada.", "countries": "Canada;France", "abstract": "Interaction is fundamental to data visualization, but what \u201cinteraction\u201d means in the context of visualization is ambiguous and confusing. We argue that this confusion is due to a lack of consensual definition. To tackle this problem, we start by synthesizing an inclusive view of interaction in the visualization community \u2013 including insights from information visualization, visual analytics and scientific visualization, as well as the input of both senior and junior visualization researchers. Once this view takes shape, we look at how interaction is defined in the field of human-computer interaction (HCI). By extracting commonalities and differences between the views of interaction in visualization and in HCI, we synthesize a definition of interaction for visualization. Our definition is meant to be a thinking tool and inspire novel and bolder interaction design practices. We hope that by better understanding what interaction in visualization is and what it can be, we will enrich the quality of interaction in visualization systems and empower those who use them.", "keywords": "interaction,visualization,data,definition,human-computer interaction", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934283", "refList": ["10.1057/ivs.2009.22", "10.1515/icom-2017-0027", "10.1145/2493102.2493104", "10.1007/978-3-319-06793-3\\_6", "10.1080/03640210801898177", "10.1109/mcg.2010.30", "10.1109/tvcg.2013.134", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2018.2865237", "10.1109/2945.981847", "10.1145/2598784.2598806", "10.1109/tvcg.2013.130", "10.1109/tvcg.2007.70577", "10.1109/mic.2015.129", "10.1145/2909132.2909267", "10.1080/01449290500330331", "10.1109/tvcg.2018.2865233", "10.1109/tvcg.2015.2467831", "10.1109/iv.2015.34", "10.1109/tvcg.2009.111", "10.1145/3290605.3300565", "10.1145/3173574.3173797", "10.1145/948496.948514", "10.1145/3025453.3025765", "10.1109/tvcg.2015.2467613", "10.1145/2659796", "10.1109/tvcg.2014.2346311", "10.1145/3025453.3025524", "10.1080/17452759.2011.558588", "10.1145/3027063.3053113", "10.1109/infvis.2005.1532136", "10.1145/2470654.2481307", "10.1109/tvcg.2018.2864913", "10.1145/345513.345267", "10.3102/00028312005004437", "10.1109/tvcg.2007.70436", "10.1037/0033-295x.106.4.643", "10.1145/2133416.2146416", "10.1109/tvcg.2013.191", "10.1109/tvcg.2010.177", "10.1145/960201.957206", "10.1109/tvcg.2007.70515", "10.1109/infvis.2000.885092", "10.1145/2636240.2636844", "10.1037/h0055392", "10.1177/1473871611413180", "10.1109/vl.1996.545307", "10.1111/j.1471-1842.2009.00848.x", "10.1145/989863.989865", "10.1109/tvcg.2013.124", "10.1109/tvcg.2008.109", "10.1145/1166253.1166265", "10.1145/2702123.2702180", "10.1109/tvcg.2016.2598620", "10.1080/07370024.2016.1226139", "10.1145/3173574.3173909", "10.1109/pacificvis.2010.5429613", "10.1111/j.1467-6478.2006.00368.x", "10.1109/tvcg.2012.204", "10.1109/tvcg.2013.120", "10.1179/1743277412y.0000000019", "10.1145/1936652.1936684", "10.2307/1269768", "10.1109/infvis.1996.559213", "10.1109/tvcg.2016.2598839", "10.1145/2642918.2647360", "10.1057/palgrave.ivs.9500099", "10.1016/j.cag.2009.06.004", "10.1109/mc.2013.178", "10.1109/tvcg.2007.70541", "10.1109/vast.2011.6102473", "10.1145/358886.358895", "10.1109/tvcg.2014.2346573", "10.1109/tvcg.2018.2865159", "10.1145/2207676.2207741", "10.1109/tvcg.2015.2396062", "10.1145/2207676.2208572", "10.1109/tvcg.2016.2598608", "10.1057/ivs.2008.31", "10.1177/001316446002000104", "10.1109/tvcg.2017.2680452", "10.1109/tvcg.2006.80", "10.1145/2598510.2598566", "10.1037/0003-066x.51.4.355", "10.7146/dpb.v16i224.7586", "10.1109/infvis.1998.729560", "10.1162/leon\\_a\\_00011", "10.1109/tvcg.2010.157", "10.1109/tvcg.2014.2359887"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2020.3030394", "title": "Direct Volume Rendering with Nonparametric Models of Uncertainty", "year": "2020", "conferenceName": "SciVis", "authors": "Tushar M. Athawale;Bo Ma 0002;Elham Sakhaee;Christopher R. Johnson;Alireza Entezari", "citationCount": "0", "affiliation": "Athawale, TM (Corresponding Author), Univ Utah, Sci Comp \\& Imaging SCI Inst, Salt Lake City, UT 84112 USA. Athawale, Tushar M.; Johnson, Chris R., Univ Utah, Sci Comp \\& Imaging SCI Inst, Salt Lake City, UT 84112 USA. Ma, Bo; Sakhaee, Elham; Entezari, Alireza, Univ Florida, Dept CISE, Gainesville, FL 32611 USA.", "countries": "USA", "abstract": "We present a nonparametric statistical framework for the quantification, analysis, and propagation of data uncertainty in direct volume rendering (DVR). The state-of-the-art statistical DVR framework allows for preserving the transfer function (TF) of the ground truth function when visualizing uncertain data; however, the existing framework is restricted to parametric models of uncertainty. In this paper, we address the limitations of the existing DVR framework by extending the DVR framework for nonparametric distributions. We exploit the quantile interpolation technique to derive probability distributions representing uncertainty in viewing-ray sample intensities in closed form, which allows for accurate and efficient computation. We evaluate our proposed nonparametric statistical models through qualitative and quantitative comparisons with the mean-field and parametric statistical models, such as uniform and Gaussian, as well as Gaussian mixtures. In addition, we present an extension of the state-of-the-art rendering parametric framework to 2D TFs for improved DVR classifications. We show the applicability of our uncertainty quantification framework to ensemble, downsampled, and bivariate versions of scalar field datasets.", "keywords": "Volumes,uncertainty,nonparametric,2D transfer function", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030394", "refList": ["10.1109/icdm.2012.80", "10.1145/1401890.1401904", "10.1109/tvcg.2017.2745139", "10.18128/d030.v6.0", "10.1111/cgf.12142", "10.1109/tvcg.2009.114", "10.1111/j.1467-8659.2009.01677.x", "10.1109/mcse.2007.55", "10.1109/icde.2012.16", "10.1198/106186008x320465", "10.1145/1835804.1835868", "10.1559/1523040054738936", "10.1109/tvcg.2019.2934432", "10.1109/34.1000236", "10.1109/infvis.2005.1532136", "10.1109/tvcg.2012.128", "10.1109/pacificvis.2017.8031573", "10.1109/tvcg.2016.2598920", "10.1109/tvcg.2018.2865021", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1109/sp.2009.22", "10.1109/tvcg.2017.2744184", "10.1145/773153.773173", "10.1145/2660267.2660348", "10.1109/icdmw.2009.93", "10.1016/j.jtrangeo.2015.09.001", "10.1109/icde.2010.5447831", "10.1007/11681878\\_14", "10.1111/cgf.13409", "10.1007/s13278-014-0205-5", "10.1109/tvcg.2011.163", "10.1145/3035918.3035940", "10.1109/sp.2008.33", "10.1145/2882903.2882931"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030396", "title": "What Makes a Data-GIF Understandable?", "year": "2020", "conferenceName": "InfoVis", "authors": "Xinhuan Shu;Aoyu Wu;Junxiu Tang;Benjamin Bach;Yingcai Wu;Huamin Qu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Shu, Xinhuan; Tang, Junxiu; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Shu, Xinhuan; Wu, Aoyu; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Shu, Xinhuan, Zhejiang Univ, Hangzhou, Peoples R China. Bach, Benjamin, Univ Edinburgh, Edinburgh, Midlothian, Scotland.", "countries": "Scotland;China", "abstract": "GIFs are enjoying increasing popularity on social media as a format for data-driven storytelling with visualization; simple visual messages are embedded in short animations that usually last less than 15 seconds and are played in automatic repetition. In this paper, we ask the question, \u201cWhat makes a data-GIF understandable?\u201d While other storytelling formats such as data videos, infographics, or data comics are relatively well studied, we have little knowledge about the design factors and principles for \u201cdata-GIFs\u201d. To close this gap, we provide results from semi-structured interviews and an online study with a total of 118 participants investigating the impact of design decisions on the understandability of data-GIFs. The study and our consequent analysis are informed by a systematic review and structured design space of 108 data-GIFs that we found online. Our results show the impact of design dimensions from our design space such as animation encoding, context preservation, or repetition on viewers understanding of the GIF's core message. The paper concludes with a list of suggestions for creating more effective Data-GIFs.", "keywords": "Data-GIFs,Data-driven Storytelling,Evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030396", "refList": ["10.1109/tvcg.2016.2598647", "10.1016/j.visinf.2020.07.001", "10.1177/1473871615594652", "10.1109/tvcg.2014.2346424", "10.1111/cgf.13195", "10.1109/vlsicircuits18222.2020.9162811", "10.1109/tvcg.2018.2864909", "10.1109/tvcg.2018.2864903", "10.1145/2702123.2702431", "10.1016/j.visinf.2019.12.002", "10.1145/3274349", "10.1111/cgf.13444", "10.1145/3206505.3206552", "10.1145/3290605.3300280", "10.1109/jstqe.2020.3021589", "10.1145/2647868.2656408", "10.1006/ijhc.1017", "10.1145/3290605.3300335", "10.1007/s12650-020-00689-0", "10.1145/2818048.2819936", "10.1111/cgf.13325", "10.1145/2858036.2858387", "10.1145/3027063.3053139", "10.1145/3290605.3300474", "10.1109/tvcg.2016.2598920", "10.1109/tvcg.2018.2864899", "10.1145/3290605.3300483", "10.1145/3173574.3173612", "10.1145/2909132.2909255", "10.1109/tvcg.2016.2598620", "10.1016/j.learninstruc.2007.09.013", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2008.125", "10.1145/3173574.3173909", "10.1109/tvcg.2019.2934397", "10.1111/cgf.13709", "10.1109/tvcg.2013.234", "10.1109/tvcg.2019.2934401", "10.1145/2858036.2858532", "10.1016/j.visinf.2020.08.001", "10.1109/tvcg.2010.179", "10.1109/cicc48029.2020.9075900"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13682", "year": "2019", "title": "Toward Understanding Representation Methods in Visualization Recommendations through Scatterplot Construction Tasks", "conferenceName": "EuroVis", "authors": "Sehi L'Yi;Youli Chang;DongHwa Shin;Jinwook Seo", "citationCount": "0", "affiliation": "Seo, J (Corresponding Author), Seoul Natl Univ, Seoul, South Korea.\nL'Yi, Sehi; Chang, Youli; Shin, DongHwa; Seo, Jinwook, Seoul Natl Univ, Seoul, South Korea.", "countries": "Korea", "abstract": "Most visualization recommendation systems predominantly rely on graphical previews to describe alternative visual encodings. However, since InfoVis novices are not familiar with visual representations (e.g., interpretation barriers {[}GTS10]), novices might have difficulty understanding and choosing recommended visual encodings. As an initial step toward understanding effective representation methods for visualization recommendations, we investigate the effectiveness of three representation methods (i.e., previews, animated transitions, and textual descriptions) under scatterplot construction tasks. Our results show how different representations individually and cooperatively help users understand and choose recommended visualizations, for example, by supporting their expect-and-confirm process. Based on our study results, we discuss design implications for visualization recommendation interfaces.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13682", "refList": ["10.1177/1473871611416549", "10.1111/cgf.12106", "10.1145/2047196.2047247", "10.1007/s00371-015-1132-9", "10.1145/2858036.2858435", "10.1057/ivs.2008.27", "10.1109/tvcg.2007.70594", "10.2312/pe.eurovisshort.eurovisshort2013.019-023", "10.1007/978-3-642-23768-3\\_23", "10.14778/2831360.2831371", "10.1145/1502650.1502695", "10.1109/pacificvis.2017.8031599", "10.1145/3025453.3025942", "10.1145/2858036.2858101", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2014.2346292", "10.1109/tvcg.2016.2598920", "10.1057/palgrave/ivs/9500002", "10.1109/tvcg.2017.2744184", "10.1111/cgf.13409", "10.1109/tvcg.2007.70539", "10.1109/2945.981851", "10.1109/tvcg.2010.164", "10.1145/3025453.3025752", "10.1109/pacificvis.2014.42", "10.1145/2213836", "10.1111/cgf.12635", "10.1109/tvcg.2015.2467191", "10.1057/palgrave.ivs.9500091", "10.1109/tvcg.2015.2413786"], "wos": 1, "children": [], "len": 1}], "len": 13}, {"doi": "10.1109/tvcg.2017.2745958", "title": "Active Reading of Visualizations", "year": "2017", "conferenceName": "InfoVis", "authors": "Jagoda Walny;Samuel Huron;Charles Perin;Tiffany Wun;Richard Pusch;Sheelagh Carpendale", "citationCount": "12", "affiliation": "Walny, J (Corresponding Author), Univ Calgary, Calgary, AB, Canada. Walny, Jagoda; Huron, Samuel; Perin, Charles; Wun, Tiffany; Pusch, Richard; Carpendale, Sheelagh, Univ Calgary, Calgary, AB, Canada. Huron, Samuel, Telecom ParisTech, Paris, France. Perin, Charles, Univ London, London, England.", "countries": "Canada;France;England", "abstract": "We investigate whether the notion of active reading for text might be usefully applied to visualizations. Through a qualitative study we explored whether people apply observable active reading techniques when reading paper-based node-link visualizations. Participants used a range of physical actions while reading, and from these we synthesized an initial set of active reading techniques for visualizations. To learn more about the potential impact such techniques may have on visualization reading, we implemented support for one type of physical action from our observations (making freeform marks) in an interactive node-link visualization. Results from our quantitative study of this implementation show that interactive support for active reading techniques can improve the accuracy of performing low-level visualization tasks. Together, our studies suggest that the active reading space is ripe for research exploration within visualization and can lead to new interactions that make for a more flexible and effective visualization reading experience.", "keywords": "active reading of visualizations,active reading,information visualization,spectrum of physical engagement", "link": "http://dx.doi.org/10.1109/TVCG.2017.2745958", "refList": ["10.1109/tvcg.2012.189", "10.1109/tvcg.2014.2346435", "10.1145/2396636.2396670", "10.1109/tvcg.2008.141", "10.1145/2675133.2675207", "10.1145/2702123.2702347", "10.1109/tvcg.2015.2467811", "10.1145/286498.286510", "10.1109/tvcg.2011.185", "10.1109/tvcg.2015.2467201", "10.1145/2207676.2208327", "10.2312/eurovisstar.20151113", "10.1109/tvcg.2015.2467195", "10.1006/ijhc.2002.1017", "10.3758/s13428-013-0330-5", "10.3758/bf03195567", "10.1145/2470654.2466441", "10.1109/tvcg.2014.2346422", "10.1109/tvcg.2014.2346573", "10.1006/ijhc.1017", "10.2307/4128941", "10.1145/2317956.2318034", "10.1109/tvcg.2014.2346984", "10.1145/2254556.2254669", "10.1111/j.1467-8659.2009.01439.x", "10.1145/2858036.2858101", "10.1109/tvcg.2014.2346279", "10.1109/infvis.2005.1532136", "10.1109/tvcg.2015.2467452", "10.1037/0096-1523.16.4.683", "10.1109/tvcg.2015.2467671", "10.1109/mcg.2015.52", "10.1109/tvcg.2014.2346292", "10.1145/2702123.2702172", "10.1109/tvcg.2005.53", "10.1109/tvcg.2016.2598594", "10.2307/2288400", "10.1145/1168149.1168168", "10.1145/2470654.2466444", "10.1145/1240624.1240781", "10.1145/2501988.2502036", "10.1145/2470654.2470724", "10.1007/s00146-010-0272-8", "10.1109/infvis.2004.1", "10.1109/tvcg.2010.164", "10.2200/s00539ed1v01y201310icr029", "10.1109/tabletop.2007.12", "10.1109/mcg.2006.70", "10.1109/tvcg.2013.164", "10.1145/2858036.2858488", "10.1111/cgf.12635", "10.1109/tvcg.2010.179", "10.1007/978-3-319-26633-6\\_13", "10.1145/2470654.2481359", "10.1037/0003-066x.60.2.170", "10.1145/1753326.1753417"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865117", "title": "Patterns and Pace: Quantifying Diverse Exploration Behavior with Visualizations on the Web", "year": "2018", "conferenceName": "InfoVis", "authors": "Mi Feng;Evan M. Peck;Lane Harrison", "citationCount": "3", "affiliation": "Feng, M (Corresponding Author), Worcester Polytech Inst, Worcester, MA 01609 USA. Feng, Mi; Harrison, Lane, Worcester Polytech Inst, Worcester, MA 01609 USA. Peck, Evan, Bucknell Univ, Lewisburg, PA 17837 USA.", "countries": "USA", "abstract": "The diverse and vibrant ecosystem of interactive visualizations on the web presents an opportunity for researchers and practitioners to observe and analyze how everyday people interact with data visualizations. However, existing metrics of visualization interaction behavior used in research do not fully reveal the breadth of peoples' open-ended explorations with visualizations. One possible way to address this challenge is to determine high-level goals for visualization interaction metrics, and infer corresponding features from user interaction data that characterize different aspects of peoples' explorations of visualizations. In this paper, we identify needs for visualization behavior measurement, and develop corresponding candidate features that can be inferred from users' interaction data. We then propose metrics that capture novel aspects of peoples' open-ended explorations, including exploration uniqueness and exploration pacing. We evaluate these metrics along with four other metrics recently proposed in visualization literature by applying them to interaction data from prior visualization studies. The results of these evaluations suggest that these new metrics 1) reveal new characteristics of peoples' use of visualizations, 2) can be used to evaluate statistical differences between visualization designs, and 3) are statistically independent of prior metrics used in visualization research. We discuss implications of these results for future studies, including the potential for applying these metrics in visualization interaction analysis, as well as emerging challenges in developing and selecting metrics depicting visualization explorations.", "keywords": "Interaction,Visualization,Quantitative Evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865117", "refList": ["10.1109/tvcg.2008.137", "10.1109/ldav.2012.6378977", "10.1145/2702123.2702452", "10.1145/2702123.2702419", "10.1109/tvcg.2014.2346575", "10.1145/2702123.2702275", "10.1109/iccv.2013.147", "10.1109/tvcg.2014.2346452", "10.1016/0306-4573(88)90021-0", "10.1109/tvcg.2015.2467613", "10.1109/tvcg.2011.229", "10.1109/tvcg.2015.2467201", "10.1109/vast.2007.4389008", "10.1007/978-1-4419-0492-8\\_2", "10.1109/tvcg.2017.2745958", "10.1145/2020408.2020581", "10.1109/tvcg.2016.2598797", "10.1109/tvcg.2015.2467871", "10.1145/2702123.2702590", "10.1109/tvcg.2016.2598466", "10.1109/tvcg.2013.200", "10.1109/tvcg.2005.53", "10.1109/vast.2007.4389009", "10.1145/2678025.2701407", "10.1111/cgf.13208", "10.1145/989863.989880", "10.1109/tvcg.2007.70515", "10.1145/365024.365325", "10.1109/tvcg.2016.2599058"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030383", "title": "II-20: Intelligent and pragmatic analytic categorization of image collections", "year": "2020", "conferenceName": "VAST", "authors": "Jan Zah\u00e1lka;Marcel Worring;Jarke J. van Wijk", "citationCount": "0", "affiliation": "Zahalka, J (Corresponding Author), Czech Tech Univ, Prague, Czech Republic. Zahalka, Jan, Czech Tech Univ, Prague, Czech Republic. Worring, Marcel, Univ Amsterdam, Amsterdam, Netherlands. van Wijk, Jarke J., Eindhoven Univ Technol, Eindhoven, Netherlands.", "countries": "Republic;Netherlands", "abstract": "In this paper, we introduce 11\u201320 (Image Insight 2020), a multimedia analytics approach for analytic categorization of image collections. Advanced visualizations for image collections exist, but they need tight integration with a machine model to support the task of analytic categorization. Directly employing computer vision and interactive learning techniques gravitates towards search. Analytic categorization, however, is not machine classification (the difference between the two is called the pragmatic gap): a human adds/redefines/deletes categories of relevance on the fly to build insight, whereas the machine classifier is rigid and non-adaptive. Analytic categorization that truly brings the user to insight requires a flexible machine model that allows dynamic sliding on the exploration-search axis, as well as semantic interactions: a human thinks about image data mostly in semantic terms. 11\u201320 brings three major contributions to multimedia analytics on image collections and towards closing the pragmatic gap. Firstly, a new machine model that closely follows the user's interactions and dynamically models her categories of relevance. II-20's machine model, in addition to matching and exceeding the state of the art's ability to produce relevant suggestions, allows the user to dynamically slide on the exploration-search axis without any additional input from her side. Secondly, the dynamic, 1-image-at-a-time Tetris metaphor that synergizes with the model. It allows a well-trained model to analyze the collection by itself with minimal interaction from the user and complements the classic grid metaphor. Thirdly, the fast-forward interaction, allowing the user to harness the model to quickly expand (\u201cfast-forward\u201d) the categories of relevance, expands the multimedia analytics semantic interaction dictionary. Automated experiments show that II-20's machine model outperforms the existing state of the art and also demonstrate the Tetris metaphor's analytic quality. User studies further confirm that II\u201320 is an intuitive, efficient, and effective multimedia analytics tool.", "keywords": "Multimedia analytics,image data,analytic categorization,pragmatic gap", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030383", "refList": ["10.1109/tvcg.2008.137", "10.1145/2882903.2882919", "10.1145/2556647.2556657", "10.1145/1142473.1142574", "10.1109/tvcg.2016.2598471", "10.1109/vast.2017.8585669", "10.1109/tvcg.2014.2346573", "10.1111/j.0956-7976.2005.00782.x", "10.1007/978-3-540-89965-5\\_27", "10.1109/visual.2019.8933611", "10.1109/vast.2010.5653598", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1109/tvcg.2012.271", "10.1109/mcg.2009.49", "10.1057/ivs.2008.31", "10.1111/cgf.12925", "10.1109/hicss.2016.183", "10.1109/tvcg.2016.2598466", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2018.2865117", "10.1109/vast47406.2019.8986948", "10.1016/s0950-7051(00)00101-5", "10.1109/tvcg.2016.2598594", "10.1109/tvcg.2006.101", "10.1111/cgf.12311", "10.1109/vast.2009.5333020", "10.1109/vast.2010.5652885", "10.1111/cgf.13670", "10.1109/wvl.1988.18020", "10.1145/2133806.2133821", "10.1109/vast.2012.6400486"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13678", "year": "2019", "title": "Characterizing Exploratory Visual Analysis: A Literature Review and Evaluation of Analytic Provenance in Tableau", "conferenceName": "EuroVis", "authors": "Leilani Battle;Jeffrey Heer", "citationCount": "2", "affiliation": "Battle, L (Corresponding Author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.\nBattle, Leilani, Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.\nHeer, Jeffrey, Univ Washington, Paul G Allen Sch Comp Sci \\& Engn, Seattle, WA 98195 USA.", "countries": "USA", "abstract": "Supporting exploratory visual analysis (EVA) is a central goal of visualization research, and yet our understanding of the process is arguably vague and piecemeal. We contribute a consistent definition of EVA through review of the relevant literature, and an empirical evaluation of existing assumptions regarding how analysts perform EVA using Tableau, a popular visual analysis tool. We present the results of a study where 27 Tableau users answered various analysis questions across 3 datasets. We measure task performance, identify recurring patterns across participants' analyses, and assess variance from task specificity and dataset. We find striking differences between existing assumptions and the collected data. Participants successfully completed a variety of tasks, with over 80\\% accuracy across focused tasks with measurably correct answers. The observed cadence of analyses is surprisingly slow compared to popular assumptions from the database community. We find significant overlap in analyses across participants, showing that EVA behaviors can be predictable. Furthermore, we find few structural differences between behavior graphs for open-ended and more focused exploration tasks.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13678", "refList": ["10.1109/dsia.2017.8339089", "10.1109/tvcg.2008.137", "10.1109/tvcg.2007.28", "10.1057/ivs.2009.22", "10.1038/526189a", "10.1109/tvcg.2006.85", "10.1145/2882903.2882919", "10.1145/2588555.2610523", "10.1109/tvcg.2014.2346575", "10.1145/2207676.2208294", "10.1145/2723372.2731084", "10.1109/tvcg.2016.2598471", "10.1145/3173574.3174168", "10.1109/visual.2005.1532788", "10.1109/tvcg.2014.2346452", "10.1109/tvcg.2015.2467613", "10.20380/gi2015.16", "10.14778/2831360.2831371", "10.1145/3173574.3174053", "10.1109/icdew.2006.75", "10.1145/2993901.2993912", "10.1145/1502650.1502695", "10.1109/tvcg.2012.219", "10.1007/978-3-642-23768-3\\_22", "10.1109/tvcg.2016.2598797", "10.1109/vl.1996.545307", "10.1111/j.1467-8659.2010.01830.x", "10.1109/tvcg.2015.2467871", "10.1145/3209900.3209901", "10.1145/2939502.2939513", "10.1145/381641.381656", "10.1109/tvcg.2013.124", "10.1109/vast.2008.4677357", "10.1145/1378773.1378788", "10.1109/mcg.2012.120", "10.1057/ivs.2008.31", "10.1109/infvis.2005.1532136", "10.1109/icde.2014.6816674", "10.1109/tvcg.2016.2598466", "10.1109/tvcg.2015.2467551", "10.1145/2939502.2939506", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2865117", "10.1145/1556262.1556276", "10.1016/s0950-7051(00)00101-5", "10.1109/tvcg.2008.109", "10.1145/2588555.2593666", "10.1109/tvcg.2018.2865040", "10.1109/tvcg.2013.179", "10.1111/cgf.13409", "10.1177/1473871616638546", "10.1109/2945.981851", "10.1109/tvcg.2010.164", "10.1109/vast.2008.4677365", "10.1109/tvcg.2015.2467191", "10.1145/2133806.2133821", "10.1145/2207676.2208412", "10.1109/tvcg.2017.2744319", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.224"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030425", "title": "Visual Analysis of Argumentation in Essays", "year": "2020", "conferenceName": "VAST", "authors": "Dora Kiesel;Patrick Riehmann;Henning Wachsmuth;Benno Stein;Bernd Fr\u00f6hlich", "citationCount": "0", "affiliation": "Kiesel, D (Corresponding Author), Bauhaus Univ Weimar, Weimar, Germany. Kiesel, Dora; Riehmann, Patrick; Stein, Benno; Froehlich, Bernd, Bauhaus Univ Weimar, Weimar, Germany. Wachsmuth, Henning, Paderborn Univ, Paderborn, Germany.", "countries": "Germany", "abstract": "This paper presents a visual analytics system for exploring, analyzing and comparing argument structures in essay corpora. We provide an overview of the corpus by a list of ArguLines which represent the argument units of each essay by a sequence of glyphs. Each glyph encodes the stance, the depth and the relative position of an argument unit. The overview can be ordered in various ways to reveal patterns and outliers. Subsets of essays can be selected and analyzed in detail using the Argument Unit Occurrence Tree which aggregates the argument structures using hierarchical histograms. This hierarchical view facilitates the estimation of statistics and trends concerning the progression of the argumentation in the essays. It also provides insights into the commonalities and differences between selected subsets. The text view is the necessary textual basis to verify conclusions from the other views and the annotation process. Linking the views and interaction techniques for visual filtering, studying the evolution of stance within a subset of essays and scrutinizing the order of argumentative units enable a deep analysis of essay corpora. Our expert reviews confirmed the utility of the system and revealed detailed and previously unknown information about the argumentation in our sample corpus.", "keywords": "Information Visualization,Text Analysis,User Interfaces,Visual Analytics,Argumentation Visualization,Glyph-based Techniques,Text and Document Data,Tree-based Visualization,Coordinated and Multiple Views,Close and Distant Reading", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030425", "refList": ["10.1109/tvcg.2014.2346575", "10.14778/2735479.2735485", "10.14778/3192965.3192971", "10.1111/cgf.12129", "10.1145/2206869.2206874", "10.1109/icde.2016.7498300", "10.14778/2831360.2831371", "10.1145/3035918.3056097", "10.1023/a:1009726021843", "10.1111/cgf.13678", "10.1145/3183713.3196905", "10.14778/3115404.3115418", "10.1109/tvcg.2012.180", "10.1109/icde.1999.754950", "10.14778/1453856.1453924", "10.1145/3209900.3209901", "10.1002/spe.2325", "10.1145/42201.42203", "10.1109/tvcg.2013.124", "10.1109/vast.2008.4677357", "10.1109/icde.2016.7498287", "10.1109/icde.2014.6816674", "10.14778/2732951.2732964", "10.1109/tvcg.2015.2467551", "10.1145/1084805.1084812", "10.1109/2.781635", "10.14778/2732279.2732280", "10.1109/icde.2015.7113427", "10.1109/tvcg.2015.2467091", "10.1007/s00778-017-0486-1", "10.1109/tvcg.2003.1196005", "10.1109/icde.2004.1320035", "10.1109/tvcg.2016.2607714", "10.14778/2732951.2732953", "10.1109/tvcg.2008.131", "10.1109/tvcg.2018.2865018", "10.1145/2133806.2133821", "10.1109/icde.2019.00035", "10.1109/tpds.2005.144", "10.14778/3236187.3236212", "10.1109/tvcg.2009.111", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14033", "year": "2020", "title": "Survey on Individual Differences in Visualization", "conferenceName": "EuroVis", "authors": "Zhengliang Liu;R. Jordan Crouser;Alvitta Ottley", "citationCount": "0", "affiliation": "Liu, ZL (Corresponding Author), Washington Univ, St Louis, MO 63110 USA.\nLiu, Zhengliang; Ottley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nCrouser, R. Jordan, Smith Coll, Northampton, MA 01063 USA.", "countries": "USA", "abstract": "Developments in data visualization research have enabled visualization systems to achieve great general usability and application across a variety of domains. These advancements have improved not only people's understanding of data, but also the general understanding of people themselves, and how they interact with visualization systems. In particular, researchers have gradually come to recognize the deficiency of having one-size-fits-all visualization interfaces, as well as the significance of individual differences in the use of data visualization systems. Unfortunately, the absence of comprehensive surveys of the existing literature impedes the development of this research. In this paper, we review the research perspectives, as well as the personality traits and cognitive abilities, visualizations, tasks, and measures investigated in the existing literature. We aim to provide a detailed summary of existing scholarship, produce evidence-based reviews, and spur future inquiry.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14033", "refList": ["10.1145/3025171.3025192", "10.1037/0278-7393.29.2.298", "10.1145/2470654.2470707", "10.1177/001872086300500103", "10.1109/tvcg.2014.2346575", "10.1146/annurev-psych-113011-143750", "10.1016/j.sbspro.2011.11.312", "10.1177/1073191102092010", "10.1109/tvcg.2016.2598471", "10.1145/169891.169925", "10.1145/2856767.2856779", "10.4102/sajip.v29i1.88", "10.1145/3301275.3302307", "10.1109/mcg.2012.120", "10.1016/0749-596x(89)90040-5", "10.1111/cgf.12393", "10.3389/fpsyg.2018.00755", "10.1037/0033-2909.121.2.219", "10.2466/pms.1978.47.2.599", "10.1037/0022-3514.44.2.419", "10.1016/j.cub.2009.12.014", "10.1002/per.704", "10.1016/j.paid.2006.03.011", "10.1038/36846", "10.1145/3025453.3025877", "10.1037/0022-3514.93.5.880", "10.1002/per.469", "10.1002/tea.3660300407", "10.1037/h0076301", "10.1002/jocb.32", "10.1371/journal.pone.0131115", "10.1002/per.588", "10.1145/2449396.2449439", "10.1108/jmd-12-2013-0160", "10.1037/0022-3514.42.1.116", "10.1016/j.intell.2003.10.005", "10.1016/s0022-5371(80)90312-6", "10.3758/bf03214546", "10.1111/j.1467-8659.2009.01442.x", "10.1002/(sici)1099-0984(200003/04)14:2", "10.1145/2470654.2470696", "10.2190/vqjd-t1yd-5wvb-rypj", "10.1145/2702123.2702590", "10.1109/infvis.2005.1532136", "10.1037/0278-7393.29.4.611", "10.1145/3025453.3025577", "10.1016/0360-8352(91)90009-u", "10.1145/2556288.2557141", "10.1016/j.sbspro.2012.01.055", "10.1016/j.sbspro.2013.04.319", "10.1007/s11257-019-09244-5", "10.1016/j.tics.2013.06.006", "10.1037/a0037009", "10.1037/0003-066x.48.1.26", "10.1109/infvis.2004.70", "10.1037/0003-066x.45.4.489", "10.1037/h0092976", "10.1177/1473871612441542", "10.1145/302979.303030", "10.1016/b978-0-12-386915-9.00003-6", "10.1080/08870449608404995", "10.1109/tvcg.2013.156", "10.1145/2110192.2110202", "10.3758/cabn.2.4.341", "10.1037/0096-1523.27.1.92", "10.1109/tvcg.2007.70515", "10.1145/1385569.1385602", "10.24963/ijcai.2017/217", "10.1080/135467897394419", "10.1007/978-3-642-31454-4\\_23", "10.1037/a0021016", "10.1109/tvcg.2014.2346452", "10.1145/3301275.3302313", "10.1145/2633043", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1109/vl.1996.545307", "10.1037/0033-2909.119.2.197", "10.1523/jneurosci.2145-09.2009", "10.1177/1473871613513227", "10.1037/0022-3514.51.4.875", "10.1109/mcg.2009.49", "10.1002/acp.1344", "10.1037/a0016127", "10.1037/1040-3590.18.2.192", "10.1111/j.1467-8659.2011.01928.x", "10.1016/s0140-6736(02)07441-x", "10.1037/0022-0663.96.3.471", "10.1207/s15327752jpa4803\\_13", "10.1007/s10798-018-9446-3", "10.1002/(sici)1097-4571(2000)51:6", "10.1109/vast.2009.5333468", "10.1109/mcg.2009.22", "10.1145/3079628.3079634", "10.1016/j.jrp.2014.05.003", "10.1016/s0079-6123(07)00020-9", "10.1002/ijop.12511", "10.1111/j.2044-8279.1982.tb00821.x", "10.5539/jel.v4n4p91", "10.1177/1473871615594652", "10.1037/0022-3514.66.5.950", "10.1037/h0042761", "10.1109/tvcg.2012.199", "10.1177/1046496403257228", "10.1080/13614569708914684", "10.1111/cgf.13678", "10.1016/j.lindif.2003.08.001", "10.1016/j.jecp.2009.11.003", "10.1109/tvcg.2014.2346984", "10.1109/vast.2011.6102445", "10.1145/2678025.2701376", "10.1057/ivs.2008.31", "10.1533/9781780630366", "10.1111/j.2044-8260.1992.tb00972.x", "10.1016/j.jrp.2011.12.003", "10.1016/0747-5632(91)90002-i", "10.1080/13546781343000222", "10.3758/s13414-013-0610-2", "10.1145/3301275.3302283", "10.1177/106907279300100107", "10.1109/vast.2012.6400535", "10.1145/3377325.3377502", "10.1037/1089-2699.10.4.249", "10.1016/j.paid.2003.08.018", "10.1016/j.paid.2010.09.015"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 13}, {"doi": "10.1109/tvcg.2018.2865231", "title": "SmartCues: A Multitouch Query Approach for Details-on-Demand through Dynamically Computed Overlays", "year": "2018", "conferenceName": "InfoVis", "authors": "Hariharan Subramonyam;Eytan Adar", "citationCount": "0", "affiliation": "Subramonyam, H (Corresponding Author), Univ Michigan, Sch Informat, Ann Arbor, MI 48109 USA. Subramonyam, Hariharan; Adar, Eytan, Univ Michigan, Sch Informat, Ann Arbor, MI 48109 USA.", "countries": "USA", "abstract": "Details-on-demand is a crucial feature in the visual information-seeking process but is often only implemented in highly constrained settings. The most common solution, hover queries (i.e., tooltips), are fast and expressive but are usually limited to single mark (e.g., a bar in a bar chart). `Queries' to retrieve details for more complex sets of objects (e.g., comparisons between pairs of elements, averages across multiple items, trend lines, etc.) are difficult for end-users to invoke explicitly. Further, the output of these queries require complex annotations and overlays which need to be displayed and dismissed on demand to avoid clutter. In this work we introduce SmartCues, a library to support details-on-demand through dynamically computed overlays. For end-users, SmartCues provides multitouch interactions to construct complex queries for a variety of details. For designers, SmartCues offers an interaction library that can be used out-of-the-box, and can be extended for new charts and detail types. We demonstrate how SmartCues can be implemented across a wide array of visualization types and, through a lab study, show that end users can effectively use SmartCues.", "keywords": "Graphical overlays,details-on-demand,graph comprehension", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865231", "refList": ["10.1002/acp.2350030302", "10.1057/palgrave.ivs.9500099", "10.1109/infvis.1995.528688", "10.1145/175276.175283", "10.1145/345513.345271", "10.1145/2449396.2449439", "10.1177/1473871611406623", "10.1037/1076-898x.4.2.75", "10.1109/tvcg.2011.185", "10.1145/2047196.2047227", "10.1109/tvcg.2017.2745958", "10.1109/vl.1996.545307", "10.1109/tvcg.2014.2346250", "10.1207/s15327051hci0804\\_3", "10.1109/pacificvis.2017.8031599", "10.1145/22949.22950", "10.1145/2470654.2481318", "10.1109/cmv.2003.1215008", "10.1109/tvcg.2013.124", "10.1145/2556288.2557231", "10.1007/978-3-642-31223-6\\_13", "10.1109/infvis.2005.1532136", "10.1109/tvcg.2013.119", "10.1006/jvlc.1996.0009", "10.1109/tvcg.2012.229", "10.1145/2470654.2481374", "10.1016/j.learninstruc.2011.02.002", "10.2307/2288400", "10.1109/tvcg.2008.109", "10.1145/2046684.2046699", "10.1145/212332.212334", "10.1109/vast.2010.5652885", "10.1145/2598153.2598163", "10.1145/2807442.2807478", "10.1145/302979.303030", "10.2307/749671", "10.1007/978-3-540-87730-1\\_30", "10.1109/tvcg.2012.204", "10.1145/2133806.2133821", "10.2312/pe/eurovisshort/eurovisshort2012/097-101", "10.1145/2470654.2466143", "10.1145/2396636.2396675", "10.14778/2732240.2732247", "10.1109/tvcg.2010.179", "10.1109/vast.2012.6400487", "10.2307/2289447", "10.1109/tvcg.2007.70515", "10.1145/2992154.2992157"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934539", "title": "Criteria for Rigor in Visualization Design Study", "year": "2019", "conferenceName": "InfoVis", "authors": "Miriah D. Meyer;Jason Dykes", "citationCount": "16", "affiliation": "Meyer, M (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Dykes, Jason, City Univ London, London, England.", "countries": "USA;England", "abstract": "We develop a new perspective on research conducted through visualization design study that emphasizes design as a method of inquiry and the broad range of knowledge-contributions achieved through it as multiple, subjective, and socially constructed. From this interpretivist position we explore the nature of visualization design study and develop six criteria for rigor. We propose that rigor is established and judged according to the extent to which visualization design study research and its reporting are INFORMED, REFLEXIVE, ABUNDANT, PLAUSIBLE, RESONANT, and TRANSPARENT. This perspective and the criteria were constructed through a four-year engagement with the discourse around rigor and the nature of knowledge in social science, information systems, and design. We suggest methods from cognate disciplines that can support visualization researchers in meeting these criteria during the planning, execution, and reporting of design study. Through a series of deliberately provocative questions, we explore implications of this new perspective for design study research in visualization, concluding that as a discipline, visualization is not yet well positioned to embrace, nurture, and fully benefit from a rigorous, interpretivist approach to design study. The perspective and criteria we present are intended to stimulate dialogue and debate around the nature of visualization design study and the broader underpinnings of the discipline.", "keywords": "design study,relativism,interpretivism,knowledge construction,qualitative research,research through design", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934539", "refList": ["10.1007/978-1-4939-0378-8\\_8", "10.1177/1049732315588501", "10.1177/146879410200200205", "10.2307/1177100", "10.1016/0142-694x(82)90040-0", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1145/2362364.2362371", "10.1080/2159676x.2017.1393221", "10.1177/1473871613510429", "10.1145/2212877.2212889", "10.1080/09650790802011973", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2015.2467195", "10.1109/mcse.2007.106", "10.1080/1750984x.2017.1317357", "10.1109/tvcg.2017.2745958", "10.1177/1525822x0101300203", "10.1080/23265507.2017.1300068", "10.1111/j.1540-4560.1946.tb02295.x", "10.1177/1468794108098034", "10.1007/978-3-7643-8472-2\\_6", "10.1145/2317956.2317968", "10.2307/1511837", "10.1177/104973202129120052", "10.3233/efi-2004-22201", "10.1109/beliv.2018.8634427", "10.1109/beliv.2018.8634261", "10.1016/s0142-694x(01)00009-6", "10.1007/978-3-7643-8472-2\\_3", "10.1145/642611.642616", "10.1177/1468794107085301", "10.1177/1077800410383121", "10.1109/tvcg.2010.137", "10.1145/2405716.2405725", "10.1145/2702123.2702172", "10.1109/tvcg.2014.2346248", "10.1109/tvcg.2011.209", "10.1111/j.1467-8659.2009.01710.x", "10.1145/3173574.3173775", "10.1145/1993060.1993065", "10.1007/978-1-4419-5653-8\\_2", "10.1177/107780049900500402", "10.1109/tvcg.2018.2864905", "10.2307/2288400", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1177/160940690400300403", "10.1145/2993901.2993916", "10.1145/1879831.1879836", "10.2307/3178066", "10.1109/tvcg.2018.2864913", "10.3102/0013189x022004016", "10.1109/tvcg.2015.2511718", "10.1109/tvcg.2018.2865241", "10.1016/j.ijnurstu.2010.06.004", "10.1109/tvcg.2012.213", "10.1111/0735-2751.00040", "10.1109/tvcg.2013.145", "10.1002/ev.1427", "10.1109/tvcg.2018.2811488", "10.1075/idj.23.1.07thu", "10.1109/tvcg.2009.111", "10.1109/mcg.2018.2874523", "10.1111/cgf.13184", "10.1111/cgf.13595"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3029413", "title": "A Design Space of Vision Science Methods for Visualization Research", "year": "2020", "conferenceName": "InfoVis", "authors": "Madison A. Elliott;Christine Nothelfer;Cindy Xiong;Danielle Albers Szafir", "citationCount": "0", "affiliation": "Elliott, MA (Corresponding Author), Univ British Columbia, Vancouver, BC, Canada. Elliott, Madison A., Univ British Columbia, Vancouver, BC, Canada. Nothelfer, Christine, Northwestern Univ, Evanston, IL 60208 USA. Xiong, Cindy, Univ Massachusetts, Amherst, MA 01003 USA. Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "Canada;USA", "abstract": "A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.", "keywords": "Perception,human vision,empirical research,evaluation,HCI", "link": "http://dx.doi.org/10.1109/TVCG.2020.3029413", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1177/0886109909354981", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1093/bioinformatics/btq110", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1177/1744987107081254", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1177/174498710501000305", "10.1017/s1049096513001789", "10.1109/tvcg.2012.213", "10.1093/nar/gkz239", "10.1093/sysbio/sys062", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030438", "title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening", "year": "2020", "conferenceName": "SciVis", "authors": "Mar\u00eda Virginia Sabando;Pavol Ulbrich;Mat\u00edas N. Selzer;Jan Byska;Jan Mican;Ignacio Ponzoni;Axel J. Soto;Maria Luj\u00e1n Ganuza;Barbora Kozl\u00edkov\u00e1", "citationCount": "0", "affiliation": "Sabando, MV (Corresponding Author), Univ Nacl Sur, Inst Comp Sci \\& Engn UNS CONICET, Bahia Blanca, Buenos Aires, Argentina. Sabando, MV (Corresponding Author), Univ Nacl Sur, Dept Comp Sci \\& Engn, Bahia Blanca, Buenos Aires, Argentina. Sabando, Maria Virginia; Selzer, Matias; Ponzoni, Ignacio; Soto, Axel J.; Ganuza, Maria Lujan, Univ Nacl Sur, Inst Comp Sci \\& Engn UNS CONICET, Bahia Blanca, Buenos Aires, Argentina. Sabando, Maria Virginia; Ponzoni, Ignacio; Soto, Axel J., Univ Nacl Sur, Dept Comp Sci \\& Engn, Bahia Blanca, Buenos Aires, Argentina. Ulbrich, Pavol; Byska, Jan; Kozlikova, Barbora, Masaryk Univ, Fac Informat, Visitlab, Brno, Czech Republic. Selzer, Matias; Ganuza, Maria Lujan, Univ Nacl Sur, VyGLab Res Lab UNS CICPBA, Dept Comp Sci \\& Engn, Bahia Blanca, Buenos Aires, Argentina. Mican, Jan, Masaryk Univ, Dept Expt Biol, Loschmidt Labs, Brno, Czech Republic. Mican, Jan, Masaryk Univ, RECETOX, Brno, Czech Republic. Mican, Jan, Masaryk Univ, Fac Med, Brno, Czech Republic.", "countries": "Argentina;Republic", "abstract": "In the modern drug discovery process, medicinal chemists deal with the complexity of analysis of large ensembles of candidate molecules. Computational tools, such as dimensionality reduction (DR) and classification, are commonly used to efficiently process the multidimensional space of features. These underlying calculations often hinder interpretability of results and prevent experts from assessing the impact of individual molecular features on the resulting representations. To provide a solution for scrutinizing such complex data, we introduce ChemVA, an interactive application for the visual exploration of large molecular ensembles and their features. Our tool consists of multiple coordinated views: Hexagonal view, Detail view, 3D view, Table view, and a newly proposed Difference view designed for the comparison of DR projections. These views display DR projections combined with biological activity, selected molecular features, and confidence scores for each of these projections. This conjunction of views allows the user to drill down through the dataset and to efficiently select candidate compounds. Our approach was evaluated on two case studies of finding structurally similar ligands with similar binding affinity to a target protein, as well as on an external qualitative evaluation. The results suggest that our system allows effective visual inspection and comparison of different high-dimensional molecular representations. Furthermore, ChemVA assists in the identification of candidate compounds while providing information on the certainty behind different molecular representations.", "keywords": "Virtual screening,visual analysis,dimensionality reduction,coordinated views,cheminformatics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030438", "refList": ["10.1109/tvcg.2008.137", "10.1057/ivs.2009.10", "10.2312/eurovisstar.20151110", "10.1109/eisic.2015.35", "10.1109/pacificvis.2014.44", "10.1145/1142473.1142574", "10.1109/tvcg.2013.223", "10.1109/tvcg.2014.2346573", "10.1109/tvcg.2019.2934539", "10.1111/cgf.13717", "10.1109/vizsec.2009.5375536", "10.1111/cgf.12925", "10.1109/tvcg.2015.2467551", "10.1109/mcg.2015.99", "10.1007/978-3-319", "10.1109/tvcg.2012.255", "10.1145/1064830.1064834", "10.1177/1473871611433713", "10.1207/s1532690xci0804\\_2", "10.1145/1168149.1168168", "10.1016/j.chb.2006.10.002", "10.1109/tvcg.2014.2346441", "10.1109/eisic.2017.15", "10.1111/1467-8721.00160", "10.1109/tvcg.2018.2865024", "10.1109/infvis.2004.2"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030435", "title": "Data Visceralization: Enabling Deeper Understanding of Data Using Virtual Reality", "year": "2020", "conferenceName": "InfoVis", "authors": "Benjamin Lee;Dave Brown;Bongshin Lee;Christophe Hurter;Steven Mark Drucker;Tim Dwyer", "citationCount": "1", "affiliation": "Lee, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Lee, Benjamin; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Brown, Dave; Lee, Bongshin; Drucker, Steven, Microsoft Res, Redmond, WA USA. Hurter, Christophe, French Civil Aviat Univ, ENAC, Toulouse, France.", "countries": "USA;France;Australia", "abstract": "A fundamental part of data visualization is transforming data to map abstract information onto visual attributes. While this abstraction is a powerful basis for data visualization, the connection between the representation and the original underlying data (i.e., what the quantities and measurements actually correspond with in reality) can be lost. On the other hand, virtual reality (VR) is being increasingly used to represent real and abstract models as natural experiences to users. In this work, we explore the potential of using VR to help restore the basic understanding of units and measures that are often abstracted away in data visualization in an approach we call data visceralization. By building VR prototypes as design probes, we identify key themes and factors for data visceralization. We do this first through a critical reflection by the authors, then by involving external participants. We find that data visceralization is an engaging way of understanding the qualitative aspects of physical measures and their real-life form, which complements analytical and quantitative understanding commonly gained from data visualization. However, data visceralization is most effective when there is a one-to-one mapping between data and representation, with transformations such as scaling affecting this understanding. We conclude with a discussion of future directions for data visceralization.", "keywords": "Data visceralization,virtual reality,exploratory study", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030435", "refList": ["10.1080/01973762.2013.761106", "10.1109/tvcg.2015.2467811", "10.1109/tvcg.2012.221", "10.1145/3284179.3284326", "10.1109/tvcg.2019.2934287", "10.1109/2945.841119", "10.1109/tvcg.2019.2934539", "10.1109/mcg.2013.101", "10.1109/tvcg.2011.175", "10.1109/tvcg.2018.2830759", "10.1109/3dvis.2014.7160096", "10.1109/mcg.2018.2878900", "10.1109/iv.2011.32", "10.1109/tvcg.2013.196", "10.1109/tvcg.2018.2865241", "10.1515/abitech-2017-0002", "10.1145/2468356.2468739", "10.16995/olh.280", "10.1080/15230406.2018.1513343", "10.1109/iv.2004.1320189", "10.1109/tvcg.2012.213", "10.1109/mcg.2006.120", "10.1109/icdar.2017.286", "10.1371/journal.pone.0146368", "10.1080/1472586x.2011.548488", "10.1109/tvcg.2014.2346574", "10.1080/0013838x.2017.1332021"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030464", "title": "Designing Narrative-Focused Role-Playing Games for Visualization Literacy in Young Children", "year": "2020", "conferenceName": "InfoVis", "authors": "Elaine Huynh;Angela Nyhout;Patricia Ganea;Fanny Chevalier", "citationCount": "0", "affiliation": "Huynh, E (Corresponding Author), Univ Toronto, Dept Comp Sci, Toronto, ON, Canada. Huynh, Elaine, Univ Toronto, Dept Comp Sci, Toronto, ON, Canada. Nyhout, Angela; Ganea, Patricia, Univ Toronto, Ontario Inst Studies Educ, Toronto, ON, Canada. Chevalier, Fanny, Univ Toronto, Dept Comp Sci \\& Stat Sci, Toronto, ON, Canada.", "countries": "Canada", "abstract": "Building on game design and education research, this paper introduces narrative-focused role-playing games as a way to promote visualization literacy in young children. Visualization literacy skills are vital in understanding the world around us and constructing meaningful visualizations, yet, how to better develop these skills at an early age remains largely overlooked and understudied. Only recently has the visualization community started to fill this gap, resulting in preliminary studies and development of educational tools for use in early education. We add to these efforts through the exploration of gamification to support learning, and identify an opportunity to apply role-playing game-based designs by leveraging the presence of narratives in data-related problems involving visualizations. We study the effects of including narrative elements on learning through a technology probe, grounded in a set of design considerations stemming from visualization, game design and education science. We create two versions of a game - one with narrative elements and one without - and evaluate our instances on 33 child participants between 11- to 13-years old using a between-subjects study design. Despite participants requiring double the amount of time to complete their game due to additional narrative elements, the inclusion of such elements were found to improve engagement without sacrificing learning; our results indicate no significant differences in development of graph-reading skills, but significant differences in engagement and overall enjoyment of the game. We report observations and qualitative feedback collected, and note areas for improvement and room for future work.", "keywords": "Visualization Literacy,Educational technology,Gamification,Narrative", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030464", "refList": ["10.1007/978-981-13-2694-3\\_2", "10.1145/2702123.2702298", "10.1145/2702123.2702558", "10.1007/978-981-13-2694-3\\_8", "10.1145/2702123.2702245", "10.1109/mis.2012.27", "10.18061/dsq.v21i4.318", "10.1109/tvcg.2013.134", "10.1109/vl.1996.545307", "10.1007/s10708-008-9186-0", "10.1093/cje/ben057", "10.5210/fm.v16i2.3316", "10.1109/tvcg.2019.2934539", "10.1109/tvcg.2016.2598608", "10.1007/978-3-319-94659-7\\_10", "10.1109/mcg.2013.28", "10.17351/ests2017.134", "10.1109/pacificvis.2014.39", "10.1145/3173574.3173728", "10.1145/2598784.2598806", "10.1145/2491500.2491501", "10.1145/1993060.1993065", "10.1109/tvcg.2018.2802520", "10.1145/3025453.3025667", "10.1145/2598510.2598566", "10.1080/15710882.2015.1081240", "10.17351/ests2017.133", "10.1109/tvcg.2014.2346431", "10.1016/j.ijhcs.2015.02.005", "10.1145/2702123.2702180", "10.1109/tvcg.2007.70577", "10.1109/mcg.2019.2923483", "10.5931/djim.v12.i1.6449", "10.1145/3240167.3240206", "10.1145/2468356.2468739", "10.1109/tvcg.2012.213", "10.1145/3025453.3025751", "10.4018/978-1-4666-6497-5.ch003"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030355", "title": "Guidelines For Pursuing and Revealing Data Abstractions", "year": "2020", "conferenceName": "InfoVis", "authors": "Alex Bigelow;Katy Williams;Katherine E. Isaacs", "citationCount": "0", "affiliation": "Bigelow, A (Corresponding Author), Univ Arizona, Tucson, AZ 85721 USA. Bigelow, Alex; Williams, Katy; Isaacs, Katherine E., Univ Arizona, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "Many data abstraction types, such as networks or set relationships, remain unfamiliar to data workers beyond the visualization research community. We conduct a survey and series of interviews about how people describe their data, either directly or indirectly. We refer to the latter as latent data abstractions. We conduct a Grounded Theory analysis that (1) interprets the extent to which latent data abstractions exist, (2) reveals the far-reaching effects that the interventionist pursuit of such abstractions can have on data workers, (3) describes why and when data workers may resist such explorations, and (4) suggests how to take advantage of opportunities and mitigate risks through transparency about visualization research perspectives and agendas. We then use the themes and codes discovered in the Grounded Theory analysis to develop guidelines for data abstraction in visualization projects. To continue the discussion, we make our dataset open along with a visual interface for further exploration.", "keywords": "Data abstraction,Grounded theory,Survey design,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030355", "refList": ["10.1080/2159676x.2016.1251701", "10.1109/infvis.2000.885092", "10.1145/2702123.2702298", "10.4135/9781848607941.n14", "10.1007/978-1-4939", "10.1109/tvcg.2014.2346331", "10.1109/tvcg.2017.2744843", "10.1177/1473871613510429", "10.1007/978-1-4939-0378-8\\_2", "10.1145/2598153.2598175", "10.1109/tvcg.2019.2934285", "10.1177/1473871613488591", "10.1145/2501105.2501106", "10.1109/tvcg.2019.2934538", "10.1109/tvcg.2019.2934539", "10.1017/s1049096510990781", "10.1145/3025453.3025837", "10.1145/3290605.3300474", "10.1145/3290605.3300356", "10.1002/nur.1025", "10.1145/2993901.2993916", "10.1145/3392826", "10.1086/269268", "10.1109/tvcg.2018.2865241", "10.1145/2998181.2998331", "10.1145/291224.291229", "10.1057/ivs.2009.13", "10.1145/2047196.2047205", "10.1109/tvcg.2012.213", "10.1145/3274405", "10.1109/tvcg.2013.145", "10.1016/0040-6031(92)85160-w", "10.1109/iv.2013.45", "10.1109/tvcg.2009.111", "10.1109/mcg.2019.2914844", "10.1109/tvcg.2009.116"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030405", "title": "Insights From Experiments With Rigor in an EvoBio Design Study", "year": "2020", "conferenceName": "InfoVis", "authors": "Jennifer Rogers;Austin H. Patton;Luke Harmon;Alexander Lex;Miriah D. Meyer", "citationCount": "0", "affiliation": "Rogers, J (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Rogers, Jen; Lex, Alexander; Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Patton, Austin H., Washington State Univ, Pullman, WA 99164 USA. Harmon, Luke, Univ Idaho, Moscow, ID 83843 USA.", "countries": "USA", "abstract": "Design study is an established approach of conducting problem-driven visualization research. The academic visualization community has produced a large body of work for reporting on design studies, informed by a handful of theoretical frameworks, and applied to a broad range of application areas. The result is an abundance of reported insights into visualization design, with an emphasis on novel visualization techniques and systems as the primary contribution of these studies. In recent work we proposed a new, interpretivist perspective on design study and six companion criteria for rigor that highlight the opportunities for researchers to contribute knowledge that extends beyond visualization idioms and software. In this work we conducted a year-long collaboration with evolutionary biologists to develop an interactive tool for visual exploration of multivariate datasets and phylogenetic trees. During this design study we experimented with methods to support three of the rigor criteria: ABUNDANT, REFLEXIVE, and TRANSPARENT. As a result we contribute two novel visualization techniques for the analysis of multivariate phylogenetic datasets, three methodological recommendations for conducting design studies drawn from reflections over our process of experimentation, and two writing devices for reporting interpretivist design study. We offer this work as an example for implementing the rigor criteria to produce a diverse range of knowledge contributions.", "keywords": "Methodologies,Application Motivated Visualization,Guidelines,Life Sciences Visualization,Health,Medicine,Biology,Bioinformatics,Genomics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030405", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1080/17493460802276893", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1109/tvcg.2012.213", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030462", "title": "Table Scraps: An Actionable Framework for Multi-Table Data Wrangling From An Artifact Study of Computational Journalism", "year": "2020", "conferenceName": "InfoVis", "authors": "Stephen Kasica;Charles Berret;Tamara Munzner", "citationCount": "0", "affiliation": "Kasica, S (Corresponding Author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada. Kasica, Stephen; Munzner, Tamara, Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada. Berret, Charles, Univ British Columbia, Sch Journalism Writing \\& Media, Vancouver, BC, Canada.", "countries": "Canada", "abstract": "For the many journalists who use data and computation to report the news, data wrangling is an integral part of their work. Despite an abundance of literature on data wrangling in the context of enterprise data analysis, little is known about the specific operations, processes, and pain points journalists encounter while performing this tedious, time-consuming task. To better understand the needs of this user group, we conduct a technical observation study of 50 public repositories of data and analysis code authored by 33 professional journalists at 26 news organizations. We develop two detailed and cross-cutting taxonomies of data wrangling in computational journalism, for actions and for processes. We observe the extensive use of multiple tables, a notable gap in previous wrangling analyses. We develop a concise, actionable framework for general multi-table data wrangling that includes wrangling operations documented in our taxonomy that are without clear parallels in other work. This framework, the first to incorporate tables as first-class objects, will support future interactive wrangling tools for both computational journalism and general-purpose use. We assess the generative and descriptive power of our framework through discussion of its relationship to our set of taxonomies.", "keywords": "Computational journalism,Data journalism,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030462", "refList": ["10.1145/1378773.1378792", "10.1109/tvcg.2012.219", "10.1109/vast47406.2019.8986909", "10.1145/1084805.1084812", "10.1007/s00778-008-0098-x", "10.1016/j.websem.2008.09.005", "10.18637/jss.v040.i01", "10.1145/989863.989865", "10.1109/tvcg.2015.2467551", "10.5281/zenodo.3509134", "10.1109/tvcg.2019.2934539", "10.1109/tvcg.2019.2934593", "10.1109/tse.2018.2796554", "10.17349/jmc117309", "10.1109/2945.981851", "10.1109/vast.2011.6102440", "10.1177/1473871611415994", "10.1145/2001269.2001288"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13964", "year": "2020", "title": "Reading Traces: Scalable Exploration in Elastic Visualizations of Cultural Heritage Data", "conferenceName": "EuroVis", "authors": "Mark{-}Jan Bludau;Viktoria Br{\\\"{u}}ggemann;Anna Busch;Marian D{\\\"{o}}rk", "citationCount": "1", "affiliation": "Bludau, MJ (Corresponding Author), Univ Appl Sci Potsdam, UCLAB, Potsdam, Germany.\nBludau, M. -J.; Brueggemann, V.; Doerk, M., Univ Appl Sci Potsdam, UCLAB, Potsdam, Germany.\nBusch, A., Univ Potsdam, Theodor Fontane Archiv, Potsdam, Germany.", "countries": "Germany", "abstract": "Through a design study, we develop an approach to data exploration that utilizes elastic visualizations designed to support varying degrees of detail and abstraction. Examining the notions of scalability and elasticity in interactive visualizations, we introduce a visualization of personal reading traces such as marginalia or markings inside the reference library of German realist author Theodor Fontane. To explore such a rich and extensive collection, meaningful visual forms of abstraction and detail are as important as the transitions between those states. Following a growing research interest in the role of fluid interactivity and animations between views, we are particularly interested in the potential of carefully designed transitions and consistent representations across scales. The resulting prototype addresses humanistic research questions about the interplay of distant and close reading with visualization research on continuous navigation along several granularity levels, using scrolling as one of the main interaction mechanisms. In addition to presenting the design process and resulting prototype, we present findings from a qualitative evaluation of the tool, which suggest that bridging between distant and close views can enhance exploration, but that transitions between views need to be crafted very carefully to facilitate comprehension.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13964", "refList": ["10.1007/s41244-017-0048-4", "10.1109/tvcg.2009.108", "10.1145/1456650.1456652", "10.1109/tvcg.2014.2346424", "10.1177/1473871611416549", "10.1111/cgf.13195", "10.1145/2207676.2208607", "10.1145/1376616.1376618", "10.1006/ijhc.2002.1017", "10.1109/tvcg.2011.185", "10.1177/1473871611413180", "10.1109/vl.1996.545307", "10.1006/ijhc.1017", "10.1109/tvcg.2019.2934539", "10.1109/tvcg.2018.2830759", "10.1145/1556262.1556300", "10.1109/tvcg.2014.2346677", "10.1145/2909132.2909255", "10.1109/tvcg.2007.70539", "10.1145/2396636.2396675", "10.1145/1978942.1979124", "10.1016/j.ijhcs.2003.08.005", "10.2312/eurovisstar.20151113", "10.1109/infvis.2005.1532127"], "wos": 1, "children": [], "len": 1}], "len": 17}, {"doi": "10.1109/tvcg.2019.2934283", "title": "What is Interaction for Data Visualization?", "year": "2019", "conferenceName": "InfoVis", "authors": "Evanthia Dimara;Charles Perin", "citationCount": "1", "affiliation": "Dimara, E (Corresponding Author), Sorbonne Univ, Paris, France. Dimara, Evanthia, Sorbonne Univ, Paris, France. Perin, Charles, Univ Victoria, Victoria, BC, Canada.", "countries": "Canada;France", "abstract": "Interaction is fundamental to data visualization, but what \u201cinteraction\u201d means in the context of visualization is ambiguous and confusing. We argue that this confusion is due to a lack of consensual definition. To tackle this problem, we start by synthesizing an inclusive view of interaction in the visualization community \u2013 including insights from information visualization, visual analytics and scientific visualization, as well as the input of both senior and junior visualization researchers. Once this view takes shape, we look at how interaction is defined in the field of human-computer interaction (HCI). By extracting commonalities and differences between the views of interaction in visualization and in HCI, we synthesize a definition of interaction for visualization. Our definition is meant to be a thinking tool and inspire novel and bolder interaction design practices. We hope that by better understanding what interaction in visualization is and what it can be, we will enrich the quality of interaction in visualization systems and empower those who use them.", "keywords": "interaction,visualization,data,definition,human-computer interaction", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934283", "refList": ["10.1057/ivs.2009.22", "10.1515/icom-2017-0027", "10.1145/2493102.2493104", "10.1007/978-3-319-06793-3\\_6", "10.1080/03640210801898177", "10.1109/mcg.2010.30", "10.1109/tvcg.2013.134", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2018.2865237", "10.1109/2945.981847", "10.1145/2598784.2598806", "10.1109/tvcg.2013.130", "10.1109/tvcg.2007.70577", "10.1109/mic.2015.129", "10.1145/2909132.2909267", "10.1080/01449290500330331", "10.1109/tvcg.2018.2865233", "10.1109/tvcg.2015.2467831", "10.1109/iv.2015.34", "10.1109/tvcg.2009.111", "10.1145/3290605.3300565", "10.1145/3173574.3173797", "10.1145/948496.948514", "10.1145/3025453.3025765", "10.1109/tvcg.2015.2467613", "10.1145/2659796", "10.1109/tvcg.2014.2346311", "10.1145/3025453.3025524", "10.1080/17452759.2011.558588", "10.1145/3027063.3053113", "10.1109/infvis.2005.1532136", "10.1145/2470654.2481307", "10.1109/tvcg.2018.2864913", "10.1145/345513.345267", "10.3102/00028312005004437", "10.1109/tvcg.2007.70436", "10.1037/0033-295x.106.4.643", "10.1145/2133416.2146416", "10.1109/tvcg.2013.191", "10.1109/tvcg.2010.177", "10.1145/960201.957206", "10.1109/tvcg.2007.70515", "10.1109/infvis.2000.885092", "10.1145/2636240.2636844", "10.1037/h0055392", "10.1177/1473871611413180", "10.1109/vl.1996.545307", "10.1111/j.1471-1842.2009.00848.x", "10.1145/989863.989865", "10.1109/tvcg.2013.124", "10.1109/tvcg.2008.109", "10.1145/1166253.1166265", "10.1145/2702123.2702180", "10.1109/tvcg.2016.2598620", "10.1080/07370024.2016.1226139", "10.1145/3173574.3173909", "10.1109/pacificvis.2010.5429613", "10.1111/j.1467-6478.2006.00368.x", "10.1109/tvcg.2012.204", "10.1109/tvcg.2013.120", "10.1179/1743277412y.0000000019", "10.1145/1936652.1936684", "10.2307/1269768", "10.1109/infvis.1996.559213", "10.1109/tvcg.2016.2598839", "10.1145/2642918.2647360", "10.1057/palgrave.ivs.9500099", "10.1016/j.cag.2009.06.004", "10.1109/mc.2013.178", "10.1109/tvcg.2007.70541", "10.1109/vast.2011.6102473", "10.1145/358886.358895", "10.1109/tvcg.2014.2346573", "10.1109/tvcg.2018.2865159", "10.1145/2207676.2207741", "10.1109/tvcg.2015.2396062", "10.1145/2207676.2208572", "10.1109/tvcg.2016.2598608", "10.1057/ivs.2008.31", "10.1177/001316446002000104", "10.1109/tvcg.2017.2680452", "10.1109/tvcg.2006.80", "10.1145/2598510.2598566", "10.1037/0003-066x.51.4.355", "10.7146/dpb.v16i224.7586", "10.1109/infvis.1998.729560", "10.1162/leon\\_a\\_00011", "10.1109/tvcg.2010.157", "10.1109/tvcg.2014.2359887"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13720", "year": "2019", "title": "Capture \\& Analysis of Active Reading Behaviors for Interactive Articles on the Web", "conferenceName": "EuroVis", "authors": "Matthew Conlen;Alex Kale;Jeffrey Heer", "citationCount": "1", "affiliation": "Conlen, M (Corresponding Author), Univ Washington, Paul G Allen Sch Comp Sci \\& Engn, Seattle, WA 98195 USA.\nConlen, Matthew; Kale, Alex; Heer, Jeffrey, Univ Washington, Paul G Allen Sch Comp Sci \\& Engn, Seattle, WA 98195 USA.", "countries": "USA", "abstract": "Journalists, educators, and technical writers are increasingly publishing interactive content on the web. However, popular analytics tools provide only coarse information about how readers interact with individual pages, and laboratory studies often fail to capture the variability of a real-world audience. We contribute extensions to the Idyll markup language to automate the detailed instrumentation of interactive articles and corresponding visual analysis tools for inspecting reader behavior at both micro- and macro-levels. We present three case studies of interactive articles that were instrumented, posted online, and promoted via social media to reach broad audiences, and share data from over 50,000 reader sessions. We demonstrate the use of our tools to characterize article-specific interaction patterns, compare behavior across desktop and mobile devices, and reveal reading patterns common across articles. Our contributed findings, tools, and corpus of behavioral data can help advance and inform more comprehensive studies of narrative visualization.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13720", "refList": ["10.1111/cgf.13195", "10.1145/2702123.2702452", "10.1371/journal.pone.0142444", "10.1145/2470654.2466442", "10.1002/wics.101", "10.1111/cgf.12392", "10.1109/tvcg.2017.2745958", "10.1007/978-3-642-25289-1\\_37", "10.1109/tvcg.2016.2598797", "10.1108/10662240410555306", "10.1080/21670811.2018.1488598", "10.1145/3025171.3025184", "10.1109/mcg.2015.99", "10.1109/tvcg.2016.2539960", "10.1109/tvcg.2013.119", "10.1016/s0079-7421(02)80005-6", "10.1109/isemc.2014.6899034", "10.1007/s11144-015-0848-x", "10.1145/3025453.3025866", "10.2307/469105", "10.1109/mc.2013.36", "10.1111/cgf.13208"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030418", "title": "CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization", "year": "2020", "conferenceName": "VAST", "authors": "Zijie J. Wang;Robert Turko;Omar Shaikh;Haekyu Park;Nilaksh Das;Fred Hohman;Minsuk Kahng;Duen Horng Chau", "citationCount": "0", "affiliation": "Wang, ZJ (Corresponding Author), Georgia Tech, Atlanta, GA 30332 USA. Wang, Zijie J.; Turko, Robert; Shaikh, Omar; Park, Haekyu; Das, Nilaksh; Hohman, Fred; Chau, Duen Horng (Polo), Georgia Tech, Atlanta, GA 30332 USA. Kahng, Minsuk, Oregon State Univ, Corvallis, OR 97331 USA.", "countries": "USA", "abstract": "Deep learning's great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN's structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern deep learning techniques.", "keywords": "Deep learning,machine learning,convolutional neural networks,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030418", "refList": ["10.1016/j.cag.2018.09.018", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2017.2744938", "10.1037/0022-0663.83.4.484", "10.1016/j.patcog.2017.10.013", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2011.185", "10.1016/s0360-1315(99)00023-8", "10.1080/07380569.2012.651422", "10.1006/s1045-926x(02)00027-7", "10.1145/1821996.1821997", "10.1109/vast.2018.8802509", "10.1109/vl.2000.874346", "10.1007/978-3-319-27857-5\\_77", "10.1145/1227504.1227384", "10.1038/nature14539", "10.1109/tvcg.2018.2816223", "10.23915/distill.00016", "10.1162/neco.1997.9.8.1735", "10.1109/tvcg.2016.2640960", "10.1006/ijhc.2000.0409", "10.1109/tvcg.2017.2744718", "10.1006/s1045-926x(02)00028-9", "10.1111/cgf.13720", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1145/782941.782998", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 45}, {"doi": "10.1109/tvcg.2018.2865117", "title": "Patterns and Pace: Quantifying Diverse Exploration Behavior with Visualizations on the Web", "year": "2018", "conferenceName": "InfoVis", "authors": "Mi Feng;Evan M. Peck;Lane Harrison", "citationCount": "3", "affiliation": "Feng, M (Corresponding Author), Worcester Polytech Inst, Worcester, MA 01609 USA. Feng, Mi; Harrison, Lane, Worcester Polytech Inst, Worcester, MA 01609 USA. Peck, Evan, Bucknell Univ, Lewisburg, PA 17837 USA.", "countries": "USA", "abstract": "The diverse and vibrant ecosystem of interactive visualizations on the web presents an opportunity for researchers and practitioners to observe and analyze how everyday people interact with data visualizations. However, existing metrics of visualization interaction behavior used in research do not fully reveal the breadth of peoples' open-ended explorations with visualizations. One possible way to address this challenge is to determine high-level goals for visualization interaction metrics, and infer corresponding features from user interaction data that characterize different aspects of peoples' explorations of visualizations. In this paper, we identify needs for visualization behavior measurement, and develop corresponding candidate features that can be inferred from users' interaction data. We then propose metrics that capture novel aspects of peoples' open-ended explorations, including exploration uniqueness and exploration pacing. We evaluate these metrics along with four other metrics recently proposed in visualization literature by applying them to interaction data from prior visualization studies. The results of these evaluations suggest that these new metrics 1) reveal new characteristics of peoples' use of visualizations, 2) can be used to evaluate statistical differences between visualization designs, and 3) are statistically independent of prior metrics used in visualization research. We discuss implications of these results for future studies, including the potential for applying these metrics in visualization interaction analysis, as well as emerging challenges in developing and selecting metrics depicting visualization explorations.", "keywords": "Interaction,Visualization,Quantitative Evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865117", "refList": ["10.1109/tvcg.2008.137", "10.1109/ldav.2012.6378977", "10.1145/2702123.2702452", "10.1145/2702123.2702419", "10.1109/tvcg.2014.2346575", "10.1145/2702123.2702275", "10.1109/iccv.2013.147", "10.1109/tvcg.2014.2346452", "10.1016/0306-4573(88)90021-0", "10.1109/tvcg.2015.2467613", "10.1109/tvcg.2011.229", "10.1109/tvcg.2015.2467201", "10.1109/vast.2007.4389008", "10.1007/978-1-4419-0492-8\\_2", "10.1109/tvcg.2017.2745958", "10.1145/2020408.2020581", "10.1109/tvcg.2016.2598797", "10.1109/tvcg.2015.2467871", "10.1145/2702123.2702590", "10.1109/tvcg.2016.2598466", "10.1109/tvcg.2013.200", "10.1109/tvcg.2005.53", "10.1109/vast.2007.4389009", "10.1145/2678025.2701407", "10.1111/cgf.13208", "10.1145/989863.989880", "10.1109/tvcg.2007.70515", "10.1145/365024.365325", "10.1109/tvcg.2016.2599058"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030383", "title": "II-20: Intelligent and pragmatic analytic categorization of image collections", "year": "2020", "conferenceName": "VAST", "authors": "Jan Zah\u00e1lka;Marcel Worring;Jarke J. van Wijk", "citationCount": "0", "affiliation": "Zahalka, J (Corresponding Author), Czech Tech Univ, Prague, Czech Republic. Zahalka, Jan, Czech Tech Univ, Prague, Czech Republic. Worring, Marcel, Univ Amsterdam, Amsterdam, Netherlands. van Wijk, Jarke J., Eindhoven Univ Technol, Eindhoven, Netherlands.", "countries": "Republic;Netherlands", "abstract": "In this paper, we introduce 11\u201320 (Image Insight 2020), a multimedia analytics approach for analytic categorization of image collections. Advanced visualizations for image collections exist, but they need tight integration with a machine model to support the task of analytic categorization. Directly employing computer vision and interactive learning techniques gravitates towards search. Analytic categorization, however, is not machine classification (the difference between the two is called the pragmatic gap): a human adds/redefines/deletes categories of relevance on the fly to build insight, whereas the machine classifier is rigid and non-adaptive. Analytic categorization that truly brings the user to insight requires a flexible machine model that allows dynamic sliding on the exploration-search axis, as well as semantic interactions: a human thinks about image data mostly in semantic terms. 11\u201320 brings three major contributions to multimedia analytics on image collections and towards closing the pragmatic gap. Firstly, a new machine model that closely follows the user's interactions and dynamically models her categories of relevance. II-20's machine model, in addition to matching and exceeding the state of the art's ability to produce relevant suggestions, allows the user to dynamically slide on the exploration-search axis without any additional input from her side. Secondly, the dynamic, 1-image-at-a-time Tetris metaphor that synergizes with the model. It allows a well-trained model to analyze the collection by itself with minimal interaction from the user and complements the classic grid metaphor. Thirdly, the fast-forward interaction, allowing the user to harness the model to quickly expand (\u201cfast-forward\u201d) the categories of relevance, expands the multimedia analytics semantic interaction dictionary. Automated experiments show that II-20's machine model outperforms the existing state of the art and also demonstrate the Tetris metaphor's analytic quality. User studies further confirm that II\u201320 is an intuitive, efficient, and effective multimedia analytics tool.", "keywords": "Multimedia analytics,image data,analytic categorization,pragmatic gap", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030383", "refList": ["10.1109/tvcg.2008.137", "10.1145/2882903.2882919", "10.1145/2556647.2556657", "10.1145/1142473.1142574", "10.1109/tvcg.2016.2598471", "10.1109/vast.2017.8585669", "10.1109/tvcg.2014.2346573", "10.1111/j.0956-7976.2005.00782.x", "10.1007/978-3-540-89965-5\\_27", "10.1109/visual.2019.8933611", "10.1109/vast.2010.5653598", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1109/tvcg.2012.271", "10.1109/mcg.2009.49", "10.1057/ivs.2008.31", "10.1111/cgf.12925", "10.1109/hicss.2016.183", "10.1109/tvcg.2016.2598466", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2018.2865117", "10.1109/vast47406.2019.8986948", "10.1016/s0950-7051(00)00101-5", "10.1109/tvcg.2016.2598594", "10.1109/tvcg.2006.101", "10.1111/cgf.12311", "10.1109/vast.2009.5333020", "10.1109/vast.2010.5652885", "10.1111/cgf.13670", "10.1109/wvl.1988.18020", "10.1145/2133806.2133821", "10.1109/vast.2012.6400486"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13678", "year": "2019", "title": "Characterizing Exploratory Visual Analysis: A Literature Review and Evaluation of Analytic Provenance in Tableau", "conferenceName": "EuroVis", "authors": "Leilani Battle;Jeffrey Heer", "citationCount": "2", "affiliation": "Battle, L (Corresponding Author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.\nBattle, Leilani, Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.\nHeer, Jeffrey, Univ Washington, Paul G Allen Sch Comp Sci \\& Engn, Seattle, WA 98195 USA.", "countries": "USA", "abstract": "Supporting exploratory visual analysis (EVA) is a central goal of visualization research, and yet our understanding of the process is arguably vague and piecemeal. We contribute a consistent definition of EVA through review of the relevant literature, and an empirical evaluation of existing assumptions regarding how analysts perform EVA using Tableau, a popular visual analysis tool. We present the results of a study where 27 Tableau users answered various analysis questions across 3 datasets. We measure task performance, identify recurring patterns across participants' analyses, and assess variance from task specificity and dataset. We find striking differences between existing assumptions and the collected data. Participants successfully completed a variety of tasks, with over 80\\% accuracy across focused tasks with measurably correct answers. The observed cadence of analyses is surprisingly slow compared to popular assumptions from the database community. We find significant overlap in analyses across participants, showing that EVA behaviors can be predictable. Furthermore, we find few structural differences between behavior graphs for open-ended and more focused exploration tasks.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13678", "refList": ["10.1109/dsia.2017.8339089", "10.1109/tvcg.2008.137", "10.1109/tvcg.2007.28", "10.1057/ivs.2009.22", "10.1038/526189a", "10.1109/tvcg.2006.85", "10.1145/2882903.2882919", "10.1145/2588555.2610523", "10.1109/tvcg.2014.2346575", "10.1145/2207676.2208294", "10.1145/2723372.2731084", "10.1109/tvcg.2016.2598471", "10.1145/3173574.3174168", "10.1109/visual.2005.1532788", "10.1109/tvcg.2014.2346452", "10.1109/tvcg.2015.2467613", "10.20380/gi2015.16", "10.14778/2831360.2831371", "10.1145/3173574.3174053", "10.1109/icdew.2006.75", "10.1145/2993901.2993912", "10.1145/1502650.1502695", "10.1109/tvcg.2012.219", "10.1007/978-3-642-23768-3\\_22", "10.1109/tvcg.2016.2598797", "10.1109/vl.1996.545307", "10.1111/j.1467-8659.2010.01830.x", "10.1109/tvcg.2015.2467871", "10.1145/3209900.3209901", "10.1145/2939502.2939513", "10.1145/381641.381656", "10.1109/tvcg.2013.124", "10.1109/vast.2008.4677357", "10.1145/1378773.1378788", "10.1109/mcg.2012.120", "10.1057/ivs.2008.31", "10.1109/infvis.2005.1532136", "10.1109/icde.2014.6816674", "10.1109/tvcg.2016.2598466", "10.1109/tvcg.2015.2467551", "10.1145/2939502.2939506", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2865117", "10.1145/1556262.1556276", "10.1016/s0950-7051(00)00101-5", "10.1109/tvcg.2008.109", "10.1145/2588555.2593666", "10.1109/tvcg.2018.2865040", "10.1109/tvcg.2013.179", "10.1111/cgf.13409", "10.1177/1473871616638546", "10.1109/2945.981851", "10.1109/tvcg.2010.164", "10.1109/vast.2008.4677365", "10.1109/tvcg.2015.2467191", "10.1145/2133806.2133821", "10.1145/2207676.2208412", "10.1109/tvcg.2017.2744319", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.224"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030425", "title": "Visual Analysis of Argumentation in Essays", "year": "2020", "conferenceName": "VAST", "authors": "Dora Kiesel;Patrick Riehmann;Henning Wachsmuth;Benno Stein;Bernd Fr\u00f6hlich", "citationCount": "0", "affiliation": "Kiesel, D (Corresponding Author), Bauhaus Univ Weimar, Weimar, Germany. Kiesel, Dora; Riehmann, Patrick; Stein, Benno; Froehlich, Bernd, Bauhaus Univ Weimar, Weimar, Germany. Wachsmuth, Henning, Paderborn Univ, Paderborn, Germany.", "countries": "Germany", "abstract": "This paper presents a visual analytics system for exploring, analyzing and comparing argument structures in essay corpora. We provide an overview of the corpus by a list of ArguLines which represent the argument units of each essay by a sequence of glyphs. Each glyph encodes the stance, the depth and the relative position of an argument unit. The overview can be ordered in various ways to reveal patterns and outliers. Subsets of essays can be selected and analyzed in detail using the Argument Unit Occurrence Tree which aggregates the argument structures using hierarchical histograms. This hierarchical view facilitates the estimation of statistics and trends concerning the progression of the argumentation in the essays. It also provides insights into the commonalities and differences between selected subsets. The text view is the necessary textual basis to verify conclusions from the other views and the annotation process. Linking the views and interaction techniques for visual filtering, studying the evolution of stance within a subset of essays and scrutinizing the order of argumentative units enable a deep analysis of essay corpora. Our expert reviews confirmed the utility of the system and revealed detailed and previously unknown information about the argumentation in our sample corpus.", "keywords": "Information Visualization,Text Analysis,User Interfaces,Visual Analytics,Argumentation Visualization,Glyph-based Techniques,Text and Document Data,Tree-based Visualization,Coordinated and Multiple Views,Close and Distant Reading", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030425", "refList": ["10.1109/tvcg.2014.2346575", "10.14778/2735479.2735485", "10.14778/3192965.3192971", "10.1111/cgf.12129", "10.1145/2206869.2206874", "10.1109/icde.2016.7498300", "10.14778/2831360.2831371", "10.1145/3035918.3056097", "10.1023/a:1009726021843", "10.1111/cgf.13678", "10.1145/3183713.3196905", "10.14778/3115404.3115418", "10.1109/tvcg.2012.180", "10.1109/icde.1999.754950", "10.14778/1453856.1453924", "10.1145/3209900.3209901", "10.1002/spe.2325", "10.1145/42201.42203", "10.1109/tvcg.2013.124", "10.1109/vast.2008.4677357", "10.1109/icde.2016.7498287", "10.1109/icde.2014.6816674", "10.14778/2732951.2732964", "10.1109/tvcg.2015.2467551", "10.1145/1084805.1084812", "10.1109/2.781635", "10.14778/2732279.2732280", "10.1109/icde.2015.7113427", "10.1109/tvcg.2015.2467091", "10.1007/s00778-017-0486-1", "10.1109/tvcg.2003.1196005", "10.1109/icde.2004.1320035", "10.1109/tvcg.2016.2607714", "10.14778/2732951.2732953", "10.1109/tvcg.2008.131", "10.1109/tvcg.2018.2865018", "10.1145/2133806.2133821", "10.1109/icde.2019.00035", "10.1109/tpds.2005.144", "10.14778/3236187.3236212", "10.1109/tvcg.2009.111", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14033", "year": "2020", "title": "Survey on Individual Differences in Visualization", "conferenceName": "EuroVis", "authors": "Zhengliang Liu;R. Jordan Crouser;Alvitta Ottley", "citationCount": "0", "affiliation": "Liu, ZL (Corresponding Author), Washington Univ, St Louis, MO 63110 USA.\nLiu, Zhengliang; Ottley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nCrouser, R. Jordan, Smith Coll, Northampton, MA 01063 USA.", "countries": "USA", "abstract": "Developments in data visualization research have enabled visualization systems to achieve great general usability and application across a variety of domains. These advancements have improved not only people's understanding of data, but also the general understanding of people themselves, and how they interact with visualization systems. In particular, researchers have gradually come to recognize the deficiency of having one-size-fits-all visualization interfaces, as well as the significance of individual differences in the use of data visualization systems. Unfortunately, the absence of comprehensive surveys of the existing literature impedes the development of this research. In this paper, we review the research perspectives, as well as the personality traits and cognitive abilities, visualizations, tasks, and measures investigated in the existing literature. We aim to provide a detailed summary of existing scholarship, produce evidence-based reviews, and spur future inquiry.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14033", "refList": ["10.1145/3025171.3025192", "10.1037/0278-7393.29.2.298", "10.1145/2470654.2470707", "10.1177/001872086300500103", "10.1109/tvcg.2014.2346575", "10.1146/annurev-psych-113011-143750", "10.1016/j.sbspro.2011.11.312", "10.1177/1073191102092010", "10.1109/tvcg.2016.2598471", "10.1145/169891.169925", "10.1145/2856767.2856779", "10.4102/sajip.v29i1.88", "10.1145/3301275.3302307", "10.1109/mcg.2012.120", "10.1016/0749-596x(89)90040-5", "10.1111/cgf.12393", "10.3389/fpsyg.2018.00755", "10.1037/0033-2909.121.2.219", "10.2466/pms.1978.47.2.599", "10.1037/0022-3514.44.2.419", "10.1016/j.cub.2009.12.014", "10.1002/per.704", "10.1016/j.paid.2006.03.011", "10.1038/36846", "10.1145/3025453.3025877", "10.1037/0022-3514.93.5.880", "10.1002/per.469", "10.1002/tea.3660300407", "10.1037/h0076301", "10.1002/jocb.32", "10.1371/journal.pone.0131115", "10.1002/per.588", "10.1145/2449396.2449439", "10.1108/jmd-12-2013-0160", "10.1037/0022-3514.42.1.116", "10.1016/j.intell.2003.10.005", "10.1016/s0022-5371(80)90312-6", "10.3758/bf03214546", "10.1111/j.1467-8659.2009.01442.x", "10.1002/(sici)1099-0984(200003/04)14:2", "10.1145/2470654.2470696", "10.2190/vqjd-t1yd-5wvb-rypj", "10.1145/2702123.2702590", "10.1109/infvis.2005.1532136", "10.1037/0278-7393.29.4.611", "10.1145/3025453.3025577", "10.1016/0360-8352(91)90009-u", "10.1145/2556288.2557141", "10.1016/j.sbspro.2012.01.055", "10.1016/j.sbspro.2013.04.319", "10.1007/s11257-019-09244-5", "10.1016/j.tics.2013.06.006", "10.1037/a0037009", "10.1037/0003-066x.48.1.26", "10.1109/infvis.2004.70", "10.1037/0003-066x.45.4.489", "10.1037/h0092976", "10.1177/1473871612441542", "10.1145/302979.303030", "10.1016/b978-0-12-386915-9.00003-6", "10.1080/08870449608404995", "10.1109/tvcg.2013.156", "10.1145/2110192.2110202", "10.3758/cabn.2.4.341", "10.1037/0096-1523.27.1.92", "10.1109/tvcg.2007.70515", "10.1145/1385569.1385602", "10.24963/ijcai.2017/217", "10.1080/135467897394419", "10.1007/978-3-642-31454-4\\_23", "10.1037/a0021016", "10.1109/tvcg.2014.2346452", "10.1145/3301275.3302313", "10.1145/2633043", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1109/vl.1996.545307", "10.1037/0033-2909.119.2.197", "10.1523/jneurosci.2145-09.2009", "10.1177/1473871613513227", "10.1037/0022-3514.51.4.875", "10.1109/mcg.2009.49", "10.1002/acp.1344", "10.1037/a0016127", "10.1037/1040-3590.18.2.192", "10.1111/j.1467-8659.2011.01928.x", "10.1016/s0140-6736(02)07441-x", "10.1037/0022-0663.96.3.471", "10.1207/s15327752jpa4803\\_13", "10.1007/s10798-018-9446-3", "10.1002/(sici)1097-4571(2000)51:6", "10.1109/vast.2009.5333468", "10.1109/mcg.2009.22", "10.1145/3079628.3079634", "10.1016/j.jrp.2014.05.003", "10.1016/s0079-6123(07)00020-9", "10.1002/ijop.12511", "10.1111/j.2044-8279.1982.tb00821.x", "10.5539/jel.v4n4p91", "10.1177/1473871615594652", "10.1037/0022-3514.66.5.950", "10.1037/h0042761", "10.1109/tvcg.2012.199", "10.1177/1046496403257228", "10.1080/13614569708914684", "10.1111/cgf.13678", "10.1016/j.lindif.2003.08.001", "10.1016/j.jecp.2009.11.003", "10.1109/tvcg.2014.2346984", "10.1109/vast.2011.6102445", "10.1145/2678025.2701376", "10.1057/ivs.2008.31", "10.1533/9781780630366", "10.1111/j.2044-8260.1992.tb00972.x", "10.1016/j.jrp.2011.12.003", "10.1016/0747-5632(91)90002-i", "10.1080/13546781343000222", "10.3758/s13414-013-0610-2", "10.1145/3301275.3302283", "10.1177/106907279300100107", "10.1109/vast.2012.6400535", "10.1145/3377325.3377502", "10.1037/1089-2699.10.4.249", "10.1016/j.paid.2003.08.018", "10.1016/j.paid.2010.09.015"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 13}, {"doi": "10.1111/cgf.13444", "year": "2018", "title": "Information Visualization Evaluation Using Crowdsourcing", "conferenceName": "EuroVis", "authors": "Rita Borgo;Luana Micallef;Benjamin Bach;Fintan McGee;Bongshin Lee", "citationCount": "10", "affiliation": "Borgo, R (Corresponding Author), Kingss Coll London, Dept Informat, London, England.\nBorgo, R., Kingss Coll London, Dept Informat, London, England.\nMicallef, L., Aalto Univ, Dept Comp Sci, Espoo, Finland.\nBach, B., Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.\nMcGee, F., LIST, Environm Informat Unit, Luxembourg, Luxembourg.\nLee, B., Microsoft Res, Redmond, WA USA.", "countries": "USA;Finland;Luxembourg;Scotland;England", "abstract": "Visualization researchers have been increasingly leveraging crowdsourcing approaches to overcome a number of limitations of controlled laboratory experiments, including small participant sample sizes and narrow demographic backgrounds of study participants. However, as a community, we have little understanding on when, where, and how researchers use crowdsourcing approaches for visualization research. In this paper, we review the use of crowdsourcing for evaluation in visualization research. We analyzed 190 crowdsourcing experiments, reported in 82 papers that were published in major visualization conferences and journals between 2006 and 2017. We tagged each experiment along 36 dimensions that we identified for crowdsourcing experiments. We grouped our dimensions into six important aspects: study design \\& procedure, task type, participants, measures \\& metrics, quality assurance, and reproducibility. We report on the main findings of our review and discuss challenges and opportunities for improvements in conducting crowdsourcing studies for visualization research.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13444", "refList": ["10.1145/2631775.2631819", "10.1111/cgf.13195", "10.1109/tvcg.2014.2346575", "10.1145/1453152.1453156", "10.1145/2858036.2858063", "10.1111/cgf.12634", "10.1145/2835776.2835835", "10.1109/tvcg.2015.2467759", "10.1145/2858036.2858440", "10.1163/156856897x00357", "10.1145/2047196.2047199", "10.1145/2702123.2702545", "10.1145/3119930", "10.1145/2702123.2702594", "10.1109/tvcg.2013.247", "10.1145/3025453.3025917", "10.1145/2207676.2208556", "10.1007/978-3-319-66435-4\\_5", "10.1109/tvcg.2010.186", "10.1145/1746259.1746260", "10.1177/1071181311551228", "10.1518/155723409x448017", "10.1111/cgf.12233", "10.1109/tvcg.2015.2467671", "10.1109/tvcg.2016.2598862", "10.1145/2063576.2063860", "10.1145/3025453.3025477", "10.1145/2470654.2481281", "10.1145/1869086.1869094", "10.5194/isprsannals-ii-3-w5-325-2015", "10.1145/2858036.2858280", "10.1111/cgf.12127", "10.1109/tvcg.2015.2465151", "10.1145/2591677", "10.1109/iv.2014.47", "10.1109/cce.2014.6916756", "10.1109/vast.2011.6102470", "10.1109/tvcg.2012.234", "10.1145/2702123.2702443", "10.1145/3025453.3025781", "10.1109/tvcg.2013.234", "10.1109/tvcg.2017.2746018", "10.1109/tvcg.2015.2467758", "10.1109/passat/socialcom.2011.203", "10.1145/989863.989880", "10.1111/cgf.12387", "10.1109/tvcg.2015.2500240", "10.1109/vast.2012.6400540", "10.1109/tvcg.2016.2599058", "10.1145/2702123.2702608", "10.1145/2660398.2660403", "10.1109/tvcg.2014.2346979", "10.1145/2702123.2702452", "10.1145/2598153.2598168", "10.1109/pacificvis.2016.7465249", "10.1145/2858036.2858107", "10.1145/3025453.3025820", "10.1109/tvcg.2014.2315995", "10.1111/j.1467-8659.2009.01442.x", "10.1109/tvcg.2015.2467451", "10.1145/2675133.2675246", "10.1145/2598153.2602248", "10.1109/tvcg.2015.2468151", "10.1145/3027063.3053113", "10.1007/978-3-319-20267-9\\_14", "10.1145/2928269", "10.1109/tvcg.2014.2346320", "10.1145/2468356.2468529", "10.1145/2810012", "10.1109/vast.2010.5652890", "10.1364/josa.35.000268", "10.1109/tvcg.2017.2674978", "10.1111/cgf.12657", "10.1145/2441776.2441912", "10.1109/tvcg.2013.164", "10.1109/tvcg.2016.2598591", "10.1371/journal.pbio.1002456", "10.1145/2818048.2820005", "10.1145/1837885.1837889", "10.1145/2598153.2602249", "10.1109/tvcg.2012.230", "10.1145/3025453.3025870", "10.1145/2470654.2481410", "10.1117/12.2076745", "10.1109/tvcg.2015.2467201", "10.1145/2441776.2441847", "10.1145/2470654.2466424", "10.1109/vl.1996.545307", "10.1145/3025453.3025592", "10.1007/s10791-012-9205-0", "10.1145/2858036.2858300", "10.1145/2488388.2488489", "10.1109/tvcg.2010.174", "10.1109/pacificvis.2012.6183570", "10.1111/cgf.13018", "10.1109/tvcg.2011.279", "10.1145/3025453.3025922", "10.1145/3131275", "10.1007/978-3-319-66435-4\\_2", "10.1109/tvcg.2012.220", "10.1145/2598153.2602225", "10.1109/tvcg.2013.183", "10.1145/1314683.1314684", "10.1109/tvcg.2012.236", "10.1145/2810188.2810192", "10.1109/tvcg.2012.196", "10.1109/tvcg.2016.2599106", "10.1109/t-affc.2012.19", "10.1145/2556288.2557379", "10.1109/vast.2014.7042496", "10.1109/tvcg.2012.199", "10.1109/pacificvis.2017.8031598", "10.1145/2669557.2669564", "10.1145/2207676.2207709", "10.1007/978-3-540-70956-5\\_2", "10.1016/j.jebo.2013.03.003", "10.1111/cgf.13009", "10.1109/mcg.2017.23", "10.1109/vast.2011.6102445", "10.1007/978-3-319-66435-4\\_4", "10.1145/3025453.3025969", "10.1145/2858036.2858101", "10.1109/tvcg.2008.155", "10.1111/cgf.12888", "10.1145/2598153.2598182", "10.1109/tvcg.2013.119", "10.1109/tvcg.2014.2346978", "10.1007/978-3-319-66435-4\\_3", "10.1145/2858036.2858237", "10.1177/0272989x07304449", "10.1109/qomex.2012.6263866", "10.1145/2858036.2858558"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934397", "title": "A Comparative Evaluation of Animation and Small Multiples for Trend Visualization on Mobile Phones", "year": "2019", "conferenceName": "InfoVis", "authors": "Matthew Brehmer;Bongshin Lee;Petra Isenberg;Eun Kyoung Choe", "citationCount": "3", "affiliation": "Brehmer, M (Corresponding Author), Microsoft Res, Redmond, WA 98052 USA. Brehmer, Matthew; Lee, Bongshin, Microsoft Res, Redmond, WA 98052 USA. Isenberg, Petra, INRIA, Rocquencourt, France. Choe, Eun Kyoung, Univ Maryland, College Pk, MD 20742 USA.", "countries": "USA;France", "abstract": "We compare the efficacy of animated and small multiples variants of scatterplots on mobile phones for comparing trends in multivariate datasets. Visualization is increasingly prevalent in mobile applications and mobile-first websites, yet there is little prior visualization research dedicated to small displays. In this paper, we build upon previous experimental research carried out on larger displays that assessed animated and non-animated variants of scatterplots. Incorporating similar experimental stimuli and tasks, we conducted an experiment where 96 crowdworker participants performed nine trend comparison tasks using their mobile phones. We found that those using a small multiples design consistently completed tasks in less time, albeit with slightly less confidence than those using an animated design. The accuracy results were more task-dependent, and we further interpret our results according to the characteristics of the individual tasks, with a specific focus on the trajectories of target and distractor data items in each task. We identify cases that appear to favor either animation or small multiples, providing new questions for further experimental research and implications for visualization design on mobile devices. Lastly, we provide a reflection on our evaluation methodology.", "keywords": "Evaluation,graphical perception,mobile phones,trend visualization,animation,small multiples,crowdsourcing", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934397", "refList": ["10.1109/mcg.2015.113", "10.1111/cgf.12106", "10.1109/tvcg.2011.185", "10.1111/cgf.13444", "10.1145/2786567.2786571", "10.1109/tvcg.2013.254", "10.1007/978-3-319-68517-5\\_1", "10.1145/3123266.3123274", "10.1007/978-3-319-66435-4\\_5", "10.1109/tvcg.2018.2865234", "10.1111/j.1467-8659.2012.03093.x", "10.1145/1133265.1133364", "10.1145/2470654.2481318", "10.1145/2993901.2993906", "10.1145/3290605.3300786", "10.1109/tvcg.2016.2598876", "10.2307/2288400", "10.1038/nmeth.2659", "10.1109/tvcg.2018.2865142", "10.1167/7.13.14", "10.1109/tvcg.2008.125", "10.1145/3025453.3025752", "10.1109/tvcg.2015.2502587", "10.1109/tvcg.2010.78", "10.1145/2396636.2396675", "10.1007/978-3-319-26633-6\\_13", "10.1145/2858036.2858558", "10.1145/3290605.3300771"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030423", "title": "MobileVisFixer: Tailoring Web Visualizations for Mobile Phones Leveraging an Explainable Reinforcement Learning Framework", "year": "2020", "conferenceName": "InfoVis", "authors": "Aoyu Wu;Wai Tong;Tim Dwyer;Bongshin Lee;Petra Isenberg;Huamin Qu", "citationCount": "0", "affiliation": "Wu, AY (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wu, Aoyu; Tong, Wai; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Lee, Bongshin, Microsoft Res, Redmond, WA USA. Isenberg, Petra, INRIA, Le Chesnay Rocquencourt, France.", "countries": "USA;China;France;Australia", "abstract": "We contribute MobileVisFixer, a new method to make visualizations more mobile-friendly. Although mobile devices have become the primary means of accessing information on the web, many existing visualizations are not optimized for small screens and can lead to a frustrating user experience. Currently, practitioners and researchers have to engage in a tedious and time-consuming process to ensure that their designs scale to screens of different sizes, and existing toolkits and libraries provide little support in diagnosing and repairing issues. To address this challenge, MobileVisFixer automates a mobile-friendly visualization re-design process with a novel reinforcement learning framework. To inform the design of MobileVisFixer, we first collected and analyzed SVG-based visualizations on the web, and identified five common mobile-friendly issues. MobileVisFixer addresses four of these issues on single-view Cartesian visualizations with linear or discrete scales by a Markov Decision Process model that is both generalizable across various visualizations and fully explainable. MobileVisFixer deconstructs charts into declarative formats, and uses a greedy heuristic based on Policy Gradient methods to find solutions to this difficult, multi-criteria optimization problem in reasonable time. In addition, MobileVisFixer can be easily extended with the incorporation of optimization algorithms for data visualizations. Quantitative evaluation on two real-world datasets demonstrates the effectiveness and generalizability of our method.", "keywords": "Mobile visualization,Responsive visualization,Machine learning for visualizations,Reinforcement learning", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030423", "refList": ["10.1109/mcg.2014.82", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2012.196", "10.1145/3092703.3092726", "10.1109/pimrc.2015.7343276", "10.1109/tvcg.2014.48", "10.1145/3025453.3025957", "10.1145/2047196.2047247", "10.1145/3173574.3174168", "10.1109/icst.2019.00027", "10.1109/tvcg.2007.70594", "10.1145/3197517.3201311", "10.1145/3126594.3126653", "10.1109/mcg.2019.2924636", "10.1145/3180155.3180262", "10.1109/tvcg.2017.2659744", "10.1145/3123266.3123274", "10.1109/tvcg.2019.2934538", "10.1109/cvpr.2018.00592", "10.1109/tvcg.2018.2865138", "10.1109/tsmcc.2012.2218595", "10.1109/tvcg.2018.2865240", "10.1111/cgf.13193", "10.1145/2775441.2775478", "10.1145/3313831.3376777", "10.1109/mc.2006.109", "10.1111/cgf.13686", "10.1109/tvcg.2015.2467091", "10.4230/dagrep.9.7.78", "10.1007/978-3-319-71249-9\\_9", "10.1007/s00778-019-00588-3", "10.1109/ase.2015.31", "10.1007/bf02124742", "10.1109/tvcg.2019.2934397", "10.1109/adprl.2007.368196", "10.1109/mcg.2020.2968244", "10.1145/3092703.3092712", "10.1109/tvcg.2017.2744320", "10.1023/a:1017992615625", "10.1145/2642918", "10.1145/2858036.2858558", "10.1145/3290605.3300358", "10.1109/tvcg.2019.2934431", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030406", "title": "Palettailor: Discriminable Colorization for Categorical Data", "year": "2020", "conferenceName": "InfoVis", "authors": "Kecheng Lu;Mi Feng;Xin Chen;Michael Sedlmair;Oliver Deussen;Dani Lischinski;Zhanglin Cheng;Yunhai Wang", "citationCount": "0", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Jinan, Peoples R China. Cheng, ZL (Corresponding Author), SIAT, Shenzhen VesuCA Key Lab, Shenzhen, Peoples R China. Lu, Kecheng; Chen, Xin; Wang, Yunhai, Shandong Univ, Jinan, Peoples R China. Feng, Mi, Twitter Inc, San Francisco, CA USA. Lu, Kecheng; Deussen, Oliver; Cheng, Zhanglin, SIAT, Shenzhen VesuCA Key Lab, Shenzhen, Peoples R China. Sedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany. Deussen, Oliver, Konstanz Univ, Constance, Germany. Lischinski, Dani, Hebrew Univ Jerusalem, Jerusalem, Israel.", "countries": "USA;Israel;Germany;China", "abstract": "We present an integrated approach for creating and assigning color palettes to different visualizations such as multi-class scatterplots, line, and bar charts. While other methods separate the creation of colors from their assignment, our approach takes data characteristics into account to produce color palettes, which are then assigned in a way that fosters better visual discrimination of classes. To do so, we use a customized optimization based on simulated annealing to maximize the combination of three carefully designed color scoring functions: point distinctness, name difference, and color discrimination. We compare our approach to state-of-the-art palettes with a controlled user study for scatterplots and line charts, furthermore we performed a case study. Our results show that Palettailor, as a fully-automated approach, generates color palettes with a higher discrimination quality than existing approaches. The efficiency of our optimization allows us also to incorporate user modifications into the color selection process.", "keywords": "Color Palette,Discriminability,Multi-Class Scatterplot,Line Chart,Bar Chart", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030406", "refList": ["10.1109/mcg.2014.82", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2012.196", "10.1145/3092703.3092726", "10.1109/tvcg.2014.48", "10.1145/3025453.3025957", "10.1145/2047196.2047247", "10.1145/3173574.3174168", "10.1109/icst.2019.00027", "10.1109/tvcg.2007.70594", "10.1145/3197517.3201311", "10.1145/3126594.3126653", "10.1109/mcg.2019.2924636", "10.1145/3180155.3180262", "10.1109/tvcg.2017.2659744", "10.1145/3123266.3123274", "10.1109/tvcg.2018.2865234", "10.1109/tvcg.2019.2934538", "10.1109/cvpr.2018.00592", "10.1145/2815833.2816956", "10.1109/tvcg.2018.2865138", "10.1109/tsmcc.2012.2218595", "10.1109/tvcg.2018.2865240", "10.1111/cgf.13193", "10.1145/2775441.2775478", "10.1145/3313831.3376777", "10.1109/mc.2006.109", "10.1111/cgf.13686", "10.1109/tvcg.2015.2467091", "10.4230/dagrep.9.7.78", "10.1007/978-3-319-71249-9\\_9", "10.1007/s00778-019-00588-3", "10.1109/ase.2015.31", "10.1007/bf02124742", "10.1145/3025453.3025768", "10.1109/tvcg.2019.2934397", "10.1109/adprl.2007.368196", "10.1109/mcg.2020.2968244", "10.1145/3092703.3092712", "10.1109/tvcg.2017.2744320", "10.1023/a:1017992615625", "10.1145/2642918", "10.1145/2858036.2858558", "10.1145/3290605.3300358", "10.1109/tvcg.2019.2934431", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030385", "title": "Staged Animation Strategies for Online Dynamic Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Tarik Crnovrsanin;Shilpika;Senthil K. Chandrasegaran;Kwan-Liu Ma", "citationCount": "0", "affiliation": "Crnovrsanin, T (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA. Crnovrsanin, Tarik; Shilpika; Chandrasegaran, Senthil; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "Dynamic networks-networks that change over time-can be categorized into two types: offline dynamic networks, where all states of the network are known, and online dynamic networks, where only the past states of the network are known. Research on staging animated transitions in dynamic networks has focused more on offline data, where rendering strategies can take into account past and future states of the network. Rendering online dynamic networks is a more challenging problem since it requires a balance between timeliness for monitoring tasks-so that the animations do not lag too far behind the events-and clarity for comprehension tasks-to minimize simultaneous changes that may be difficult to follow. To illustrate the challenges placed by these requirements, we explore three strategies to stage animations for online dynamic networks: time-based, event-based, and a new hybrid approach that we introduce by combining the advantages of the first two. We illustrate the advantages and disadvantages of each strategy in representing low- and high-throughput data and conduct a user study involving monitoring and comprehension of dynamic networks. We also conduct a follow-up, think-aloud study combining monitoring and comprehension with experts in dynamic network visualization. Our findings show that animation staging strategies that emphasize comprehension do better for participant response times and accuracy. However, the notion of \u201ccomprehension\u201d is not always clear when it comes to complex changes in highly dynamic networks, requiring some iteration in staging that the hybrid approach affords. Based on our results, we make recommendations for balancing event-based and time-based parameters for our hybrid approach.", "keywords": "Dynamic networks,graph visualization,animation,mental map,user study", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030385", "refList": ["10.1109/infvis.2004.18", "10.1109/tvcg.2008.141", "10.1017/s0033291717001751", "10.1109/tvcg.2014.2346424", "10.1007/978-3-319-73915-1\\_31", "10.1145/2702123.27023123", "10.1111/cgf.12791", "10.1109/tvcg.2011.226", "10.1145/985692.985748", "10.1109/tvcg.2008.11", "10.1007/s10654-016-0149-3", "10.1109/tvcg.2011.185", "10.1145/1143518.1143521", "10.1109/tvcg.2011.213", "10.1109/tvcg.2013.254", "10.1006/ijhc.1017", "10.1109/tvcg.2011.169", "10.1111/j.1467-8659.2012.03093.x", "10.1016/j.ins.2015.04.017", "10.1057/ivs.2010.10", "10.1145/2576099", "10.1145/2858036.2858387", "10.1007/3-540-31190-4", "10.1109/tvcg.2013.238", "10.1093/aje/kwx259", "10.1145/3290607.3310432", "10.1109/visual.2019.8933748", "10.1109/tvcg", "10.1007/1155526124", "10.1145/1165734.1165736", "10.1080/00031305.2016.1154108", "10.1109/tvcg.2016.2592906", "10.1002/spe.4380211102", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2006.166", "10.1109/tvcg.2018.2886901", "10.1109/tvcg.2019.2934397", "10.1109/tvcg.2015.2467751", "10.1109/wsc.2003.1261490", "10.1109/tvcg.2010.78", "10.1109/mc.2018.2890217", "10.1007/3", "10.1038/sdata.2014.56", "10.1111/cgf.12804", "10.1111/j.1467-8659.2009.01687.x", "10.2312/eurovisshort.20141149", "10.1109/tvcg.2013.205"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030396", "title": "What Makes a Data-GIF Understandable?", "year": "2020", "conferenceName": "InfoVis", "authors": "Xinhuan Shu;Aoyu Wu;Junxiu Tang;Benjamin Bach;Yingcai Wu;Huamin Qu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Shu, Xinhuan; Tang, Junxiu; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Shu, Xinhuan; Wu, Aoyu; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Shu, Xinhuan, Zhejiang Univ, Hangzhou, Peoples R China. Bach, Benjamin, Univ Edinburgh, Edinburgh, Midlothian, Scotland.", "countries": "Scotland;China", "abstract": "GIFs are enjoying increasing popularity on social media as a format for data-driven storytelling with visualization; simple visual messages are embedded in short animations that usually last less than 15 seconds and are played in automatic repetition. In this paper, we ask the question, \u201cWhat makes a data-GIF understandable?\u201d While other storytelling formats such as data videos, infographics, or data comics are relatively well studied, we have little knowledge about the design factors and principles for \u201cdata-GIFs\u201d. To close this gap, we provide results from semi-structured interviews and an online study with a total of 118 participants investigating the impact of design decisions on the understandability of data-GIFs. The study and our consequent analysis are informed by a systematic review and structured design space of 108 data-GIFs that we found online. Our results show the impact of design dimensions from our design space such as animation encoding, context preservation, or repetition on viewers understanding of the GIF's core message. The paper concludes with a list of suggestions for creating more effective Data-GIFs.", "keywords": "Data-GIFs,Data-driven Storytelling,Evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030396", "refList": ["10.1109/tvcg.2016.2598647", "10.1016/j.visinf.2020.07.001", "10.1177/1473871615594652", "10.1109/tvcg.2014.2346424", "10.1111/cgf.13195", "10.1109/vlsicircuits18222.2020.9162811", "10.1109/tvcg.2018.2864909", "10.1109/tvcg.2018.2864903", "10.1145/2702123.2702431", "10.1016/j.visinf.2019.12.002", "10.1145/3274349", "10.1111/cgf.13444", "10.1145/3206505.3206552", "10.1145/3290605.3300280", "10.1109/jstqe.2020.3021589", "10.1145/2647868.2656408", "10.1006/ijhc.1017", "10.1145/3290605.3300335", "10.1007/s12650-020-00689-0", "10.1145/2818048.2819936", "10.1111/cgf.13325", "10.1145/2858036.2858387", "10.1145/3027063.3053139", "10.1145/3290605.3300474", "10.1109/tvcg.2016.2598920", "10.1109/tvcg.2018.2864899", "10.1145/3290605.3300483", "10.1145/3173574.3173612", "10.1145/2909132.2909255", "10.1109/tvcg.2016.2598620", "10.1016/j.learninstruc.2007.09.013", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2008.125", "10.1145/3173574.3173909", "10.1109/tvcg.2019.2934397", "10.1111/cgf.13709", "10.1109/tvcg.2013.234", "10.1109/tvcg.2019.2934401", "10.1145/2858036.2858532", "10.1016/j.visinf.2020.08.001", "10.1109/tvcg.2010.179", "10.1109/cicc48029.2020.9075900"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934784", "title": "A Comparison of Radial and Linear Charts for Visualizing Daily Patterns", "year": "2019", "conferenceName": "InfoVis", "authors": "Manuela Waldner;Alexandra Diehl;Denis Gracanin;Rainer Splechtna;Claudio Delrieux;Kresimir Matkovic", "citationCount": "0", "affiliation": "Waldner, M (Corresponding Author), TU Wien, Vienna, Austria. Waldner, Manuela, TU Wien, Vienna, Austria. Diehl, Alexandra, Univ Zurich, Zurich, Switzerland. Gracanin, Denis, Virginia Tech, Blacksburg, VA USA. Splechtna, Rainer; Matkovic, Kresimir, VRVis Res Ctr, Vienna, Austria. Delrieux, Claudio, Univ Nacl Sur, Elect \\& Comp Eng Dept, Bahia Blanca, Buenos Aires, Argentina.", "countries": "Argentina;Switzerland;USA;Austria", "abstract": "Radial charts are generally considered less effective than linear charts. Perhaps the only exception is in visualizing periodical time-dependent data, which is believed to be naturally supported by the radial layout. It has been demonstrated that the drawbacks of radial charts outweigh the benefits of this natural mapping. Visualization of daily patterns, as a special case, has not been systematically evaluated using radial charts. In contrast to yearly or weekly recurrent trends, the analysis of daily patterns on a radial chart may benefit from our trained skill on reading radial clocks that are ubiquitous in our culture. In a crowd-sourced experiment with 92 non-expert users, we evaluated the accuracy, efficiency, and subjective ratings of radial and linear charts for visualizing daily traffic accident patterns. We systematically compared juxtaposed 12-hours variants and single 24-hours variants for both layouts in four low-level tasks and one high-level interpretation task. Our results show that over all tasks, the most elementary 24-hours linear bar chart is most accurate and efficient and is also preferred by the users. This provides strong evidence for the use of linear layouts \u2013 even for visualizing periodical daily patterns.", "keywords": "Radial charts,time series series data,daily patterns,crowd-sourced experiment", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934784", "refList": ["10.1109/infvis.2000.885091", "10.1109/tvcg.2018.2865158", "10.1177/1473871611406623", "10.1111/cgf.13444", "10.1038/scientificamerican0384-128", "10.1109/tvcg.2018.2865234", "10.1109/tvcg.2015.2467771", "10.1109/tvcg.2018.2865077", "10.1109/tvcg.2013.184", "10.1109/tvcg.2014.2346320", "10.1145/2858036.2858300", "10.1109/tvcg.2017.2674958", "10.1109/iv.2013.12", "10.1145/2470654.2466443", "10.1007/s10763-012-9362-z", "10.1207/s15427625tcq1402\\_3", "10.2307/2288400", "10.1136/jnnp.64.5.588", "10.1109/tvcg.2014.2346426", "10.1109/infvis.1998.729557", "10.1007/bf03217308", "10.1145/506443.506505", "10.1109/infvis.2001.963273", "10.1111/j.1467-8659.2011.01947.x", "10.1109/tvcg.2013.234", "10.1109/38.974517", "10.2312/pe/eurovisshort/eurovisshort2012/097-101", "10.1145/1743546.1743567", "10.1109/tvcg.2010.162", "10.2307/2289447", "10.1145/2110192.2110202", "10.1109/tvcg.2009.23", "10.3102/10769986030004353", "10.1109/tvcg.2010.209"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3028891", "title": "A Structured Review of Data Management Technology for Interactive Visualization and Analysis", "year": "2020", "conferenceName": "InfoVis", "authors": "Leilani Battle;Carlos Scheidegger", "citationCount": "0", "affiliation": "Battle, L (Corresponding Author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA. Battle, Leilani, Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA. Scheidegger, Carlos, Univ Arizona, Dept Comp Sci, HDC Lab, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "In the last two decades, interactive visualization and analysis have become a central tool in data-driven decision making. Concurrently to the contributions in data visualization, research in data management has produced technology that directly benefits interactive analysis. Here, we contribute a systematic review of 30 years of work in this adjacent field, and highlight techniques and principles we believe to be underappreciated in visualization work. We structure our review along two axes. First, we use task taxonomies from the visualization literature to structure the space of interactions in usual systems. Second, we created a categorization of data management work that strikes a balance between specificity and generality. Concretely, we contribute a characterization of 131 research papers along these two axes. We find that five notions in data management venues fit interactive visualization systems well: materialized views, approximate query processing, user modeling and query prediction, muiti-query optimization, lineage techniques, and indexing techniques. In addition, we find a preponderance of work in materialized views and approximate query processing, most targeting a limited subset of the interaction tasks in the taxonomy we used. This suggests natural avenues of future research both in visualization and data management. Our categorization both changes how we visualization researchers design and build our systems, and highlights where future work is necessary.", "keywords": "", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028891", "refList": ["10.1109/tvcg.2012.233", "10.1016/s0022-5371(74)80015-0", "10.1109/tvcg.2017.2744359", "10.1037/0096-3445.136.4.623", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2012.196", "10.1037/a0029856", "10.1109/tvcg.2014.2346979", "10.1037/h0030300", "10.1109/tvcg.2016.2598918", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2018.2864909", "10.1111/cgf.13079", "10.3389/fpsyg.2012.00355", "10.1145/2858036.2858465", "10.1080/01621459.1989.10478821", "10.1037/0278-7393.24.3.732", "10.1109/tvcg.2011.127", "10.1145/2858036.2858063", "10.4249/scholarpedia.3325", "10.4324/9781410611949", "10.1111/cgf.13444", "10.1145/2993901.2993909", "10.1037/0033-295x.96.2.267", "10.1006/ijhc.1017", "10.1086/405615", "10.1109/tvcg.2019.2934801", "10.1038/17953", "10.1037/xhp0000314", "10.1109/tvcg.2019.2934400", "10.1145/2470654.2470723", "10.1037/0096-1523.16.2.332", "10.1167/16.5.11", "10.3758/s13423-016-1174-7", "10.3758/bf03207704", "10.1146/annurev.psych.55.090902.141415", "10.2307/2288400", "10.3758/bf03204258", "10.1109/tvcg.2011.279", "10.1109/vissoft.2014.36", "10.3758/s13423-011-0055-3", "10.1145/3025453.3025922", "10.1109/tvcg.2019.2934284", "10.3758/bf03210498", "10.3758/bf03200774", "10.2307/1419876", "10.1038/s41562-017-0058", "10.1109/tvcg.2010.237", "10.1109/pacificvis.2012.6183556", "10.1109/infvis.1997.636792", "10.1093/acprof:oso/9780198523192.003.0005", "10.1073/pnas.1117465109", "10.1109/tvcg.2013.234", "10.1038/nn.3655", "10.1111/cgf.12379", "10.1146/annurev-psych-010416-044232", "10.1111/cgf.13695", "10.1037/0033-295x.107.3.500", "10.1109/tvcg.2013.183", "10.1146/annurev.psych.53.100901.135125", "10.1037//0022-3514.79.6.995", "10.1559/152304003100010929", "10.1109/tvcg.2018.2865147", "10.1037/0096-1523.18.3.849", "10.1111/j.1467-8659.2009.01694.x", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030440", "title": "Context-aware Sampling of Large Networks via Graph Representation Learning", "year": "2020", "conferenceName": "InfoVis", "authors": "Zhiguang Zhou;Chen Shi;Xilong Shen;Lihong Cai;Haoxuan Wang;Yuhua Liu;Ying Zhao;Wei Chen", "citationCount": "0", "affiliation": "Zhao, Y (Corresponding Author), Cent South Univ, Changsha, Peoples R China. Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China. Zhou, Zhiguang; Shi, Chen; Shen, Xilong; Cai, Lihong; Wang, Haoxuan; Liu, Yuhua, Zhejiang Univ Finance \\& Econ, Sch Informat, Hangzhou, Peoples R China. Zhao, Ying, Cent South Univ, Changsha, Peoples R China. Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou, Peoples R China.", "countries": "China", "abstract": "Numerous sampling strategies have been proposed to simplify large-scale networks for highly readable visualizations. It is of great challenge to preserve contextual structures formed by nodes and edges with tight relationships in a sampled graph, because they are easily overlooked during the process of sampling due to their irregular distribution and immunity to scale. In this paper, a new graph sampling method is proposed oriented to the preservation of contextual structures. We first utilize a graph representation learning (GRL) model to transform nodes into vectors so that the contextual structures in a network can be effectively extracted and organized. Then, we propose a multi-objective blue noise sampling model to select a subset of nodes in the vectorized space to preserve contextual structures with the retention of relative data and cluster densities in addition to those features of significance, such as bridging nodes and graph connections. We also design a set of visual interfaces enabling users to interactively conduct context-aware sampling, visually compare results with various sampling strategies, and deeply explore large networks. Case studies and quantitative comparisons based on real-world datasets have demonstrated the effectiveness of our method in the abstraction and exploration of large networks.", "keywords": "Graph sampling,Graph representation learning,Blue noise sampling,Graph evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030440", "refList": ["10.1145/2491159.2491168", "10.1016/j.physa.2015.04.035", "10.1145/1830252.1830274", "10.1109/icdmw.2007.91", "10.1002/net.21834", "10.1109/tvcg.2018.2864503", "10.1016/j.cag.2018.01.010", "10.1109/icc.2016.7511156", "10.1111/cgf.13444", "10.1145/956750.956831", "10.1145/364099.364331", "10.1007/s00180-016-0663-5", "10.1109/tvcg.2013.223", "10.1007/s12650-018-0530-2", "10.1103/physreve.73.016102", "10.1109/access.2018.2870684", "10.1007/978-3-319-06793-3\\_1", "10.2312/vissym/eurovis05/239-246", "10.1016/j.ins.2015.02.014", "10.1145/2339530.2339723", "10.1109/icde.2015.7113345", "10.1109/tvcg.2011.233", "10.14778/2809974.2809980", "10.1109/glocom.2015.7417471", "10.1145/2578153.2578175", "10.1214/aoms/1177705148", "10.1109/tvcg.2008.130", "10.14232/actacyb.20.1.2011.6", "10.1504/ijitm.2019.099809", "10.1109/tvcg.2018.2865020", "10.1145/956750", "10.1002/cpe.4330060203", "10.1145/1150402.1150479", "10.1103/physreve.72.036118", "10.1109/tvcg.2017.2744098", "10.1145/2020408.2020512", "10.1142/s0129183114400075", "10.1109/jsac.2011.111005", "10.1016/j.camwa.2011.11.057", "10.1145/2470654.2466444", "10.1109/tvcg.2017.2674999", "10.1214/aos/1013203451", "10.1109/icdcsw.2011.34", "10.1016/j.physa.2013.11.015", "10.1145/1081870.1081893", "10.1109/tnet.2008.2001730", "10.1109/access.2016.2633485", "10.1145/1879141.1879192", "10.1371/journal.pone.0098679", "10.1126/science.220.4598.671", "10.1109/pacificvis.2015.7156355", "10.1088/1475-7516/2011/08/011", "10.1007/978-3-319-27261-0\\_41", "10.1111/cgf.13410", "10.1109/tvcg.2018.2865139", "10.1109/tvcg.2016.2598831", "10.1016/j.physa.2014.06.065"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030365", "title": "Modeling the Influence of Visual Density on Cluster Perception in Scatterplots Using Topology", "year": "2020", "conferenceName": "InfoVis", "authors": "Ghulam Jilani Quadri;Paul Rosen", "citationCount": "0", "affiliation": "Quadri, GJ (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Quadri, Ghulam Jilani; Rosen, Paul, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "Scatterplots are used for a variety of visual analytics tasks, including cluster identification, and the visual encodings used on a scatterplot play a deciding role on the level of visual separation of clusters. For visualization designers, optimizing the visual encodings is crucial to maximizing the clarity of data. This requires accurately modeling human perception of cluster separation, which remains challenging. We present a multi-stage user study focusing on four factors-distribution size of clusters, number of points, size of points, and opacity of points-that influence cluster identification in scatterplots. From these parameters, we have constructed two models, a distance-based model, and a density-based model, using the merge tree data structure from Topological Data Analysis. Our analysis demonstrates that these factors play an important role in the number of clusters perceived, and it verifies that the distance-based and density-based models can reasonably estimate the number of clusters a user observes. Finally, we demonstrate how these models can be used to optimize visual encodings on real-world data.", "keywords": "Scatterplot,clustering,perception,empirical evaluation,visual encoding,crowdsourcing,topological data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030365", "refList": ["10.1109/tvcg.2017.2744359", "10.1145/1778765", "10.1109/tvcg.2019.2934799", "10.1111/cgf.12889", "10.1109/tvcg.2011.127", "10.1111/cgf.13444", "10.1145/1964897.1964910", "10.1167/8.7.6", "10.1145/3173574.3173991", "10.1145/1556262.1556289", "10.1145/1056808.1056914", "10.2307/2288400", "10.1109/tvcg.2014.2346594", "10.1109/iv.2002.1028760", "10.1177/001316447303300111", "10.1007/bf02289823", "10.1109/tvcg.2017.2744339", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1109/tvcg.2013.65", "10.1109/mcg.2012.37", "10.1109/pacificvis.2010.5429604", "10.1002/acp.2350050106", "10.1109/infvis.2005.1532136", "10.1177/1473871612465214", "10.1109/tvcg.2017.2674978", "10.1002/jhbs.20078", "10.1007/978-3-642-35142-6\\_14", "10.1007/978-3-319-71507-0", "10.1109/mcg.201.7.6", "10.3758/bf03193961", "10.1364/josa.49.000280", "10.1145/3025453.3025905", "10.1109/tvcg.201.9.2934541", "10.1109/tvcg.2018.2829750", "10.1177/1473871615606187", "10.1111/cgf.13408", "10.1109/pacificvis.2016.7465252", "10.1109/pacificvis.2016.7465244", "10.1109/inevis.2005.1532142", "10.1111/cgf.13414", "10.1111/j.1467-8659.2012.03125.x", "10.1167/16.5.11", "10.1109/infvis.2005.1532142", "10.1145/2858036.2858155", "10.1038/nmeth1210-941", "10.1177/1473871616638892", "10.1109/tcbb.2014.2306840", "10.1111/cgf.13684", "10.1177/0301006615602599", "10.1214/aoms/1177731915", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1109/tvcg.2014.2346572", "10.1109/tvcg.2018.2875702", "10.1109/tvcg.2017.2754480", "10.1109/vast.2014.7042493", "10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2019.2934541", "10.1068/p030033", "10.1140/epjds/s13688-017-0109-5", "10.1201/b17511", "10.1016/s0925-7721(02)00093-7", "10.1109/pacificvis.2012.6183557", "10.1109/tvcg.2014.2346983", "10.1109/5.726791", "10.1080/01621459.1926.10502165", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2018.2864907", "10.1111/cgf.12109", "10.1109/tvcg.2017.2744184", "10.1146/annurev-statistics-031017-100045", "10.1111/cgf.13409", "10.1145/3002151.3002162", "10.1016/s0378-4371(98)00494-4", "10.3138/y308-2422-8615-1233", "10.1111/cgf.12632", "10.1145/3290605.3300771"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030396", "title": "What Makes a Data-GIF Understandable?", "year": "2020", "conferenceName": "InfoVis", "authors": "Xinhuan Shu;Aoyu Wu;Junxiu Tang;Benjamin Bach;Yingcai Wu;Huamin Qu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Shu, Xinhuan; Tang, Junxiu; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Shu, Xinhuan; Wu, Aoyu; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Shu, Xinhuan, Zhejiang Univ, Hangzhou, Peoples R China. Bach, Benjamin, Univ Edinburgh, Edinburgh, Midlothian, Scotland.", "countries": "Scotland;China", "abstract": "GIFs are enjoying increasing popularity on social media as a format for data-driven storytelling with visualization; simple visual messages are embedded in short animations that usually last less than 15 seconds and are played in automatic repetition. In this paper, we ask the question, \u201cWhat makes a data-GIF understandable?\u201d While other storytelling formats such as data videos, infographics, or data comics are relatively well studied, we have little knowledge about the design factors and principles for \u201cdata-GIFs\u201d. To close this gap, we provide results from semi-structured interviews and an online study with a total of 118 participants investigating the impact of design decisions on the understandability of data-GIFs. The study and our consequent analysis are informed by a systematic review and structured design space of 108 data-GIFs that we found online. Our results show the impact of design dimensions from our design space such as animation encoding, context preservation, or repetition on viewers understanding of the GIF's core message. The paper concludes with a list of suggestions for creating more effective Data-GIFs.", "keywords": "Data-GIFs,Data-driven Storytelling,Evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030396", "refList": ["10.1109/tvcg.2016.2598647", "10.1016/j.visinf.2020.07.001", "10.1177/1473871615594652", "10.1109/tvcg.2014.2346424", "10.1111/cgf.13195", "10.1109/vlsicircuits18222.2020.9162811", "10.1109/tvcg.2018.2864909", "10.1109/tvcg.2018.2864903", "10.1145/2702123.2702431", "10.1016/j.visinf.2019.12.002", "10.1145/3274349", "10.1111/cgf.13444", "10.1145/3206505.3206552", "10.1145/3290605.3300280", "10.1109/jstqe.2020.3021589", "10.1145/2647868.2656408", "10.1006/ijhc.1017", "10.1145/3290605.3300335", "10.1007/s12650-020-00689-0", "10.1145/2818048.2819936", "10.1111/cgf.13325", "10.1145/2858036.2858387", "10.1145/3027063.3053139", "10.1145/3290605.3300474", "10.1109/tvcg.2016.2598920", "10.1109/tvcg.2018.2864899", "10.1145/3290605.3300483", "10.1145/3173574.3173612", "10.1145/2909132.2909255", "10.1109/tvcg.2016.2598620", "10.1016/j.learninstruc.2007.09.013", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2008.125", "10.1145/3173574.3173909", "10.1109/tvcg.2019.2934397", "10.1111/cgf.13709", "10.1109/tvcg.2013.234", "10.1109/tvcg.2019.2934401", "10.1145/2858036.2858532", "10.1016/j.visinf.2020.08.001", "10.1109/tvcg.2010.179", "10.1109/cicc48029.2020.9075900"], "wos": 1, "children": [], "len": 1}], "len": 21}, {"doi": "10.1111/cgf.13718", "year": "2019", "title": "Investigating the Manual View Specification and Visualization by Demonstration Paradigms for Visualization Construction", "conferenceName": "EuroVis", "authors": "Bahador Saket;Alex Endert", "citationCount": "1", "affiliation": "Saket, B (Corresponding Author), Georgia Inst Technol, Sch Interact Comp, Atlanta, GA 30332 USA.\nSaket, Bahador; Endert, Alex, Georgia Inst Technol, Sch Interact Comp, Atlanta, GA 30332 USA.", "countries": "USA", "abstract": "Interactivity plays an important role in data visualization. Therefore, understanding how people create visualizations given different interaction paradigms provides empirical evidence to inform interaction design. We present a two-phase study comparing people's visualization construction processes using two visualization tools: one implementing the manual view specification paradigm (Polestar) and another implementing visualization by demonstration (VisExemplar). Findings of our study indicate that the choice of interaction paradigm influences the visualization construction in terms of: 1) the overall effectiveness, 2) how participants phrase their goals, and 3) their perceived control and engagement. Based on our findings, we discuss trade-offs and open challenges with these interaction paradigms.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13718", "refList": ["10.1111/cgf.12887", "10.1023/a:1008716330212", "10.1145/2470654.2466255", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2012.275", "10.1145/2493102.2493104", "10.1109/tvcg.2016.2598839", "10.1145/948496.948514", "10.1109/tvcg.2015.2467201", "10.2312/pe.eurovisshort.eurovisshort2013.019-023", "10.1109/tvcg.2007.70541", "10.1109/tvcg.2016.2598446", "10.1109/vl.1996.545307", "10.1109/tvcg.2014.2346250", "10.1145/3025453.3025942", "10.1145/2207676.2207741", "10.1145/3173574.3174212", "10.1109/tvcg.2015.2467615", "10.1145/2598784.2598806", "10.1109/tvcg.2017.2680452", "10.1145/2598510.2598566", "10.1109/tvcg.2007.70577", "10.1109/tvcg.2016.2598620", "10.2307/2530428", "10.1109/infvis.1998.729560", "10.1109/tvcg.2014.2346291", "10.1109/tvcg.2010.164", "10.1109/tvcg.2017.2743859", "10.1109/tvcg.2013.191", "10.1109/2.153286", "10.1109/tvcg.2015.2467191", "10.1145/3173574.3173697", "10.1145/1502650.1502667", "10.1109/tvcg.2007.70515"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934534", "title": "Investigating Direct Manipulation of Graphical Encodings as a Method for User Interaction", "year": "2019", "conferenceName": "InfoVis", "authors": "Bahador Saket;Samuel Huron;Charles Perin;Alex Endert", "citationCount": "0", "affiliation": "Saket, B (Corresponding Author), Georgia Tech, Atlanta, GA 30332 USA. Saket, Bahador; Endert, Alex, Georgia Tech, Atlanta, GA 30332 USA. Huron, Samuel, Univ Paris Saclay, Paris, France. Perin, Charles, Univ Victoria, Victoria, BC, Canada.", "countries": "Canada;USA;France", "abstract": "We investigate direct manipulation of graphical encodings as a method for interacting with visualizations. There is an increasing interest in developing visualization tools that enable users to perform operations by directly manipulating graphical encodings rather than external widgets such as checkboxes and sliders. Designers of such tools must decide which direct manipulation operations should be supported, and identify how each operation can be invoked. However, we lack empirical guidelines for how people convey their intended operations using direct manipulation of graphical encodings. We address this issue by conducting a qualitative study that examines how participants perform 15 operations using direct manipulation of standard graphical encodings. From this study, we 1) identify a list of strategies people employ to perform each operation, 2) observe commonalities in strategies across operations, and 3) derive implications to help designers leverage direct manipulation of graphical encoding as a method for user interaction.", "keywords": "Direct Manipulation,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934534", "refList": ["10.1145/2556288.2557379", "10.1109/tvcg.2016.2598839", "10.1145/2858036.2858435", "10.1145/2702123.2702237", "10.1111/j.1467-8659.2009.01678.x", "10.1109/tvcg.2011.185", "10.1109/mcg.2016.90", "10.1177/1473871611413180", "10.1111/cgf.13718", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2014.2346984", "10.1145/2207676.2207741", "10.1109/mcg.2019.2903711", "10.1109/tvcg.2018.2865075", "10.1109/tvcg.2014.2346279", "10.1109/iv.1999.781570", "10.1109/tvcg.2015.2467615", "10.1109/tvcg.2014.2346292", "10.1145/2984511.2984588", "10.1109/tvcg.2017.2680452", "10.1145/1166253.1166265", "10.1109/tvcg.2017.2745258", "10.1109/tvcg.2016.2598620", "10.1109/tvcg.2014.2346291", "10.2307/2530428", "10.1109/tvcg.2012.204", "10.1145/3173574.3173697", "10.1207/s15327051hci0104\\_2", "10.1109/vast.2012.6400486", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030476", "title": "StructGraphics: Flexible Visualization Design through Data-Agnostic and Reusable Graphical Structures", "year": "2020", "conferenceName": "InfoVis", "authors": "Theophanis Tsandilas", "citationCount": "0", "affiliation": "Tsandilas, T (Corresponding Author), Univ Paris Saclay, INRIA, Gif Sur Yvette, France. Tsandilas, T (Corresponding Author), CNRS, F-75700 Paris, France. Tsandilas, Theophanis, Univ Paris Saclay, INRIA, Gif Sur Yvette, France. Tsandilas, Theophanis, CNRS, F-75700 Paris, France.", "countries": "France", "abstract": "Information visualization research has developed powerful systems that enable users to author custom data visualizations without textual programming. These systems can support graphics-driven practices by bridging lazy data-binding mechanisms with vector-graphics editing tools. Yet, despite their expressive power, visualization authoring systems often assume that users want to generate visual representations that they already have in mind rather than explore designs. They also impose a data-to-graphics workflow, where binding data dimensions to graphical properties is a necessary step for generating visualization layouts. In this paper, we introduce StructGraphics, an approach for creating data-agnostic and fully reusable visualization designs. StructGraphics enables designers to construct visualization designs by drawing graphics on a canvas and then structuring their visual properties without relying on a concrete dataset or data schema. In StructGraphics, tabular data structures are derived directly from the structure of the graphics. Later, designers can link these structures with real datasets through a spreadsheet user interface. StructGraphics supports the design and reuse of complex data visualizations by combining graphical property sharing, by-example design specification, and persistent layout constraints. We demonstrate the power of the approach through a gallery of visualization examples and reflect on its strengths and limitations in interaction with graphic designers and data visualization experts.", "keywords": "Visualization design,graphical structures,visualization grammars,layout constraints,infographics,flexible data binding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030476", "refList": ["10.1145/344949.344959", "10.1057/ivs.2009.22", "10.1109/mcg.1987.277079", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/2858036.2858435", "10.1145/3173574.3173610", "10.1145/3173574.3174106", "10.1109/tvcg.2011.185", "10.1109/tvcg.2007.70515", "10.1109/iv.2008.66", "10.1111/cgf.13718", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2017.2744198", "10.1145/22949.22950", "10.1145/1925844.1926423", "10.1109/mcg.2019.2903711", "10.1006/cogp.1994.1010", "10.1109/tvcg.2018.2865240", "10.1145/3125571.3125585", "10.1145/3022671.2984020", "10.1111/cgf.12391", "10.1109/tvcg.2015.2467091", "10.1109/infvis.2004.12", "10.1109/tvcg.2016.2598620", "10.1016/j.jvlc.2017.10.001", "10.1109/tvcg.2014.2346291", "10.1109/2945.981851", "10.1073/pnas.1807184115", "10.1109/tvcg.2010.177", "10.1145/3173574.3173697", "10.1080/1369118x.2016.1153126", "10.1109/tvcg.2015.2414454", "10.1145/2740908.2742849", "10.1023/a:1025671410623", "10.1111/cgf.12903", "10.1207/s15327051hci0104\\_2", "10.1145/3290605.3300358", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13169", "year": "2017", "title": "Empirically Measuring Soft Knowledge in Visualization", "conferenceName": "EuroVis", "authors": "Natchaya Kijmongkolchai;Alfie Abdul{-}Rahman;Min Chen", "citationCount": "3", "affiliation": "Kijmongkolchai, N (Corresponding Author), Univ Oxford, Oxford, England.\nKijmongkolchai, Natchaya; Abdul-Rahman, Alfie; Chen, Min, Univ Oxford, Oxford, England.", "countries": "England", "abstract": "In this paper, we present an empirical study designed to evaluate the hypothesis that humans' soft knowledge can enhance the cost-benefit ratio of a visualization process by reducing the potential distortion. In particular, we focused on the impact of three classes of soft knowledge: (i) knowledge about application contexts, (ii) knowledge about the patterns to be observed (i.e., in relation to visualization task), and (iii) knowledge about statistical measures. We mapped these classes into three control variables, and used real-world time series data to construct stimuli. The results of the study confirmed the positive contribution of each class of knowledge towards the reduction of the potential distortion, while the knowledge about the patterns prevents distortion more effectively than the other two classes.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13169", "refList": ["10.1111/cgf.12887", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2006.194", "10.1109/tvcg.2013.166", "10.1109/tvcg.2014.2346420", "10.1111/cgf.12634", "10.1111/cgf.12889", "10.1109/tvcg.2015.2467759", "10.1109/tvcg.2013.187", "10.1109/tvcg.2012.197", "10.1109/tvcg.2012.180", "10.1109/tvcg.2015.2467322", "10.1109/tvcg.2016.2598862", "10.1111/cgf.12127", "10.1111/cgf.12104", "10.1109/tvcg.2015.2513410", "10.1161/01.cir.101.23.e215", "10.1109/tvcg.2012.163", "10.1109/tvcg.2013.234", "10.1109/tvcg.2015.2467758", "10.1111/j.1467-8659.2009.01694.x", "10.1109/tvcg.2014.2346428", "10.1109/tvcg.2012.233", "10.1109/tvcg.2014.2346979", "10.1109/tvcg.2016.2598898", "10.1111/cgf.12638", "10.1111/cgf.12880", "10.1111/j.1467-8659.2012.03129.x", "10.1111/cgf.12092", "10.1109/tvcg.2014.2346320", "10.1109/tvcg.2010.150", "10.1109/tvcg.2016.2598466", "10.1111/cgf.12633", "10.1109/tvcg.2016.2598594", "10.1109/tvcg.2015.2467752", "10.1109/tvcg.2015.2502587", "10.1111/j.1467-8659.2012.03092.x", "10.1109/tvcg.2016.2520921", "10.1145/2858036.2858272", "10.1109/tvcg.2012.215", "10.1109/tvcg.2012.230", "10.1109/tvcg.2014.2371858", "10.1109/visual.2001.964505", "10.1109/tvcg.2015.2467201", "10.1109/tvcg.2014.2346422", "10.1111/cgf.12656", "10.1057/ivs.2008.13", "10.1109/tvcg.2015.2424872", "10.1109/tvcg.2012.279", "10.1109/tvcg.2014.2346998", "10.1109/tvcg.2012.220", "10.1109/tvcg.2015.2467951", "10.1109/tvcg.2013.183", "10.1109/tvcg.2012.223", "10.1109/tvcg.2012.189", "10.1109/tvcg.2012.222", "10.1109/tvcg.2014.2330617", "10.1109/tvcg.2012.196", "10.1109/tvcg.2016.2599106", "10.1109/tvcg.2014.2346424", "10.1109/tvcg.2012.199", "10.1109/tvcg.2016.2518158", "10.1109/tvcg.2012.221", "10.1111/cgf.13009", "10.1109/tvcg.2014.2346983", "10.1111/j.1467-8659.2012.03093.x", "10.1109/tvcg.2013.170", "10.1111/cgf.12888", "10.1109/tvcg.2012.245", "10.1109/tvcg.2014.2346426", "10.1109/tvcg.2016.2598544", "10.1109/tvcg.2015.2467751", "10.1109/tvcg.2014.2346298", "10.1109/tvcg.2012.144", "10.1109/tvcg.2012.251", "10.1109/tvcg.2016.2598885"], "wos": 1, "children": [{"doi": "10.1109/vast.2017.8585498", "title": "The Role of Explicit Knowledge: A Conceptual Model of Knowledge-Assisted Visual Analytics", "year": "2017", "conferenceName": "VAST", "authors": "Paolo Federico;Markus Wagner 0008;Alexander Rind;Albert Amor-Amoros;Silvia Miksch;Wolfgang Aigner", "citationCount": "3", "affiliation": "Federico, P (Corresponding Author), TU Wien, Vienna, Austria. Federico, Paolo; Wagner, Markus; Rind, Alexander; Amor-Amoros, Albert; Miksch, Silvia; Aigner, Wolfgang, TU Wien, Vienna, Austria. Wagner, Markus; Rind, Alexander; Aigner, Wolfgang, St Poelten Univ Appl Sci, Sankt Polten, Austria.", "countries": "Austria", "abstract": "Visual Analytics (VA) aims to combine the strengths of humans and computers for effective data analysis. In this endeavor, humans' tacit knowledge from prior experience is an important asset that can be leveraged by both human and computer to improve the analytic process. While VA environments are starting to include features to formalize, store, and utilize such knowledge, the mechanisms and degree in which these environments integrate explicit knowledge varies widely. Additionally, this important class of VA environments has never been elaborated on by existing work on VA theory. This paper proposes a conceptual model of Knowledge-assisted VA conceptually grounded on the visualization model by van Wijk. We apply the model to describe various examples of knowledge-assisted VA from the literature and elaborate on three of them in finer detail. Moreover, we illustrate the utilization of the model to compare different design alternatives and to evaluate existing approaches with respect to their use of knowledge. Finally, the model can inspire designers to generate novel VA environments using explicit knowledge effectively.", "keywords": "Automated analysis,tacit knowledge,explicit knowledge,visual analytics,information visualization,theory and model", "link": "http://dx.doi.org/10.1109/VAST.2017.8585498", "refList": ["10.1016/j.artmed.2006.03.001", "10.1057/ivs.2009.22", "10.1109/infvis.2000.885092", "10.2312/pe/eurovast/eurova11/009-012", "10.1109/tvcg.2016.2598839", "10.1109/tvcg.2014.2346575", "10.1111/cgf.13169", "10.1186/1471-2105-13-s8-s3", "10.1007/s00371-015-1132-9", "10.1109/tvcg.2013.146", "10.1109/tvcg.2016.2598471", "10.1016/j.cag.2009.06.004", "10.1145/2993901.2993915", "10.1111/cgf.12090", "10.1177/1473871611412817", "10.1145/2598153.2598172", "10.1016/j.cag.2009.06.006", "10.1109/tvcg.2016.2598460", "10.1016/j.autcon.2014.03.012", "10.1145/989863.989865", "10.1109/21.44068", "10.1057/palgrave.ivs.9500045", "10.2312/eurova.20151108", "10.1002/9781444303179.ch3", "10.1109/hicss.2016.183", "10.1177/0165551506070706", "10.1145/2494188.2494202", "10.1145/1556262.1556327", "10.1016/j.artmed.2006.04.002", "10.1007/978-3-540-71080-6\\_6", "10.1111/j.1467-8659.2008.01230.x", "10.1109/pacificvis.2011.5742371", "10.1109/vast.2014.7042530", "10.1109/mcg.2010.8", "10.1109/mcg.2015.25", "10.1109/vast.2012.6400555", "10.1109/tvcg.2014.2346481", "10.1145/2836034.2836040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2008.109", "10.1016/j.artmed.2005.10.003", "10.1109/mcg.2014.73", "10.1016/s0004-3702(96)00025-2", "10.1093/intqhc/mzm007", "10.1016/j.dss.2012.06.009", "10.1016/j.cose.2017.02.003", "10.1109/infvis.1998.729560", "10.1109/vast.2010.5654451", "10.1109/tvcg.2016.2598829", "10.1109/vast.2007.4389021", "10.1145/1562849.1562851", "10.1145/302979.303030", "10.1145/985692.985706", "10.1109/infvis.1997.636792", "10.1111/j.1467-8659.2012.03092.x", "10.1109/mcg.2009.6", "10.1057/ivs.2008.28", "10.1109/mcg.2005.91", "10.1016/j.artmed.2010.02.001", "10.1177/0272989x14565822", "10.1109/mcg.2010.15", "10.1007/s10844-014-0304-9", "10.2312/pe.eurovast.eurova13.043-047", "10.1109/mcg.2014.33", "10.1109/tvcg.2014.2346574", "10.1111/j.1467-8659.2009.01708.x", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934654", "title": "Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections", "year": "2019", "conferenceName": "VAST", "authors": "Mennatallah El-Assady;Rebecca Kehlbeck;Christopher Collins;Daniel A. Keim;Oliver Deussen", "citationCount": "3", "affiliation": "El-Assady, M (Corresponding Author), Univ Konstanz, Constance, Germany. El-Assady, M (Corresponding Author), Ontario Tech Univ, Oshawa, ON, Canada. El-Assady, Mennatallah; Kehlbeck, Rebecca; Keim, Daniel; Deussen, Oliver, Univ Konstanz, Constance, Germany. El-Assady, Mennatallah; Collins, Christopher, Ontario Tech Univ, Oshawa, ON, Canada.", "countries": "Canada;Germany", "abstract": "We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decision-making process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.", "keywords": "Topic Model Optimization,Word Embedding,Mixed-Initiative Refinement,Guided Visual Analytics,Semantic Mapping", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934654", "refList": ["10.1109/vast.2014.7042493", "10.1145/2133806.2133826", "10.1016/j.visinf.2018.09.003", "10.1007/978-3-319-67008-9\\_26", "10.1109/tvcg.2013.126", "10.1162/tacl\\_a\\_00140", "10.1007/s13202-018-0509-5", "10.1109/bdva.2018.8534018", "10.1108/eb026526", "10.1007/s10994-013-5413-0", "10.1145/564376.564421", "10.1016/j.visinf.2017.01.006", "10.3115/v1/p14-2050", "10.1007/bf00288933", "10.1109/tvcg.2013.212", "10.1145/2207676.2207741", "10.1109/tvcg.2013.162", "10.1109/tvcg.2016.2515592", "10.1109/tvcg.2017.2745080", "10.1109/mcg.2013.53", "10.1109/tvcg.2017.2744199", "10.1145/3091108", "10.18653/v1/p17-4009", "10.1162/jmlr.2003.3.4-5.951", "10.1109/vast.2017.8585498", "10.1109/tvcg.2017.2723397", "10.1109/tvcg.2018.2864769", "10.1007/s10618-005-0361-3", "10.3115/v1/d14-1167", "10.1007/bf01840357", "10.1162/jmlr.2003.3.4-5.993", "10.1145/2678025.2701370", "10.1016/j.ins.2016.06.040", "10.1109/tvcg.2017.2746018", "10.1109/vast.2011.6102461", "10.1111/cgf.13092", "10.3115/1117729.1117730", "10.1109/mcg.2015.91", "10.1145/2669557.2669572"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13959", "year": "2020", "title": "Knowledge-Assisted Comparative Assessment of Breast Cancer using Dynamic Contrast-Enhanced Magnetic Resonance Imaging", "conferenceName": "EuroVis", "authors": "Kai Nie;Pascal A. Baltzer;Bernhard Preim;Gabriel Mistelbauer", "citationCount": "0", "affiliation": "Nie, K (Corresponding Author), Otto von Guericke Univ, Dept Simulat \\& Graph, Magdeburg, Germany.\nNie, K.; Preim, B.; Mistelbauer, G., Otto von Guericke Univ, Dept Simulat \\& Graph, Magdeburg, Germany.\nBaltzer, P., Med Univ Vienna, Dept Biomed Imaging \\& Image Guided Therapy, Vienna, Austria.", "countries": "Germany;Austria", "abstract": "Breast perfusion data are dynamic medical image data that depict perfusion characteristics of the investigated tissue. These data consist of a series of static datasets that are acquired at different time points and aggregated into time intensity curves (TICs) for each voxel. The characteristics of these TICs provide important information about a lesion's composition, but their analysis is time-consuming due to their large number. Subsequently, these TICs are used to classify a lesion as benign or malignant. This lesion scoring is commonly done manually by physicians and may therefore be subject to bias. We propose an approach that addresses both of these problems by combining an automated lesion classification with a visual confirmatory analysis, especially for uncertain cases. Firstly, we cluster the TICs of a lesion using ordering points to identify the clustering structure (OPTICS) and then visualize these clusters. Together with their relative size, they are added to a library. We then model fuzzy inference rules by using the lesion's TIC clusters as antecedents and its score as consequent. Using a fuzzy scoring system, we can suggest a score for a new lesion. Secondly, to allow physicians to confirm the suggestion in uncertain cases, we display the TIC clusters together with their spatial distribution and allow them to compare two lesions side by side. With our knowledge-assisted comparative visual analysis, physicians can explore and classify breast lesions. The true positive prediction accuracy of our scoring system achieved 71.4\\% in one-fold cross-validation using 14 lesions.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13959", "refList": ["10.1016/j.patcog.2017.08.004", "10.1109/icinfa.2017.8078962", "10.1109/iccv.2013.222", "10.1002/jmri.1123", "10.1016/j.ejrad.2017.01.020", "10.1007/s00330-016-4612-z", "10.1002/widm.30", "10.1002/jmri.26721", "10.1109/tvcg.2008.95", "10.1007/s00330-015-4075-7", "10.1007/s00330-007-0762-3", "10.1148/radiology.213.3.r99dc01881", "10.1007/s10278-010-9298-1", "10.1016/j.datak.2006.01.013", "10.1016/j.cag.2010.05.016", "10.5121/mlaij.2016.3103", "10.1089/10665270360688057", "10.1117/1.jmi.5.1.014502", "10.1002/mp.12408", "10.1198/jcgs.2011.09224", "10.1109/cbms.2013.6627768", "10.1109/vast.2017.8585498", "10.1118/1.4937787", "10.1016/j.acra.2009.03.017", "10.1145/2503210.2503255", "10.1007/978-3-319-68548-9\\_44", "10.1109/tvcg.2007.70569", "10.1016/j.compmedimag.2007.02.007", "10.1109/tmi.2013.2281984", "10.1148/radiol.2442051620", "10.1016/j.jacr.2009.07.023", "10.1002/j.1538-7305.1957.tb01515.x", "10.1148/radiol.14121031", "10.3322/caac.21492", "10.1148/radiology.211.1.r99ap38101", "10.1016/j.compbiomed.2014.10.006", "10.1016/j.eswa.2016.01.004", "10.3238/arztebl.2018.0316", "10.1559/152304003100010929", "10.1016/s0002-9610(01)00726-7", "10.1016/j.ejrad.2020.108819", "10.1109/tmi.2012.2191302"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/tvcg.2018.2864913", "title": "A Framework for Externalizing Implicit Error Using Visualization", "year": "2018", "conferenceName": "InfoVis", "authors": "Nina McCurdy;Julie Gerdes;Miriah D. Meyer", "citationCount": "8", "affiliation": "McCurdy, N (Corresponding Author), Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA. McCurdy, Nina; Meyer, Miriah, Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA. Gerdes, Julie, Texas Tech Univ, Coll Arts \\& Sci, Lubbock, TX 79409 USA.", "countries": "USA", "abstract": "This paper presents a framework for externalizing and analyzing expert knowledge about discrepancies in data through the use of visualization. Grounded in an 18-month design study with global health experts, the framework formalizes the notion of data discrepancies as implicit error, both in global health data and more broadly. We use the term implicit error to describe measurement error that is inherent to and pervasive throughout a dataset, but that isn't explicitly accounted for or defined. Instead, implicit error exists in the minds of experts, is mainly qualitative, and is accounted for subjectively during expert interpretation of the data. Externalizing knowledge surrounding implicit error can assist in synchronizing, validating, and enhancing interpretation, and can inform error analysis and mitigation. The framework consists of a description of implicit error components that are important for downstream analysis, along with a process model for externalizing and analyzing implicit error using visualization. As a second contribution, we provide a rich, reflective, and verifiable description of our research process as an exemplar summary toward the ongoing inquiry into ways of increasing the validity and transferability of design study research.", "keywords": "implicit error,knowledge externalization,design study", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864913", "refList": ["10.1197/jamia.m2342", "10.1109/tvcg.2013.132", "10.1111/cgf.13169", "10.1179/1743277414y.0000000099", "10.1016/j.cag.2009.06.004", "10.1111/j.1467-8659.2009.01678.x", "10.1111/j.0361-3666.2003.00237.x", "10.1111/cgf.12392", "10.1016/j.jvlc.2011.04.002", "10.1186/1471-2334-11-37", "10.1371/journal.pmed.1000376", "10.1109/infvis.2005.1532134", "10.1109/pacificvis.2017.8031599", "10.1109/tvcg.2017.2743898", "10.1197/jamia.m2544", "10.1145/3025453.3025592", "10.1109/tvcg.2015.2468151", "10.1007/978-1-4471-6497-5\\_1", "10.3233/978-1-60750-533-4-23", "10.1177/0165551506070706", "10.1145/642611.642616", "10.6064/2012/875253", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2015.2465151", "10.1111/j.1467-9604.2007.00468.x", "10.1109/mcg.2012.31", "10.1109/tvcg.2007.70589", "10.1145/2993901.2993916", "10.1109/vast.2010.5652885", "10.1145/3025453.3025738", "10.1109/38.689662", "10.1016/s0925-7535(97)00052-0", "10.9745/ghsp-d-15-00207", "10.1518/001872095779049543", "10.1080/15323269.2011.587100", "10.1117/12.587254", "10.1016/j.cie.2014.11.025", "10.1109/tvcg.2017.2745240", "10.1109/tvcg.2012.213", "10.1109/vast.2011.6102457", "10.1109/mcg.2015.50", "10.1145/1385569.1385582", "10.1016/j.jbi.2014.04.006", "10.1136/amiajnl-2011-000486"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934539", "title": "Criteria for Rigor in Visualization Design Study", "year": "2019", "conferenceName": "InfoVis", "authors": "Miriah D. Meyer;Jason Dykes", "citationCount": "16", "affiliation": "Meyer, M (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Dykes, Jason, City Univ London, London, England.", "countries": "USA;England", "abstract": "We develop a new perspective on research conducted through visualization design study that emphasizes design as a method of inquiry and the broad range of knowledge-contributions achieved through it as multiple, subjective, and socially constructed. From this interpretivist position we explore the nature of visualization design study and develop six criteria for rigor. We propose that rigor is established and judged according to the extent to which visualization design study research and its reporting are INFORMED, REFLEXIVE, ABUNDANT, PLAUSIBLE, RESONANT, and TRANSPARENT. This perspective and the criteria were constructed through a four-year engagement with the discourse around rigor and the nature of knowledge in social science, information systems, and design. We suggest methods from cognate disciplines that can support visualization researchers in meeting these criteria during the planning, execution, and reporting of design study. Through a series of deliberately provocative questions, we explore implications of this new perspective for design study research in visualization, concluding that as a discipline, visualization is not yet well positioned to embrace, nurture, and fully benefit from a rigorous, interpretivist approach to design study. The perspective and criteria we present are intended to stimulate dialogue and debate around the nature of visualization design study and the broader underpinnings of the discipline.", "keywords": "design study,relativism,interpretivism,knowledge construction,qualitative research,research through design", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934539", "refList": ["10.1007/978-1-4939-0378-8\\_8", "10.1177/1049732315588501", "10.1177/146879410200200205", "10.2307/1177100", "10.1016/0142-694x(82)90040-0", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1145/2362364.2362371", "10.1080/2159676x.2017.1393221", "10.1177/1473871613510429", "10.1145/2212877.2212889", "10.1080/09650790802011973", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2015.2467195", "10.1109/mcse.2007.106", "10.1080/1750984x.2017.1317357", "10.1109/tvcg.2017.2745958", "10.1177/1525822x0101300203", "10.1080/23265507.2017.1300068", "10.1111/j.1540-4560.1946.tb02295.x", "10.1177/1468794108098034", "10.1007/978-3-7643-8472-2\\_6", "10.1145/2317956.2317968", "10.2307/1511837", "10.1177/104973202129120052", "10.3233/efi-2004-22201", "10.1109/beliv.2018.8634427", "10.1109/beliv.2018.8634261", "10.1016/s0142-694x(01)00009-6", "10.1007/978-3-7643-8472-2\\_3", "10.1145/642611.642616", "10.1177/1468794107085301", "10.1177/1077800410383121", "10.1109/tvcg.2010.137", "10.1145/2405716.2405725", "10.1145/2702123.2702172", "10.1109/tvcg.2014.2346248", "10.1109/tvcg.2011.209", "10.1111/j.1467-8659.2009.01710.x", "10.1145/3173574.3173775", "10.1145/1993060.1993065", "10.1007/978-1-4419-5653-8\\_2", "10.1177/107780049900500402", "10.1109/tvcg.2018.2864905", "10.2307/2288400", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1177/160940690400300403", "10.1145/2993901.2993916", "10.1145/1879831.1879836", "10.2307/3178066", "10.1109/tvcg.2018.2864913", "10.3102/0013189x022004016", "10.1109/tvcg.2015.2511718", "10.1109/tvcg.2018.2865241", "10.1016/j.ijnurstu.2010.06.004", "10.1109/tvcg.2012.213", "10.1111/0735-2751.00040", "10.1109/tvcg.2013.145", "10.1002/ev.1427", "10.1109/tvcg.2018.2811488", "10.1075/idj.23.1.07thu", "10.1109/tvcg.2009.111", "10.1109/mcg.2018.2874523", "10.1111/cgf.13184", "10.1111/cgf.13595"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3029413", "title": "A Design Space of Vision Science Methods for Visualization Research", "year": "2020", "conferenceName": "InfoVis", "authors": "Madison A. Elliott;Christine Nothelfer;Cindy Xiong;Danielle Albers Szafir", "citationCount": "0", "affiliation": "Elliott, MA (Corresponding Author), Univ British Columbia, Vancouver, BC, Canada. Elliott, Madison A., Univ British Columbia, Vancouver, BC, Canada. Nothelfer, Christine, Northwestern Univ, Evanston, IL 60208 USA. Xiong, Cindy, Univ Massachusetts, Amherst, MA 01003 USA. Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "Canada;USA", "abstract": "A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.", "keywords": "Perception,human vision,empirical research,evaluation,HCI", "link": "http://dx.doi.org/10.1109/TVCG.2020.3029413", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1177/0886109909354981", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1093/bioinformatics/btq110", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1177/1744987107081254", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1177/174498710501000305", "10.1017/s1049096513001789", "10.1109/tvcg.2012.213", "10.1093/nar/gkz239", "10.1093/sysbio/sys062", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030438", "title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in Virtual Screening", "year": "2020", "conferenceName": "SciVis", "authors": "Mar\u00eda Virginia Sabando;Pavol Ulbrich;Mat\u00edas N. Selzer;Jan Byska;Jan Mican;Ignacio Ponzoni;Axel J. Soto;Maria Luj\u00e1n Ganuza;Barbora Kozl\u00edkov\u00e1", "citationCount": "0", "affiliation": "Sabando, MV (Corresponding Author), Univ Nacl Sur, Inst Comp Sci \\& Engn UNS CONICET, Bahia Blanca, Buenos Aires, Argentina. Sabando, MV (Corresponding Author), Univ Nacl Sur, Dept Comp Sci \\& Engn, Bahia Blanca, Buenos Aires, Argentina. Sabando, Maria Virginia; Selzer, Matias; Ponzoni, Ignacio; Soto, Axel J.; Ganuza, Maria Lujan, Univ Nacl Sur, Inst Comp Sci \\& Engn UNS CONICET, Bahia Blanca, Buenos Aires, Argentina. Sabando, Maria Virginia; Ponzoni, Ignacio; Soto, Axel J., Univ Nacl Sur, Dept Comp Sci \\& Engn, Bahia Blanca, Buenos Aires, Argentina. Ulbrich, Pavol; Byska, Jan; Kozlikova, Barbora, Masaryk Univ, Fac Informat, Visitlab, Brno, Czech Republic. Selzer, Matias; Ganuza, Maria Lujan, Univ Nacl Sur, VyGLab Res Lab UNS CICPBA, Dept Comp Sci \\& Engn, Bahia Blanca, Buenos Aires, Argentina. Mican, Jan, Masaryk Univ, Dept Expt Biol, Loschmidt Labs, Brno, Czech Republic. Mican, Jan, Masaryk Univ, RECETOX, Brno, Czech Republic. Mican, Jan, Masaryk Univ, Fac Med, Brno, Czech Republic.", "countries": "Argentina;Republic", "abstract": "In the modern drug discovery process, medicinal chemists deal with the complexity of analysis of large ensembles of candidate molecules. Computational tools, such as dimensionality reduction (DR) and classification, are commonly used to efficiently process the multidimensional space of features. These underlying calculations often hinder interpretability of results and prevent experts from assessing the impact of individual molecular features on the resulting representations. To provide a solution for scrutinizing such complex data, we introduce ChemVA, an interactive application for the visual exploration of large molecular ensembles and their features. Our tool consists of multiple coordinated views: Hexagonal view, Detail view, 3D view, Table view, and a newly proposed Difference view designed for the comparison of DR projections. These views display DR projections combined with biological activity, selected molecular features, and confidence scores for each of these projections. This conjunction of views allows the user to drill down through the dataset and to efficiently select candidate compounds. Our approach was evaluated on two case studies of finding structurally similar ligands with similar binding affinity to a target protein, as well as on an external qualitative evaluation. The results suggest that our system allows effective visual inspection and comparison of different high-dimensional molecular representations. Furthermore, ChemVA assists in the identification of candidate compounds while providing information on the certainty behind different molecular representations.", "keywords": "Virtual screening,visual analysis,dimensionality reduction,coordinated views,cheminformatics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030438", "refList": ["10.1109/tvcg.2008.137", "10.1057/ivs.2009.10", "10.2312/eurovisstar.20151110", "10.1109/eisic.2015.35", "10.1109/pacificvis.2014.44", "10.1145/1142473.1142574", "10.1109/tvcg.2013.223", "10.1109/tvcg.2014.2346573", "10.1109/tvcg.2019.2934539", "10.1111/cgf.13717", "10.1109/vizsec.2009.5375536", "10.1111/cgf.12925", "10.1109/tvcg.2015.2467551", "10.1109/mcg.2015.99", "10.1007/978-3-319", "10.1109/tvcg.2012.255", "10.1145/1064830.1064834", "10.1177/1473871611433713", "10.1207/s1532690xci0804\\_2", "10.1145/1168149.1168168", "10.1016/j.chb.2006.10.002", "10.1109/tvcg.2014.2346441", "10.1109/eisic.2017.15", "10.1111/1467-8721.00160", "10.1109/tvcg.2018.2865024", "10.1109/infvis.2004.2"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030435", "title": "Data Visceralization: Enabling Deeper Understanding of Data Using Virtual Reality", "year": "2020", "conferenceName": "InfoVis", "authors": "Benjamin Lee;Dave Brown;Bongshin Lee;Christophe Hurter;Steven Mark Drucker;Tim Dwyer", "citationCount": "1", "affiliation": "Lee, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Lee, Benjamin; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Brown, Dave; Lee, Bongshin; Drucker, Steven, Microsoft Res, Redmond, WA USA. Hurter, Christophe, French Civil Aviat Univ, ENAC, Toulouse, France.", "countries": "USA;France;Australia", "abstract": "A fundamental part of data visualization is transforming data to map abstract information onto visual attributes. While this abstraction is a powerful basis for data visualization, the connection between the representation and the original underlying data (i.e., what the quantities and measurements actually correspond with in reality) can be lost. On the other hand, virtual reality (VR) is being increasingly used to represent real and abstract models as natural experiences to users. In this work, we explore the potential of using VR to help restore the basic understanding of units and measures that are often abstracted away in data visualization in an approach we call data visceralization. By building VR prototypes as design probes, we identify key themes and factors for data visceralization. We do this first through a critical reflection by the authors, then by involving external participants. We find that data visceralization is an engaging way of understanding the qualitative aspects of physical measures and their real-life form, which complements analytical and quantitative understanding commonly gained from data visualization. However, data visceralization is most effective when there is a one-to-one mapping between data and representation, with transformations such as scaling affecting this understanding. We conclude with a discussion of future directions for data visceralization.", "keywords": "Data visceralization,virtual reality,exploratory study", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030435", "refList": ["10.1080/01973762.2013.761106", "10.1109/tvcg.2015.2467811", "10.1109/tvcg.2012.221", "10.1145/3284179.3284326", "10.1109/tvcg.2019.2934287", "10.1109/2945.841119", "10.1109/tvcg.2019.2934539", "10.1109/mcg.2013.101", "10.1109/tvcg.2011.175", "10.1109/tvcg.2018.2830759", "10.1109/3dvis.2014.7160096", "10.1109/mcg.2018.2878900", "10.1109/iv.2011.32", "10.1109/tvcg.2013.196", "10.1109/tvcg.2018.2865241", "10.1515/abitech-2017-0002", "10.1145/2468356.2468739", "10.16995/olh.280", "10.1080/15230406.2018.1513343", "10.1109/iv.2004.1320189", "10.1109/tvcg.2012.213", "10.1109/mcg.2006.120", "10.1109/icdar.2017.286", "10.1371/journal.pone.0146368", "10.1080/1472586x.2011.548488", "10.1109/tvcg.2014.2346574", "10.1080/0013838x.2017.1332021"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030464", "title": "Designing Narrative-Focused Role-Playing Games for Visualization Literacy in Young Children", "year": "2020", "conferenceName": "InfoVis", "authors": "Elaine Huynh;Angela Nyhout;Patricia Ganea;Fanny Chevalier", "citationCount": "0", "affiliation": "Huynh, E (Corresponding Author), Univ Toronto, Dept Comp Sci, Toronto, ON, Canada. Huynh, Elaine, Univ Toronto, Dept Comp Sci, Toronto, ON, Canada. Nyhout, Angela; Ganea, Patricia, Univ Toronto, Ontario Inst Studies Educ, Toronto, ON, Canada. Chevalier, Fanny, Univ Toronto, Dept Comp Sci \\& Stat Sci, Toronto, ON, Canada.", "countries": "Canada", "abstract": "Building on game design and education research, this paper introduces narrative-focused role-playing games as a way to promote visualization literacy in young children. Visualization literacy skills are vital in understanding the world around us and constructing meaningful visualizations, yet, how to better develop these skills at an early age remains largely overlooked and understudied. Only recently has the visualization community started to fill this gap, resulting in preliminary studies and development of educational tools for use in early education. We add to these efforts through the exploration of gamification to support learning, and identify an opportunity to apply role-playing game-based designs by leveraging the presence of narratives in data-related problems involving visualizations. We study the effects of including narrative elements on learning through a technology probe, grounded in a set of design considerations stemming from visualization, game design and education science. We create two versions of a game - one with narrative elements and one without - and evaluate our instances on 33 child participants between 11- to 13-years old using a between-subjects study design. Despite participants requiring double the amount of time to complete their game due to additional narrative elements, the inclusion of such elements were found to improve engagement without sacrificing learning; our results indicate no significant differences in development of graph-reading skills, but significant differences in engagement and overall enjoyment of the game. We report observations and qualitative feedback collected, and note areas for improvement and room for future work.", "keywords": "Visualization Literacy,Educational technology,Gamification,Narrative", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030464", "refList": ["10.1007/978-981-13-2694-3\\_2", "10.1145/2702123.2702298", "10.1145/2702123.2702558", "10.1007/978-981-13-2694-3\\_8", "10.1145/2702123.2702245", "10.1109/mis.2012.27", "10.18061/dsq.v21i4.318", "10.1109/tvcg.2013.134", "10.1109/vl.1996.545307", "10.1007/s10708-008-9186-0", "10.1093/cje/ben057", "10.5210/fm.v16i2.3316", "10.1109/tvcg.2019.2934539", "10.1109/tvcg.2016.2598608", "10.1007/978-3-319-94659-7\\_10", "10.1109/mcg.2013.28", "10.17351/ests2017.134", "10.1109/pacificvis.2014.39", "10.1145/3173574.3173728", "10.1145/2598784.2598806", "10.1145/2491500.2491501", "10.1145/1993060.1993065", "10.1109/tvcg.2018.2802520", "10.1145/3025453.3025667", "10.1145/2598510.2598566", "10.1080/15710882.2015.1081240", "10.17351/ests2017.133", "10.1109/tvcg.2014.2346431", "10.1016/j.ijhcs.2015.02.005", "10.1145/2702123.2702180", "10.1109/tvcg.2007.70577", "10.1109/mcg.2019.2923483", "10.5931/djim.v12.i1.6449", "10.1145/3240167.3240206", "10.1145/2468356.2468739", "10.1109/tvcg.2012.213", "10.1145/3025453.3025751", "10.4018/978-1-4666-6497-5.ch003"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030355", "title": "Guidelines For Pursuing and Revealing Data Abstractions", "year": "2020", "conferenceName": "InfoVis", "authors": "Alex Bigelow;Katy Williams;Katherine E. Isaacs", "citationCount": "0", "affiliation": "Bigelow, A (Corresponding Author), Univ Arizona, Tucson, AZ 85721 USA. Bigelow, Alex; Williams, Katy; Isaacs, Katherine E., Univ Arizona, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "Many data abstraction types, such as networks or set relationships, remain unfamiliar to data workers beyond the visualization research community. We conduct a survey and series of interviews about how people describe their data, either directly or indirectly. We refer to the latter as latent data abstractions. We conduct a Grounded Theory analysis that (1) interprets the extent to which latent data abstractions exist, (2) reveals the far-reaching effects that the interventionist pursuit of such abstractions can have on data workers, (3) describes why and when data workers may resist such explorations, and (4) suggests how to take advantage of opportunities and mitigate risks through transparency about visualization research perspectives and agendas. We then use the themes and codes discovered in the Grounded Theory analysis to develop guidelines for data abstraction in visualization projects. To continue the discussion, we make our dataset open along with a visual interface for further exploration.", "keywords": "Data abstraction,Grounded theory,Survey design,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030355", "refList": ["10.1080/2159676x.2016.1251701", "10.1109/infvis.2000.885092", "10.1145/2702123.2702298", "10.4135/9781848607941.n14", "10.1007/978-1-4939", "10.1109/tvcg.2014.2346331", "10.1109/tvcg.2017.2744843", "10.1177/1473871613510429", "10.1007/978-1-4939-0378-8\\_2", "10.1145/2598153.2598175", "10.1109/tvcg.2019.2934285", "10.1177/1473871613488591", "10.1145/2501105.2501106", "10.1109/tvcg.2019.2934538", "10.1109/tvcg.2019.2934539", "10.1017/s1049096510990781", "10.1145/3025453.3025837", "10.1145/3290605.3300474", "10.1145/3290605.3300356", "10.1002/nur.1025", "10.1145/2993901.2993916", "10.1145/3392826", "10.1086/269268", "10.1109/tvcg.2018.2865241", "10.1145/2998181.2998331", "10.1145/291224.291229", "10.1057/ivs.2009.13", "10.1145/2047196.2047205", "10.1109/tvcg.2012.213", "10.1145/3274405", "10.1109/tvcg.2013.145", "10.1016/0040-6031(92)85160-w", "10.1109/iv.2013.45", "10.1109/tvcg.2009.111", "10.1109/mcg.2019.2914844", "10.1109/tvcg.2009.116"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030405", "title": "Insights From Experiments With Rigor in an EvoBio Design Study", "year": "2020", "conferenceName": "InfoVis", "authors": "Jennifer Rogers;Austin H. Patton;Luke Harmon;Alexander Lex;Miriah D. Meyer", "citationCount": "0", "affiliation": "Rogers, J (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Rogers, Jen; Lex, Alexander; Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Patton, Austin H., Washington State Univ, Pullman, WA 99164 USA. Harmon, Luke, Univ Idaho, Moscow, ID 83843 USA.", "countries": "USA", "abstract": "Design study is an established approach of conducting problem-driven visualization research. The academic visualization community has produced a large body of work for reporting on design studies, informed by a handful of theoretical frameworks, and applied to a broad range of application areas. The result is an abundance of reported insights into visualization design, with an emphasis on novel visualization techniques and systems as the primary contribution of these studies. In recent work we proposed a new, interpretivist perspective on design study and six companion criteria for rigor that highlight the opportunities for researchers to contribute knowledge that extends beyond visualization idioms and software. In this work we conducted a year-long collaboration with evolutionary biologists to develop an interactive tool for visual exploration of multivariate datasets and phylogenetic trees. During this design study we experimented with methods to support three of the rigor criteria: ABUNDANT, REFLEXIVE, and TRANSPARENT. As a result we contribute two novel visualization techniques for the analysis of multivariate phylogenetic datasets, three methodological recommendations for conducting design studies drawn from reflections over our process of experimentation, and two writing devices for reporting interpretivist design study. We offer this work as an example for implementing the rigor criteria to produce a diverse range of knowledge contributions.", "keywords": "Methodologies,Application Motivated Visualization,Guidelines,Life Sciences Visualization,Health,Medicine,Biology,Bioinformatics,Genomics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030405", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1080/17493460802276893", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1109/tvcg.2012.213", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030462", "title": "Table Scraps: An Actionable Framework for Multi-Table Data Wrangling From An Artifact Study of Computational Journalism", "year": "2020", "conferenceName": "InfoVis", "authors": "Stephen Kasica;Charles Berret;Tamara Munzner", "citationCount": "0", "affiliation": "Kasica, S (Corresponding Author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada. Kasica, Stephen; Munzner, Tamara, Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada. Berret, Charles, Univ British Columbia, Sch Journalism Writing \\& Media, Vancouver, BC, Canada.", "countries": "Canada", "abstract": "For the many journalists who use data and computation to report the news, data wrangling is an integral part of their work. Despite an abundance of literature on data wrangling in the context of enterprise data analysis, little is known about the specific operations, processes, and pain points journalists encounter while performing this tedious, time-consuming task. To better understand the needs of this user group, we conduct a technical observation study of 50 public repositories of data and analysis code authored by 33 professional journalists at 26 news organizations. We develop two detailed and cross-cutting taxonomies of data wrangling in computational journalism, for actions and for processes. We observe the extensive use of multiple tables, a notable gap in previous wrangling analyses. We develop a concise, actionable framework for general multi-table data wrangling that includes wrangling operations documented in our taxonomy that are without clear parallels in other work. This framework, the first to incorporate tables as first-class objects, will support future interactive wrangling tools for both computational journalism and general-purpose use. We assess the generative and descriptive power of our framework through discussion of its relationship to our set of taxonomies.", "keywords": "Computational journalism,Data journalism,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030462", "refList": ["10.1145/1378773.1378792", "10.1109/tvcg.2012.219", "10.1109/vast47406.2019.8986909", "10.1145/1084805.1084812", "10.1007/s00778-008-0098-x", "10.1016/j.websem.2008.09.005", "10.18637/jss.v040.i01", "10.1145/989863.989865", "10.1109/tvcg.2015.2467551", "10.5281/zenodo.3509134", "10.1109/tvcg.2019.2934539", "10.1109/tvcg.2019.2934593", "10.1109/tse.2018.2796554", "10.17349/jmc117309", "10.1109/2945.981851", "10.1109/vast.2011.6102440", "10.1177/1473871611415994", "10.1145/2001269.2001288"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13964", "year": "2020", "title": "Reading Traces: Scalable Exploration in Elastic Visualizations of Cultural Heritage Data", "conferenceName": "EuroVis", "authors": "Mark{-}Jan Bludau;Viktoria Br{\\\"{u}}ggemann;Anna Busch;Marian D{\\\"{o}}rk", "citationCount": "1", "affiliation": "Bludau, MJ (Corresponding Author), Univ Appl Sci Potsdam, UCLAB, Potsdam, Germany.\nBludau, M. -J.; Brueggemann, V.; Doerk, M., Univ Appl Sci Potsdam, UCLAB, Potsdam, Germany.\nBusch, A., Univ Potsdam, Theodor Fontane Archiv, Potsdam, Germany.", "countries": "Germany", "abstract": "Through a design study, we develop an approach to data exploration that utilizes elastic visualizations designed to support varying degrees of detail and abstraction. Examining the notions of scalability and elasticity in interactive visualizations, we introduce a visualization of personal reading traces such as marginalia or markings inside the reference library of German realist author Theodor Fontane. To explore such a rich and extensive collection, meaningful visual forms of abstraction and detail are as important as the transitions between those states. Following a growing research interest in the role of fluid interactivity and animations between views, we are particularly interested in the potential of carefully designed transitions and consistent representations across scales. The resulting prototype addresses humanistic research questions about the interplay of distant and close reading with visualization research on continuous navigation along several granularity levels, using scrolling as one of the main interaction mechanisms. In addition to presenting the design process and resulting prototype, we present findings from a qualitative evaluation of the tool, which suggest that bridging between distant and close views can enhance exploration, but that transitions between views need to be crafted very carefully to facilitate comprehension.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13964", "refList": ["10.1007/s41244-017-0048-4", "10.1109/tvcg.2009.108", "10.1145/1456650.1456652", "10.1109/tvcg.2014.2346424", "10.1177/1473871611416549", "10.1111/cgf.13195", "10.1145/2207676.2208607", "10.1145/1376616.1376618", "10.1006/ijhc.2002.1017", "10.1109/tvcg.2011.185", "10.1177/1473871611413180", "10.1109/vl.1996.545307", "10.1006/ijhc.1017", "10.1109/tvcg.2019.2934539", "10.1109/tvcg.2018.2830759", "10.1145/1556262.1556300", "10.1109/tvcg.2014.2346677", "10.1145/2909132.2909255", "10.1109/tvcg.2007.70539", "10.1145/2396636.2396675", "10.1145/1978942.1979124", "10.1016/j.ijhcs.2003.08.005", "10.2312/eurovisstar.20151113", "10.1109/infvis.2005.1532127"], "wos": 1, "children": [], "len": 1}], "len": 17}, {"doi": "10.1109/tvcg.2019.2934283", "title": "What is Interaction for Data Visualization?", "year": "2019", "conferenceName": "InfoVis", "authors": "Evanthia Dimara;Charles Perin", "citationCount": "1", "affiliation": "Dimara, E (Corresponding Author), Sorbonne Univ, Paris, France. Dimara, Evanthia, Sorbonne Univ, Paris, France. Perin, Charles, Univ Victoria, Victoria, BC, Canada.", "countries": "Canada;France", "abstract": "Interaction is fundamental to data visualization, but what \u201cinteraction\u201d means in the context of visualization is ambiguous and confusing. We argue that this confusion is due to a lack of consensual definition. To tackle this problem, we start by synthesizing an inclusive view of interaction in the visualization community \u2013 including insights from information visualization, visual analytics and scientific visualization, as well as the input of both senior and junior visualization researchers. Once this view takes shape, we look at how interaction is defined in the field of human-computer interaction (HCI). By extracting commonalities and differences between the views of interaction in visualization and in HCI, we synthesize a definition of interaction for visualization. Our definition is meant to be a thinking tool and inspire novel and bolder interaction design practices. We hope that by better understanding what interaction in visualization is and what it can be, we will enrich the quality of interaction in visualization systems and empower those who use them.", "keywords": "interaction,visualization,data,definition,human-computer interaction", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934283", "refList": ["10.1057/ivs.2009.22", "10.1515/icom-2017-0027", "10.1145/2493102.2493104", "10.1007/978-3-319-06793-3\\_6", "10.1080/03640210801898177", "10.1109/mcg.2010.30", "10.1109/tvcg.2013.134", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2018.2865237", "10.1109/2945.981847", "10.1145/2598784.2598806", "10.1109/tvcg.2013.130", "10.1109/tvcg.2007.70577", "10.1109/mic.2015.129", "10.1145/2909132.2909267", "10.1080/01449290500330331", "10.1109/tvcg.2018.2865233", "10.1109/tvcg.2015.2467831", "10.1109/iv.2015.34", "10.1109/tvcg.2009.111", "10.1145/3290605.3300565", "10.1145/3173574.3173797", "10.1145/948496.948514", "10.1145/3025453.3025765", "10.1109/tvcg.2015.2467613", "10.1145/2659796", "10.1109/tvcg.2014.2346311", "10.1145/3025453.3025524", "10.1080/17452759.2011.558588", "10.1145/3027063.3053113", "10.1109/infvis.2005.1532136", "10.1145/2470654.2481307", "10.1109/tvcg.2018.2864913", "10.1145/345513.345267", "10.3102/00028312005004437", "10.1109/tvcg.2007.70436", "10.1037/0033-295x.106.4.643", "10.1145/2133416.2146416", "10.1109/tvcg.2013.191", "10.1109/tvcg.2010.177", "10.1145/960201.957206", "10.1109/tvcg.2007.70515", "10.1109/infvis.2000.885092", "10.1145/2636240.2636844", "10.1037/h0055392", "10.1177/1473871611413180", "10.1109/vl.1996.545307", "10.1111/j.1471-1842.2009.00848.x", "10.1145/989863.989865", "10.1109/tvcg.2013.124", "10.1109/tvcg.2008.109", "10.1145/1166253.1166265", "10.1145/2702123.2702180", "10.1109/tvcg.2016.2598620", "10.1080/07370024.2016.1226139", "10.1145/3173574.3173909", "10.1109/pacificvis.2010.5429613", "10.1111/j.1467-6478.2006.00368.x", "10.1109/tvcg.2012.204", "10.1109/tvcg.2013.120", "10.1179/1743277412y.0000000019", "10.1145/1936652.1936684", "10.2307/1269768", "10.1109/infvis.1996.559213", "10.1109/tvcg.2016.2598839", "10.1145/2642918.2647360", "10.1057/palgrave.ivs.9500099", "10.1016/j.cag.2009.06.004", "10.1109/mc.2013.178", "10.1109/tvcg.2007.70541", "10.1109/vast.2011.6102473", "10.1145/358886.358895", "10.1109/tvcg.2014.2346573", "10.1109/tvcg.2018.2865159", "10.1145/2207676.2207741", "10.1109/tvcg.2015.2396062", "10.1145/2207676.2208572", "10.1109/tvcg.2016.2598608", "10.1057/ivs.2008.31", "10.1177/001316446002000104", "10.1109/tvcg.2017.2680452", "10.1109/tvcg.2006.80", "10.1145/2598510.2598566", "10.1037/0003-066x.51.4.355", "10.7146/dpb.v16i224.7586", "10.1109/infvis.1998.729560", "10.1162/leon\\_a\\_00011", "10.1109/tvcg.2010.157", "10.1109/tvcg.2014.2359887"], "wos": 1, "children": [], "len": 1}], "len": 21}, {"doi": "10.1109/tvcg.2018.2865025", "title": "An Information-Theoretic Approach to the Cost-benefit Analysis of Visualization in Virtual Environments", "year": "2018", "conferenceName": "VAST", "authors": "Min Chen;Kelly P. Gaither;Nigel W. John;Brian McCann", "citationCount": "1", "affiliation": "Chen, M (Corresponding Author), Univ Oxford, Oxford, England. Chen, Min, Univ Oxford, Oxford, England. Gaither, Kelly; McCann, Brian, Univ Texas Austin, Austin, TX 78712 USA. John, Nigel W., Univ Chester, Chester, Cheshire, England.", "countries": "USA;England", "abstract": "Visualization and virtual environments (VEs) have been two interconnected parallel strands in visual computing for decades. Some VEs have been purposely developed for visualization applications, while many visualization applications are exemplary showcases in general-purpose VEs. Because of the development and operation costs of VEs, the majority of visualization applications in practice have yet to benefit from the capacity of VEs. In this paper, we examine this status quo from an information-theoretic perspective. Our objectives are to conduct cost-benefit analysis on typical VE systems (including augmented and mixed reality, theater-based systems, and large powerwalls), to explain why some visualization applications benefit more from VEs than others, and to sketch out pathways for the future development of visualization applications in VEs. We support our theoretical propositions and analysis using theories and discoveries in the literature of cognitive sciences and the practical evidence reported in the literatures of visualization and VEs.", "keywords": "Theory of visualization,virtual environments,four levels of visualization,virtual reality,augmented reality,mixed reality,cost-benefit analysis,information theory,cognitive sciences,visualization applications,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865025", "refList": ["10.1109/tvcg.2011.231", "10.1007/s00268-007-9307-9", "10.1162/pres.1995.4.1.64", "10.1038/81497", "10.1523/jneurosci.0647-08.2008", "10.1037/a0029856", "10.1038/nature03390", "10.1016/s1364-6613(97)01080-2", "10.1111/cgf.13169", "10.1364/josaa.20.001419", "10.1109/tvcg.2013.127", "10.1037/0033-295x.101.2.343", "10.1109/tvcg.2012.42", "10.1007/s002210100745", "10.1109/vl.1996.545307", "10.1016/s0959-4388(98)80140-2", "10.1007/s00221-006-0804-0", "10.1016/0001-6918(67)90080-7", "10.1523/jneurosci.4319-03.2004", "10.1037/0096-3445.109.2.160", "10.1145/2330667.2330687", "10.1109/tvcg.2010.132", "10.1037/0033-295x.113.4.766", "10.1145/22949.22950", "10.1002/j.1538-7305.1948.tb00917.x", "10.1109/jdt.2008.2001575", "10.1109/tvcg.2008.142", "10.1016/s0079-6123(06)55002-2", "10.1109/mcg.2018.032421653", "10.1038/17953", "10.1162/105474602760204309", "10.1145/253284.253301", "10.1007/978-3-662-43790-2\\_6", "10.1038/nn963", "10.1067/mob.2002.127361", "10.1037/a0033101", "10.1109/ldav.2012.6378981", "10.1113/jphysiol.1964.sp007485", "10.1016/j.tics.2005.02.009", "10.1146/annurev.ne.18.030195.001205", "10.1109/infvis.2004.59", "10.1111/j.1460-2466.1992.tb00812.x", "10.1089/109493101300117884", "10.1109/tvcg.2010.131", "10.1016/s0896-6273(02)01003-6", "10.1016/s0042-6989(01)00102-x", "10.1016/0010-0285(80)90005-5", "10.1109/mcg.2014.18", "10.1109/38.963459", "10.1109/tvcg.2012.133", "10.1007/978-1-4899-5379-7\\_8", "10.1145/2556288.2557020", "10.1109/mcg.2013.37", "10.1109/tvcg.2014.20", "10.3758/bf03200774", "10.1097/sla.0b013e318288c40b", "10.1016/j.cub.2009.12.014", "10.1016/j.cag.2012.04.007", "10.1109/mcg.2014.80", "10.1111/j.1467-8659.2012.03114.x", "10.1111/j.1600-0412.2012.01482.x", "10.1109/tvcg.2006.184", "10.1007/s11548-013-0929-0", "10.1007/978-1-4899-5379-7", "10.1016/0042-6989(84)90041-5", "10.1109/tvcg.2014.2346325", "10.1145/2702123.2702406", "10.1162/pres.1992.1.1.120", "10.1038/36846", "10.1167/7.5.6", "10.1146/annurev.psych.53.100901.135125", "10.1145/1128923.1128948"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934264", "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;David S. Ebert;Arif Ghafoor", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Karimzadeh, Morteza; Ebert, David S.; Ghafoor, Arif, Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.", "keywords": "Summative evaluation,usefulness,evaluation process,taxonomy,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934264", "refList": ["10.1109/tvcg.2017.2744478", "10.1109/tvcg.2018.2865025", "10.1109/tvcg.2006.85", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2017.2745181", "10.1111/cgf.13677", "10.1109/tvcg.2018.2864844", "10.1109/tvcg.2013.126", "10.1109/tvcg.2018.2864811", "10.1109/infvis.2005.1532147", "10.1177/0956797613504966", "10.1145/2669557.2669579", "10.1109/mcg.2005.102", "10.1109/visual.2003.1250426", "10.1136/bmj.39489.470347.ad", "10.1109/tvcg.2017.2744080", "10.1109/mcg.2009.53", "10.1111/j.1467-8527.2005.00307.x", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2864886", "10.1109/tvcg.2018.2864843", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2018.2865051", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2018.2865026", "10.1007/978-3-540-71080-6\\_6", "10.1109/tvcg.2018.2865020", "10.1177/1473871611407399", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2018.2864526", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2864905", "10.1049/sej.1991.0040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2711030", "10.1109/tvcg.2011.279", "10.1109/vast.2017.8585505", "10.1147/jrd.2010.2042914", "10.1016/s0378-7206(98)00044-5", "10.1145/2993901.2993913", "10.1109/tvcg.2018.2865041", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744758", "10.1145/1168149.1168158", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2012.213", "10.1109/tvcg.2017.2744738", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2018.2864826", "10.1145/1377966.1377974", "10.1109/apec.2009.4802646", "10.1145/1168149.1168152", "10.1016/j.jss.2008.03.059", "10.1109/vast.2017.8585484", "10.1109/tvcg.2017.2744818", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2865042", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030388", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "year": "2020", "conferenceName": "SciVis", "authors": "Pepe Eulzer;Sabine Bauer;Francis Kilian;Kai Lawonn", "citationCount": "0", "affiliation": "Eulzer, P (Corresponding Author), Univ Jena, Jena, Germany. Eulzer, Pepe; Lawonn, Kai, Univ Jena, Jena, Germany. Bauer, Sabine, Univ Koblenz Landau, Koblenz, Germany. Kilian, Francis, Cath Clin Koblenz Montabaur, Dept Spine Surg, Koblenz, Germany.", "countries": "Germany", "abstract": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. $\\mathrm{By}$ linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. $\\mathrm{in}$ a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.", "keywords": "Medical visualization,bioinformatics,coordinated views,focus and context,biomechanical simulation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030388", "refList": ["10.1109/tvcg.2018.2864903", "10.1177/1473871613510429", "10.1093/ehjqcco/qcz052", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2018.2865076", "10.1055/s-0039-1687862", "10.1109/visual.1990.146375", "10.1109/tvcg.2017.2744198", "10.1016/j.ijmedinf.2014.10.001", "10.1109/tvcg.2013.124", "10.1016/j.jacc", "10.1111/cgf.13167", "10.17705/1thci.00055", "10.1136/bmjqs.2009.037895", "10.1109/tvcg.2013.238", "10.1109/tvcg.2018.2865240", "10.1186/1471-2261-6-34", "10.1109/tvcg.2019.2934264", "10.1109/tvcg.2013.200", "10.1109/tvcg.2011.209", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467091", "10.1136/bmjopen-2019-033208", "10.1109/beliv.2018.8634027", "10.1109/tvcg.2012.213", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2015.2467325", "10.1145/2133806.2133821", "10.1145/1806799.1806866", "10.1108/02635570610688869", "10.1002/hbm.20701", "10.1561/1100000039", "10.1145/3025453.3025645", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 5}], "len": 35}, {"doi": "10.1111/cgf.13207", "year": "2017", "title": "Internal and External Visual Cue Preferences for Visualizations in Presentations", "conferenceName": "EuroVis", "authors": "Ha Kyung Kong;Zhicheng Liu;Karrie Karahalios", "citationCount": "5", "affiliation": "Kong, HK (Corresponding Author), Univ Illinois, Champaign, IL 61820 USA.\nKong, Ha-Kyung; Karahalios, Karrie, Univ Illinois, Champaign, IL 61820 USA.\nLiu, Zhicheng; Karahalios, Karrie, Adobe Res, Champaign, IL USA.", "countries": "USA", "abstract": "Presenters, such as analysts briefing to an executive committee, often use visualizations to convey information. In these cases, providing clear visual guidance is important to communicate key concepts without confusion. This paper explores visual cues that guide attention to a particular area of a visualization. We developed a visual cue taxonomy distinguishing internal from external cues, designed a web tool based on the taxonomy, and conducted a user study with 24 participants to understand user preferences in choosing visual cues. Participants perceived internal cues (e.g., transparency, brightness, and magnification) as the most useful visual cues and often combined them with other internal or external cues to emphasize areas of focus for their audience. Interviews also revealed that the choice of visual cues depends on not only the chart type, but also the presentation setting, the audience, and the function cues are serving. Considering the complexity of choosing visual cues, we provide design implications for improving the organization, consistency, and integration of visual cues within existing workflows.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13207", "refList": ["10.1002/acp.1348", "10.1111/cgf.12106", "10.1109/tvcg.2015.2467201", "10.1109/vl.1996.545307", "10.1007/3-540-30790-7\\_18", "10.1109/iv.2010.21", "10.1109/38.974515", "10.1007/978-3-642-78069-1\\_10", "10.1109/mcg.2015.99", "10.1080/15230406.2013.803706", "10.1007/s10648-009-9098-7", "10.1109/tvcg.2012.229", "10.1016/j.compedu.2010.10.007", "10.1145/2556288.2557141", "10.1145/2470654.2481374", "10.1146/annurev.psych.48.1.269", "10.1145/180171.180173", "10.1016/j.learninstruc.2009.02.010", "10.1109/tvcg.2014.2346352", "10.1559/15230406384373", "10.1007/978-3-642-12670-3.3", "10.1109/tvcg.2010.179", "10.1002/acp.1346", "10.1145/1056808.1057073"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865145", "title": "Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication", "year": "2018", "conferenceName": "InfoVis", "authors": "Arjun Srinivasan;Steven Mark Drucker;Alex Endert;John T. Stasko", "citationCount": "12", "affiliation": "Srinivasan, A (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA. Srinivasan, Arjun; Endert, Alex; Stasko, John, Georgia Inst Technol, Atlanta, GA 30332 USA. Drucker, Steven M., Microsoft Res, Washington, DC USA.", "countries": "USA", "abstract": "Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.", "keywords": "Natural Language Generation,Mixed-initiative Interaction,Visualization Recommendation,Data-driven Communication", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865145", "refList": ["10.2307/1269768", "10.1109/tvcg.2017.2744843", "10.1007/s00371-015-1132-9", "10.1145/3172944.3173007", "10.1109/tvcg.2007.70594", "10.1109/visual.1990.146375", "10.1145/1502650.1502695", "10.1145/108360.108361", "10.14778/2733004.2733035", "10.1109/pacificvis.2017.8031599", "10.1109/tvcg.2013.124", "10.1145/2556288.2557241", "10.1109/visual.1992.235203", "10.1109/infvis.2005.1532136", "10.1109/tvcg.2017.2745219", "10.1109/tvcg.2013.119", "10.1145/2984511.2984588", "10.1109/tvcg.2012.229", "10.1145/3025453.3025866", "10.1145/3035918.3035922", "10.1145/2807442.2807478", "10.1109/tvcg.2010.164", "10.1145/302979.303030", "10.1109/mcg.2006.70", "10.1109/mc.2013.36", "10.1109/tvcg.2015.2467191", "10.1017/s1351324997001502", "10.1111/cgf.13207", "10.1109/mcg.2009.22", "10.1145/2702123.2702608"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934669", "title": "Exploranative Code Quality Documents", "year": "2019", "conferenceName": "VAST", "authors": "Haris Mumtaz;Shahid Latif;Fabian Beck;Daniel Weiskopf", "citationCount": "0", "affiliation": "Mumtaz, H (Corresponding Author), Univ Stuttgart, VISUS, Stuttgart, Germany. Mumtaz, Haris; Weiskopf, Daniel, Univ Stuttgart, VISUS, Stuttgart, Germany. Latif, Shahid; Beck, Fabian, Univ Duisburg Essen, Paluno, Duisburg, Germany.", "countries": "Germany", "abstract": "Good code quality is a prerequisite for efficiently developing maintainable software. In this paper, we present a novel approach to generate exploranative (explanatory and exploratory) data-driven documents that report code quality in an interactive, exploratory environment. We employ a template-based natural language generation method to create textual explanations about the code quality, dependent on data from software metrics. The interactive document is enriched by different kinds of visualization, including parallel coordinates plots and scatterplots for data exploration and graphics embedded into text. We devise an interaction model that allows users to explore code quality with consistent linking between text and visualizations; through integrated explanatory text, users are taught background knowledge about code quality aspects. Our approach to interactive documents was developed in a design study process that included software engineering and visual analytics experts. Although the solution is specific to the software engineering scenario, we discuss how the concept could generalize to multivariate data and report lessons learned in a broader scope.", "keywords": "Code quality,interactive documents,natural language generation,sparklines", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934669", "refList": ["10.1109/tvcg.2014.2346435", "10.1109/tvcg.2018.2865022", "10.1016/j.jvlc.2018.10.001", "10.1002/smr.521", "10.1109/tvcg.2006.69", "10.1016/0004-3702(93)90022-4", "10.1145/3173574.3174106", "10.1007/s10648-010-9136-5", "10.3115/974557.974594", "10.1109/vl.1996.545307", "10.1109/tse.1976.233837", "10.1109/vissoft.2018.00010", "10.2312/vissym/vissym04/261-266", "10.1046/j.1365-2575.2002.00117.x", "10.1109/wcre.2002.1173068", "10.1109/icpc.2013.6613830", "10.1145/1985362.1985365", "10.3115/v1/w14-4401", "10.1007/s00766-007-0054-0", "10.1109/vissoft.2017.11", "10.1007/s10515-011-0098-8", "10.1109/vissof.2011", "10.1109/tvcg.2017.2674958", "10.1002/smr.404", "10.1109/32.979986", "10.1016/b978-0-12-397174-6.00010-6", "10.1145/3242587.3242617", "10.1109/mcg.2018.032421649", "10.1613/jair.5477", "10.1109/tfuzz.2014.2328011", "10.1145/2095654.2095665", "10.1109/icpc.2013.6613834", "10.1109/tvcg.2018.2865145", "10.1145/2597008.2597149", "10.1109/live.2013.6617345", "10.1145/1985793.1985868", "10.1002/int.21835", "10.1145/3139295.3139312", "10.1109/32.295895", "10.1109/scam.2014.14", "10.5281/zenodo.3336019", "10.1016/j.visinf.2019.03.004", "10.1109/aswec.2010.18", "10.2312/eurovisshort.20181084"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030403", "title": "Calliope: Automatic Visual Data Story Generation from a Spreadsheet", "year": "2020", "conferenceName": "InfoVis", "authors": "Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi 0007;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, Intelligent Big Data Visualizat Lab, Shanghai, Peoples R China. Shi, Danqing; Xu, Xinyue; Sun, Fuling; Shi, Yang; Cao, Nan, Tongji Univ, Intelligent Big Data Visualizat Lab, Shanghai, Peoples R China.", "countries": "China", "abstract": "Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.", "keywords": "Information Visualization,Visual Storytelling,Data Story", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030403", "refList": ["10.1007/s11063-017-9759-3", "10.1109/tvcg.2016.2598647", "10.1162/0891201054223977", "10.1109/tvcg.2015.2467732", "10.3390/info9030065", "10.1109/tvcg.2019.2934398", "10.1016/j.visinf.2018.12.001", "10.1145/2002353.2002355", "10.1109/tvcg.2007.70594", "10.1111/cgf.12392", "10.14778/2831360.2831371", "10.1093/biomet/33.3.239", "10.1109/mcg.2019.2924636", "10.1109/tvcg.2017.2659744", "10.1109/pacificvis.2009.4906837", "10.1109/pacificvis.2017.8031599", "10.4103/1755-6783.179101", "10.1145/2362394.2362398", "10.1111/cgf.12925", "10.1109/icde.2018.00019", "10.1109/tvcg.2018.2865240", "10.1109/mcg.2015.99", "10.1109/vds48975.2019.8973383", "10.1038/nature16961", "10.1109/tciaig.2012.2186810", "10.1145/3035918.3035922", "10.1145/3197517.3201362", "10.1109/tvcg.2019.2934281", "10.1109/tvcg.2016.2598620", "10.1155/2019/8480905", "10.1109/iccchina.2013.6671183", "10.1109/tvcg.2018.2865145", "10.1145/3299869.3314037", "10.1017/s1351324907004664", "10.1109/tvcg.2019.2934785", "10.1177/1473871618806555", "10.1613/jair.2989", "10.1109/tvcg.2010.179", "10.1145/3303766"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.1043", "year": "2020", "title": "AutoCaption: An Approach to Generate Natural Language Description from Visualization Automatically", "conferenceName": "PacificVis", "authors": "Can Liu;Liwenhan Xie;Yun Han;Datong Wei;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.\nYuan, XR (Corresponding Author), Peking Univ, Sch EECS, Beijing, Peoples R China.\nYuan, XR (Corresponding Author), Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China.\nLiu, Can; Xie, Liwenhan; Han, Yun; Wei, Datong; Yuan, Xiaoru, Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.\nLiu, Can; Xie, Liwenhan; Han, Yun; Wei, Datong; Yuan, Xiaoru, Peking Univ, Sch EECS, Beijing, Peoples R China.\nYuan, Xiaoru, Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China.", "countries": "China", "abstract": "In this paper, we propose a novel approach to generate captions for visualization charts automatically. In the proposed method, visual marks and visual channels, together with the associated text information in the original charts, are first extracted and identified with a multilayer perceptron classifier. Meanwhile, data information can also be retrieved by parsing visual marks with extracted mapping relationships. Then a 1-D convolutional residual network is employed to analyze the relationship between visual elements, and recognize significant features of the visualization charts, with both data and visual information as input. In the final step, the full description of the visual charts can be generated through a template-based approach. The generated captions can effectively cover the main visual features of the visual charts and support major feature types in commons charts. We further demonstrate the effectiveness of our approach through several cases.", "keywords": "", "link": "https://doi.org/10.1109/PacificVis48177.2020.1043", "refList": ["10.1145/1414471.1414525", "10.1111/cgf.13193", "10.1109/tvcg.2018.2865145", "10.1109/is.2002.1044219", "10.1145/2470654.2481374", "10.1080/13614568.2010.534186", "10.1017/s1351324997001502", "10.1145/3035918.3035922", "10.1109/cvpr.2016.90", "10.1145/2047196.2047247", "10.1007/s11257-006-9002-9", "10.1109/cvpr.2015.7298935", "10.1145/1148170.1148270", "10.1109/72.279181", "10.1109/icsmc.2011.6084067"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934555", "title": "Pattern-Driven Navigation in 2D Multiscale Visualizations with Scalable Insets", "year": "2019", "conferenceName": "InfoVis", "authors": "Fritz Lekschas;Michael Behrisch;Benjamin Bach;Peter Kerpedjiev;Nils Gehlenborg;Hanspeter Pfister", "citationCount": "2", "affiliation": "Lekschas, F (Corresponding Author), Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Lekschas, Fritz; Behrisch, Michael; Pfister, Hanspeter, Harvard Univ, Sch Engn \\& Appl Sci, Cambridge, MA 02138 USA. Bach, Benjamin, Univ Edinburgh, Edinburgh, Midlothian, Scotland. Kerpedjiev, Peter; Gehlenborg, Nils, Harvard Med Sch, Boston, MA 02115 USA.", "countries": "Scotland;USA", "abstract": "We present Scalable Insets, a technique for interactively exploring and navigating large numbers of annotated patterns in multiscale visualizations such as gigapixel images, matrices, or maps. Exploration of many but sparsely-distributed patterns in multiscale visualizations is challenging as visual representations change across zoom levels, context and navigational cues get lost upon zooming, and navigation is time consuming. Our technique visualizes annotated patterns too small to be identifiable at certain zoom levels using insets, i.e., magnified thumbnail views of the annotated patterns. Insets support users in searching, comparing, and contextualizing patterns while reducing the amount of navigation needed. They are dynamically placed either within the viewport or along the boundary of the viewport to offer a compromise between locality and context preservation. Annotated patterns are interactively clustered by location and type. They are visually represented as an aggregated inset to provide scalable exploration within a single viewport. In a controlled user study with 18 participants, we found that Scalable Insets can speed up visual search and improve the accuracy of pattern comparison at the cost of slower frequency estimation compared to a baseline technique. A second study with 6 experts in the field of genomics showed that Scalable Insets is easy to learn and provides first insights into how Scalable Insets can be applied in an open-ended data exploration scenario.", "keywords": "Guided Navigation,Pattern Exploration,Multiscale Visualizations,Gigapixel Images,Geospatial Maps,Genomics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934555", "refList": ["10.1109/tvcg.2011.231", "10.1111/cgf.12333", "10.1109/vast.2009.5333443", "10.1109/tvcg.2013.154", "10.1109/tvcg.2011.185", "10.1016/j.comgeo.2009.03.006", "10.1109/tvcg.2013.213", "10.1111/cgf.12871", "10.1063/1.1699114", "10.1145/22339.22342", "10.1145/2984511.2984578", "10.1016/j.comgeo.2006.05.003", "10.1145/223355.223749", "10.1016/j.cell.2014.11.021", "10.1145/302979.303148", "10.1111/j.1467-8659.2009.01450.x", "10.1126/science.298.5594.824", "10.1145/2470654.2466444", "10.1109/tvcg.2017.2745978", "10.1109/tvcg.2007.70589", "10.1145/212332.212334", "10.1007/s00453-015-0028-4", "10.1109/tvcg.2014.2346441", "10.1111/cgf.12615", "10.1109/tvcg.2006.136", "10.1007/978-3-319-27261-0\\_42", "10.1559/15230406384373", "10.1145/1618452.1618513", "10.1109/tvcg.2014.2346352", "10.1117/12.2231191", "10.1007/978-3-642-12670-3.3", "10.1038/nature23884", "10.1111/j.1467-8659.2011.01935.x", "10.3138/u3n2-6363-130n-h870", "10.1186/s13059-018-1486-1", "10.1111/cgf.12936", "10.1109/tvcg.2009.65", "10.1111/cgf.13207"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030378", "title": "NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries", "year": "2020", "conferenceName": "InfoVis", "authors": "Arpit Narechania;Arjun Srinivasan;John T. Stasko", "citationCount": "1", "affiliation": "Narechania, A (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA. Narechania, Arpit; Srinivasan, Arjun; Stasko, John, Georgia Inst Technol, Atlanta, GA 30332 USA.", "countries": "USA", "abstract": "Natural language interfaces (NLls) have shown great promise for visual data analysis, allowing people to flexibly specify and interact with visualizations. However, developing visualization NLIs remains a challenging task, requiring low-level implementation of natural language processing (NLP) techniques as well as knowledge of visual analytic tasks and visualization design. We present NL4DV, a toolkit for natural language-driven data visualization. NL4DV is a Python package that takes as input a tabular dataset and a natural language query about that dataset. In response, the toolkit returns an analytic specification modeled as a JSON object containing data attributes, analytic tasks, and a list of Vega-Lite specifications relevant to the input query. In doing so, NL4DV aids visualization developers who may not have a background in NLP, enabling them to create new visualization NLIs or incorporate natural language input within their existing systems. We demonstrate NL4DV's usage and capabilities through four examples: 1) rendering visualizations using natural language in a Jupyter notebook, 2) developing a NLI to specify and edit Vega-Lite charts, 3) recreating data ambiguity widgets from the DataTone system, and 4) incorporating speech input to create a multimodal visualization system.", "keywords": "Natural Language Interfaces,Visualization Toolkits", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030378", "refList": ["10.1109/tvcg.2014.2346249", "10.1145/3230623", "10.1111/cgf.12106", "10.1109/visual.2005.1532788", "10.1016/j.infsof.2018.01.005", "10.1109/tvcg.2012.237", "10.1109/tvcg.2011.185", "10.1109/vlhcc.2004.13", "10.1109/oecc.2012.6276803", "10.1111/j.1467-8659.2012.03093.x", "10.1029/2018jd029522", "10.1109/tvcg.2007.70535", "10.1080/14626260701532033", "10.1038/s41592-018-0175-z", "10.1016/j.cell.2014.11.021", "10.1080/07370024.2013.812411", "10.1109/tvcg.2017.2745978", "10.1007/s11263-009-0275-4", "10.1109/tvcg.2015.2467851", "10.1109/tvcg.2008.125", "10.1111/cgf.12615", "10.1109/tvcg.2015.2502587", "10.1109/2945.942695", "10.1109/cvprw.2009.5206848", "10.1145/376929.376932", "10.1109/tvcg.2019.2934555", "10.1057/palgrave.ivs.9500025", "10.1145/357423.357430"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934785", "title": "Text-to-Viz: Automatic Generation of Infographics from Proportion-Related Natural Language Statements", "year": "2019", "conferenceName": "InfoVis", "authors": "Weiwei Cui;Xiaoyu Zhang;Yun Wang 0012;He Huang;Bei Chen;Lei Fang;Haidong Zhang;Jian-Guan Lou;Dongmei Zhang", "citationCount": "1", "affiliation": "Cui, WW (Corresponding Author), Microsoft Res Asia, Beijing, Peoples R China. Cui, Weiwei; Zhang, Xiaoyu; Wang, Yun; Huang, He; Chen, Bei; Fang, Lei; Zhang, Haidong; Lou, Jian-Guan; Zhang, Dongmei, Microsoft Res Asia, Beijing, Peoples R China. Zhang, Xiaoyu, Univ Calif Davis, ViDi Res Grp, Davis, CA 95616 USA.", "countries": "USA;China", "abstract": "Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.", "keywords": "Visualization for the masses,infographic,automatic visualization,presentation,and dissemination", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934785", "refList": ["10.1145/504704.504705", "10.1109/tvcg.2016.2598647", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2015.2467732", "10.1016/0010-0285(72)90002-3", "10.1145/2702123.2702275", "10.1109/tvcg.2015.2467471", "10.1145/2858036.2858435", "10.1109/tvcg.2012.221", "10.1109/tvcg.2007.70594", "10.1145/2702123.2702545", "10.1109/tvcg.2012.197", "10.1109/mcg.2016.2", "10.1109/pacificvis.2017.8031599", "10.1109/caia.1991.120841", "10.1002/spe.4380111102", "10.1109/icde.2018.00019", "10.1109/tvcg.2018.2865240", "10.1007/978-3-319-91376-6\\_21", "10.1109/tvcg.2015.2467321", "10.1145/2984511.2984588", "10.1111/cgf.12127", "10.1109/tvcg.2016.2598876", "10.1145/2470654.2481374", "10.1016/j.ergon.2008.02.020", "10.1111/cgf.12391", "10.1109/tvcg.2007.70577", "10.3115/1119176.1119206", "10.1109/tvcg.2016.2598620", "10.1109/tvcg.2014.2346291", "10.1177/1473871611415996", "10.1145/2807442.2807478", "10.3115/974557.974586", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2013.234", "10.1177/1473871618806555", "10.1145/3173574.3173697", "10.1109/tvcg.2010.179", "10.1109/tnn.2003.820440", "10.1111/cgf.13207"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030403", "title": "Calliope: Automatic Visual Data Story Generation from a Spreadsheet", "year": "2020", "conferenceName": "InfoVis", "authors": "Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi 0007;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, Intelligent Big Data Visualizat Lab, Shanghai, Peoples R China. Shi, Danqing; Xu, Xinyue; Sun, Fuling; Shi, Yang; Cao, Nan, Tongji Univ, Intelligent Big Data Visualizat Lab, Shanghai, Peoples R China.", "countries": "China", "abstract": "Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.", "keywords": "Information Visualization,Visual Storytelling,Data Story", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030403", "refList": ["10.1007/s11063-017-9759-3", "10.1109/tvcg.2016.2598647", "10.1162/0891201054223977", "10.1109/tvcg.2015.2467732", "10.3390/info9030065", "10.1109/tvcg.2019.2934398", "10.1016/j.visinf.2018.12.001", "10.1145/2002353.2002355", "10.1109/tvcg.2007.70594", "10.1111/cgf.12392", "10.14778/2831360.2831371", "10.1093/biomet/33.3.239", "10.1109/mcg.2019.2924636", "10.1109/tvcg.2017.2659744", "10.1109/pacificvis.2009.4906837", "10.1109/pacificvis.2017.8031599", "10.4103/1755-6783.179101", "10.1145/2362394.2362398", "10.1111/cgf.12925", "10.1109/icde.2018.00019", "10.1109/tvcg.2018.2865240", "10.1109/mcg.2015.99", "10.1109/vds48975.2019.8973383", "10.1038/nature16961", "10.1109/tciaig.2012.2186810", "10.1145/3035918.3035922", "10.1145/3197517.3201362", "10.1109/tvcg.2019.2934281", "10.1109/tvcg.2016.2598620", "10.1155/2019/8480905", "10.1109/iccchina.2013.6671183", "10.1109/tvcg.2018.2865145", "10.1145/3299869.3314037", "10.1017/s1351324907004664", "10.1109/tvcg.2019.2934785", "10.1177/1473871618806555", "10.1613/jair.2989", "10.1109/tvcg.2010.179", "10.1145/3303766"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14004", "year": "2020", "title": "Infomages: Embedding Data into Thematic Images", "conferenceName": "EuroVis", "authors": "Darius Coelho;Klaus Mueller", "citationCount": "0", "affiliation": "Coelho, D (Corresponding Author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.\nCoelho, D.; Mueller, K., SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.", "countries": "USA", "abstract": "Recent studies have indicated that visually embellished charts such as infographics have the ability to engage viewers and positively affect memorability. Fueled by these findings, researchers have proposed a variety of infographic design tools. However, these tools do not cover the entire design space. In this work, we identify a subset of infographics that we call infomages. Infomages are casual visuals of data in which a data chart is embedded into a thematic image such that the content of the image reflects the subject and the designer's interpretation of the data. Creating an effective infomage, however, can require a fair amount of design expertise and is thus out of reach for most people. In order to also afford non-artists with the means to design convincing infomages, we first study the principled design of existing infomages and identify a set of key chart embedding techniques. Informed by these findings we build a design tool that links web-scale image search with a set of interactive image processing tools to empower novice users with the ability to design a wide variety of infomages. As the embedding process might introduce some amount of visual distortion of the data our tool also aids users to gauge the amount of this distortion, if any. We experimentally demonstrate the usability of our tool and conclude with a discussion of infomages and our design tool.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14004", "refList": ["10.1109/mcg.2018.2879066", "10.1109/tvcg.2015.2467732", "10.1002/acp.2350030302", "10.1145/361237.361242", "10.1109/tvcg.2019.2934398", "10.1145/2702123.2702275", "10.1109/tvcg.2019.2934810", "10.1111/cgf.12634", "10.1109/tvcg.2012.221", "10.1145/2702123.2702545", "10.1145/2501988.2502046", "10.1109/tvcg.2012.197", "10.1109/tvcg.2016.2598609", "10.1086/209244", "10.1145/1015706.1015720", "10.1109/tvcg.2011.175", "10.1111/cgf.12888", "10.1109/tvcg.2015.2467321", "10.2307/2288400", "10.1109/tvcg.2006.163", "10.1109/tvcg.2015.2440241", "10.1080/10447319509526110", "10.1109/tvcg.2016.2598620", "10.1109/tvcg.2013.234", "10.1109/tvcg.2019.2934785", "10.1126/science.220.4598.671", "10.1111/j.1469-8986.1993.tb03352.x", "10.1179/000870403235002042", "10.1145/2702123.2702608"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13719", "year": "2019", "title": "Linking and Layout: Exploring the Integration of Text and Visualization in Storytelling", "conferenceName": "EuroVis", "authors": "Qiyu Zhi;Alvitta Ottley;Ronald A. Metoyer", "citationCount": "0", "affiliation": "Zhi, Q (Corresponding Author), Univ Notre Dame, Notre Dame, IN 46556 USA.\nZhi, Qiyu; Metoyer, Ronald, Univ Notre Dame, Notre Dame, IN 46556 USA.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.", "countries": "USA", "abstract": "Modern web technologies are enabling authors to create various forms of text visualization integration for storytelling. This integration may shape the stories' flow and thereby affect the reading experience. In this paper, we seek to understand two text visualization integration forms: (i) different text and visualization spatial arrangements (layout), namely, vertical and slideshow; and (ii) interactive linking of text and visualization (linking). Here, linking refers to a bidirectional interaction mode that explicitly highlights the explanatory visualization element when selecting narrative text and vice versa. Through a crowdsourced study with 180 participants, we measured the effect of layout and linking on the degree to which users engage with the story (user engagement), their understanding of the story content (comprehension), and their ability to recall the story information (recall). We found that participants performed significantly better in comprehension tasks with the slideshow layout. Participant recall was better with the slideshow layout under conditions with linking versus no linking. We also found that linking significantly increased user engagement. Additionally, linking and the slideshow layout were preferred by the participants. We also explored user reading behaviors with different conditions.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13719", "refList": ["10.1111/cgf.13195", "10.1145/2702123.2702452", "10.1109/tvcg.2014.2346575", "10.1145/3025453.3025870", "10.1145/3172944.3173007", "10.1109/tvcg.2011.185", "10.1145/381641.381653", "10.1111/cgf.12392", "10.1006/ijhc.2000.0418", "10.1145/2702123.2702248", "10.1145/2556288.2557241", "10.1109/infvis.2005.1532136", "10.1016/s0079-7421(02)80005-6", "10.1145/2556288.2557141", "10.1145/2470654.2481374", "10.1016/j.compedu.2012.07.011", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2015.2467531", "10.1109/tvcg.2013.191", "10.1109/mc.2013.36", "10.1002/asi.21229", "10.1109/tvcg.2013.234", "10.1109/tvcg.2010.179", "10.1109/tvcg.2011.255", "10.1145/2993901.2993903", "10.1111/cgf.13207", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 23}], "len": 163}, "index": 441, "embedding": [-2.180616855621338, 3.9572458267211914, -1.0741424560546875, -2.5161209106445312, -0.6870212554931641, 0.16650904715061188, 2.5134811401367188, 1.3868077993392944, 3.1229264736175537, 11.189961433410645, -1.3442381620407104, 2.3114640712738037, 4.3741984367370605, 2.768977403640747, 12.4292573928833, 4.908034324645996, 0.8030754923820496, 10.414639472961426, 1.2541606426239014, 3.312558889389038, -0.06630208343267441, 10.578361511230469, 6.280398368835449, 10.884745597839355, -2.565751552581787, 15.142683982849121, -0.5670700073242188, 5.165809631347656, 0.9075107574462891, 2.9222495555877686, 2.7963342666625977, 7.253758430480957], "projection": [1.0535919666290283, 12.440445899963379], "size": 82, "height": 5, "width": 32}