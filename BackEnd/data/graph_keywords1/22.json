{"data": {"doi": "10.1109/pacificvis.2014.24", "year": "2014", "title": "Transfer Function Map", "conferenceName": "PacificVis", "authors": "Hanqi Guo;Wei Li;Xiaoru Yuan", "citationCount": "5", "affiliation": "Guo, HQ (Corresponding Author), Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing, Peoples R China.\nGuo, Hanqi; Li, Wei; Yuan, Xiaoru, Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing, Peoples R China.\nGuo, Hanqi; Li, Wei; Yuan, Xiaoru, Peking Univ, Sch EECS, Beijing, Peoples R China.\nGuo, Hanqi; Yuan, Xiaoru, Peking Univ, Ctr Computat Sci \\& Engn, Beijing, Peoples R China.", "countries": "China", "abstract": "Transfer function design in volume visualization has been a challenging problem due to the huge design space. In this work, we present a system which is capable of integrating the transfer function design results from a group of users. For a specified volume dataset, intermediate and final transfer function designs for many users with different backgrounds are collected. A 2D representation of the transfer function feature space, called transfer function map, is then constructed for each volume data set by MDS projection of the collected transfer function samples. With the proposed transfer function map, interactions, including flexible navigation in the transfer function feature space and transfer function design recommendation, have been developed.", "keywords": "Volume Visualization; Collaborative Visualization; Transfer Function Design", "link": "https://doi.org/10.1109/PacificVis.2014.24", "refList": ["10.1109/tvcg.2008.162", "10.2312/vg/vg-pbg08/041-048", "10.1109/visual.1997.663875", "10.1109/visual.1998.745319", "10.1109/visual.1996.568113", "10.1109/tvcg.2002.1021579", "10.1109/tvcg.2011.261", "10.1109/visual.1999.809871", "10.1109/tvcg.2007.1051", "10.1109/38.511", "10.1109/38.920623", "10.1109/tvcg.2009.189", "10.1109/tvcg.2009.120", "10.1109/visual.1999.809932", "10.1109/visual.2003.1250414", "10.1109/tvcg.2010.35"], "wos": 1, "children": [{"doi": "10.1111/cgf.12934", "year": "2016", "title": "State of the Art in Transfer Functions for Direct Volume Rendering", "conferenceName": "EuroVis", "authors": "Patric Ljung;Jens H. Kr{\\\"{u}}ger;M. Eduard Gr{\\\"{o}}ller;Markus Hadwiger;Charles D. Hansen;Anders Ynnerman", "citationCount": "33", "affiliation": "Ljung, P (Corresponding Author), Linkoping Univ, S-58183 Linkoping, Sweden.\nLjung, Patric; Ynnerman, Anders, Linkoping Univ, S-58183 Linkoping, Sweden.\nKrueger, Jens, Univ Duisburg Essen, CoViDAG, Essen, Germany.\nKrueger, Jens; Hansen, Charles D., Univ Utah, Sci Comp \\& Imaging Inst, Salt Lake City, UT 84112 USA.\nGroeller, Eduard, TU Wien, Vienna, Austria.\nGroeller, Eduard, Univ Bergen, N-5020 Bergen, Norway.\nHadwiger, Markus, King Abdullah Univ Sci \\& Technol, Thuwal, Saudi Arabia.", "countries": "USA;Arabia;Germany;Austria;Sweden;Norway", "abstract": "A central topic in scientific visualization is the transfer function (TF) for volume rendering. The TF serves a fundamental role in translating scalar and multivariate data into color and opacity to express and reveal the relevant features present in the data studied. Beyond this core functionality, TFs also serve as a tool for encoding and utilizing domain knowledge and as an expression for visual design of material appearances. TFs also enable interactive volumetric exploration of complex data. The purpose of this state-of-the-art report (STAR) is to provide an overview of research into the various aspects of TFs, which lead to interpretation of the underlying data through the use of meaningful visual representations. The STAR classifies TF research into the following aspects: dimensionality, derived attributes, aggregated attributes, rendering aspects, automation, and user interfaces. The STAR concludes with some interesting research challenges that form the basis of an agenda for the development of next generation TF tools and methodologies.", "keywords": "", "link": "https://doi.org/10.1111/cgf.12934", "refList": ["10.1109/tvcg.2008.198", "10.2312/vissym/vissym04/017-024.8", "10.2312/vissym/vissym02/115-124", "10.1109/tvcg.2011.97", "10.1109/visual.2003.1250413", "10.1109/tvcg.2015.2467031", "10.1109/tvcg.2009.120", "10.1109/pccga.2004.1348348", "10.1111/j.1467-8659.2011.01944.x", "10.1109/tvcg.2008.170", "10.1109/svv.1998.729588", "10.1111/cgf.12365", "10.1109/tvcg.2009.115", "10.1371/journal.pone.0038586", "10.1109/tvcg.2010.195", "10.1109/tvcg.2006.148", "10.1111/cgf.12623", "10.1109/visual.2003.1250386", "10.1109/sccg.2001.945360", "10.1109/ldav.2014.7013202", "10.1109/pacificvis.2014.24", "10.1109/visual.1999.809932", "10.1109/38.865879", "10.1109/tvcg.2011.261", "10.1109/pacificvis.2009.4906854", "10.1109/tvcg.2008.162", "10.2312/vg/vg-pbg08/041-048", "10.2312/vg/vg06/001-008", "10.1109/tvcg.2014.2346411", "10.1109/2945.998670", "10.2312/vissym/eurovis05/263-270", "10.2312/conf/eg2012/stars/075-094", "10.1016/j.cag.2012.02.007", "10.1109/tvcg.2015.2467294", "10.1109/tvcg.2014.2346351", "10.1109/tvcg.2008.25", "10.2312/vissym/eurovis07/115-122", "10.1109/visual.1999.809886", "10.1111/cgf.12624", "10.1109/tvcg.2008.169", "10.1109/tvcg.2002.1021579", "10.1109/tvcg.2007.1051", "10.1109/tvcg.2010.239", "10.1111/cgf.12371", "10.1109/tvcg.2007.70518", "10.1109/tvcg.2014.2346324", "10.1109/visual.1998.745319", "10.1109/jbhi.2013.2263227", "10.1109/tvcg.2012.80", "10.1109/tvcg.2006.100", "10.1007/s10915-011-9501-7", "10.1109/pacificvis.2013.6596129", "10.1109/pacificvis.2010.5429615", "10.1109/tvcg.2006.124", "10.1145/1375714.1375729", "10.1109/visual.2004.48", "10.1109/ldav.2011.6092313", "10.1109/38.920623", "10.2312/vissym/eurovis05/069-076", "10.1109/2945.646238", "10.1016/j.gmod.2003.08.002", "10.1109/tvcg.2010.35", "10.1109/svv.1998.729580", "10.1109/visual.1995.480803", "10.2312/vissym/eurovis06/251-258", "10.2312/vissym/eurovis06/227-234", "10.1109/tvcg.2015.2467431", "10.2312/vissym/vissym04/017-024", "10.1109/tvcg.2006.39", "10.1016/j.cmpb.2007.03.008", "10.1109/pacificvis.2011.5742368", "10.1109/tvcg.2007.47", "10.1111/j.1467-8659.2007.01095.x", "10.1109/tvcg.2010.170", "10.2312/vcbm/vcbm08/101-108", "10.2312/vissym/eurovis06/243-250", "10.1109/visual.2000.885678", "10.2312/vg/vg07/001-008", "10.1109/tvcg.2009.185", "10.1016/j.cag.2008.08.006", "10.1109/icma.2007.4303986", "10.1109/tvcg.2005.38", "10.1109/tvcg.2006.96", "10.1111/j.1467-8659.2009.01474.x", "10.1109/tvcg.2009.189", "10.2312/vissym/eurovis07/131-138", "10.1109/pccga.2002.1167880", "10.1111/j.1467-8659.2012.03123.x", "10.2312/vissym/eurovis05/271-278", "10.1109/tvcg.2009.25", "10.1111/j.1467-8659.2008.01216.x", "10.1109/tvcg.2012.231", "10.1109/tvcg.2011.258", "10.1111/j.1467-8659.2005.00855.x", "10.1109/visual.1996.568113", "10.1109/tvcg.2005.62", "10.1109/tvcg.2011.23", "10.1109/tvcg.2014.2359462", "10.1109/icics.2009.5397587", "10.1109/2945.942694", "10.1109/pacificvis.2009.4906857", "10.1109/tvcg.2007.70591", "10.1109/pacificvis.2010.5429624", "10.1109/visual.2003.1250414", "10.2312/vg/vg05/137-145", "10.1109/tvcg.2012.105", "10.1111/j.1467-8659.2012.03122.x", "10.1109/38.511", "10.1109/2945.856994", "10.1109/tvcg.2006.72", "10.1109/2945.468400", "10.2312/vg/vg10/077-083", "10.1057/ivs.2010.6"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2017.2744078", "title": "An Intelligent System Approach for Probabilistic Volume Rendering Using Hierarchical 3D Convolutional Sparse Coding", "year": "2017", "conferenceName": "SciVis", "authors": "Tran Minh Quan;Junyoung Choi;Haejin Jeong;Won-Ki Jeong", "citationCount": "4", "affiliation": "Ulsan Nat'l Inst. of Science and Technology (UNIST);Ulsan Nat'l Inst. of Science and Technology (UNIST);Ulsan Nat'l Inst. of Science and Technology (UNIST);Ulsan Nat'l Inst. of Science and Technology (UNIST)", "countries": "Ulsan Nat'l Inst. of Science and Technology (UNIST)", "abstract": "In this paper, we propose a novel machine learning-based voxel classification method for highly-accurate volume rendering. Unlike conventional voxel classification methods that incorporate intensity-based features, the proposed method employs dictionary based features learned directly from the input data using hierarchical multi-scale 3D convolutional sparse coding, a novel extension of the state-of-the-art learning-based sparse feature representation method. The proposed approach automatically generates high-dimensional feature vectors in up to 75 dimensions, which are then fed into an intelligent system built on a random forest classifier for accurately classifying voxels from only a handful of selection scribbles made directly on the input data by the user. We apply the probabilistic transfer function to further customize and refine the rendered result. The proposed method is more intuitive to use and more robust to noise in comparison with conventional intensity-based classification methods. We evaluate the proposed method using several synthetic and real-world volume datasets, and demonstrate the methods usability through a user study.", "keywords": "Volume Rendering,Machine Learning,Hierarchically Convolutional Sparse Coding", "link": "http://dx.doi.org/10.1109/TVCG.2017.2744078", "refList": ["10.1207/s15327051hci0701\\_3", "10.1109/tvcg.2008.162", "10.1109/cvpr.2013.57", "10.1109/cvpr.2015.7299149", "10.1109/isbi.2015.7164109", "10.1109/38.920623", "10.1109/tvcg.2009.25", "10.1109/tvcg.2012.231", "10.1109/cvpr.2010.5539957", "10.1007/978-3-319-46726-9\\_56", "10.1109/tsp.2006.881199", "10.1109/cvpr.2006.142", "10.1109/svv.1998.729588", "10.1109/cvpr.2014.394", "10.1111/cgf.12623", "10.1109/icassp.2014.6854992", "10.1111/cgf.12624", "10.1109/tvcg.2002.1021579", "10.1145/54852.378484", "10.1111/cgf.12934", "10.4135/9781412961288.n154", "10.1109/tip.2015.2495260", "10.1109/tvcg.2012.105", "10.1145/3065386", "10.1109/38.511", "10.1109/tvcg.2005.38", "10.1007/978-3-319-12643-2\\_31", "10.1145/360825.360839", "10.2307/1932409", "10.1109/pacificvis.2013.6596129", "10.1109/tvcg.2011.261", "10.1023/a:1010933404324"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00041", "year": "2019", "title": "DNN-VolVis: Interactive Volume Visualization Supported by Deep Neural Network", "conferenceName": "PacificVis", "authors": "Fan Hong;Can Liu;Xiaoru Yuan", "citationCount": "2", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.\nYuan, XR (Corresponding Author), Peking Univ, Sch EECS, Beijing, Peoples R China.\nYuan, XR (Corresponding Author), Peking Univ, Beijing Engn Technol Res Ctr Virtual Simulat \\& Vi, Beijing, Peoples R China.\nHong, Fan; Liu, Can; Yuan, Xiaoru, Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.\nHong, Fan; Liu, Can; Yuan, Xiaoru, Peking Univ, Sch EECS, Beijing, Peoples R China.\nYuan, Xiaoru, Peking Univ, Beijing Engn Technol Res Ctr Virtual Simulat \\& Vi, Beijing, Peoples R China.", "countries": "China", "abstract": "In this work, we propose a novel approach of volume visualization without explicit traditional rendering pipeline. In our proposed method, volumetric images can be interactively `reversed' given the volumetric data and a static volume rendered image under the desired rendering effect. Our pipeline enables 3D-navigation on it for exploring the given volumetric data without explicit transfer function. In our approach, deep neural networks, combined usage of Generative Adversarial Networks (GANs) and Convolutional Neural Networks (CNN) are employed to synthesize high-resolution and perceptually authentic images directly, inheriting the desired transfer function and viewing parameter implicitly given by the input images respectively.", "keywords": "Deep learning; volume rendering; transfer function; generative adversarial network; machine learning", "link": "https://doi.org/10.1109/PacificVis.2019.00041", "refList": ["10.1109/iccv.2017.629", "10.1109/tvcg.2008.162", "10.2312/vissym/eurovis05/271-278", "10.1109/38.920623", "10.1109/tvcg.2009.25", "10.1109/tvcg.2017.2744078", "10.1109/visual.2003.1250413", "10.1109/cvpr.2017.19", "10.1109/tvcg.2010.35", "10.1109/cvpr.2015.7298594", "10.1109/visual.1996.568113", "10.1109/5.726791", "10.1109/svv.1998.729588", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/83.535842", "10.1016/0893-6080(91)90009-t", "10.1109/tvcg.2006.148", "10.2312/vissym/eurovis07/115-122", "10.1038/323533a0", "10.1109/tvcg.2002.1021579", "10.1109/tvcg.2007.1051", "10.1145/1830483.1830503", "10.1109/cvpr.2016.90", "10.1109/tvcg.2012.80", "10.1007/978-3-319-24574-4\\_28", "10.1109/tvcg.2005.38", "10.1109/tvcg.2009.189", "10.1109/pacificvis.2013.6596129", "10.1109/visual.1999.809932", "10.1007/978-3-319-46493-0\\_47", "10.1109/tvcg.2011.261", "10.1109/pccga.2002.1167880"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030374", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data", "year": "2020", "conferenceName": "SciVis", "authors": "Yifan Wang;Guoli Yan;Haikuan Zhu;Sagar Buch;Ying Wang;E. Mark Haacke;Jing Hua;Zichun Zhong", "citationCount": "0", "affiliation": "Wang, YF (Corresponding Author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Wang, Yifan; Yan, Guoli; Zhu, Haikuan; Hua, Jing; Zhong, Zichun, Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Buch, Sagar; Wang, Ying; Haacke, Ewart Mark, Wayne State Univ, Dept Radiol, Detroit, MI 48201 USA.", "countries": "USA", "abstract": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.", "keywords": "Deep neural network,3D cerebrovascular segmentation and visualization,maximum intensity projection (MIP),joint embedding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030374", "refList": ["10.1109/cluster.2018.00036", "10.1109/iccv.2015.123", "10.1109/tvcg.2013.133", "10.1109/cvpr.2017.19", "10.1109/tvcg.2019.2934312", "10.1007/978-3-319-46487-9\\_40", "10.1109/tvcg.2006.175", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2018.2880207", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/tvcg.2018.2816059", "10.1109/pacificvis.2009.4906852", "10.1109/tvcg.2018.2864808", "10.1109/mcg.2018.2881523", "10.1109/tvcg.2015.2467431", "10.1109/tpami.2015.2439281", "10.1109/pacificvis48177.2020.8737", "10.1109/cvpr.2019.00244", "10.1109/tvcg.2006.165", "10.1109/bigdata.2018.8622520", "10.1007/978-3-319-46466-4\\_29", "10.1109/visual.2019.8933759", "10.1145/3197517.3201304", "10.1109/pacificvis.2011.5742378", "10.1109/cvpr.2006.91", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1007/978-3-319-24574-4\\_28", "10.1109/pacificvis.2019.00041", "10.1109/tvcg.2019.2934332", "10.1109/cvpr.2018.00916", "10.1007/978-3-319-46475-6\\_43"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.8737", "year": "2020", "title": "SSR-VFD: Spatial Super-Resolution for Vector Field Data Analysis and Visualization", "conferenceName": "PacificVis", "authors": "Li Guo;Shaojie Ye;Jun Han;Hao Zheng;Han Gao;Danny Z. Chen;Jian{-}Xun Wang;Chaoli Wang", "citationCount": "1", "affiliation": "Guo, L (Corresponding Author), Nankai Univ, Tianjin, Peoples R China.\nGuo, Li, Nankai Univ, Tianjin, Peoples R China.\nYe, Shaojie, Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.\nHan, Jun; Zheng, Hao; Gao, Han; Chen, Danny Z.; Wang, Jian-Xun; Wang, Chaoli, Univ Notre Dame, Notre Dame, IN 46556 USA.", "countries": "USA;China", "abstract": "We present SSR-VFD, a novel deep learning framework that produces coherent spatial super-resolution (SSR) of three-dimensional vector field data (VFD). SSR-VFD is the first work that advocates a machine learning approach to generate high-resolution vector fields from low-resolution ones. The core of SSR-VFD lies in the use of three separate neural nets that take the three components of a low-resolution vector field as input and jointly output a synthesized high-resolution vector field. To capture spatial coherence, we take into account magnitude and angle losses in network optimization. Our method can work in the in situ scenario where VFD are down-sampled at simulation time for storage saving and these reduced VFD are upsampled back to their original resolution during postprocessing. To demonstrate the effectiveness of SSR-VFD, we show quantitative and qualitative results with several vector field data sets of different characteristics and compare our method against volume upscaling using bicubic interpolation, and two solutions based on CNN and GAN, respectively.", "keywords": "Spatial super-resolution; vector field data; convolutional neural network; deep learning", "link": "https://doi.org/10.1109/PacificVis48177.2020.8737", "refList": ["10.1016/j.ijvsm.2017.05.001", "10.1016/j.jvs.2005.01.020", "10.1109/iccv.2015.123", "10.1111/cgf.13620", "10.1109/cvpr.2019.00831", "10.1109/cvpr.2019.00817", "10.1109/cvpr.2019.00399", "10.1109/tvcg.2019.2934312", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2018.2816059", "10.1109/mcg.2018.2881523", "10.1109/tpami.2015.2439281", "10.1109/bigdata.2018.8622520", "10.1145/3197517.3201304", "10.1109/tvcg.2018.2796085", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1109/pacificvis.2019.00041", "10.1111/cgf.13689"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030374", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data", "year": "2020", "conferenceName": "SciVis", "authors": "Yifan Wang;Guoli Yan;Haikuan Zhu;Sagar Buch;Ying Wang;E. Mark Haacke;Jing Hua;Zichun Zhong", "citationCount": "0", "affiliation": "Wang, YF (Corresponding Author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Wang, Yifan; Yan, Guoli; Zhu, Haikuan; Hua, Jing; Zhong, Zichun, Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Buch, Sagar; Wang, Ying; Haacke, Ewart Mark, Wayne State Univ, Dept Radiol, Detroit, MI 48201 USA.", "countries": "USA", "abstract": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.", "keywords": "Deep neural network,3D cerebrovascular segmentation and visualization,maximum intensity projection (MIP),joint embedding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030374", "refList": ["10.1109/cluster.2018.00036", "10.1109/iccv.2015.123", "10.1109/tvcg.2013.133", "10.1109/cvpr.2017.19", "10.1109/tvcg.2019.2934312", "10.1007/978-3-319-46487-9\\_40", "10.1109/tvcg.2006.175", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2018.2880207", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/tvcg.2018.2816059", "10.1109/pacificvis.2009.4906852", "10.1109/tvcg.2018.2864808", "10.1109/mcg.2018.2881523", "10.1109/tvcg.2015.2467431", "10.1109/tpami.2015.2439281", "10.1109/pacificvis48177.2020.8737", "10.1109/cvpr.2019.00244", "10.1109/tvcg.2006.165", "10.1109/bigdata.2018.8622520", "10.1007/978-3-319-46466-4\\_29", "10.1109/visual.2019.8933759", "10.1145/3197517.3201304", "10.1109/pacificvis.2011.5742378", "10.1109/cvpr.2006.91", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1007/978-3-319-24574-4\\_28", "10.1109/pacificvis.2019.00041", "10.1109/tvcg.2019.2934332", "10.1109/cvpr.2018.00916", "10.1007/978-3-319-46475-6\\_43"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.14037", "year": "2020", "title": "State of the Art in Time-Dependent Flow Topology: Interpreting Physical Meaningfulness Through Mathematical Properties", "conferenceName": "EuroVis", "authors": "Roxana Bujack;Lin Yan;Ingrid Hotz;Christoph Garth;Bei Wang", "citationCount": "0", "affiliation": "Bujack, R (Corresponding Author), Los Alamos Natl Lab, Los Alamos, NM 87545 USA.\nBujack, Roxana, Los Alamos Natl Lab, Los Alamos, NM 87545 USA.\nYan, Lin; Wang, Bei, Univ Utah, Sci Comp \\& Imaging Inst, Salt Lake City, UT 84112 USA.\nHotz, Ingrid, Linkopings Univ, Sci Visualizat Grp, Linkoping, Sweden.\nGarth, Christoph, Univ Kaiserslautern, Kaiserslautern, Germany.", "countries": "Sweden;Germany;USA", "abstract": "We present a state-of-the-art report on time-dependent flow topology. We survey representative papers in visualization and provide a taxonomy of existing approaches that generalize flow topology from time-independent to time-dependent settings. The approaches are classified based upon four categories: tracking of steady topology, reference frame adaption, pathline classification or clustering, and generalization of critical points. Our unique contributions include introducing a set of desirable mathematical properties to interpret physical meaningfulness for time-dependent flow visualization, inferring mathematical properties associated with selective research papers, and utilizing such properties for classification. The five most important properties identified in the existing literature include coincidence with the steady case, induction of a partition within the domain, Lagrangian invariance, objectivity, and Galilean invariance.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14037", "refList": ["10.2514/6.1995-1715", "10.1111/cgf.12100", "10.1007/978-3-540-70823-0\\_1", "10.1111/cgf.12885", "10.1063/1.857730", "10.1109/tvcg.2019.2934312", "10.1109/tvcg.2013.92", "10.1063/1.166399", "10.1109/tvcg.2005.68.3", "10.1063/1.4971788", "10.1111/cgf.12933", "10.1145/3072959.3073684", "10.1063/1.4982720", "10.1109/tvcg.2018.2864432", "10.1017/s0022112004002526", "10.1109/tvcg.2010.93", "10.1109/pacificvis.2016.7465253", "10.1109/tvcg.2018.2864505", "10.1063/1.858828", "10.1109/tvcg.2019.2934255", "10.5194/npg-9-237-2002", "10.1063/1.4800210", "10.1007/978-3-540-88606-8\\_12", "10.1111/j.1467-8659.2011.01942.x", "10.1109/2945.928168", "10.1007/978-3-662-10388-31", "10.1016/j.physd.2013.01.013", "10.1111/cgf.12358", "10.1017/s002211209900720x", "10.1017/s0022112097008057", "10.1111/j.1467-8659.2009.01546.x", "10.1109/2.35197", "10.1111/j.1467-8659.2012.03089.x", "10.1111/cgf.13319", "10.1109/tvcg.2019.2934375.3", "10.1016/j.crme.2015.08.002.4", "10.1016/s0167-2789(00)00142-1", "10.1111/cgf.12359", "10.1103/physreve.93.063107", "10.1016/j.physd.2009.05.005", "10.1063/1.1477449", "10.1111/j.1467-8659.2009.01686.x", "10.1109/pacificvis.2011.5742374", "10.1017/jfm.2013.391", "10.1111/cgf.12121", "10.1186/1743-422x-3-15", "10.1063/1.3502450", "10.1063/1.868323", "10.1016/j.cnsns.2013.05.002", "10.1109/visual.2004.99", "10.1109/visual.2004.107", "10.1111/j.1467-8659.2009.01604.x", "10.1146/annurev-fluid-010313-141322", "10.1063/1.3690153", "10.1109/tvcg.2019.2934375", "10.1109/tvcg.2013.143", "10.1137/130940633", "10.1016/j.cag.2014.01.007", "10.1016/s0097-8493(02)00056-0", "10.1109/vl.1996.545307", "10.1109/tvcg.2011.284", "10.1109/tvcg.2011.265", "10.1017/jfm.2016.792", "10.1109/tvcg.2013.208", "10.1007/s12650-016-0348-8", "10.1023/b:elas.0000005548.36767.e7", "10.1007/978-1-4939-0419-8\\_\\_9", "10.1007/bf00849110", "10.1615/int.j.uncertaintyquantification.2012003956", "10.1017/s0022112096001802", "10.1017/s0962492902000065", "10.1109/pacificvis.2019.00041", "10.1111/j.1467-8659.2011.01901.x", "10.1109/tvcg.2008.33", "10.1016/j.physd.2005.10.007", "10.1016/s0894-1777(96)00090-8", "10.1145/2517327.2442526", "10.1109/tvcg.2017.2743938", "10.1109/tvcg.2018.2816059", "10.1109/tvcg.2019.2934242", "10.2514/6.1995-1715.4", "10.1111/cgf.12109", "10.1109/tvcg.2011.269", "10.1109/visual.1990.146359", "10.1111/j.1467-8659.2003.00723.x", "10.5194/npg-18-977-2011", "10.1109/visual.1998.745296", "10.1109/tvcg.2007.70557", "10.1109/tvcg.2014.2312012"], "wos": 1, "children": [], "len": 1}], "len": 9}], "len": 11}, {"doi": "10.1109/tvcg.2018.2864816", "title": "Interactive Visualization of 3D Histopathology in Native Resolution", "year": "2018", "conferenceName": "SciVis", "authors": "Martin Falk;Anders Ynnerman;Darren Treanor;Claes Lundstr\u00f6m", "citationCount": "1", "affiliation": "Falk, M (Corresponding Author), Linkoping Univ, Dept Sci \\& Technol, Linkoping, Sweden. Falk, Martin; Ynnerman, Anders, Linkoping Univ, Dept Sci \\& Technol, Linkoping, Sweden. Treanor, Darren, Leeds Teaching Hosp NHS Trust, Leeds, W Yorkshire, England. Treanor, Darren; Lundstrom, Claes, Linkoping Univ, Ctr Med Image Sci \\& Visualizat CMIV, Linkoping, Sweden. Lundstrom, Claes, Sectra AB, Linkoping, Sweden.", "countries": "Sweden;England", "abstract": "We present a visualization application that enables effective interactive visual analysis of large-scale 3D histopathology, that is, high-resolution 3D microscopy data of human tissue. Clinical work flows and research based on pathology have, until now, largely been dominated by 2D imaging. As we will show in the paper, studying volumetric histology data will open up novel and useful opportunities for both research and clinical practice. Our starting point is the current lack of appropriate visualization tools in histopathology, which has been a limiting factor in the uptake of digital pathology. Visualization of 3D histology data does pose difficult challenges in several aspects. The full-color datasets are dense and large in scale, on the order of 100,000 \u00d7 100,000 \u00d7 100 voxels. This entails serious demands on both rendering performance and user experience design. Despite this, our developed application supports interactive study of 3D histology datasets at native resolution. Our application is based on tailoring and tuning of existing methods, system integration work, as well as a careful study of domain specific demands emanating from a close participatory design process with domain experts as team members. Results from a user evaluation employing the tool demonstrate a strong agreement among the 14 participating pathologists that 3D histopathology will be a valuable and enabling tool for their work.", "keywords": "Histology,Pathology,Volume Rendering,Expert Evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864816", "refList": ["10.4103/2153-3539.129452", "10.2312/vissym/vissym02/115-124", "10.1371/journal.pone.0126817", "10.1093/ajcp/138.suppl1.288", "10.1109/tvcg.2009.150", "10.4103/2153-3539.151890", "10.1109/mcg.2010.26", "10.1109/tbme.2014.2303294", "10.1109/tvcg.2012.240", "10.1111/cgf.12605", "10.1007/s00371-008-0261-9", "10.1109/tvcg.2002.1021579", "10.1016/j.mri.2012.05.001", "10.1109/pacificvis.2017.8031591", "10.1111/his.12629", "10.2312/vissym/vissym00/137-146", "10.4103/2153-3539.151894", "10.1186/s12859-017-1934-z", "10.1117/12.813756", "10.1145/2834117", "10.1111/his.13452", "10.1111/cgf.12934", "10.1001/jama.2017.14585", "10.1016/j.ajpath.2012.01.033", "10.1109/visual.2002.1183757", "10.4103/2153-3539.114206", "10.17629/www.diagnosticpathology.eu-2016-2:232", "10.1109/mcg.2013.55", "10.1073/pnas.1710742114", "10.1117/12.529137", "10.2312/vg/vg-pbg08/163-170", "10.4103/2153-3539.119005"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13183", "year": "2017", "title": "Visual Analysis of Confocal Raman Spectroscopy Data using Cascaded Transfer Function Design", "conferenceName": "EuroVis", "authors": "Christoph M. Schikora;Markus Plack;Rainer Bornemann;Peter Haring Bol{\\'{\\i}}var;Andreas Kolb", "citationCount": "0", "affiliation": "Schikora, CM (Corresponding Author), Univ Siegen, Comp Graph \\& Multimedia Syst, Siegen, Germany.\nSchikora, Christoph M.; Plack, Markus; Kolb, Andreas, Univ Siegen, Comp Graph \\& Multimedia Syst, Siegen, Germany.\nBornemann, Rainer; Bolivar, Peter Haring, Univ Siegen, High Frequency \\& Quantum Elect, Siegen, Germany.", "countries": "Germany", "abstract": "2D Confocal Raman Microscopy (CRM) data consist of high dimensional per-pixel spectral data of 1000 bands and allows for complex spectral and spatial-spectral analysis tasks, i.e., in material discrimination, material thickness, and spatial material distributions. Currently, simple integral methods are commonly applied as visual analysis solutions to CRM data which exhibit restricted discrimination power in various regards. In this paper we present a novel approach for the visual analysis of 2D multispectral CRM data using multi-variate visualization techniques. Due to the large amount of data and the demand of an explorative approach without a-priori restriction, our system allows for arbitrary interactive (de)selection of varaibles w/o limitation and an unrestricted online definition/construction of new, combined properties. Our approach integrates CRM specific quantitative measures and handles material-related features for mixed materials in a quantitative manner. Technically, we realize the online definition/construction of new, combined properties as semi-automatic, cascaded, 1D and 2D multidimensional transfer functions (MD-TFs). By interactively incorporating new (raw or derived) properties, the dimensionality of the MD-TF space grows during the exploration procedure and is virtually unlimited. The final visualization is achieved by an enhanced color mixing step which improves saturation and contrast.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13183", "refList": ["10.1016/j.sab.2006.12.002", "10.1021/nl061702a", "10.1007/978-3-642-12522-5\\_4", "10.1016/j.carbon.2016.01.001", "10.1137/040616024", "10.1016/j.cag.2012.02.007", "10.1111/cgf.12365", "10.2312/vissym/eur0vis05/117-123", "10.1007/s11664-009-0803-6", "10.1109/mcse.2012.27", "10.1109/tvcg.2002.1021579", "10.1007/978-3-540-85567-5\\_50", "10.1109/pacificvis.2010.5429612", "10.1109/tgrs.2010.2051553", "10.1109/tvcg.2007.70591", "10.1021/ac034173t", "10.1366/000370210792434350", "10.1111/cgf.12934", "10.1109/tvcg.2012.110", "10.1109/tvcg.2012.105", "10.1109/tvcg.2006.164", "10.1109/tvcg.2009.199", "10.1109/visual.2003.1250412", "10.1109/igarss.2011.6049397", "10.1007/978-3-642-12522-5", "10.1109/pacificvis.2013.6596129", "10.1039/c4an01061b", "10.1109/tvcg.2011.261"], "wos": 1, "children": [], "len": 1}], "len": 17}], "len": 19}, "index": 22, "embedding": [-0.6901872158050537, 0.5296863913536072, -1.060706377029419, -1.5809603929519653, -0.6745967864990234, 0.16650904715061188, -0.700136661529541, 0.665309727191925, -0.43282294273376465, 0.8086163401603699, -0.6336372494697571, -0.4594820737838745, 0.5517458915710449, 0.1507781594991684, -0.5093683004379272, 0.8355732560157776, -0.17821377515792847, 1.821367859840393, -0.650787889957428, 1.4306484460830688, -0.06630208343267441, 1.1832729578018188, -0.7503138780593872, 1.1733040809631348, -2.570812940597534, 1.8247865438461304, -0.5500518083572388, 0.27192428708076477, 0.6967512369155884, -1.5152453184127808, -0.44741010665893555, 0.427256315946579], "projection": [-0.35313543677330017, 8.333064079284668], "size": 10, "height": 6, "width": 3}