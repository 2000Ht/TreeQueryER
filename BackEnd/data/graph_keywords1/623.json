{"data": {"doi": "10.1109/tvcg.2016.2599049", "title": "GlyphLens: View-Dependent Occlusion Management in the Interactive Glyph Visualization", "year": "2016", "conferenceName": "SciVis", "authors": "Xin Tong;Cheng Li;Han-Wei Shen", "citationCount": "13", "affiliation": "Tong, X (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA. Tong, Xin; Li, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Glyph as a powerful multivariate visualization technique is used to visualize data through its visual channels. To visualize 3D volumetric dataset, glyphs are usually placed on 2D surface, such as the slicing plane or the feature surface, to avoid occluding each other. However, the 3D spatial structure of some features may be missing. On the other hand, placing large number of glyphs over the entire 3D space results in occlusion and visual clutter that make the visualization ineffective. To avoid the occlusion, we propose a view-dependent interactive 3D lens that removes the occluding glyphs by pulling the glyphs aside through the animation. We provide two space deformation models and two lens shape models to displace the glyphs based on their spatial distributions. After the displacement, the glyphs around the user-interested region are still visible as the context information, and their spatial structures are preserved. Besides, we attenuate the brightness of the glyphs inside the lens based on their depths to provide more depth cue. Furthermore, we developed an interactive glyph visualization system to explore different glyph-based visualization applications. In the system, we provide a few lens utilities that allows users to pick a glyph or a feature and look at it from different view directions. We compare different display/interaction techniques to visualize/manipulate our lens and glyphs.", "keywords": "View-dependent visualization;focus + context techniques;manipulation and deformation;glyph-based techniques;human-computer interaction", "link": "http://dx.doi.org/10.1109/TVCG.2016.2599049", "refList": ["10.1109/visual.2003.1250400", "10.1145/989863.989871", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2013.100", "10.1145/988834.988870", "10.1111/j.1467-8659.2006.01000.x", "10.1109/visual.1993.398849", "10.1109/pacificvis.2015.7156349", "10.1109/pacificvis.2009.4906851", "10.1145/2766890", "10.1109/pacificvis.2015.7156385", "10.1145/1239451.1239482", "10.2312/vissym/vissym04/147-154", "10.1109/tvcg.2015.2443804", "10.1109/tvcg.2006.167", "10.1145/166117.166126", "10.1007/bf01897116", "10.1111/1467-8659.t01-3-00700", "10.1111/j.1467-8659.2012.03115.x", "10.3390/s130506380", "10.1109/mcg.1984.275995", "10.1145/237091.237098", "10.1109/tmi.2009.2016561", "10.1145/1618452.1618504", "10.1109/tvcg.2010.199", "10.1109/tvcg.2010.127", "10.1109/tvcg.2013.121", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2007.1059", "10.1109/ismar.2004.36", "10.1111/cgf.12099", "10.1109/tvcg.2010.157", "10.1109/infvis.1996.559215"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2864690", "title": "Interactive obstruction-free lensing for volumetric data visualization", "year": "2018", "conferenceName": "SciVis", "authors": "Michael Traor\u00e9;Christophe Hurter;Alexandru Telea", "citationCount": "1", "affiliation": "Traore, M (Corresponding Author), French Civil Aviat Univ, ENAC, Toulouse, France. Traore, Michael; Hurter, Christophe, French Civil Aviat Univ, ENAC, Toulouse, France. Telea, Alexandru, Univ Groningen, Inst Johan Bernoulli, Groningen, Netherlands.", "countries": "France;Netherlands", "abstract": "Occlusion is an issue in volumetric visualization as it prevents direct visualization of the region of interest. While many techniques such as transfer functions, volume segmentation or view distortion have been developed to address this, there is still room for improvement to better support the understanding of objects' vicinity. However, most existing Focus+Context fail to solve partial occlusion in datasets where the target and the occluder are very similar density-wise. For these reasons, we investigate a new technique which maintains the general structure of the investigated volumetric dataset while addressing occlusion issues. With our technique, the user interactively defines an area of interest where an occluded region or object is partially visible. Then our lens starts pushing at its border occluding objects, thus revealing hidden volumetric data. Next, the lens is modified with an extended field of view (fish-eye deformation) to better see the vicinity of the selected region. Finally, the user can freely explore the surroundings of the area under investigation within the lens. To provide real-time exploration, we implemented our lens using a GPU accelerated ray-casting framework to handle ray deformations, local lighting, and local viewpoint manipulation. We illustrate our technique with five application scenarios in baggage inspection, 3D fluid flow visualization, chest radiology, air traffic planning, and DTI fiber exploration.", "keywords": "Interaction techniques,focus + context,volume visualization,volume rendering,raycasting", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864690", "refList": ["10.1109/visual.1999.809865", "10.1145/1409060.1409107", "10.1109/visual.2003.1250400", "10.1145/2598153.2598200", "10.1007/978-1-4614-7657-3\\_19", "10.1109/tvcg.2008.59", "10.1145/989863.989871", "10.1109/tvcg.2016.2599049", "10.1145/345513.345271", "10.1016/s1470-2045(17)30438-2", "10.1109/pacificvis.2014.61", "10.1111/j.1467-8659.2006.00979.x", "10.1109/tvcg.2010.35", "10.2312/conf/eg2012/stars/075-094", "10.1111/cgf.12927", "10.1111/cgf.12871", "10.1145/2024156.2024165", "10.1007/978-3-540-85412-8\\_16", "10.1057/ivs.2009.32", "10.1109/tvcg.2006.140", "10.1109/tvcg.2007.70565", "10.1109/tvcg.2015.2403323", "10.1109/visual.2004.32", "10.2200/s00688ed1v01y201512vis006", "10.1109/tvcg.2015.2443804", "10.1109/tvcg.2006.144", "10.1109/tvcg.2016.2515611", "10.1109/tvcg.2007.48", "10.1109/tvcg.2010.193", "10.1109/tvcg.2007.1051", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.223", "10.1109/38.595268", "10.1111/j.1467-8659.2012.03079.x", "10.1109/tvcg.2009.145", "10.1109/pacificvis.2017.8031594", "10.1145/2425296.2425325", "10.1109/tvcg.2012.265", "10.1109/mcg.2017.10", "10.1007/s12031-007-0029-0", "10.1109/tvcg.2010.127", "10.1016/j.trc.2014.03.005", "10.1109/tvcg.2009.138", "10.1109/tvcg.2006.124"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934332", "title": "LassoNet: Deep Lasso-Selection of 3D Point Clouds", "year": "2019", "conferenceName": "SciVis", "authors": "Zhutian Chen;Wei Zeng 0004;Zhiguang Yang;Lingyun Yu;Chi-Wing Fu;Huamin Qu", "citationCount": "4", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Zhutian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei; Yang, Zhiguang, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yu, Lingyun, Univ Groningen, Groningen, Netherlands. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China.", "countries": "China;Netherlands", "abstract": "Selection is a fundamental task in exploratory analysis and visualization of 3D point clouds. Prior researches on selection methods were developed mainly based on heuristics such as local point density, thus limiting their applicability in general data. Specific challenges root in the great variabilities implied by point clouds (e.g., dense vs. sparse), viewpoint (e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this work, we introduce LassoNet, a new deep neural network for lasso selection of 3D point clouds, attempting to learn a latent mapping from viewpoint and lasso to point cloud regions. To achieve this, we couple user-target points with viewpoint and lasso information through 3D coordinate transform and naive selection, and improve the method scalability via an intention filtering and farthest point sampling. A hierarchical network is trained using a dataset with over 30K lasso-selection records on two different point cloud data. We conduct a formal user study to compare LassoNet with two state-of-the-art lasso-selection methods. The evaluations confirm that our approach improves the selection effectiveness and efficiency across different combinations of 3D point clouds, viewpoints, and lasso selections. Project Website: https://LassoNet.github.io", "keywords": "Point Clouds,Lasso Selection,Deep Learning", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934332", "refList": ["10.1111/j.1467-8659.2009.01515.x", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2016.2599049", "10.1109/iccv.2015.114", "10.1109/vppc.2018.8604993", "10.1111/cgf.13405", "10.1109/tvcg.2018.2843369", "10.1109/cvpr.2018.00278", "10.1145/3025453.3025957", "10.1109/mc.2013.178", "10.1145/2980179.2980238", "10.1109/msp.2017.2693418", "10.1145/237091.237105", "10.1145/2835487", "10.1109/cvpr.2010.5539838", "10.1016/j.visinf.2017.01.006", "10.1109/tcst.2018.2819965", "10.1109/tvcg.2018.2865138", "10.1109/83.623193", "10.1145/3072959.3073608", "10.1145/1053427.1053445", "10.1016/s0039-9140(96)02179-0", "10.1109/cvpr.2015.7298801", "10.1109/cvpr.2017.693", "10.1007/s12650-014-0206-5", "10.1145/3272127.3275110", "10.1109/cvpr.2015.7298845", "10.1109/iccvw.2015.112", "10.1109/iros.2015.7353481", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2012.217", "10.1109/tridui.2006.1618279", "10.1109/tvcg.2012.292", "10.1109/cvpr.2016.609", "10.1016/j.cag.2012.12.003"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028947", "title": "A Fluid Flow Data Set for Machine Learning and its Application to Neural Flow Map Interpolation", "year": "2020", "conferenceName": "SciVis", "authors": "Jakob Jakob;Markus H. Gross;Tobias G\u00fcnther", "citationCount": "0", "affiliation": "Jakob, J (Corresponding Author), Swiss Fed Inst Technol, Zurich, Switzerland. Jakob, Jakob; Gross, Markus; Guenther, Tobias, Swiss Fed Inst Technol, Zurich, Switzerland.", "countries": "Switzerland", "abstract": "In recent years, deep learning has opened countless research opportunities across many different disciplines. At present, visualization is mainly applied to explore and explain neural networks. Its counterpart-the application of deep learning to visualization problems-requires us to share data more openly in order to enable more scientists to engage in data-driven research. In this paper, we construct a large fluid flow data set and apply it to a deep learning problem in scientific visualization. Parameterized by the Reynolds number, the data set contains a wide spectrum of laminar and turbulent fluid flow regimes. The full data set was simulated on a high-performance compute cluster and contains 8000 time-dependent 2D vector fields, accumulating to more than 16 TB in size. Using our public fluid data set, we trained deep convolutional neural networks in order to set a benchmark for an improved post-hoc Lagrangian fluid flow analysis. In in-situ settings, flow maps are exported and interpolated in order to assess the transport characteristics of time-dependent fluids. Using deep learning, we improve the accuracy of flow map interpolations, allowing a more precise flow analysis at a reduced memory IO footprint.", "keywords": "Scientific visualization,deep learning,flow maps", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028947", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1145/3355089.3356560", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1007/978-3-030-00533-7\\_36", "10.3390/rs11161921", "10.1007/978-3-030-48457-6\\_1", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/mcg.2018.2881523", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1145/3072959.3073643", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/tvcg.2013.128", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1007/978-3-319-46475-6\\_43", "10.1126/science.1127647", "10.1007/978-3-319-46475-6\\_25", "10.1111/cgf.13689"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030346", "title": "V2V: A Deep Learning Approach to Variable-to-Variable Selection and Translation for Multivariate Time-Varying Data", "year": "2020", "conferenceName": "SciVis", "authors": "Jun Han;Hao Zheng 0006;Yunhao Xing;Danny Ziyi Chen;Chaoli Wang", "citationCount": "0", "affiliation": "Han, J (Corresponding Author), Univ Notre Dame, Notre Dame, IN 46556 USA. Han, Jun; Zheng, Hao; Chen, Danny Z.; Wang, Chaoli, Univ Notre Dame, Notre Dame, IN 46556 USA. Xing, Yunhao, Sichuan Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China;USA", "abstract": "We present V2V, a novel deep learning framework, as a general-purpose solution to the variable-to-variable (V2V) selection and translation problem for multivariate time-varying data (MTVD) analysis and visualization. V2V leverages a representation learning algorithm to identify transferable variables and utilizes Kullback-Leibler divergence to determine the source and target variables. It then uses a generative adversarial network (GAN) to learn the mapping from the source variable to the target variable via the adversarial, volumetric, and feature losses. V2V takes the pairs of time steps of the source and target variable as input for training, Once trained, it can infer unseen time steps of the target variable given the corresponding time steps of the source variable. Several multivariate time-varying data sets of different characteristics are used to demonstrate the effectiveness of V2V, both quantitatively and qualitatively. We compare V2V against histogram matching and two other deep learning solutions (Pix2Pix and CycleGAN).", "keywords": "Multivariate time-varying data,variable selection and translation,generative adversarial network,data extrapolation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030346", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/cvpr.2017.19", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1146/annurev", "10.3390/rs11161921", "10.1109/tvcg.2007.70551", "10.1145/3309993", "10.1007/978-3-030-48457-6\\_1", "10.1109/igarss.2018.8518411", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1146/annurev-fluid-010313-141322", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1126/science.1127647"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030374", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data", "year": "2020", "conferenceName": "SciVis", "authors": "Yifan Wang;Guoli Yan;Haikuan Zhu;Sagar Buch;Ying Wang;E. Mark Haacke;Jing Hua;Zichun Zhong", "citationCount": "0", "affiliation": "Wang, YF (Corresponding Author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Wang, Yifan; Yan, Guoli; Zhu, Haikuan; Hua, Jing; Zhong, Zichun, Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Buch, Sagar; Wang, Ying; Haacke, Ewart Mark, Wayne State Univ, Dept Radiol, Detroit, MI 48201 USA.", "countries": "USA", "abstract": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.", "keywords": "Deep neural network,3D cerebrovascular segmentation and visualization,maximum intensity projection (MIP),joint embedding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030374", "refList": ["10.1109/cluster.2018.00036", "10.1109/iccv.2015.123", "10.1109/tvcg.2013.133", "10.1109/cvpr.2017.19", "10.1109/tvcg.2019.2934312", "10.1007/978-3-319-46487-9\\_40", "10.1109/tvcg.2006.175", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2018.2880207", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/tvcg.2018.2816059", "10.1109/pacificvis.2009.4906852", "10.1109/tvcg.2018.2864808", "10.1109/mcg.2018.2881523", "10.1109/tvcg.2015.2467431", "10.1109/tpami.2015.2439281", "10.1109/pacificvis48177.2020.8737", "10.1109/cvpr.2019.00244", "10.1109/tvcg.2006.165", "10.1109/bigdata.2018.8622520", "10.1007/978-3-319-46466-4\\_29", "10.1109/visual.2019.8933759", "10.1145/3197517.3201304", "10.1109/pacificvis.2011.5742378", "10.1109/cvpr.2006.91", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1007/978-3-319-24574-4\\_28", "10.1109/pacificvis.2019.00041", "10.1109/tvcg.2019.2934332", "10.1109/cvpr.2018.00916", "10.1007/978-3-319-46475-6\\_43"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2018.00035", "year": "2018", "title": "An Automatic Deformation Approach for Occlusion Free Egocentric Data Exploration", "conferenceName": "PacificVis", "authors": "Cheng Li;Joachim Moortgat;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Moortgat, Joachim; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Occlusion management is an important task for three dimension data exploration. For egocentric data exploration, the occlusion problems, caused by the camera being too close to opaque data elements, have not been well addressed by previous studies. In this paper, we propose an automatic approach to resolve these problems and provide an occlusion free egocentric data exploration. Our system utilizes a state transition model to monitor both the camera and the data, and manages the initiation, duration, and termination of deformation with animation. Our method can be applied to multiple types of scientific datasets, including volumetric data, polygon mesh data, and particle data. We demonstrate our method with different exploration tasks, including camera navigation, isovalue adjustment, transfer function adjustment, and time varying exploration. We have collaborated with a domain expert and received positive feedback.", "keywords": "Data Deformation; Occlusion Management; Data Exploration; Egocentric Visualization", "link": "https://doi.org/10.1109/PacificVis.2018.00035", "refList": ["10.1145/989863.989871", "10.1109/tvcg.2008.59", "10.1111/j.1467-8659.2008.01332.x", "10.1109/tvcg.2016.2599049", "10.1109/pacificvis.2014.14", "10.1109/tvcg.2007.70433", "10.1007/s10596-015-9501-z", "10.1109/38.610209", "10.1109/tvcg.2003.1207447", "10.1111/j.1467-8659.2008.01181.x", "10.1109/tvcg.2012.42", "10.1109/tvcg.2016.2599217", "10.1002/fld.3764", "10.1109/pacificvis.2013.6596123", "10.1109/tvcg.2006.140", "10.1109/tvcg.2012.143", "10.1109/tvcg.2007.70565", "10.1109/scivis.2015.7429485", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.167", "10.1145/505008.505039", "10.1109/tvcg.2006.144", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/38.595268", "10.1109/tvcg.2016.2518338", "10.1109/tvcg.2014.20", "10.1145/1980462.1980487", "10.1111/cgf.12466", "10.2312/compaesth/compaesth05/209-216", "10.1109/tvcg.2010.127", "10.1016/j.advwatres.2016.01.002", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2009.144", "10.1145/1462055.1462056"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/pacificvis.2017.8031579", "year": "2017", "title": "Virtual retractor: An interactive data exploration system using physically based deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Xin Tong;Han{-}Wei Shen", "citationCount": "2", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Tong, Xin; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Interactive data exploration plays a fundamental role in analyzing three dimensional scientific data. Occlusion management and context preservation are among the key factors to ensure effective identification and extraction of three-dimensional features. In this paper, we present an interactive data exploration system that utilizes a physically based deformation method to investigate hidden structures of data in three dimensional data sets. While non-physically based methods are popular for visual analytic applications due to their lower computational cost, physically based deformation methods can often better preserve features and their context. Our physically based deformation method preserves data features by setting the mesh properties according to interesting data attributes. We design effective and intuitive interfaces by using a metaphor of virtual retractor, which reflects the cutting and splitting of data that our system is simulating. We demonstrate case studies on multiple particle datasets and volume datasets, and present feedback from a domain user.", "keywords": "K.6.1 {[}Management of Computing and Information Systems]: Project and People Management-Life Cycle; K.7.m {[}The Computing Profession]: Miscellaneous-Ethics", "link": "https://doi.org/10.1109/PACIFICVIS.2017.8031579", "refList": ["10.1145/1409060.1409107", "10.1109/visual.2003.1250400", "10.1145/989863.989871", "10.1109/tvcg.2008.59", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1145/988834.988870", "10.1109/pacificvis.2014.61", "10.1111/j.1467-8659.2006.01000.x", "10.1111/j.1467-8659.2006.00979.x", "10.1145/1239451.1239482", "10.1109/tvcg.2006.140", "10.1109/tvcg.2007.70565", "10.1088/1742-6596/125/1/012076", "10.1109/tvcg.2015.2443804", "10.1109/tvcg.2006.144", "10.1111/j.1467-8659.2012.03115.x", "10.3390/s130506380", "10.1145/237091.237098", "10.1109/tvcg.2015.2502583", "10.1016/j.cag.2007.09.006", "10.1109/38.595268", "10.1145/1618452.1618504", "10.1145/1980462.1980487", "10.3171/2012.5.jns112334", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.1109/tvcg.2010.127", "10.1109/tvcg.2013.121", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.1109/ismar.2004.36", "10.1109/tvcg.2010.157"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis.2018.00035", "year": "2018", "title": "An Automatic Deformation Approach for Occlusion Free Egocentric Data Exploration", "conferenceName": "PacificVis", "authors": "Cheng Li;Joachim Moortgat;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Moortgat, Joachim; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Occlusion management is an important task for three dimension data exploration. For egocentric data exploration, the occlusion problems, caused by the camera being too close to opaque data elements, have not been well addressed by previous studies. In this paper, we propose an automatic approach to resolve these problems and provide an occlusion free egocentric data exploration. Our system utilizes a state transition model to monitor both the camera and the data, and manages the initiation, duration, and termination of deformation with animation. Our method can be applied to multiple types of scientific datasets, including volumetric data, polygon mesh data, and particle data. We demonstrate our method with different exploration tasks, including camera navigation, isovalue adjustment, transfer function adjustment, and time varying exploration. We have collaborated with a domain expert and received positive feedback.", "keywords": "Data Deformation; Occlusion Management; Data Exploration; Egocentric Visualization", "link": "https://doi.org/10.1109/PacificVis.2018.00035", "refList": ["10.1145/989863.989871", "10.1109/tvcg.2008.59", "10.1111/j.1467-8659.2008.01332.x", "10.1109/tvcg.2016.2599049", "10.1109/pacificvis.2014.14", "10.1109/tvcg.2007.70433", "10.1007/s10596-015-9501-z", "10.1109/38.610209", "10.1109/tvcg.2003.1207447", "10.1111/j.1467-8659.2008.01181.x", "10.1109/tvcg.2012.42", "10.1109/tvcg.2016.2599217", "10.1002/fld.3764", "10.1109/pacificvis.2013.6596123", "10.1109/tvcg.2006.140", "10.1109/tvcg.2012.143", "10.1109/tvcg.2007.70565", "10.1109/scivis.2015.7429485", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.167", "10.1145/505008.505039", "10.1109/tvcg.2006.144", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/38.595268", "10.1109/tvcg.2016.2518338", "10.1109/tvcg.2014.20", "10.1145/1980462.1980487", "10.1111/cgf.12466", "10.2312/compaesth/compaesth05/209-216", "10.1109/tvcg.2010.127", "10.1016/j.advwatres.2016.01.002", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2009.144", "10.1145/1462055.1462056"], "wos": 1, "children": [{"doi": "10.1109/pacificvis.2019.00011", "year": "2019", "title": "Object-in-Hand Feature Displacement with Physically-Based Deformation", "conferenceName": "PacificVis", "authors": "Cheng Li;Han{-}Wei Shen", "citationCount": "1", "affiliation": "Li, C (Corresponding Author), Ohio State Univ, Columbus, OH 43210 USA.\nLi, Cheng; Shen, Han-Wei, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Data deformation has been widely used in visualization to obtain an improved view that better helps the comprehension of the data. It has been a consistent pursuit to conduct interactive deformation by operations that are natural to users. In this paper, we propose a deformation system following the object-in-hand metaphor. We utilize a touchscreen to directly manipulate the shape of the data by using fingers. Users can drag data features and move them along with the fingers. Users can also press their fingers to hold other parts of the data fixed during the deformation, or perform cutting on the data using a finger. The deformation is executed using a physically-based mesh, which is constructed to incorporate data properties to make the deformation authentic as well as informative. By manipulating data features as if handling an object in hand, we can successfully achieve less occluded view of the data, or improved feature layout for better view comparison. We present case studies on various types of scientific datasets, including particle data, volumetric data, and streamlines.", "keywords": "Deformation; Physically-Based; Touchscreen", "link": "https://doi.org/10.1109/PacificVis.2019.00011", "refList": ["10.5132/eec.2016.01.01", "10.1109/visual.2003.1250400", "10.1109/tvcg.2008.59", "10.1109/tvcg.2015.2467202", "10.1145/2076354.2076390", "10.1109/tvcg.2013.100", "10.1109/tvcg.2016.2599049", "10.1109/visual.1996.567609", "10.1145/2992154.2996779", "10.1109/tvcg.2016.2599217", "10.1145/2766890", "10.1109/pacificvis.2018.00035", "10.1007/978-3-319-45853-3\\_6", "10.1109/tvcg.2006.140", "10.1111/cgf.12166", "10.1109/tvcg.2007.70565", "10.1109/pccga.2001.962877", "10.1109/pacificvis.2017.8031579", "10.1109/tvcg.2006.144", "10.3897/mycokeys.36.25986", "10.1016/j.cag.2010.01.007", "10.1109/tvcg.2007.48", "10.1111/j.1467-8659.2012.03115.x", "10.1109/tvcg.2015.2502583", "10.1109/tvcg.2011.283", "10.1109/tvcg.2012.70", "10.1145/3025453.3025890", "10.1111/j.1467-8659.2007.01102.x", "10.1145/2993148.2993152", "10.1145/1980462.1980487", "10.3171/jns.1999.90.4.0780", "10.1109/tvcg.2008.132", "10.2312/vg/vg06/009-016", "10.1109/tvcg.2010.34", "10.20380/gi2009.16", "10.1145/3025453.3025863", "10.1109/tvcg.2011.224", "10.1109/tvcg.2010.157"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 7}], "len": 27}, "index": 623, "embedding": [0.6763377785682678, 1.933347463607788, -1.0254276990890503, -2.4638969898223877, -0.6602144837379456, 0.16650904715061188, 3.0059874057769775, 4.178219318389893, -0.43060755729675293, 1.4688706398010254, 0.11467245221138, 0.8377580642700195, -0.13366082310676575, 1.1873377561569214, 1.7637100219726562, 3.9814670085906982, 0.6384739279747009, 2.6495778560638428, -0.6607173681259155, 0.10036624222993851, -0.06630208343267441, 3.4917795658111572, 0.5941677689552307, 1.6404879093170166, -0.5324347019195557, 0.401729017496109, -0.5077023506164551, 1.032248854637146, 3.1099531650543213, 3.888918161392212, 0.0022018111776560545, 2.462890148162842], "projection": [-3.630192279815674, -2.2698681354522705], "size": 14, "height": 4, "width": 7}