{"data": {"doi": "10.1111/cgf.13402", "year": "2018", "title": "Chart Constellations: Effective Chart Summarization for Collaborative and Multi-User Analyses", "conferenceName": "EuroVis", "authors": "Shenyu Xu;Chris Bryan;Jianping Kelvin Li;Jian Zhao;Kwan{-}Liu Ma", "citationCount": "4", "affiliation": "Xu, SY (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA.\nXu, Shenyu; Bryan, Chris; Li, Jianping Kelvin; Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.\nZhao, Jian, FX Palo Alto Lab, Palo Alto, CA USA.", "countries": "USA", "abstract": "Many data problems in the real world are complex and require multiple analysts working together to uncover embedded insights by creating chart-driven data stories. How, as a subsequent analysis step, do we interpret and learn from these collections of charts? We present Chart Constellations, a system to interactively support a single analyst in the review and analysis of data stories created by other collaborative analysts. Instead of iterating through the individual charts for each data story, the analyst can project, cluster, filter, and connect results from all users in a meta-visualization approach. Constellations supports deriving summary insights about prior investigations and supports the exploration of new, unexplored regions in the dataset. To evaluate our system, we conduct a user study comparing it against data science notebooks. Results suggest that Constellations promotes the discovery of both broad and high-level insights, including theme and trend analysis, subjective evaluation, and hypothesis generation.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13402", "refList": ["10.1109/vast.2010.5652880", "10.1126/science.1136800", "10.1145/2669485.2669518", "10.20380/gi2015.16", "10.1109/visual.1996.568118", "10.1177/1473871611412817", "10.1109/vl.1996.545307", "10.1109/tvcg.2014.2346573", "10.1057/palgrave.ivs.9500167", "10.1109/tvcg.2009.162", "10.1109/tvcg.2013.124", "10.1109/tvcg.2009.122", "10.1109/tvcg.2009.104", "10.1109/tvcg.2015.2467615", "10.1109/tvcg.2013.119", "10.1109/visual.1999.809871", "10.1177/104973239300300403", "10.1093/comjnl/27.2.97", "10.1109/vast.2008.4677358", "10.1145/3025453.3025866", "10.1007/s11665-016-2173-6", "10.1109/tvcg.2006.65", "10.1002/meet.2008.1450450234", "10.1145/2396636.2396673", "10.1109/tnn.2003.820440", "10.1002/meet.2009.1450460219", "10.1109/tvcg.2016.2599030", "10.1179/000870403235002042"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030383", "title": "II-20: Intelligent and pragmatic analytic categorization of image collections", "year": "2020", "conferenceName": "VAST", "authors": "Jan Zah\u00e1lka;Marcel Worring;Jarke J. van Wijk", "citationCount": "0", "affiliation": "Zahalka, J (Corresponding Author), Czech Tech Univ, Prague, Czech Republic. Zahalka, Jan, Czech Tech Univ, Prague, Czech Republic. Worring, Marcel, Univ Amsterdam, Amsterdam, Netherlands. van Wijk, Jarke J., Eindhoven Univ Technol, Eindhoven, Netherlands.", "countries": "Republic;Netherlands", "abstract": "In this paper, we introduce 11\u201320 (Image Insight 2020), a multimedia analytics approach for analytic categorization of image collections. Advanced visualizations for image collections exist, but they need tight integration with a machine model to support the task of analytic categorization. Directly employing computer vision and interactive learning techniques gravitates towards search. Analytic categorization, however, is not machine classification (the difference between the two is called the pragmatic gap): a human adds/redefines/deletes categories of relevance on the fly to build insight, whereas the machine classifier is rigid and non-adaptive. Analytic categorization that truly brings the user to insight requires a flexible machine model that allows dynamic sliding on the exploration-search axis, as well as semantic interactions: a human thinks about image data mostly in semantic terms. 11\u201320 brings three major contributions to multimedia analytics on image collections and towards closing the pragmatic gap. Firstly, a new machine model that closely follows the user's interactions and dynamically models her categories of relevance. II-20's machine model, in addition to matching and exceeding the state of the art's ability to produce relevant suggestions, allows the user to dynamically slide on the exploration-search axis without any additional input from her side. Secondly, the dynamic, 1-image-at-a-time Tetris metaphor that synergizes with the model. It allows a well-trained model to analyze the collection by itself with minimal interaction from the user and complements the classic grid metaphor. Thirdly, the fast-forward interaction, allowing the user to harness the model to quickly expand (\u201cfast-forward\u201d) the categories of relevance, expands the multimedia analytics semantic interaction dictionary. Automated experiments show that II-20's machine model outperforms the existing state of the art and also demonstrate the Tetris metaphor's analytic quality. User studies further confirm that II\u201320 is an intuitive, efficient, and effective multimedia analytics tool.", "keywords": "Multimedia analytics,image data,analytic categorization,pragmatic gap", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030383", "refList": ["10.1109/tvcg.2008.137", "10.1145/2882903.2882919", "10.1145/2556647.2556657", "10.1145/1142473.1142574", "10.1109/tvcg.2016.2598471", "10.1109/vast.2017.8585669", "10.1109/tvcg.2014.2346573", "10.1111/j.0956-7976.2005.00782.x", "10.1007/978-3-540-89965-5\\_27", "10.1109/visual.2019.8933611", "10.1109/vast.2010.5653598", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1109/tvcg.2012.271", "10.1109/mcg.2009.49", "10.1057/ivs.2008.31", "10.1111/cgf.12925", "10.1109/hicss.2016.183", "10.1109/tvcg.2016.2598466", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2018.2865117", "10.1109/vast47406.2019.8986948", "10.1016/s0950-7051(00)00101-5", "10.1109/tvcg.2016.2598594", "10.1109/tvcg.2006.101", "10.1111/cgf.12311", "10.1109/vast.2009.5333020", "10.1109/vast.2010.5652885", "10.1111/cgf.13670", "10.1109/wvl.1988.18020", "10.1145/2133806.2133821", "10.1109/vast.2012.6400486"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13997", "year": "2020", "title": "Resolving Conflicting Insights in Asynchronous Collaborative Visual Analysis", "conferenceName": "EuroVis", "authors": "Jianping Kelvin Li;Shenyu Xu;Yecong (Chris) Ye;Kwan{-}Liu Ma", "citationCount": "0", "affiliation": "Li, JK (Corresponding Author), Univ Calif Davis, Davis, CA 95616 USA.\nLi, Jianping Kelvin; Xu, Shenyu; Ye, Yecong (Chris); Ma, Kwan-Liu, Univ Calif Davis, Davis, CA 95616 USA.", "countries": "USA", "abstract": "Analyzing large and complex datasets for critical decision making can benefit from a collective effort involving a team of analysts. However, insights and findings from different analysts are often incomplete, disconnected, or even conflicting. Most existing analysis tools lack proper support for examining and resolving the conflicts among the findings in order to consolidate the results of collaborative data analysis. In this paper, we present CoVA, a visual analytics system incorporating conflict detection and resolution for supporting asynchronous collaborative data analysis. By using a declarative visualization language and graph representation for managing insights and insight provenance, CoVA effectively leverages distributed revision control workflow from software engineering to automatically detect and properly resolve conflicts in collaborative analysis results. In addition, CoVA provides an effective visual interface for resolving conflicts as well as combining the analysis results. We conduct a user study to evaluate CoVA for collaborative data analysis. The results show that CoVA allows better understanding and use of the findings from different analysts.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13997", "refList": ["10.1109/vast.2010.5652880", "10.1145/2642918.2647360", "10.1109/cluster.2017.26", "10.1145/1142473.1142574", "10.1109/vast.2011.6102438", "10.1145/2872332", "10.1145/2669485.2669518", "10.1177/1473871611412817", "10.1109/ms.2012.61", "10.3233/978-1-61499-649-1-87", "10.1109/tvcg.2014.2346573", "10.1057/palgrave.ivs.9500167", "10.1109/pacificvis.2009.4906837", "10.1109/visual.1993.398857", "10.1145/2207676.2208293", "10.1145/381641.381656", "10.1111/cgf.13717", "10.1111/cgf.13402", "10.1057/palgrave.ivs.9500131", "10.1109/vast.2011.6102447", "10.1109/tvcg.2015.2467551", "10.1109/vast.2008.4677358", "10.1177/104973239300300403", "10.1007/978-1-4419-5874-7\\_12", "10.1109/tst.2013.6509100", "10.1109/vast.2014.7042526", "10.1016/s0950-7051(00)00101-5", "10.1109/tvcg.2015.2467091", "10.1109/vast.2010.5652885", "10.1145/3126594.3126642", "10.1109/tvcg.2006.65", "10.1057/palgrave.ivs.9500180", "10.1109/vast.2008.4677365", "10.1109/vast.2009.5333878", "10.1109/tvcg.2016.2598543", "10.1145/1979742.1979570", "10.1109/tvcg.2003.1207445", "10.1109/mcg.2015.50", "10.1080/09546550701246817", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 7}, "index": 1438, "embedding": [2.6326382160186768, -0.2197529822587967, -0.13548921048641205, 2.6577303409576416, 1.3020726442337036, 0.16650904715061188, -0.6933266520500183, 0.39264294505119324, 1.685951590538025, 0.35150691866874695, 2.239323854446411, 0.7290458083152771, 0.5846039652824402, 0.5559218525886536, -0.9044764637947083, -0.5381391644477844, 0.19158639013767242, 0.06749609857797623, 0.46242010593414307, 1.477435827255249, -0.06630208343267441, -0.5573102831840515, 0.5379562973976135, -0.14990495145320892, -0.7554067969322205, 0.16835027933120728, -0.561912477016449, -0.2069312334060669, 0.9709426760673523, -1.2252838611602783, 0.719817042350769, -0.10338609665632248], "projection": [-0.8999763131141663, 6.415163516998291], "size": 4, "height": 2, "width": 3}