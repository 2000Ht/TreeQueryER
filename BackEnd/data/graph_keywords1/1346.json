{"data": {"doi": "10.1111/cgf.12901", "year": "2016", "title": "Towards Quantitative Visual Analytics with Structured Brushing and Linked Statistics", "conferenceName": "EuroVis", "authors": "S. Rados;Rainer Splechtna;Kresimir Matkovic;Mario Duras;M. Eduard Gr{\\\"{o}}ller;Helwig Hauser", "citationCount": "12", "affiliation": "Rados, S (Corresponding Author), VRVis Res Ctr Vienna, Vienna, Austria.\nRados, S.; Splechtna, R.; Matkovic, K., VRVis Res Ctr Vienna, Vienna, Austria.\nDuras, M., AVL Zagreb, Zagreb, Croatia.\nGroeller, E., TU Wien, Vienna, Austria.\nGroeller, E.; Hauser, H., Univ Bergen, N-5020 Bergen, Norway.", "countries": "Croatia;Norway;Austria", "abstract": "Until now a lot of visual analytics predominantly delivers qualitative resultsbased, for example, on a continuous color map or a detailed spatial encoding. Important target applications, however, such as medical diagnosis and decision making, clearly benefit from quantitative analysis results. In this paper we propose several specific extensions to the well-established concept of linking\\&brushing in order to make the analysis results more quantitative. We structure the brushing space in order to improve the reproducibility of the brushing operation, e.g., by introducing the percentile grid. We also enhance the linked visualization with overlaid descriptive statistics to enable a more quantitative reading of the resulting focus+context visualization. Additionally, we introduce two novel brushing techniques: the percentile brush and the Mahalanobis brush. Both use the underlying data to support statistically meaningful interactions with the data. We illustrate the use of the new techniques in the context of two case studies, one based on meteorological data and the other one focused on data from the automotive industry where we evaluate a shaft design in the context of mechanical power transmission in cars.", "keywords": "", "link": "https://doi.org/10.1111/cgf.12901", "refList": ["10.2307/1269768", "10.1109/sibgrapi-t.2010.9", "10.1145/1345448.1345453", "10.1145/2002353.2002355", "10.1006/ijhc.2002.1017", "10.1109/tvcg.2013.254", "10.1007/3-540-30790-7\\_18", "10.1006/ijhc.1017", "10.1111/j.1467-8659.2009.01697.x", "10.1111/j.1467-9280.1997.tb00427.x", "10.1109/visual.1994.346302", "10.1145/102377.115768", "10.1109/tvcg.2008.125", "10.1109/tvcg.2007.70539", "10.1109/2945.856996", "10.1109/tvcg.2012.110", "10.2307/2684298", "10.2307/1422689", "10.1109/visual.1995.485139"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2017.2743859", "title": "MyBrush: Brushing and Linking with Personal Agency", "year": "2017", "conferenceName": "InfoVis", "authors": "Philipp Koytek;Charles Perin;Jo Vermeulen;Elisabeth Andr\u00e9;Sheelagh Carpendale", "citationCount": "5", "affiliation": "Koytek, P (Corresponding Author), Univ Calgary, Calgary, AB, Canada. Koytek, P (Corresponding Author), Augsburg Univ, Augsburg, Germany. Koytek, Philipp; Perin, Charles; Vermeulen, Jo; Carpendale, Sheelagh, Univ Calgary, Calgary, AB, Canada. Koytek, Philipp; Andre, Elisabeth, Augsburg Univ, Augsburg, Germany. Perin, Charles, City Univ London, London, England.", "countries": "Canada;Germany;England", "abstract": "We extend the popular brushing and linking technique by incorporating personal agency in the interaction. We map existing research related to brushing and linking into a design space that deconstructs the interaction technique into three components: source (what is being brushed), link (the expression of relationship between source and target), and target (what is revealed as related to the source). Using this design space, we created MyBrush, a unified interface that offers personal agency over brushing and linking by giving people the flexibility to configure the source, link, and target of multiple brushes. The results of three focus groups demonstrate that people with different backgrounds leveraged personal agency in different ways, including performing complex tasks and showing links explicitly. We reflect on these results, paving the way for future research on the role of personal agency in information visualization.", "keywords": "Brushing,linking,personal agency,coordinated multiple views,interaction,design space,information visualization", "link": "http://dx.doi.org/10.1109/TVCG.2017.2743859", "refList": ["10.1109/tvcg.2011.183", "10.1057/palgrave.ivs.9500057", "10.1109/tvcg.2008.153", "10.2307/1269768", "10.1109/vast.2009.5333443", "10.1057/ivs.2009.22", "10.1109/tvcg.2014.2371858", "10.2307/1390772", "10.1145/345513.345271", "10.1109/tvcg.2013.154", "10.1145/1731903.1731936", "10.1109/iv.1998.694193", "10.1109/tvcg.2014.2346260", "10.1109/tvcg.2011.185", "10.1109/visual.1991.175794", "10.1111/cgf.12901", "10.1007/3-540-30790-7\\_18", "10.1057/palgrave.ivs.9500167", "10.1109/tvcg.2008.116", "10.1109/cmv.2007.20", "10.1109/tvcg.2009.162", "10.1109/cmv.2003.1215008", "10.1145/2207676.2208293", "10.1109/tvcg.2006.99", "10.1109/tvcg.2014.2346279", "10.1111/cgf.12902", "10.1109/2945.981847", "10.1109/tvcg.2011.201", "10.1109/infvis.1999.801858", "10.3389/fpsyg.2016.01272", "10.1109/tvcg.2006.147", "10.1109/visual.1994.346302", "10.1109/visual.2000.885739", "10.18637/jss.v007.i11", "10.1109/infvis.2004.12", "10.1023/a:1021271517844", "10.1109/tvcg.2010.138", "10.3389/fnhum.2013.00514", "10.1145/345513.345282", "10.1109/tvcg.2007.70521", "10.1109/infvis.2004.64", "10.1109/pacificvis.2010.5429609", "10.1109/pacificvis.2012.6183556", "10.1006/obhd.1998.2758", "10.1145/882262.882291", "10.1559/15230406384373", "10.1006/obhd.1998.2756", "10.1109/visual.1996.567800", "10.1080/13658810903214203", "10.1111/j.1467-8659.2012.03121.x", "10.1145/2207676.2208350", "10.1145/1054972.1055031", "10.1111/j.1467-8721.2009.01644.x", "10.1109/visual.1995.485139", "10.1109/infvis.2002.1173157"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865159", "title": "Dynamic Composite Data Physicalization Using Wheeled Micro-Robots", "year": "2018", "conferenceName": "InfoVis", "authors": "Mathieu Le Goc;Charles Perin;Sean Follmer;Jean-Daniel Fekete;Pierre Dragicevic", "citationCount": "3", "affiliation": "Le Goc, M (Corresponding Author), Stanford Univ, Stanford, CA 94305 USA. Le Goc, Mathieu; Follmer, Sean, Stanford Univ, Stanford, CA 94305 USA. Perin, Charles, Univ Victoria, Victoria, BC, Canada. Perin, Charles, City Univ London, London, England. Fekete, Jean-Daniel; Dragicevic, Pierre, INRIA, Saclay, France.", "countries": "Canada;USA;England;France", "abstract": "This paper introduces dynamic composite physicalizations, a new class of physical visualizations that use collections of self-propelled objects to represent data. Dynamic composite physicalizations can be used both to give physical form to well-known interactive visualization techniques, and to explore new visualizations and interaction paradigms. We first propose a design space characterizing composite physicalizations based on previous work in the fields of Information Visualization and Human Computer Interaction. We illustrate dynamic composite physicalizations in two scenarios demonstrating potential benefits for collaboration and decision making, as well as new opportunities for physical interaction. We then describe our implementation using wheeled micro-robots capable of locating themselves and sensing user input, before discussing limitations and opportunities for future work.", "keywords": "information visualization,data physicalization,tangible user interfaces", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865159", "refList": ["10.1145/1057237.1057242", "10.1145/2858036.2858058", "10.1109/tvcg.2008.153", "10.1109/tvcg.2014.2346424", "10.1145/2642918.2647377", "10.1145/2556288.2557379", "10.1145/2702123.2702275", "10.1007/978-3-319-67687-6\\_25", "10.1016/j.intcom.2006.03.006", "10.1559/152304086783900068", "10.1109/tvcg.2012.199", "10.1145/571985.572011", "10.1109/tvcg.2013.227", "10.1057/palgrave.ivs.9500099", "10.1109/mcg.2010.101", "10.1145/2702123.2702237", "10.1109/roman.2013.6628441", "10.1145/2501988.2502032", "10.1109/tvcg.2013.134", "10.1145/1413634.1413696", "10.1109/tvcg.2014.2346250", "10.1145/258549.258803", "10.1109/tvcg.2014.2346984", "10.1145/2807442.2807488", "10.1145/2207676.2208691", "10.1145/2556288.2557231", "10.1111/cgf.12935", "10.1109/tvcg.2014.2346279", "10.1109/tvcg.2014.2346292", "10.1109/tvcg.2016.2598920", "10.1145/3173574.3173728", "10.1145/3025453.3025512", "10.1007/s00779-009-0279-7", "10.1145/1226969.1226984", "10.1109/tvcg.2016.2598498", "10.2307/2288400", "10.1145/2702123.2702180", "10.1016/0004-3702(94)00017-u", "10.1109/tvcg.2007.70539", "10.1145/2858036.2858041", "10.1109/tvcg.2017.2743859", "10.1109/sibgrapi.2007.21", "10.1177/014662168000400305", "10.1145/2396636.2396675", "10.1145/1226969", "10.1145/2702123.2702604", "10.1145/2207676.2208350", "10.1007/s10551-008-9665-8", "10.1145/3130931", "10.1109/comcomap.2012.6154871", "10.1145/2470654.2481359", "10.1145/3025453.3025877", "10.1145/2984511.2984547", "10.1109/tvcg.2014.2359887"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934283", "title": "What is Interaction for Data Visualization?", "year": "2019", "conferenceName": "InfoVis", "authors": "Evanthia Dimara;Charles Perin", "citationCount": "1", "affiliation": "Dimara, E (Corresponding Author), Sorbonne Univ, Paris, France. Dimara, Evanthia, Sorbonne Univ, Paris, France. Perin, Charles, Univ Victoria, Victoria, BC, Canada.", "countries": "Canada;France", "abstract": "Interaction is fundamental to data visualization, but what \u201cinteraction\u201d means in the context of visualization is ambiguous and confusing. We argue that this confusion is due to a lack of consensual definition. To tackle this problem, we start by synthesizing an inclusive view of interaction in the visualization community \u2013 including insights from information visualization, visual analytics and scientific visualization, as well as the input of both senior and junior visualization researchers. Once this view takes shape, we look at how interaction is defined in the field of human-computer interaction (HCI). By extracting commonalities and differences between the views of interaction in visualization and in HCI, we synthesize a definition of interaction for visualization. Our definition is meant to be a thinking tool and inspire novel and bolder interaction design practices. We hope that by better understanding what interaction in visualization is and what it can be, we will enrich the quality of interaction in visualization systems and empower those who use them.", "keywords": "interaction,visualization,data,definition,human-computer interaction", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934283", "refList": ["10.1057/ivs.2009.22", "10.1515/icom-2017-0027", "10.1145/2493102.2493104", "10.1007/978-3-319-06793-3\\_6", "10.1080/03640210801898177", "10.1109/mcg.2010.30", "10.1109/tvcg.2013.134", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2018.2865237", "10.1109/2945.981847", "10.1145/2598784.2598806", "10.1109/tvcg.2013.130", "10.1109/tvcg.2007.70577", "10.1109/mic.2015.129", "10.1145/2909132.2909267", "10.1080/01449290500330331", "10.1109/tvcg.2018.2865233", "10.1109/tvcg.2015.2467831", "10.1109/iv.2015.34", "10.1109/tvcg.2009.111", "10.1145/3290605.3300565", "10.1145/3173574.3173797", "10.1145/948496.948514", "10.1145/3025453.3025765", "10.1109/tvcg.2015.2467613", "10.1145/2659796", "10.1109/tvcg.2014.2346311", "10.1145/3025453.3025524", "10.1080/17452759.2011.558588", "10.1145/3027063.3053113", "10.1109/infvis.2005.1532136", "10.1145/2470654.2481307", "10.1109/tvcg.2018.2864913", "10.1145/345513.345267", "10.3102/00028312005004437", "10.1109/tvcg.2007.70436", "10.1037/0033-295x.106.4.643", "10.1145/2133416.2146416", "10.1109/tvcg.2013.191", "10.1109/tvcg.2010.177", "10.1145/960201.957206", "10.1109/tvcg.2007.70515", "10.1109/infvis.2000.885092", "10.1145/2636240.2636844", "10.1037/h0055392", "10.1177/1473871611413180", "10.1109/vl.1996.545307", "10.1111/j.1471-1842.2009.00848.x", "10.1145/989863.989865", "10.1109/tvcg.2013.124", "10.1109/tvcg.2008.109", "10.1145/1166253.1166265", "10.1145/2702123.2702180", "10.1109/tvcg.2016.2598620", "10.1080/07370024.2016.1226139", "10.1145/3173574.3173909", "10.1109/pacificvis.2010.5429613", "10.1111/j.1467-6478.2006.00368.x", "10.1109/tvcg.2012.204", "10.1109/tvcg.2013.120", "10.1179/1743277412y.0000000019", "10.1145/1936652.1936684", "10.2307/1269768", "10.1109/infvis.1996.559213", "10.1109/tvcg.2016.2598839", "10.1145/2642918.2647360", "10.1057/palgrave.ivs.9500099", "10.1016/j.cag.2009.06.004", "10.1109/mc.2013.178", "10.1109/tvcg.2007.70541", "10.1109/vast.2011.6102473", "10.1145/358886.358895", "10.1109/tvcg.2014.2346573", "10.1109/tvcg.2018.2865159", "10.1145/2207676.2207741", "10.1109/tvcg.2015.2396062", "10.1145/2207676.2208572", "10.1109/tvcg.2016.2598608", "10.1057/ivs.2008.31", "10.1177/001316446002000104", "10.1109/tvcg.2017.2680452", "10.1109/tvcg.2006.80", "10.1145/2598510.2598566", "10.1037/0003-066x.51.4.355", "10.7146/dpb.v16i224.7586", "10.1109/infvis.1998.729560", "10.1162/leon\\_a\\_00011", "10.1109/tvcg.2010.157", "10.1109/tvcg.2014.2359887"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2018.2865235", "title": "Multiple Coordinated Views at Large Displays for Multiple Users: Empirical Findings on User Behavior, Movements, and Distances", "year": "2018", "conferenceName": "InfoVis", "authors": "Ricardo Langner;Ulrike Kister;Raimund Dachselt", "citationCount": "5", "affiliation": "Langner, R (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Langner, Ricardo; Kister, Ulrike; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany.", "countries": "Germany", "abstract": "Interactive wall-sized displays benefit data visualization. Due to their sheer display size, they make it possible to show large amounts of data in multiple coordinated views (MCV) and facilitate collaborative data analysis. In this work, we propose a set of important design considerations and contribute a fundamental input vocabulary and interaction mapping for MCV functionality. We also developed a fully functional application with more than 45 coordinated views visualizing a real-world, multivariate data set of crime activities, which we used in a comprehensive qualitative user study investigating how pairs of users behave. Most importantly, we found that flexible movement is essential and-depending on user goals-is connected to collaboration, perception, and interaction. Therefore, we argue that for future systems interaction from the distance is required and needs good support. We show that our consistent design for both direct touch at the large display and distant interaction using mobile phones enables the seamless exploration of large-scale MCV at wall-sized displays. Our MCV application builds on design aspects such as simplicity, flexibility, and visual consistency and, therefore, supports realistic workflows. We believe that in the future, many visual data analysis scenarios will benefit from wall-sized displays presenting numerous coordinated visualizations, for which our findings provide a valuable foundation.", "keywords": "Multiple coordinated views,wall-sized displays,mobile devices,distant interaction,physical navigation,user behavior,user movements,multi-user,collaborative data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865235", "refList": ["10.1109/mcg.2014.82", "10.1111/j.1467-8659.2009.01444.x", "10.1016/j.cag.2007.01.029", "10.1109/vast.2010.5652880", "10.1109/tvcg.2013.166", "10.1145/3025453.3025594", "10.1109/tvcg.2012.275", "10.1145/2254556.2254652", "10.1145/1866029.1866034", "10.1145/1099203.1099209", "10.1145/2470654.2470695", "10.1109/tvcg.2017.2744019", "10.1109/tvcg.2012.237", "10.1145/3173574.3173593", "10.1109/tvcg.2013.134", "10.1109/vast.2016.7883506", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1111/cgf.12871", "10.1109/tvcg.2017.2744198", "10.1145/3025453.3026006", "10.1109/cmv.2007.20", "10.1145/2470654.2481318", "10.1145/2598153.2598195", "10.1109/tvcg.2009.162", "10.1145/3173574.3173747", "10.1145/2576099", "10.1145/2207676.2208691", "10.1145/2858036.2858039", "10.1145/2556288.2557231", "10.1145/2556288.2556956", "10.1109/tvcg.2017.2745219", "10.1177/1473871617725907", "10.1145/2557500.2557541", "10.1007/978-3-319-22698-9\\_31", "10.1145/2785830.2785849", "10.1145/2207676.2208690", "10.1007/s00779-013-0727-2", "10.1109/tvcg.2016.2592906", "10.1111/cgf.13206", "10.1145/2207676.2208639", "10.1145/2817721.2817726", "10.1145/2556288.2557020", "10.1145/2598153.2598163", "10.1145/2254556.2254708", "10.1145/2669485.2669507", "10.1109/tvcg.2006.184", "10.1109/tvcg.2017.2743859", "10.1109/tvcg.2012.204", "10.1145/2207676.2208297", "10.1145/1897239.1897250", "10.1145/2858036.2858118", "10.1145/2396636.2396675", "10.1145/1731903.1731926", "10.1145/2702123.2702406", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1145/1936652.1936693", "10.1177/1473871611415997", "10.1145/2556288.2557170", "10.1145/2992154.2992157", "10.1145/3206505.3206506", "10.1007/978-3-319-45853-3\\_5"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030338", "title": "Composition and Configuration Patterns in Multiple-View Visualizations", "year": "2020", "conferenceName": "InfoVis", "authors": "Xi Chen;Wei Zeng 0004;Yanna Lin;Hayder Al-Maneea;Jonathan Roberts 0002;Remco Chang", "citationCount": "0", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Xi; Zeng, Wei; Lin, Yanna, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. AI-maneea, Hayder Mahdi; Roberts, Jonathan, Bangor Univ, Bangor, Gwynedd, Wales. Chang, Remco, Tufts Univ, Medford, MA 02155 USA.", "countries": "USA;Wales;China", "abstract": "Multiple-view visualization (MV) is a layout design technique often employed to help users see a large number of data attributes and values in a single cohesive representation. Because of its generalizability, the MV design has been widely adopted by the visualization community to help users examine and interact with large, complex, and high-dimensional data. However, although ubiquitous, there has been little work to categorize and analyze MVs in order to better understand its design space. As a result, there has been little to no guideline in how to use the MV design effectively. In this paper, we present an in-depth study of how MVs are designed in practice. We focus on two fundamental measures of multiple-view patterns: composition, which quantifies what view types and how many are there; and configuration, which characterizes spatial arrangement of view layouts in the display space. We build a new dataset containing 360 images of MVs collected from IEEE VIS, EuroVis, and PacificVis publications 2011 to 2019, and make fine-grained annotations of view types and layouts for these visualization images. From this data we conduct composition and configuration analyses using quantitative metrics of term frequency and layout topology. We identify common practices around MVs, including relationship of view types, popular view layouts, and correlation between view types and layouts. We combine the findings into a MV recommendation system, providing interactive tools to explore the design space, and support example-based design.", "keywords": "Multiple views,design pattern,quantitative analysis,example-based design", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030338", "refList": ["10.1109/tvcg.2018.2865235", "10.1109/tvcg.2016.2615308", "10.1145/2642918.2647398", "10.1177/1473871611416549", "10.1109/tvcg.2019.2934810", "10.1109/tvcg.2014.48", "10.1109/tvcg.2018.2864903", "10.1145/345513.345271", "10.1109/iv.1998.694193", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2017.2744019", "10.1109/tvcg.2011.185", "10.1109/tvcg.2015.2467194", "10.1109/visual.1991.175815", "10.14778/2831360.2831371", "10.1109/mcg.2019.2924636", "10.1111/cgf.13673", "10.1111/cgf.12131", "10.1109/tvcg.2017.2744198", "10.1145/108360.108361", "10.1109/vl.1996.545307", "10.1109/tvcg.2017.2745140", "10.1109/cmv.2007.20", "10.1145/198366.198376", "10.1145/2508363.2508405", "10.1111/cgf.12114", "10.1109/icde.2018.00019", "10.1109/tvcg.2017.2787113", "10.1109/tvcg.2018.2865240", "10.1111/cgf.13380", "10.1111/cgf.12902", "10.1109/vast.2015.7347628", "10.2307/2288400", "10.1109/infvis.2004.12", "10.1145/102377.115768", "10.1109/tvcg.2009.179", "10.1109/2945.981851", "10.1109/tvcg.2007.70521", "10.1109/pacificvis.2012.6183556", "10.1145/2213836", "10.1109/tvcg.2013.234", "10.1109/iv.2008.87"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13995", "year": "2020", "title": "GTMapLens: Interactive Lens for Geo-Text Data Browsing on Map", "conferenceName": "EuroVis", "authors": "Chao Ma;Ye Zhao;Shamal Al{-}Dohuki;Jing Yang;Xinyue Ye;Farah Kamw;Md. Amiruzzaman", "citationCount": "0", "affiliation": "Zhao, Y (Corresponding Author), Kent State Univ, Kent, OH 44240 USA.\nMa, Chao; Zhao, Ye; Amiruzzaman, Md, Kent State Univ, Kent, OH 44240 USA.\nAl-Dohuki, Shamal, Univ Duhok, Duhok, Iraq.\nYang, Jing, Univ N Carolina, Charlotte, NC USA.\nYe, Xinyue, New Jersey Inst Technol, Newark, NJ 07102 USA.\nKamw, Farah, Concordia Univ, Ann Arbor, MI USA.", "countries": "USA;Iraq", "abstract": "Data containing geospatial semantics, such as geotagged tweets, travel blogs, and crime reports, associates natural language texts with geographical locations. This paper presents a lens-based visual interaction technique, GTMapLens, to flexibly browse the geo-text data on a map. It allows users to perform dynamic focus+context exploration by using movable lenses to browse geographical regions, find locations of interest, and perform comparative and drill-down studies. Geo-text data is visualized in a way that users can easily perceive the underlying geospatial semantics along with lens moving. Based on a requirement analysis with a cohort of multidisciplinary domain experts, a set of lens interaction techniques are developed including keywords control, path management, context visualization, and snapshot anchors. They allow users to achieve a guided and controllable exploration of geo-text data. A hierarchical data model enables the interactive lens operations by accelerated data retrieval from a geo-text database. Evaluation with real-world datasets is presented to show the usability and effectiveness of GTMapLens.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13995", "refList": ["10.1109/tvcg.2018.2865235", "10.1145/1936652.1936673", "10.1145/2598153.2598200", "10.1145/1456650.1456652", "10.1109/vast.2011.6102456", "10.1145/3290605.3300864", "10.1177/1473871611413180", "10.1109/iv.2011.43", "10.1109/tvcg.2015.2467971", "10.1109/iv.2016.62", "10.1080/13658816.2017.1325488", "10.1111/cgf.12871", "10.1111/cgf.12132", "10.1109/mc.2012.430", "10.1109/tvcg.2016.2598585", "10.1109/tvcg.2011.195", "10.1109/tvcg.2015.2467619", "10.1109/pacificvis.2013.6596122", "10.13140/rg.2.2.36636.59521", "10.1145/3170427.3188506", "10.1016/b978-155860915-0/50040-8", "10.1109/infvis.2003.1249008", "10.1109/tvcg.2018.2850781", "10.1016/j.datak.2006.01.013", "10.1109/tvcg.2015.2467991", "10.1111/gec3.12404", "10.1109/tvcg.2008.149", "10.1109/visual.1998.745317", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2011.176", "10.1016/j.visinf.2018.04.006.2", "10.1111/cgf.13264", "10.1080/13658816.2010.508043", "10.1007/s41651-017-0002-6", "10.1109/tvcg.2009.65", "10.1109/vast.2011.6102498", "10.1145/3025453.3025777", "10.1109/tvcg.2006.138"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13979", "year": "2020", "title": "Short-Contact Touch-Manipulation of Scatterplot Matrices on Wall Displays", "conferenceName": "EuroVis", "authors": "Patrick Riehmann;Gabriela Molina Le{\\'{o}}n;Joshua Reibert;Florian Echtler;Bernd Fr{\\\"{o}}hlich", "citationCount": "0", "affiliation": "Riehmann, P (Corresponding Author), Bauhaus Univ Weimar, Weimar, Germany.\nRiehmann, P.; Leon, G. Molina; Reibert, J.; Echtler, F.; Froehlich, B., Bauhaus Univ Weimar, Weimar, Germany.\nReibert, J., German Aerosp Ctr DLR, Inst Data Sci, Jena, Germany.\nLeon, G. Molina, Univ Bremen, Bremen, Germany.", "countries": "Germany", "abstract": "This paper presents a short-contact multitouch vocabulary for interacting with scatterplot matrices (SPLOMs) on wall-sized displays. Fling-based gestures overcome central interaction challenges of such large displays by avoiding long swipes on the typically blunt surfaces, frequent physical navigation by walking for accessing screen areas beyond arm's reach in the horizontal direction and uncomfortable postures for accessing screen areas in the vertical direction. Furthermore, we make use of the display's high resolution and large size by supporting the efficient specification of two-tiered focus + context regions which are consistently propagated across the SPLOM. These techniques are complemented by axis-centered and lasso-based selection techniques for specifying subsets of the data. An expert review as well as a user study confirmed the potential and general usability of our seamlessly integrated multitouch interaction techniques for SPLOMs on large vertical displays.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13979", "refList": ["10.1109/tvcg.2018.2865235", "10.1145/3152832.3152852", "10.1109/tvcg.2008.153", "10.1016/j.cag.2007.01.029", "10.1145/3025453.3025594", "10.1109/vast.2009.5332595", "10.1145/2971485.2971503", "10.1145/3173574.3173593", "10.1145/2817721.2817735", "10.1145/2670444.2670445", "10.1145/2470654.2481318", "10.1109/mcg.2005.88", "10.1145/1936652.1936707", "10.1145/2556288.2557231", "10.1111/cgf.12902", "10.1201/9781498710411", "10.1109/tvcg.2016.2592906", "10.1111/cgf.13206", "10.1145/1520340.1520467", "10.1109/tvcg.2010.130", "10.1145/2598153.2598163", "10.1145/2909132.2909258", "10.1002/hbm.20701", "10.1145/2396636.2396675", "10.1145/2702123.2702406", "10.1109/tvcg.2012.251", "10.2312/eurovisshort.20181088", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/22339.22342", "10.1109/mcg.2013.24", "10.1145/2669485.2669507"], "wos": 1, "children": [], "len": 1}], "len": 11}, {"doi": "10.1111/cgf.13673", "year": "2019", "title": "Multiple Views: different meanings and collocated words", "conferenceName": "EuroVis", "authors": "Jonathan C. Roberts;Hayder Al{-}Maneea;Peter W. S. Butcher;Robert Lew;Geraint Rees;Nirwan Sharma;Ana Frankenberg{-}Garcia", "citationCount": "1", "affiliation": "Roberts, JC (Corresponding Author), Bangor Univ, Bangor, Gwynedd, Wales.\nRoberts, J. C.; Al-maneea, H.; Butcher, P. W. S.; Sharma, N., Bangor Univ, Bangor, Gwynedd, Wales.\nAl-maneea, H., Basrah Univ, Basrah, Iraq.\nLew, R.; Rees, G., Adam Mickiewicz Univ, Poznan, Poland.\nFrankenberg-Garcia, A., Univ Surrey, Guildford, Surrey, England.", "countries": "Poland;Wales;England;Iraq", "abstract": "We report on an in-depth corpus linguistic study on multiple views' terminology and word collocation. We take a broad interpretation of these terms, and explore the meaning and diversity of their use in visualisation literature. First we explore senses of the term multiple views' (e.g., multiple views' can mean juxtaposition, many viewport projections or several alternative opinions). Second, we investigate term popularity and frequency of occurrences, investigating usage of multiple' and view' (e.g., multiple views, multiple visualisations, multiple sets). Third, we investigate word collocations and terms that have a similar sense (e.g., multiple views, side-by-side, small multiples). We built and used several corpora, including a 6-million-word corpus of all IEEE Visualisation conference articles published in IEEE Transactions on Visualisation and Computer Graphics 2012 to 2017. We draw on our substantial experience from early work in coordinated and multiple views, and with collocation analysis develop several lists of terms. This research provides insight into term use, a reference for novice and expert authors in visualisation, and contributes a taxonomy of multiple view' terms.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13673", "refList": ["10.1109/tvcg.2017.2744359", "10.1109/38.31462", "10.1109/cmv.2003.1215002", "10.2307/2289444", "10.1016/j.ijhcs.2011.02.007", "10.1017/s0022112091210654", "10.1016/b978-008044531-1/50426-7", "10.1109/cmv.2003.1215001", "10.1109/tvcg.2013.134", "10.1117/12.378894", "10.1037/0096-1523.21.6.1494", "10.1057/palgrave.ivs.9500086", "10.1109/iv.2011.42", "10.1109/tvcg.2017.2744199", "10.4324/9780203810088", "10.1145/1321440.1321580", "10.1109/tvcg.2015.2467271", "10.1016/j.learninstruc.2006.03.001", "10.1145/345513.345282", "10.1007/s40607-014-0009-9", "10.1109/tvcg.2017.2743859", "10.1109/infvis.1997.636761", "10.1109/tvcg.2013.219", "10.1007/978-3-319-55627-7", "10.1109/tvcg.2009.111", "10.1109/iv.2008.21", "10.1109/infvis.2002.1173157", "10.1109/mcg.2014.82", "10.1145/3143699.3143717", "10.1109/tvcg.2012.226", "10.1145/345513.345271", "10.2478/jazcas-2018-0006", "10.1145/2556288.2556969", "10.1109/visual.1998.745282", "10.1145/1276377.1276427", "10.1109/infvis.2001.963283", "10.1007/978-1-4471-6497-5\\_1", "10.1109/visual.1994.346302", "10.1057/palgrave.ivs.9500068", "10.1109/tvcg.2006.160", "10.1109/visual.1990.146374", "10.1109/tvcg.2016.2614803", "10.1177/1473871611416549", "10.1109/tvcg.2017.2745878", "10.1109/tvcg.2006.69", "10.1109/tpami.1983.4767367", "10.1109/visual.1991.175794", "10.1109/tvcg.2017.2744159", "10.1109/tvcg.2017.2744198", "10.1109/32.328995", "10.1016/j.jeap.2018.07.003", "10.1016/j.geomorph.2012.08.021", "10.1109/tvcg.2014.2346920", "10.1109/2.917550", "10.1017/s0958344018000150", "10.1109/tvcg.2009.94", "10.1179/2051819613z.0000000003", "10.1109/vast.2008.4677370", "10.1117/12.309533", "10.1109/tvcg.2014.2346747", "10.1075/ijcl.13.4.06ray", "10.1007/s12650-015-0323-9", "10.1109/tvcg.2006.178", "10.1109/tvcg.2016.2615308", "10.2307/1269768", "10.2307/4132312", "10.1109/tvcg.2018.2864903", "10.1109/mis.2006.100", "10.1109/iv.1998.694193", "10.1007/s11192-015-1830-0", "10.1109/tvcg.2017.2744080", "10.1111/j.1467-9280.1997.tb00442.x", "10.1109/cmv.2007.20", "10.1109/infvis.2003.1249006", "10.1109/tvcg.2017.2745941", "10.1016/s1364-6613(99)01332-7", "10.1145/989863.989893", "10.1109/tse.1985.232211", "10.1109/tvcg.2017.2744184", "10.1109/infvis.2004.12", "10.1075/ijcl.17.3.04har", "10.1109/tvcg.2010.179", "10.1016/j.jss.2006.05.024", "10.1109/cmv.2003.1215005", "10.1109/tvcg.2017.2744358", "10.1145/2992154.2996365", "10.1109/tvcg.2016.2598827"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030338", "title": "Composition and Configuration Patterns in Multiple-View Visualizations", "year": "2020", "conferenceName": "InfoVis", "authors": "Xi Chen;Wei Zeng 0004;Yanna Lin;Hayder Al-Maneea;Jonathan Roberts 0002;Remco Chang", "citationCount": "0", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Xi; Zeng, Wei; Lin, Yanna, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. AI-maneea, Hayder Mahdi; Roberts, Jonathan, Bangor Univ, Bangor, Gwynedd, Wales. Chang, Remco, Tufts Univ, Medford, MA 02155 USA.", "countries": "USA;Wales;China", "abstract": "Multiple-view visualization (MV) is a layout design technique often employed to help users see a large number of data attributes and values in a single cohesive representation. Because of its generalizability, the MV design has been widely adopted by the visualization community to help users examine and interact with large, complex, and high-dimensional data. However, although ubiquitous, there has been little work to categorize and analyze MVs in order to better understand its design space. As a result, there has been little to no guideline in how to use the MV design effectively. In this paper, we present an in-depth study of how MVs are designed in practice. We focus on two fundamental measures of multiple-view patterns: composition, which quantifies what view types and how many are there; and configuration, which characterizes spatial arrangement of view layouts in the display space. We build a new dataset containing 360 images of MVs collected from IEEE VIS, EuroVis, and PacificVis publications 2011 to 2019, and make fine-grained annotations of view types and layouts for these visualization images. From this data we conduct composition and configuration analyses using quantitative metrics of term frequency and layout topology. We identify common practices around MVs, including relationship of view types, popular view layouts, and correlation between view types and layouts. We combine the findings into a MV recommendation system, providing interactive tools to explore the design space, and support example-based design.", "keywords": "Multiple views,design pattern,quantitative analysis,example-based design", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030338", "refList": ["10.1109/tvcg.2018.2865235", "10.1109/tvcg.2016.2615308", "10.1145/2642918.2647398", "10.1177/1473871611416549", "10.1109/tvcg.2019.2934810", "10.1109/tvcg.2014.48", "10.1109/tvcg.2018.2864903", "10.1145/345513.345271", "10.1109/iv.1998.694193", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2017.2744019", "10.1109/tvcg.2011.185", "10.1109/tvcg.2015.2467194", "10.1109/visual.1991.175815", "10.14778/2831360.2831371", "10.1109/mcg.2019.2924636", "10.1111/cgf.13673", "10.1111/cgf.12131", "10.1109/tvcg.2017.2744198", "10.1145/108360.108361", "10.1109/vl.1996.545307", "10.1109/tvcg.2017.2745140", "10.1109/cmv.2007.20", "10.1145/198366.198376", "10.1145/2508363.2508405", "10.1111/cgf.12114", "10.1109/icde.2018.00019", "10.1109/tvcg.2017.2787113", "10.1109/tvcg.2018.2865240", "10.1111/cgf.13380", "10.1111/cgf.12902", "10.1109/vast.2015.7347628", "10.2307/2288400", "10.1109/infvis.2004.12", "10.1145/102377.115768", "10.1109/tvcg.2009.179", "10.1109/2945.981851", "10.1109/tvcg.2007.70521", "10.1109/pacificvis.2012.6183556", "10.1145/2213836", "10.1109/tvcg.2013.234", "10.1109/iv.2008.87"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.13718", "year": "2019", "title": "Investigating the Manual View Specification and Visualization by Demonstration Paradigms for Visualization Construction", "conferenceName": "EuroVis", "authors": "Bahador Saket;Alex Endert", "citationCount": "1", "affiliation": "Saket, B (Corresponding Author), Georgia Inst Technol, Sch Interact Comp, Atlanta, GA 30332 USA.\nSaket, Bahador; Endert, Alex, Georgia Inst Technol, Sch Interact Comp, Atlanta, GA 30332 USA.", "countries": "USA", "abstract": "Interactivity plays an important role in data visualization. Therefore, understanding how people create visualizations given different interaction paradigms provides empirical evidence to inform interaction design. We present a two-phase study comparing people's visualization construction processes using two visualization tools: one implementing the manual view specification paradigm (Polestar) and another implementing visualization by demonstration (VisExemplar). Findings of our study indicate that the choice of interaction paradigm influences the visualization construction in terms of: 1) the overall effectiveness, 2) how participants phrase their goals, and 3) their perceived control and engagement. Based on our findings, we discuss trade-offs and open challenges with these interaction paradigms.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13718", "refList": ["10.1111/cgf.12887", "10.1023/a:1008716330212", "10.1145/2470654.2466255", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2012.275", "10.1145/2493102.2493104", "10.1109/tvcg.2016.2598839", "10.1145/948496.948514", "10.1109/tvcg.2015.2467201", "10.2312/pe.eurovisshort.eurovisshort2013.019-023", "10.1109/tvcg.2007.70541", "10.1109/tvcg.2016.2598446", "10.1109/vl.1996.545307", "10.1109/tvcg.2014.2346250", "10.1145/3025453.3025942", "10.1145/2207676.2207741", "10.1145/3173574.3174212", "10.1109/tvcg.2015.2467615", "10.1145/2598784.2598806", "10.1109/tvcg.2017.2680452", "10.1145/2598510.2598566", "10.1109/tvcg.2007.70577", "10.1109/tvcg.2016.2598620", "10.2307/2530428", "10.1109/infvis.1998.729560", "10.1109/tvcg.2014.2346291", "10.1109/tvcg.2010.164", "10.1109/tvcg.2017.2743859", "10.1109/tvcg.2013.191", "10.1109/2.153286", "10.1109/tvcg.2015.2467191", "10.1145/3173574.3173697", "10.1145/1502650.1502667", "10.1109/tvcg.2007.70515"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934534", "title": "Investigating Direct Manipulation of Graphical Encodings as a Method for User Interaction", "year": "2019", "conferenceName": "InfoVis", "authors": "Bahador Saket;Samuel Huron;Charles Perin;Alex Endert", "citationCount": "0", "affiliation": "Saket, B (Corresponding Author), Georgia Tech, Atlanta, GA 30332 USA. Saket, Bahador; Endert, Alex, Georgia Tech, Atlanta, GA 30332 USA. Huron, Samuel, Univ Paris Saclay, Paris, France. Perin, Charles, Univ Victoria, Victoria, BC, Canada.", "countries": "Canada;USA;France", "abstract": "We investigate direct manipulation of graphical encodings as a method for interacting with visualizations. There is an increasing interest in developing visualization tools that enable users to perform operations by directly manipulating graphical encodings rather than external widgets such as checkboxes and sliders. Designers of such tools must decide which direct manipulation operations should be supported, and identify how each operation can be invoked. However, we lack empirical guidelines for how people convey their intended operations using direct manipulation of graphical encodings. We address this issue by conducting a qualitative study that examines how participants perform 15 operations using direct manipulation of standard graphical encodings. From this study, we 1) identify a list of strategies people employ to perform each operation, 2) observe commonalities in strategies across operations, and 3) derive implications to help designers leverage direct manipulation of graphical encoding as a method for user interaction.", "keywords": "Direct Manipulation,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934534", "refList": ["10.1145/2556288.2557379", "10.1109/tvcg.2016.2598839", "10.1145/2858036.2858435", "10.1145/2702123.2702237", "10.1111/j.1467-8659.2009.01678.x", "10.1109/tvcg.2011.185", "10.1109/mcg.2016.90", "10.1177/1473871611413180", "10.1111/cgf.13718", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2014.2346984", "10.1145/2207676.2207741", "10.1109/mcg.2019.2903711", "10.1109/tvcg.2018.2865075", "10.1109/tvcg.2014.2346279", "10.1109/iv.1999.781570", "10.1109/tvcg.2015.2467615", "10.1109/tvcg.2014.2346292", "10.1145/2984511.2984588", "10.1109/tvcg.2017.2680452", "10.1145/1166253.1166265", "10.1109/tvcg.2017.2745258", "10.1109/tvcg.2016.2598620", "10.1109/tvcg.2014.2346291", "10.2307/2530428", "10.1109/tvcg.2012.204", "10.1145/3173574.3173697", "10.1207/s15327051hci0104\\_2", "10.1109/vast.2012.6400486", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030476", "title": "StructGraphics: Flexible Visualization Design through Data-Agnostic and Reusable Graphical Structures", "year": "2020", "conferenceName": "InfoVis", "authors": "Theophanis Tsandilas", "citationCount": "0", "affiliation": "Tsandilas, T (Corresponding Author), Univ Paris Saclay, INRIA, Gif Sur Yvette, France. Tsandilas, T (Corresponding Author), CNRS, F-75700 Paris, France. Tsandilas, Theophanis, Univ Paris Saclay, INRIA, Gif Sur Yvette, France. Tsandilas, Theophanis, CNRS, F-75700 Paris, France.", "countries": "France", "abstract": "Information visualization research has developed powerful systems that enable users to author custom data visualizations without textual programming. These systems can support graphics-driven practices by bridging lazy data-binding mechanisms with vector-graphics editing tools. Yet, despite their expressive power, visualization authoring systems often assume that users want to generate visual representations that they already have in mind rather than explore designs. They also impose a data-to-graphics workflow, where binding data dimensions to graphical properties is a necessary step for generating visualization layouts. In this paper, we introduce StructGraphics, an approach for creating data-agnostic and fully reusable visualization designs. StructGraphics enables designers to construct visualization designs by drawing graphics on a canvas and then structuring their visual properties without relying on a concrete dataset or data schema. In StructGraphics, tabular data structures are derived directly from the structure of the graphics. Later, designers can link these structures with real datasets through a spreadsheet user interface. StructGraphics supports the design and reuse of complex data visualizations by combining graphical property sharing, by-example design specification, and persistent layout constraints. We demonstrate the power of the approach through a gallery of visualization examples and reflect on its strengths and limitations in interaction with graphic designers and data visualization experts.", "keywords": "Visualization design,graphical structures,visualization grammars,layout constraints,infographics,flexible data binding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030476", "refList": ["10.1145/344949.344959", "10.1057/ivs.2009.22", "10.1109/mcg.1987.277079", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/2858036.2858435", "10.1145/3173574.3173610", "10.1145/3173574.3174106", "10.1109/tvcg.2011.185", "10.1109/tvcg.2007.70515", "10.1109/iv.2008.66", "10.1111/cgf.13718", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2017.2744198", "10.1145/22949.22950", "10.1145/1925844.1926423", "10.1109/mcg.2019.2903711", "10.1006/cogp.1994.1010", "10.1109/tvcg.2018.2865240", "10.1145/3125571.3125585", "10.1145/3022671.2984020", "10.1111/cgf.12391", "10.1109/tvcg.2015.2467091", "10.1109/infvis.2004.12", "10.1109/tvcg.2016.2598620", "10.1016/j.jvlc.2017.10.001", "10.1109/tvcg.2014.2346291", "10.1109/2945.981851", "10.1073/pnas.1807184115", "10.1109/tvcg.2010.177", "10.1145/3173574.3173697", "10.1080/1369118x.2016.1153126", "10.1109/tvcg.2015.2414454", "10.1145/2740908.2742849", "10.1023/a:1025671410623", "10.1111/cgf.12903", "10.1207/s15327051hci0104\\_2", "10.1145/3290605.3300358", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13976", "year": "2020", "title": "Many At Once: Capturing Intentions to Create And Use Many Views At Once In Large Display Environments", "conferenceName": "EuroVis", "authors": "Jillian Aurisano;Abhinav Kumar;Abeer Alsaiari;Barbara Di Eugenio;Andrew E. Johnson", "citationCount": "0", "affiliation": "Aurisano, J (Corresponding Author), Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.\nAurisano, J (Corresponding Author), Elect Visualizat Lab, Chicago, IL 60607 USA.\nAurisano, Jillian; Kumar, Abhinav; Alsaiari, Abeer; Di Eugenio, Barbara; Johnson, Andrew, Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.\nAurisano, Jillian; Alsaiari, Abeer; Johnson, Andrew, Elect Visualizat Lab, Chicago, IL 60607 USA.", "countries": "USA", "abstract": "This paper describes results from an observational, exploratory study of visual data exploration in a large, multi-view, flexible canvas environment. Participants were provided with a set of data exploration sub-tasks associated with a local crime dataset and were instructed to pose questions to a remote mediator who would respond by generating and organizing visualizations on the large display. We observed that participants frequently posed requests to cast a net around one or several subsets of the data or a set of data attributes. They accomplished this directly and by utilizing existing views in unique ways, including by requesting to copy and pivot a group of views collectively and posing a set of parallel requests on target views expressed in one command. These observed actions depart from multi-view flexible canvas environments that typically provide interfaces in support of generating one view at a time or actions that operate on one view at a time. We describe how participants used these `cast-a-net' requests for tasks that spanned more than one view and describe design considerations for multi-view environments that would support the observed multi-view generation actions.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13976", "refList": ["10.1109/vast47406.2019.8986918", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1177/0011000006287390", "10.1109/tvcg.2014.2346293", "10.1111/cgf.12106", "10.1109/visual.2005.1532788", "10.1145/2399016.2399102", "10.1109/tvcg.2014.2346260", "10.1145/3173574.3173593", "10.1145/2559206.2581202", "10.1111/cgf.12131", "10.4108/icst.collaboratecom.2014.257337", "10.1145/2207676.2208293", "10.1145/2576099", "10.1145/1936652.1936676", "10.1109/tvcg.2013.163", "10.1109/mcg.2013.37", "10.1186/1471-2105-16-s11-s6", "10.1109/2945.981851", "10.1145/2807442.2807478", "10.1109/tvcg.2010.164", "10.1109/tvcg.2014.2337337", "10.1109/tvcg.2017.2743859", "10.1109/tvcg.2006.184", "10.1109/tvcg.2015.2467191", "10.1145/2702123.2702406", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997"], "wos": 1, "children": [], "len": 1}], "len": 29}, {"doi": "10.1111/cgf.13405", "year": "2018", "title": "Fast and Accurate CNN-based Brushing in Scatterplots", "conferenceName": "EuroVis", "authors": "Chaoran Fan;Helwig Hauser", "citationCount": "6", "affiliation": "Fan, CR (Corresponding Author), Univ Bergen, Bergen, Norway.\nFan, Chaoran; Hauser, Helwig, Univ Bergen, Bergen, Norway.", "countries": "Norway", "abstract": "Brushing plays a central role in most modern visual analytics solutions and effective and efficient techniques for data selection are key to establishing a successful human-computer dialogue. With this paper, we address the need for brushing techniques that are both fast, enabling a fluid interaction in visual data exploration and analysis, and also accurate, i.e., enabling the user to effectively select specific data subsets, even when their geometric delimination is non-trivial. We present a new solution for a near-perfect sketch-based brushing technique, where we exploit a convolutional neural network (CNN) for estimating the intended data selection from a fast and simple click-and-drag interaction and from the data distribution in the visualization. Our key contributions include a drastically reduced error ratenow below 3\\%, i.e., less than half of the so far best accuracyand an extension to a larger variety of selected data subsets, going beyond previous limitations due to linear estimation models.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13405", "refList": ["10.2307/2685209", "10.1007/s00158-002-0174-6", "10.2307/1269768", "10.1109/tpami.2016.2577031", "10.1177/1473871611413180", "10.1111/cgf.12901", "10.1113/jphysiol.1962.sp006837", "10.1007/3-540-30790-7\\_18", "10.1111/j.1467-8659.2008.01207.x", "10.1109/cmv.2007.20", "10.1109/5.726791", "10.1109/tvcg.2006.99", "10.1007/978-3-642-46466-9\\_18", "10.1007/978-3-319-10590-1\\_53", "10.1109/tvcg.2011.223", "10.1109/tvcg.2016.2598470", "10.1111/dsu.12130", "10.1145/37402.37422", "10.1145/2567948.2577348", "10.2307/1932409", "10.1109/visual.1995.485139", "10.2312/vmv.20171262"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934332", "title": "LassoNet: Deep Lasso-Selection of 3D Point Clouds", "year": "2019", "conferenceName": "SciVis", "authors": "Zhutian Chen;Wei Zeng 0004;Zhiguang Yang;Lingyun Yu;Chi-Wing Fu;Huamin Qu", "citationCount": "4", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Zhutian; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Zeng, Wei; Yang, Zhiguang, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Yu, Lingyun, Univ Groningen, Groningen, Netherlands. Fu, Chi-Wing, Chinese Univ Hong Kong, Hong Kong, Peoples R China.", "countries": "China;Netherlands", "abstract": "Selection is a fundamental task in exploratory analysis and visualization of 3D point clouds. Prior researches on selection methods were developed mainly based on heuristics such as local point density, thus limiting their applicability in general data. Specific challenges root in the great variabilities implied by point clouds (e.g., dense vs. sparse), viewpoint (e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this work, we introduce LassoNet, a new deep neural network for lasso selection of 3D point clouds, attempting to learn a latent mapping from viewpoint and lasso to point cloud regions. To achieve this, we couple user-target points with viewpoint and lasso information through 3D coordinate transform and naive selection, and improve the method scalability via an intention filtering and farthest point sampling. A hierarchical network is trained using a dataset with over 30K lasso-selection records on two different point cloud data. We conduct a formal user study to compare LassoNet with two state-of-the-art lasso-selection methods. The evaluations confirm that our approach improves the selection effectiveness and efficiency across different combinations of 3D point clouds, viewpoints, and lasso selections. Project Website: https://LassoNet.github.io", "keywords": "Point Clouds,Lasso Selection,Deep Learning", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934332", "refList": ["10.1111/j.1467-8659.2009.01515.x", "10.1109/tvcg.2015.2467202", "10.1109/tvcg.2016.2599049", "10.1109/iccv.2015.114", "10.1109/vppc.2018.8604993", "10.1111/cgf.13405", "10.1109/tvcg.2018.2843369", "10.1109/cvpr.2018.00278", "10.1145/3025453.3025957", "10.1109/mc.2013.178", "10.1145/2980179.2980238", "10.1109/msp.2017.2693418", "10.1145/237091.237105", "10.1145/2835487", "10.1109/cvpr.2010.5539838", "10.1016/j.visinf.2017.01.006", "10.1109/tcst.2018.2819965", "10.1109/tvcg.2018.2865138", "10.1109/83.623193", "10.1145/3072959.3073608", "10.1145/1053427.1053445", "10.1016/s0039-9140(96)02179-0", "10.1109/cvpr.2015.7298801", "10.1109/cvpr.2017.693", "10.1007/s12650-014-0206-5", "10.1145/3272127.3275110", "10.1109/cvpr.2015.7298845", "10.1109/iccvw.2015.112", "10.1109/iros.2015.7353481", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2012.217", "10.1109/tridui.2006.1618279", "10.1109/tvcg.2012.292", "10.1109/cvpr.2016.609", "10.1016/j.cag.2012.12.003"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028947", "title": "A Fluid Flow Data Set for Machine Learning and its Application to Neural Flow Map Interpolation", "year": "2020", "conferenceName": "SciVis", "authors": "Jakob Jakob;Markus H. Gross;Tobias G\u00fcnther", "citationCount": "0", "affiliation": "Jakob, J (Corresponding Author), Swiss Fed Inst Technol, Zurich, Switzerland. Jakob, Jakob; Gross, Markus; Guenther, Tobias, Swiss Fed Inst Technol, Zurich, Switzerland.", "countries": "Switzerland", "abstract": "In recent years, deep learning has opened countless research opportunities across many different disciplines. At present, visualization is mainly applied to explore and explain neural networks. Its counterpart-the application of deep learning to visualization problems-requires us to share data more openly in order to enable more scientists to engage in data-driven research. In this paper, we construct a large fluid flow data set and apply it to a deep learning problem in scientific visualization. Parameterized by the Reynolds number, the data set contains a wide spectrum of laminar and turbulent fluid flow regimes. The full data set was simulated on a high-performance compute cluster and contains 8000 time-dependent 2D vector fields, accumulating to more than 16 TB in size. Using our public fluid data set, we trained deep convolutional neural networks in order to set a benchmark for an improved post-hoc Lagrangian fluid flow analysis. In in-situ settings, flow maps are exported and interpolated in order to assess the transport characteristics of time-dependent fluids. Using deep learning, we improve the accuracy of flow map interpolations, allowing a more precise flow analysis at a reduced memory IO footprint.", "keywords": "Scientific visualization,deep learning,flow maps", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028947", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1145/3355089.3356560", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1007/978-3-030-00533-7\\_36", "10.3390/rs11161921", "10.1007/978-3-030-48457-6\\_1", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/mcg.2018.2881523", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1145/3072959.3073643", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/tvcg.2013.128", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1007/978-3-319-46475-6\\_43", "10.1126/science.1127647", "10.1007/978-3-319-46475-6\\_25", "10.1111/cgf.13689"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030337", "title": "QLens: Visual Analytics of MUlti-step Problem-solving Behaviors for Improving Question Design", "year": "2020", "conferenceName": "VAST", "authors": "Meng Xia;Reshika Palaniyappan Velumani;Yong Wang;Huamin Qu;Xiaojuan Ma", "citationCount": "0", "affiliation": "Xia, M (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Xia, Meng; Velumani, Reshika Palaniyappan; Wang, Yong; Qu, Huamin; Ma, Xiaojuan, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.", "countries": "China", "abstract": "With the rapid development of online education in recent years, there has been an increasing number of learning platforms that provide students with multi-step questions to cultivate their problem-solving skills. To guarantee the high quality of such learning materials, question designers need to inspect how students' problem-solving processes unfold step by step to infer whether students' problem-solving logic matches their design intent. They also need to compare the behaviors of different groups (e.g., students from different grades) to distribute questions to students with the right level of knowledge. The availability of fine-grained interaction data, such as mouse movement trajectories from the online platforms, provides the opportunity to analyze problem-solving behaviors. However, it is still challenging to interpret, summarize, and compare the high dimensional problem-solving sequence data. In this paper, we present a visual analytics system, QLens, to help question designers inspect detailed problem-solving trajectories, compare different student groups, distill insights for design improvements. In particular, QLens models problem-solving behavior as a hybrid state transition graph and visualizes it through a novel glyph-embedded Sankey diagram, which reflects students' problem-solving logic, engagement, and encountered difficulties. We conduct three case studies and three expert interviews to demonstrate the usefulness of QLens on real-world datasets that consist of thousands of problem-solving traces.", "keywords": "Learning Behavior Analysis,Visual Analytics,Time Series Data", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030337", "refList": ["10.3389/frobt.2019.00082", "10.1109/tvcg.2017.2745181", "10.1109/vr.2019.8798065", "10.1016/j.visinf.2018.12.001", "10.1109/pacificvis.2017.8031577", "10.1109/vr.2019.8798125", "10.1109/vr.2019.8797717", "10.1109/mprv.2017.35", "10.1109/tvcg.2013.192", "10.1109/tvcg.2017.2744218", "10.1016/j.visinf.2017.01.006", "10.1007/s11042-011-0833-y", "10.1109/visual.2001.964496", "10.1111/cgf.13325", "10.1109/mcg.2016.101", "10.1109/icdmw.2017.12", "10.1111/cgf.13189", "10.1016/j.visinf.2017.11.002", "10.1007/s11432-018-9801-4", "10.1109/tvcg.2018.2865041", "10.1109/vast.2014.7042477", "10.1109/vast.2014.7042478", "10.1167/11.5.13", "10.1109/vr.2019.8798227", "10.1145/3025453.3025566", "10.1109/tvcg.2018.2865192", "10.1109/vr.2018.8446350", "10.1109/tvcg.2018.2865191", "10.1109/tvcg.2019.2934332", "10.1109/tvcg.2014.2346445", "10.1016/j.cag.2012.12.003", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030346", "title": "V2V: A Deep Learning Approach to Variable-to-Variable Selection and Translation for Multivariate Time-Varying Data", "year": "2020", "conferenceName": "SciVis", "authors": "Jun Han;Hao Zheng 0006;Yunhao Xing;Danny Ziyi Chen;Chaoli Wang", "citationCount": "0", "affiliation": "Han, J (Corresponding Author), Univ Notre Dame, Notre Dame, IN 46556 USA. Han, Jun; Zheng, Hao; Chen, Danny Z.; Wang, Chaoli, Univ Notre Dame, Notre Dame, IN 46556 USA. Xing, Yunhao, Sichuan Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China;USA", "abstract": "We present V2V, a novel deep learning framework, as a general-purpose solution to the variable-to-variable (V2V) selection and translation problem for multivariate time-varying data (MTVD) analysis and visualization. V2V leverages a representation learning algorithm to identify transferable variables and utilizes Kullback-Leibler divergence to determine the source and target variables. It then uses a generative adversarial network (GAN) to learn the mapping from the source variable to the target variable via the adversarial, volumetric, and feature losses. V2V takes the pairs of time steps of the source and target variable as input for training, Once trained, it can infer unseen time steps of the target variable given the corresponding time steps of the source variable. Several multivariate time-varying data sets of different characteristics are used to demonstrate the effectiveness of V2V, both quantitatively and qualitatively. We compare V2V against histogram matching and two other deep learning solutions (Pix2Pix and CycleGAN).", "keywords": "Multivariate time-varying data,variable selection and translation,generative adversarial network,data extrapolation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030346", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/cvpr.2017.19", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1146/annurev", "10.3390/rs11161921", "10.1109/tvcg.2007.70551", "10.1145/3309993", "10.1007/978-3-030-48457-6\\_1", "10.1109/igarss.2018.8518411", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1146/annurev-fluid-010313-141322", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1126/science.1127647"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030374", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data", "year": "2020", "conferenceName": "SciVis", "authors": "Yifan Wang;Guoli Yan;Haikuan Zhu;Sagar Buch;Ying Wang;E. Mark Haacke;Jing Hua;Zichun Zhong", "citationCount": "0", "affiliation": "Wang, YF (Corresponding Author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Wang, Yifan; Yan, Guoli; Zhu, Haikuan; Hua, Jing; Zhong, Zichun, Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Buch, Sagar; Wang, Ying; Haacke, Ewart Mark, Wayne State Univ, Dept Radiol, Detroit, MI 48201 USA.", "countries": "USA", "abstract": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.", "keywords": "Deep neural network,3D cerebrovascular segmentation and visualization,maximum intensity projection (MIP),joint embedding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030374", "refList": ["10.1109/cluster.2018.00036", "10.1109/iccv.2015.123", "10.1109/tvcg.2013.133", "10.1109/cvpr.2017.19", "10.1109/tvcg.2019.2934312", "10.1007/978-3-319-46487-9\\_40", "10.1109/tvcg.2006.175", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2018.2880207", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/tvcg.2018.2816059", "10.1109/pacificvis.2009.4906852", "10.1109/tvcg.2018.2864808", "10.1109/mcg.2018.2881523", "10.1109/tvcg.2015.2467431", "10.1109/tpami.2015.2439281", "10.1109/pacificvis48177.2020.8737", "10.1109/cvpr.2019.00244", "10.1109/tvcg.2006.165", "10.1109/bigdata.2018.8622520", "10.1007/978-3-319-46466-4\\_29", "10.1109/visual.2019.8933759", "10.1145/3197517.3201304", "10.1109/pacificvis.2011.5742378", "10.1109/cvpr.2006.91", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1007/978-3-319-24574-4\\_28", "10.1109/pacificvis.2019.00041", "10.1109/tvcg.2019.2934332", "10.1109/cvpr.2018.00916", "10.1007/978-3-319-46475-6\\_43"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/tvcg.2020.3028947", "title": "A Fluid Flow Data Set for Machine Learning and its Application to Neural Flow Map Interpolation", "year": "2020", "conferenceName": "SciVis", "authors": "Jakob Jakob;Markus H. Gross;Tobias G\u00fcnther", "citationCount": "0", "affiliation": "Jakob, J (Corresponding Author), Swiss Fed Inst Technol, Zurich, Switzerland. Jakob, Jakob; Gross, Markus; Guenther, Tobias, Swiss Fed Inst Technol, Zurich, Switzerland.", "countries": "Switzerland", "abstract": "In recent years, deep learning has opened countless research opportunities across many different disciplines. At present, visualization is mainly applied to explore and explain neural networks. Its counterpart-the application of deep learning to visualization problems-requires us to share data more openly in order to enable more scientists to engage in data-driven research. In this paper, we construct a large fluid flow data set and apply it to a deep learning problem in scientific visualization. Parameterized by the Reynolds number, the data set contains a wide spectrum of laminar and turbulent fluid flow regimes. The full data set was simulated on a high-performance compute cluster and contains 8000 time-dependent 2D vector fields, accumulating to more than 16 TB in size. Using our public fluid data set, we trained deep convolutional neural networks in order to set a benchmark for an improved post-hoc Lagrangian fluid flow analysis. In in-situ settings, flow maps are exported and interpolated in order to assess the transport characteristics of time-dependent fluids. Using deep learning, we improve the accuracy of flow map interpolations, allowing a more precise flow analysis at a reduced memory IO footprint.", "keywords": "Scientific visualization,deep learning,flow maps", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028947", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1145/3355089.3356560", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1007/978-3-030-00533-7\\_36", "10.3390/rs11161921", "10.1007/978-3-030-48457-6\\_1", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/mcg.2018.2881523", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1145/3072959.3073643", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/tvcg.2013.128", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1007/978-3-319-46475-6\\_43", "10.1126/science.1127647", "10.1007/978-3-319-46475-6\\_25", "10.1111/cgf.13689"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030346", "title": "V2V: A Deep Learning Approach to Variable-to-Variable Selection and Translation for Multivariate Time-Varying Data", "year": "2020", "conferenceName": "SciVis", "authors": "Jun Han;Hao Zheng 0006;Yunhao Xing;Danny Ziyi Chen;Chaoli Wang", "citationCount": "0", "affiliation": "Han, J (Corresponding Author), Univ Notre Dame, Notre Dame, IN 46556 USA. Han, Jun; Zheng, Hao; Chen, Danny Z.; Wang, Chaoli, Univ Notre Dame, Notre Dame, IN 46556 USA. Xing, Yunhao, Sichuan Univ, Chengdu, Sichuan, Peoples R China.", "countries": "China;USA", "abstract": "We present V2V, a novel deep learning framework, as a general-purpose solution to the variable-to-variable (V2V) selection and translation problem for multivariate time-varying data (MTVD) analysis and visualization. V2V leverages a representation learning algorithm to identify transferable variables and utilizes Kullback-Leibler divergence to determine the source and target variables. It then uses a generative adversarial network (GAN) to learn the mapping from the source variable to the target variable via the adversarial, volumetric, and feature losses. V2V takes the pairs of time steps of the source and target variable as input for training, Once trained, it can infer unseen time steps of the target variable given the corresponding time steps of the source variable. Several multivariate time-varying data sets of different characteristics are used to demonstrate the effectiveness of V2V, both quantitatively and qualitatively. We compare V2V against histogram matching and two other deep learning solutions (Pix2Pix and CycleGAN).", "keywords": "Multivariate time-varying data,variable selection and translation,generative adversarial network,data extrapolation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030346", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/cvpr.2017.19", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1146/annurev", "10.3390/rs11161921", "10.1109/tvcg.2007.70551", "10.1145/3309993", "10.1007/978-3-030-48457-6\\_1", "10.1109/igarss.2018.8518411", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1146/annurev-fluid-010313-141322", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1126/science.1127647"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13689", "year": "2019", "title": "Robust Reference Frame Extraction from Unsteady 2D Vector Fields with Convolutional Neural Networks", "conferenceName": "EuroVis", "authors": "Byungsoo Kim;Tobias G{\\\"{u}}nther", "citationCount": "3", "affiliation": "Kim, B (Corresponding Author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.\nKim, Byungsoo; Guenther, Tobias, Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.", "countries": "Switzerland", "abstract": "Robust feature extraction is an integral part of scientific visualization. In unsteady vector field analysis, researchers recently directed their attention towards the computation of near-steady reference frames for vortex extraction, which is a numerically challenging endeavor. In this paper, we utilize a convolutional neural network to combine two steps of the visualization pipeline in an end-to-end manner: the filtering and the feature extraction. We use neural networks for the extraction of a steady reference frame for a given unsteady 2D vector field. By conditioning the neural network to noisy inputs and resampling artifacts, we obtain numerically stabler results than existing optimization-based approaches. Supervised deep learning typically requires a large amount of training data. Thus, our second contribution is the creation of a vector field benchmark data set, which is generally useful for any local deep learning-based feature extraction. Based on Vatistas velocity profile, we formulate a parametric vector field mixture model that we parameterize based on numerically-computed example vector fields in near-steady reference frames. Given the parametric model, we can efficiently synthesize thousands of vector fields that serve as input to our deep learning architecture. The proposed network is evaluated on an unseen numerical fluid flow simulation.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13689", "refList": ["10.1016/0377-0257(79)87004-4", "10.1111/cgf.12358", "10.1109/tvcg.2018.2864828", "10.1016/0167-2789(91)90088-q", "10.1007/978-3-540-70823-0\\_9", "10.1016/b978-012387582-2/50040-x", "10.1111/cgf.13405", "10.1111/cgf.13319", "10.1109/tvcg.2018.2843369", "10.1007/bf00538235", "10.23940/ijpe.18.03", "10.1017/jfm.2016.151", "10.1109/visual.1999.809896", "10.1146/annurev.fluid.23.1.601", "10.1109/tvcg.2007.1036", "10.1109/tvcg.2018.2864839", "10.1007/978-3-319-54024-5\\_6", "10.1109/tvcg.2013.189", "10.1145/3072959.3073684", "10.1109/tvcg.2015.2467203", "10.2514/2.957", "10.1016/0960-0779(94)90137-6", "10.1109/visual.1991.175773", "10.1109/visual.1996.568137", "10.1017/s0022112004002526", "10.1109/pacificvis.2016.7465253", "10.1007/bf00849110", "10.1007/bf00198434", "10.1109/tvcg.2007.70545", "10.3390/informatics4030027", "10.1137/140983665", "10.1016/0011-7471(70)90059-8", "10.1017/s0022112095000462", "10.1145/1183287.1183290"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028947", "title": "A Fluid Flow Data Set for Machine Learning and its Application to Neural Flow Map Interpolation", "year": "2020", "conferenceName": "SciVis", "authors": "Jakob Jakob;Markus H. Gross;Tobias G\u00fcnther", "citationCount": "0", "affiliation": "Jakob, J (Corresponding Author), Swiss Fed Inst Technol, Zurich, Switzerland. Jakob, Jakob; Gross, Markus; Guenther, Tobias, Swiss Fed Inst Technol, Zurich, Switzerland.", "countries": "Switzerland", "abstract": "In recent years, deep learning has opened countless research opportunities across many different disciplines. At present, visualization is mainly applied to explore and explain neural networks. Its counterpart-the application of deep learning to visualization problems-requires us to share data more openly in order to enable more scientists to engage in data-driven research. In this paper, we construct a large fluid flow data set and apply it to a deep learning problem in scientific visualization. Parameterized by the Reynolds number, the data set contains a wide spectrum of laminar and turbulent fluid flow regimes. The full data set was simulated on a high-performance compute cluster and contains 8000 time-dependent 2D vector fields, accumulating to more than 16 TB in size. Using our public fluid data set, we trained deep convolutional neural networks in order to set a benchmark for an improved post-hoc Lagrangian fluid flow analysis. In in-situ settings, flow maps are exported and interpolated in order to assess the transport characteristics of time-dependent fluids. Using deep learning, we improve the accuracy of flow map interpolations, allowing a more precise flow analysis at a reduced memory IO footprint.", "keywords": "Scientific visualization,deep learning,flow maps", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028947", "refList": ["10.1007/978-3-319-10593-2\\_25", "10.1145/3355089.3356560", "10.1109/access.2019.2903582", "10.1111/cgf.13405", "10.1109/tmm.2019.2919431", "10.1017/jfm.2016.151", "10.1016/j.physd.2005.10.007", "10.1007/978-3-030-00533-7\\_36", "10.3390/rs11161921", "10.1007/978-3-030-48457-6\\_1", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2007.70554", "10.1109/mcg.2018.2881523", "10.1109/cvpr.2016.207", "10.1145/1073204.1073264", "10.1145/3072959.3073643", "10.1109/tpami.2015.2439281", "10.2312/pgv", "10.4208/cicp.oa-2018-0035", "10.1109/cvpr.2017.693", "10.1109/tvcg.2013.128", "10.1109/igarss.2018.8519261", "10.1063/1.3270044", "10.2312/pgv.20191115", "10.1145/3197517.3201304", "10.1145/3065386", "10.1145/1360612.1360649", "10.1109/mcse.2015.103", "10.1145/3355089.3356575", "10.1109/mcg.2018.2881502", "10.1029/2019jd032121", "10.3390/informatics4030027", "10.1109/access.2019.2931781", "10.1007/s12650-018-0523-1", "10.1109/tvcg.2019.2934332", "10.1146/annurev.fluid.32.1.165", "10.1007/978-3-319-46475-6\\_43", "10.1126/science.1127647", "10.1007/978-3-319-46475-6\\_25", "10.1111/cgf.13689"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.8737", "year": "2020", "title": "SSR-VFD: Spatial Super-Resolution for Vector Field Data Analysis and Visualization", "conferenceName": "PacificVis", "authors": "Li Guo;Shaojie Ye;Jun Han;Hao Zheng;Han Gao;Danny Z. Chen;Jian{-}Xun Wang;Chaoli Wang", "citationCount": "1", "affiliation": "Guo, L (Corresponding Author), Nankai Univ, Tianjin, Peoples R China.\nGuo, Li, Nankai Univ, Tianjin, Peoples R China.\nYe, Shaojie, Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.\nHan, Jun; Zheng, Hao; Gao, Han; Chen, Danny Z.; Wang, Jian-Xun; Wang, Chaoli, Univ Notre Dame, Notre Dame, IN 46556 USA.", "countries": "USA;China", "abstract": "We present SSR-VFD, a novel deep learning framework that produces coherent spatial super-resolution (SSR) of three-dimensional vector field data (VFD). SSR-VFD is the first work that advocates a machine learning approach to generate high-resolution vector fields from low-resolution ones. The core of SSR-VFD lies in the use of three separate neural nets that take the three components of a low-resolution vector field as input and jointly output a synthesized high-resolution vector field. To capture spatial coherence, we take into account magnitude and angle losses in network optimization. Our method can work in the in situ scenario where VFD are down-sampled at simulation time for storage saving and these reduced VFD are upsampled back to their original resolution during postprocessing. To demonstrate the effectiveness of SSR-VFD, we show quantitative and qualitative results with several vector field data sets of different characteristics and compare our method against volume upscaling using bicubic interpolation, and two solutions based on CNN and GAN, respectively.", "keywords": "Spatial super-resolution; vector field data; convolutional neural network; deep learning", "link": "https://doi.org/10.1109/PacificVis48177.2020.8737", "refList": ["10.1016/j.ijvsm.2017.05.001", "10.1016/j.jvs.2005.01.020", "10.1109/iccv.2015.123", "10.1111/cgf.13620", "10.1109/cvpr.2019.00831", "10.1109/cvpr.2019.00817", "10.1109/cvpr.2019.00399", "10.1109/tvcg.2019.2934312", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/tvcg.2018.2816059", "10.1109/mcg.2018.2881523", "10.1109/tpami.2015.2439281", "10.1109/bigdata.2018.8622520", "10.1145/3197517.3201304", "10.1109/tvcg.2018.2796085", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1109/pacificvis.2019.00041", "10.1111/cgf.13689"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030374", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and Visualization of Highly Sparse and Noisy Image Data", "year": "2020", "conferenceName": "SciVis", "authors": "Yifan Wang;Guoli Yan;Haikuan Zhu;Sagar Buch;Ying Wang;E. Mark Haacke;Jing Hua;Zichun Zhong", "citationCount": "0", "affiliation": "Wang, YF (Corresponding Author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Wang, Yifan; Yan, Guoli; Zhu, Haikuan; Hua, Jing; Zhong, Zichun, Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA. Buch, Sagar; Wang, Ying; Haacke, Ewart Mark, Wayne State Univ, Dept Radiol, Detroit, MI 48201 USA.", "countries": "USA", "abstract": "The fundamental motivation of the proposed work is to present a new visualization-guided computing paradigm to combine direct 3D volume processing and volume rendered clues for effective 3D exploration. For example, extracting and visualizing microstructures in-vivo have been a long-standing challenging problem. However, due to the high sparseness and noisiness in cerebrovasculature data as well as highly complex geometry and topology variations of micro vessels, it is still extremely challenging to extract the complete 3D vessel structure and visualize it in 3D with high fidelity. In this paper, we present an end-to-end deep learning method, VC-Net, for robust extraction of 3D microvascular structure through embedding the image composition, generated by maximum intensity projection (MIP), into the 3D volumetric image learning process to enhance the overall performance. The core novelty is to automatically leverage the volume visualization technique (e.g., MIP - a volume rendering scheme for 3D volume images) to enhance the 3D data exploration at the deep learning level. The MIP embedding features can enhance the local vessel signal (through canceling out the noise) and adapt to the geometric variability and scalability of vessels, which is of great importance in microvascular tracking. A multi-stream convolutional neural network (CNN) framework is proposed to effectively learn the 3D volume and 2D MIP feature vectors, respectively, and then explore their inter-dependencies in a joint volume-composition embedding space by unprojecting the 2D feature vectors into the 3D volume embedding space. It is noted that the proposed framework can better capture the small/micro vessels and improve the vessel connectivity. To our knowledge, this is the first time that a deep learning framework is proposed to construct a joint convolutional embedding space, where the computed vessel probabilities from volume rendering based 2D projection and 3D volume can be explored and integrated synergistically. Experimental results are evaluated and compared with the traditional 3D vessel segmentation methods and the state-of-the-art in deep learning, by using extensive public and real patient (micro- )cerebrovascular image datasets. The application of this accurate segmentation and visualization of sparse and complicated 3D microvascular structure facilitated by our method demonstrates the potential in a powerful MR arteriogram and venogram diagnosis of vascular disease.", "keywords": "Deep neural network,3D cerebrovascular segmentation and visualization,maximum intensity projection (MIP),joint embedding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030374", "refList": ["10.1109/cluster.2018.00036", "10.1109/iccv.2015.123", "10.1109/tvcg.2013.133", "10.1109/cvpr.2017.19", "10.1109/tvcg.2019.2934312", "10.1007/978-3-319-46487-9\\_40", "10.1109/tvcg.2006.175", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2018.2880207", "10.1145/3309993", "10.1109/pacificvis.2018.00018", "10.1109/iccv.2017.244", "10.1109/cvpr.2017.632", "10.1109/tvcg.2018.2816059", "10.1109/pacificvis.2009.4906852", "10.1109/tvcg.2018.2864808", "10.1109/mcg.2018.2881523", "10.1109/tvcg.2015.2467431", "10.1109/tpami.2015.2439281", "10.1109/pacificvis48177.2020.8737", "10.1109/cvpr.2019.00244", "10.1109/tvcg.2006.165", "10.1109/bigdata.2018.8622520", "10.1007/978-3-319-46466-4\\_29", "10.1109/visual.2019.8933759", "10.1145/3197517.3201304", "10.1109/pacificvis.2011.5742378", "10.1109/cvpr.2006.91", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2019.2934255", "10.1109/cvpr.2016.90", "10.1007/978-3-319-24574-4\\_28", "10.1109/pacificvis.2019.00041", "10.1109/tvcg.2019.2934332", "10.1109/cvpr.2018.00916", "10.1007/978-3-319-46475-6\\_43"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 7}, {"doi": "10.1111/cgf.13670", "year": "2019", "title": "Follow The Clicks: Learning and Anticipating Mouse Interactions During Exploratory Data Analysis", "conferenceName": "EuroVis", "authors": "Alvitta Ottley;Roman Garnett;Ran Wan", "citationCount": "1", "affiliation": "Ottley, A (Corresponding Author), Washington Univ, Comp Sci \\& Engn, St Louis, MO 63110 USA.\nOttley, Alvitta; Garnett, Roman; Wan, Ran, Washington Univ, Comp Sci \\& Engn, St Louis, MO 63110 USA.", "countries": "USA", "abstract": "The goal of visual analytics is to create a symbiosis between human and computer by leveraging their unique strengths. While this model has demonstrated immense success, we are yet to realize the full potential of such a human-computer partnership. In a perfect collaborative mixed-initiative system, the computer must possess skills for learning and anticipating the users' needs. Addressing this gap, we propose a framework for inferring attention from passive observations of the user's click, thereby allowing accurate predictions of future events. We demonstrate this technique with a crime map and found that users' clicks can appear in our prediction set 92\\% - 97\\% of the time. Further analysis shows that we can achieve high prediction accuracy typically after three clicks. Altogether, we show that passive observations of interaction data can reveal valuable information that will allow the system to learn and anticipate future events.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13670", "refList": ["10.1057/ivs.2009.22", "10.1145/2882903.2882919", "10.1109/tvcg.2014.2346575", "10.1145/846183.846188", "10.1111/cgf.13405", "10.1109/tvcg.2016.2598471", "10.1145/3173574.3174168", "10.1145/948496.948514", "10.1109/mcg.2006.30", "10.1109/tvcg.2012.195", "10.1016/s0042-6989(99)00163-7", "10.1007/978-3-540-70956-5", "10.1145/643477.643478", "10.1038/35058500", "10.1023/a:1008935410038", "10.1002/9781118360491.ch10", "10.1007/978-94-009-3833-5\\_5", "10.1145/2702123.2702590", "10.1057/ivs.2008.31", "10.1145/1054972.1055012", "10.1109/tvcg.2015.2467551", "10.1145/636772.636798", "10.1109/tpami.2012.89", "10.1109/34.730558", "10.1094/pdis-11-11-0999-pdn", "10.1145/1476589.1476628", "10.1145/964442.964461", "10.1145/302979.303030", "10.1109/5254.796083", "10.1145/360402.360406", "10.1109/icarcv.2018.8581221", "10.1049/ip-f-2.1993.0015", "10.1109/compsac.2017.270", "10.1109/vast.2008.4677361", "10.1145/1979742.1979570", "10.1109/hicss.2005.286", "10.1145/2110192.2110202", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2016.2598468"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030383", "title": "II-20: Intelligent and pragmatic analytic categorization of image collections", "year": "2020", "conferenceName": "VAST", "authors": "Jan Zah\u00e1lka;Marcel Worring;Jarke J. van Wijk", "citationCount": "0", "affiliation": "Zahalka, J (Corresponding Author), Czech Tech Univ, Prague, Czech Republic. Zahalka, Jan, Czech Tech Univ, Prague, Czech Republic. Worring, Marcel, Univ Amsterdam, Amsterdam, Netherlands. van Wijk, Jarke J., Eindhoven Univ Technol, Eindhoven, Netherlands.", "countries": "Republic;Netherlands", "abstract": "In this paper, we introduce 11\u201320 (Image Insight 2020), a multimedia analytics approach for analytic categorization of image collections. Advanced visualizations for image collections exist, but they need tight integration with a machine model to support the task of analytic categorization. Directly employing computer vision and interactive learning techniques gravitates towards search. Analytic categorization, however, is not machine classification (the difference between the two is called the pragmatic gap): a human adds/redefines/deletes categories of relevance on the fly to build insight, whereas the machine classifier is rigid and non-adaptive. Analytic categorization that truly brings the user to insight requires a flexible machine model that allows dynamic sliding on the exploration-search axis, as well as semantic interactions: a human thinks about image data mostly in semantic terms. 11\u201320 brings three major contributions to multimedia analytics on image collections and towards closing the pragmatic gap. Firstly, a new machine model that closely follows the user's interactions and dynamically models her categories of relevance. II-20's machine model, in addition to matching and exceeding the state of the art's ability to produce relevant suggestions, allows the user to dynamically slide on the exploration-search axis without any additional input from her side. Secondly, the dynamic, 1-image-at-a-time Tetris metaphor that synergizes with the model. It allows a well-trained model to analyze the collection by itself with minimal interaction from the user and complements the classic grid metaphor. Thirdly, the fast-forward interaction, allowing the user to harness the model to quickly expand (\u201cfast-forward\u201d) the categories of relevance, expands the multimedia analytics semantic interaction dictionary. Automated experiments show that II-20's machine model outperforms the existing state of the art and also demonstrate the Tetris metaphor's analytic quality. User studies further confirm that II\u201320 is an intuitive, efficient, and effective multimedia analytics tool.", "keywords": "Multimedia analytics,image data,analytic categorization,pragmatic gap", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030383", "refList": ["10.1109/tvcg.2008.137", "10.1145/2882903.2882919", "10.1145/2556647.2556657", "10.1145/1142473.1142574", "10.1109/tvcg.2016.2598471", "10.1109/vast.2017.8585669", "10.1109/tvcg.2014.2346573", "10.1111/j.0956-7976.2005.00782.x", "10.1007/978-3-540-89965-5\\_27", "10.1109/visual.2019.8933611", "10.1109/vast.2010.5653598", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1109/tvcg.2012.271", "10.1109/mcg.2009.49", "10.1057/ivs.2008.31", "10.1111/cgf.12925", "10.1109/hicss.2016.183", "10.1109/tvcg.2016.2598466", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2018.2865117", "10.1109/vast47406.2019.8986948", "10.1016/s0950-7051(00)00101-5", "10.1109/tvcg.2016.2598594", "10.1109/tvcg.2006.101", "10.1111/cgf.12311", "10.1109/vast.2009.5333020", "10.1109/vast.2010.5652885", "10.1111/cgf.13670", "10.1109/wvl.1988.18020", "10.1145/2133806.2133821", "10.1109/vast.2012.6400486"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 31}, {"doi": "10.1111/cgf.13731", "year": "2019", "title": "The State of the Art in Visual Analysis Approaches for Ocean and Atmospheric Datasets", "conferenceName": "EuroVis", "authors": "Shehzad Afzal;Mohamad Mazen Hittawe;Sohaib Ghani;Tahira Jamil;Omar M. Knio;Markus Hadwiger;Kevin I.{-}J. Ho", "citationCount": "0", "affiliation": "Afzal, S (Corresponding Author), King Abdullah Univ Sci \\& Technol, Thuwal, Saudi Arabia.\nAfzal, S.; Hittawe, M. M.; Ghani, S.; Jamil, T.; Knio, O.; Hadwiger, M.; Hoteit, I, King Abdullah Univ Sci \\& Technol, Thuwal, Saudi Arabia.", "countries": "Arabia", "abstract": "The analysis of ocean and atmospheric datasets offers a unique set of challenges to scientists working in different application areas. These challenges include dealing with extremely large volumes of multidimensional data, supporting interactive visual analysis, ensembles exploration and visualization, exploring model sensitivities to inputs, mesoscale ocean features analysis, predictive analytics, heterogeneity and complexity of observational data, representing uncertainty, and many more. Researchers across disciplines collaborate to address such challenges, which led to significant research and development advances in ocean and atmospheric sciences, and also in several relevant areas such as visualization and visual analytics, big data analytics, machine learning and statistics. In this report, we perform an extensive survey of research advances in the visual analysis of ocean and atmospheric datasets. First, we survey the task requirements by conducting interviews with researchers, domain experts, and end users working with these datasets on a spectrum of analytics problems in the domain of ocean and atmospheric sciences. We then discuss existing models and frameworks related to data analysis, sense-making, and knowledge discovery for visual analytics applications. We categorize the techniques, systems, and tools presented in the literature based on the taxonomies of task requirements, interaction methods, visualization techniques, machine learning and statistical methods, evaluation methods, data types, data dimensions and size, spatial scale and application areas. We then evaluate the task requirements identified based on our interviews with domain experts in the context of categorized research based on our taxonomies, and existing models and frameworks of visual analytics to determine the extent to which they fulfill these task requirements, and identify the gaps in current research. In the last part of this report, we summarize the trends, challenges, and opportunities for future research in this area. (see http://www.acm.org/about/class/class/2012) )", "keywords": "", "link": "https://doi.org/10.1111/cgf.13731", "refList": ["10.1111/cgf.12898", "10.1109/icdmw.2009.55", "10.1109/tvcg.2013.144", "10.1109/tvcg.2015.2467754", "10.1109/tvcg.2015.2507569", "10.1109/tvcg.2014.2346455", "10.1111/cgf.12901", "10.1109/tvcg.2008.184", "10.1109/mc.2013.119", "10.1111/j.1467-8659.2009.01697.x", "10.1109/tvcg.2009.200", "10.1111/cgf.12649", "10.1109/2945.981847", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2012.190", "10.1109/mis.2006.75", "10.1109/iv.2015.13", "10.1111/cgf.12135", "10.1109/vast.2009.5332586", "10.1109/pacificvis.2011.5742369", "10.1109/tvcg.2008.131", "10.1109/tvcg.2013.131", "10.1109/tvcg.2010.82", "10.1109/pacificvis.2015.7156374", "10.1111/cgf.12886", "10.1109/tvcg.2016.2598869", "10.1109/vast.2012.6400553", "10.1109/pacificvis.2013.6596144", "10.1109/tvcg.2017.2745178", "10.1109/tvcg.2015.2410278", "10.1111/cgf.12931", "10.1109/tvcg.2009.155", "10.1109/pacificvis.2015.7156366", "10.1109/mcg.2015.121", "10.1007/978-1-4471-2804-5\\_6", "10.1109/ldav.2015.7348068", "10.1007/978-1-4471-6497-5\\_1", "10.1109/pacificvis.2009.4906852", "10.1111/cgf.12646", "10.1109/tvcg.2016.2607204", "10.1109/tvcg.2011.162", "10.1177/1473871612465214", "10.1109/mcg.2017.3621228", "10.1109/tvcg.2012.110", "10.1109/sc.2014.40", "10.5194/gmd-8-2329-2015", "10.1109/tvcg.2012.80", "10.1109/tvcg.2014.2346448", "10.1109/ldav.2012.6378978", "10.1007/s10915-011-9501-7", "10.1109/tvcg.2016.2598830", "10.1109/tvcg.2007.70515", "10.1109/vast.2011.6102460", "10.1109/pacificvis.2017.8031584", "10.1109/tvcg.2016.2637904", "10.1175/2009jtecha1374.1", "10.1109/tvcg.2015.2467591", "10.1109/tvcg.2015.2467204", "10.1109/tvcg.2008.59", "10.1109/tvcg.2013.143", "10.1109/tvcg.2008.119", "10.1109/tvcg.2015.2467411", "10.1109/icdmw.2009.91", "10.1109/tvcg.2016.2598868", "10.1109/tvcg.2017.2661309", "10.1109/vl.1996.545307", "10.1109/tvcg.2007.70523", "10.1109/tvcg.2008.140", "10.1109/icra.2012.6224689", "10.1109/tvcg.2010.80", "10.1167/tvst.7.1.16", "10.1109/pacificvis.2018.00037", "10.1111/cgf.13210", "10.1109/tvcg.2017.2698041", "10.1109/iv.2010.51", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2010.170", "10.1103/physrevd.71.077102", "10.5194/npg-22-545-2015", "10.1109/tvcg.2014.2307892", "10.1111/cgf.12650", "10.1109/iv.2009.38", "10.2312/pe.envirvis.envirvis13.053-057", "10.1145/3122948.3122952", "10.1109/ldav.2014.7013208", "10.1038/nature14956", "10.1109/vast.2015.7347671", "10.1109/vast.2015.7347634", "10.1109/tvcg.2014.2346755", "10.1177/1473871613481692", "10.1109/ldav.2017.8231849", "10.1109/tvcg.2017.2773071", "10.1109/iv.2011.79", "10.1109/tvcg.2013.10", "10.1109/tvcg.2008.157", "10.1111/j.1467-8659.2009.01664.x", "10.1109/pacificvis.2016.7465251", "10.1109/vast.2014.7042489", "10.1109/mis.2006.100", "10.1109/tvcg.2010.247", "10.1109/tvcg.2016.2534560", "10.1109/tvcg.2010.181", "10.1109/tvcg.2018.2864817", "10.1109/vast.2015.7347635", "10.1109/tvcg.2018.2864901", "10.1109/hicss.2016.183", "10.1109/iv.2010.32", "10.1111/j.1467-8659.2011.01948.x", "10.1109/tvcg.2017.2743989", "10.1109/pacificvis.2016.7465272", "10.1109/tvcg.2008.69", "10.1109/tvcg.2008.139", "10.1109/iv.2011.12", "10.1111/cgf.12520"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030466", "title": "Uncertainty in Continuous Scatterplots, Continuous Parallel Coordinates, and Fibers", "year": "2020", "conferenceName": "SciVis", "authors": "Boyan Zheng;Filip Sadlo", "citationCount": "0", "affiliation": "Zheng, BY (Corresponding Author), Heidelberg Univ, Heidelberg, Germany. Zheng, Boyan; Sadlo, Filip, Heidelberg Univ, Heidelberg, Germany.", "countries": "Germany", "abstract": "In this paper, we introduce uncertainty to continuous scatterplots and continuous parallel coordinates. We derive respective models, validate them with sampling-based brute-force schemes, and present acceleration strategies for their computation. At the same time, we show that our approach lends itself as well for introducing uncertainty into the definition of fibers in bivariate data. Finally, we demonstrate the properties and the utility of our approach using specifically designed synthetic cases and simulated data.", "keywords": "Multivariate data,uncertainty visualization,uncertain continuous scatterplots,uncertain continuous parallel coordinates,uncertain fibers", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030466", "refList": ["10.1109/tvcg.2018.2865193", "10.1109/tvcg.2013.143", "10.1109/tvcg.2015.2467204", "10.1109/pacificvis.2013.6596144", "10.1109/tvcg.2017.2745178", "10.1109/tmm.2016.2614227", "10.1109/scivis.2015.7429488", "10.1111/cgf.12100", "10.1109/pacificvis.2016.7465251", "10.1111/cgf.12898", "10.1109/tvcg.2015.2410278", "10.1109/icdmw.2009.55", "10.1109/tvcg.2015.2467754", "10.1109/tvcg.2010.247", "10.1109/tvcg.2019.2934312", "10.1111/cgf.13397", "10.1109/tvcg.2016.2598868", "10.1111/j.1467-8659.2011.01944.x", "10.1109/tvcg.2015.2507569", "10.1109/tvcg.2013.92", "10.1145/1268517.1268563", "10.1109/tvcg.2014.2346455", "10.1109/tvcg.2010.181", "10.1109/tvcg.2018.2853721", "10.1109/tvcg.2008.140", "10.1007/s12650-015-0341-7", "10.1109/mcg.2014.52", "10.1109/tvcg.2018.2864815", "10.1111/j.1467-8659.2012.03095.x", "10.1109/tvcg.2013.138", "10.1109/tvcg.2019.2934242", "10.3390/e20070540", "10.1016/j.jcp.2007.02.014", "10.1111/cgf.13999", "10.1109/tvcg.2017.2779501", "10.1111/cgf.12390", "10.1109/tvcg.2014.2307892", "10.1111/cgf.13531", "10.1109/tvcg.2013.152", "10.1038/nature14956", "10.1007/978-3-540-88606-8\\_4", "10.1109/mcg.2005.71", "10.1111/j.1467-8659.2011.01942.x", "10.1109/tvcg.2019.2934800", "10.1109/tvcg.2016.2598830", "10.1109/cvpr.2005.188", "10.1109/tvcg.2011.261", "10.1111/cgf.13731", "10.1109/tvcg.2017.2754480"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 67}, "index": 1346, "embedding": [-1.2106531858444214, 1.2896493673324585, -1.0626764297485352, -2.5201807022094727, -0.2168128341436386, 0.16650904715061188, -0.7139257192611694, 1.6108592748641968, 5.2916035652160645, 2.5891904830932617, 0.723839282989502, 0.7929927110671997, 3.745927572250366, 3.848360776901245, 2.139086961746216, 0.682343602180481, 0.8204331398010254, 0.07420150190591812, 1.4000067710876465, 4.81790828704834, -0.06630208343267441, 2.1324799060821533, 2.0833001136779785, 4.5457587242126465, -2.5787017345428467, 8.454176902770996, -0.5483691096305847, 0.6281458735466003, 0.3906826972961426, -2.4690706729888916, 1.907996416091919, 1.9717274904251099], "projection": [1.3974225521087646, 10.40720272064209], "size": 34, "height": 5, "width": 17}