{"data": {"doi": "10.1109/tvcg.2016.2599042", "title": "Progressive Direct Volume-to-Volume Transformation", "year": "2016", "conferenceName": "SciVis", "authors": "Steffen Frey;Thomas Ertl", "citationCount": "5", "affiliation": "Frey, S (Corresponding Author), Univ Stuttgart, Stuttgart, Germany. Frey, Steffen; Ertl, Thomas, Univ Stuttgart, Stuttgart, Germany.", "countries": "Germany", "abstract": "We present a novel technique to generate transformations between arbitrary volumes, providing both expressive distances and smooth interpolates. In contrast to conventional morphing or warping approaches, our technique requires no user guidance, intermediate representations (like extracted features), or blending, and imposes no restrictions regarding shape or structure. Our technique operates directly on the volumetric data representation, and while linear programming approaches could solve the underlying problem optimally, their polynomial complexity makes them infeasible for high-resolution volumes. We therefore propose a progressive refinement approach designed for parallel execution that is able to quickly deliver approximate results that are iteratively improved toward the optimum. On this basis, we further present a new approach for the streaming selection of time steps in temporal data that allows for the reconstruction of the full sequence with a user-specified error bound. We finally demonstrate the utility of our technique for different applications, compare our approach against alternatives, and evaluate its characteristics with a variety of different data sets.", "keywords": "Volume transformation;Volume visualization;progressive;automatic;parallel;time-varying data;streaming data", "link": "http://dx.doi.org/10.1109/TVCG.2016.2599042", "refList": ["10.1109/pacificvis.2009.4906831", "10.1002/jcc.20084", "10.1145/2024156.2024192", "10.1145/1142473.1142574", "10.1016/s0167-8396(99)00039-4", "10.1023/a:1026543900054", "10.1016/j.cag.2013.05.009", "10.1109/visual.1994.346333", "10.1145/1268517.1268563", "10.1016/j.amc.2015.05.095", "10.1109/tvcg.2008.140", "10.1109/pacificvis.2013.6596136", "10.1007/s11390-011-1154-3", "10.1109/tvcg.2010.39", "10.1109/tvcg.2009.200", "10.1016/j.cag.2010.01.007", "10.1109/ldav.2012.6378962", "10.1109/2945.597796", "10.1109/tvcg.2011.54", "10.1109/visual.2002.1183809", "10.2312/vg/vg03/027-034", "10.1109/cvpr.1994.323794", "10.1109/pacificvis.2011.5742372", "10.1109/ldav.2012.6378975", "10.1007/978-3-642-24734-7\\_16", "10.1109/tvcg.2012.284", "10.1145/12130.12144", "10.1145/2601097.2601124", "10.1007/s00371-012-0688-x", "10.1109/tvcg.2008.143"], "wos": 1, "children": [{"doi": "10.1111/cgf.13438", "year": "2018", "title": "Spatio-Temporal Contours from Deep Volume Raycasting", "conferenceName": "EuroVis", "authors": "Steffen Frey", "citationCount": "2", "affiliation": "Frey, S (Corresponding Author), Univ Stuttgart, Visualizat Res Ctr, Stuttgart, Germany.\nFrey, S., Univ Stuttgart, Visualizat Res Ctr, Stuttgart, Germany.", "countries": "Germany", "abstract": "We visualize contours for spatio-temporal processes to indicate where and when non-continuous changes occur or spatial bounds are encountered. All time steps are comprised densely in one visualization, with contours allowing to efficiently analyze processes in the data even in case of spatial or temporal overlap. Contours are determined on the basis of deep raycasting that collects samples across time and depth along each ray. For each sample along a ray, its closest neighbors from adjacent rays are identified, considering time, depth, and value in the process. Large distances are represented as contours in image space, using color to indicate temporal occurrence. This contour representation can easily be combined with volume rendering-based techniques, providing both full spatial detail for individual time steps and an outline of the whole time series in one view. Our view-dependent technique supports efficient progressive computation, and requires no prior assumptions regarding the shape or nature of processes in the data. We discuss and demonstrate the performance and utility of our approach via a variety of data sets, comparison and combination with an alternative technique, and feedback by a domain scientist.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13438", "refList": ["10.1109/pacificvis.2009.4906831", "10.1109/tvcg.2008.105", "10.1145/1073204.1073222", "10.1145/1268517.1268528", "10.1145/2461912.2461928", "10.1109/pacificvis.2015.7156386", "10.1109/tvcg.2016.2599042", "10.1023/a:1026543900054", "10.2312/conf/eg2012/stars/075-094", "10.1145/1268517.1268563", "10.1016/j.amc.2015.05.095", "10.1109/tvcg.2008.140", "10.1111/cgf.13201", "10.1038/nature05428", "10.1109/tvcg.2009.200", "10.1109/ldav.2012.6378962", "10.1109/2945.597796", "10.1111/j.1467-8659.2010.01650.x", "10.1111/j.1467-8659.2008.01235.x", "10.1109/tvcg.2017.2692781", "10.1109/tvcg.2007.47", "10.1109/ldav.2012.6378975", "10.1109/tvcg.2012.284", "10.1109/visual.2003.1250402", "10.1111/cgf.13070", "10.1111/j.1467-8659.2003.00723.x", "10.3390/informatics4030027", "10.1109/tvcg.2008.143", "10.1109/visual.2001.964554", "10.1111/cgf.12804"], "wos": 1, "children": [], "len": 1}], "len": 3}, "index": 621, "embedding": [0.0429462231695652, -0.11783739924430847, -1.0254276990890503, 0.13930334150791168, 0.2573772072792053, 0.16650904715061188, -0.706787109375, 0.869491457939148, 0.7837430834770203, 0.311816543340683, 0.31978270411491394, -0.2687914967536926, -0.13366082310676575, -1.1824467182159424, -0.7231093645095825, -0.4608985483646393, 0.18585053086280823, 0.06482167541980743, -0.6607173681259155, 0.23750987648963928, -0.06630208343267441, -0.28009453415870667, -0.8408960700035095, 0.021335532888770103, -1.0167875289916992, 0.08472851663827896, -0.5077023506164551, -0.17138656973838806, 0.6413806080818176, -1.460415005683899, -0.4162452816963196, -0.4846896231174469], "projection": [-1.7701795101165771, 8.581881523132324], "size": 2, "height": 2, "width": 1}