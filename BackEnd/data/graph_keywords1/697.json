{"data": {"doi": "10.1109/tvcg.2017.2744843", "title": "Graphiti: Interactive Specification of Attribute-Based Edges for Network Modeling and Visualization", "year": "2017", "conferenceName": "VAST", "authors": "Arjun Srinivasan;Hyunwoo Park;Alex Endert;Rahul C. Basole", "citationCount": "5", "affiliation": "Srinivasan, A (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA. Srinivasan, Arjun; Endert, Alex; Basole, Rahul C., Georgia Inst Technol, Atlanta, GA 30332 USA. Park, Hyunwoo, Ohio State Univ, Columbus, OH 43210 USA.", "countries": "USA", "abstract": "Network visualizations, often in the form of node-link diagrams, are an effective means to understand relationships between entities, discover entities with interesting characteristics, and to identify clusters. While several existing tools allow users to visualize pre-defined networks, creating these networks from raw data remains a challenging task, often requiring users to program custom scripts or write complex SQL commands. Some existing tools also allow users to both visualize and model networks. Interaction techniques adopted by these tools often assume users know the exact conditions for defining edges in the resulting networks. This assumption may not always hold true, however. In cases where users do not know much about attributes in the dataset or when there are several attributes to choose from, users may not know which attributes they could use to formulate linking conditions. We propose an alternate interaction technique to model networks that allows users to demonstrate to the system a subset of nodes and links they wish to see in the resulting network. The system, in response, recommends conditions that can be used to model networks based on the specified nodes and links. In this paper, we show how such a demonstration-based interaction technique can be used to model networks by employing it in a prototype tool, Graphiti. Through multiple usage scenarios, we show how Graphiti not only allows users to model networks from a tabular dataset but also facilitates updating a pre-defined network with additional edge types.", "keywords": "Network modeling,visual analytics,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2017.2744843", "refList": ["10.1109/tvcg.2009.108", "10.1109/tvcg.2016.2598839", "10.1145/2909132.2909246", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2011.185", "10.1111/cgf.12880", "10.1145/1124772.1124891", "10.1109/2945.981849", "10.1093/bioinformatics/btq675", "10.1145/286498.286511", "10.1136/qshc.2004.010033", "10.1177/1473871613488591", "10.1109/2945.841119", "10.1145/985692.985767", "10.1016/j.socnet.2007.04.006", "10.1145/263407.263525", "10.1007/978-3-642-03658-3\\_47", "10.1057/palgrave.ivs.9500143", "10.1109/tvcg.2009.151", "10.1177/1473871612462152", "10.1109/tvcg.2006.166", "10.1145/1378773.1378792", "10.1147/sj.164.0324", "10.1109/vast.2010.5652520", "10.1162/jmlr.2003.3.4-5.993", "10.1103/physrevx.3.041022", "10.1109/tvcg.2006.106", "10.1017/cb09780511996368", "10.1145/1502650.1502667", "10.1109/iv.2010.28", "10.1111/cgf.12644"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865145", "title": "Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication", "year": "2018", "conferenceName": "InfoVis", "authors": "Arjun Srinivasan;Steven Mark Drucker;Alex Endert;John T. Stasko", "citationCount": "12", "affiliation": "Srinivasan, A (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA. Srinivasan, Arjun; Endert, Alex; Stasko, John, Georgia Inst Technol, Atlanta, GA 30332 USA. Drucker, Steven M., Microsoft Res, Washington, DC USA.", "countries": "USA", "abstract": "Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.", "keywords": "Natural Language Generation,Mixed-initiative Interaction,Visualization Recommendation,Data-driven Communication", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865145", "refList": ["10.2307/1269768", "10.1109/tvcg.2017.2744843", "10.1007/s00371-015-1132-9", "10.1145/3172944.3173007", "10.1109/tvcg.2007.70594", "10.1109/visual.1990.146375", "10.1145/1502650.1502695", "10.1145/108360.108361", "10.14778/2733004.2733035", "10.1109/pacificvis.2017.8031599", "10.1109/tvcg.2013.124", "10.1145/2556288.2557241", "10.1109/visual.1992.235203", "10.1109/infvis.2005.1532136", "10.1109/tvcg.2017.2745219", "10.1109/tvcg.2013.119", "10.1145/2984511.2984588", "10.1109/tvcg.2012.229", "10.1145/3025453.3025866", "10.1145/3035918.3035922", "10.1145/2807442.2807478", "10.1109/tvcg.2010.164", "10.1145/302979.303030", "10.1109/mcg.2006.70", "10.1109/mc.2013.36", "10.1109/tvcg.2015.2467191", "10.1017/s1351324997001502", "10.1111/cgf.13207", "10.1109/mcg.2009.22", "10.1145/2702123.2702608"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934669", "title": "Exploranative Code Quality Documents", "year": "2019", "conferenceName": "VAST", "authors": "Haris Mumtaz;Shahid Latif;Fabian Beck;Daniel Weiskopf", "citationCount": "0", "affiliation": "Mumtaz, H (Corresponding Author), Univ Stuttgart, VISUS, Stuttgart, Germany. Mumtaz, Haris; Weiskopf, Daniel, Univ Stuttgart, VISUS, Stuttgart, Germany. Latif, Shahid; Beck, Fabian, Univ Duisburg Essen, Paluno, Duisburg, Germany.", "countries": "Germany", "abstract": "Good code quality is a prerequisite for efficiently developing maintainable software. In this paper, we present a novel approach to generate exploranative (explanatory and exploratory) data-driven documents that report code quality in an interactive, exploratory environment. We employ a template-based natural language generation method to create textual explanations about the code quality, dependent on data from software metrics. The interactive document is enriched by different kinds of visualization, including parallel coordinates plots and scatterplots for data exploration and graphics embedded into text. We devise an interaction model that allows users to explore code quality with consistent linking between text and visualizations; through integrated explanatory text, users are taught background knowledge about code quality aspects. Our approach to interactive documents was developed in a design study process that included software engineering and visual analytics experts. Although the solution is specific to the software engineering scenario, we discuss how the concept could generalize to multivariate data and report lessons learned in a broader scope.", "keywords": "Code quality,interactive documents,natural language generation,sparklines", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934669", "refList": ["10.1109/tvcg.2014.2346435", "10.1109/tvcg.2018.2865022", "10.1016/j.jvlc.2018.10.001", "10.1002/smr.521", "10.1109/tvcg.2006.69", "10.1016/0004-3702(93)90022-4", "10.1145/3173574.3174106", "10.1007/s10648-010-9136-5", "10.3115/974557.974594", "10.1109/vl.1996.545307", "10.1109/tse.1976.233837", "10.1109/vissoft.2018.00010", "10.2312/vissym/vissym04/261-266", "10.1046/j.1365-2575.2002.00117.x", "10.1109/wcre.2002.1173068", "10.1109/icpc.2013.6613830", "10.1145/1985362.1985365", "10.3115/v1/w14-4401", "10.1007/s00766-007-0054-0", "10.1109/vissoft.2017.11", "10.1007/s10515-011-0098-8", "10.1109/vissof.2011", "10.1109/tvcg.2017.2674958", "10.1002/smr.404", "10.1109/32.979986", "10.1016/b978-0-12-397174-6.00010-6", "10.1145/3242587.3242617", "10.1109/mcg.2018.032421649", "10.1613/jair.5477", "10.1109/tfuzz.2014.2328011", "10.1145/2095654.2095665", "10.1109/icpc.2013.6613834", "10.1109/tvcg.2018.2865145", "10.1145/2597008.2597149", "10.1109/live.2013.6617345", "10.1145/1985793.1985868", "10.1002/int.21835", "10.1145/3139295.3139312", "10.1109/32.295895", "10.1109/scam.2014.14", "10.5281/zenodo.3336019", "10.1016/j.visinf.2019.03.004", "10.1109/aswec.2010.18", "10.2312/eurovisshort.20181084"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030403", "title": "Calliope: Automatic Visual Data Story Generation from a Spreadsheet", "year": "2020", "conferenceName": "InfoVis", "authors": "Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi 0007;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, Intelligent Big Data Visualizat Lab, Shanghai, Peoples R China. Shi, Danqing; Xu, Xinyue; Sun, Fuling; Shi, Yang; Cao, Nan, Tongji Univ, Intelligent Big Data Visualizat Lab, Shanghai, Peoples R China.", "countries": "China", "abstract": "Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.", "keywords": "Information Visualization,Visual Storytelling,Data Story", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030403", "refList": ["10.1007/s11063-017-9759-3", "10.1109/tvcg.2016.2598647", "10.1162/0891201054223977", "10.1109/tvcg.2015.2467732", "10.3390/info9030065", "10.1109/tvcg.2019.2934398", "10.1016/j.visinf.2018.12.001", "10.1145/2002353.2002355", "10.1109/tvcg.2007.70594", "10.1111/cgf.12392", "10.14778/2831360.2831371", "10.1093/biomet/33.3.239", "10.1109/mcg.2019.2924636", "10.1109/tvcg.2017.2659744", "10.1109/pacificvis.2009.4906837", "10.1109/pacificvis.2017.8031599", "10.4103/1755-6783.179101", "10.1145/2362394.2362398", "10.1111/cgf.12925", "10.1109/icde.2018.00019", "10.1109/tvcg.2018.2865240", "10.1109/mcg.2015.99", "10.1109/vds48975.2019.8973383", "10.1038/nature16961", "10.1109/tciaig.2012.2186810", "10.1145/3035918.3035922", "10.1145/3197517.3201362", "10.1109/tvcg.2019.2934281", "10.1109/tvcg.2016.2598620", "10.1155/2019/8480905", "10.1109/iccchina.2013.6671183", "10.1109/tvcg.2018.2865145", "10.1145/3299869.3314037", "10.1017/s1351324907004664", "10.1109/tvcg.2019.2934785", "10.1177/1473871618806555", "10.1613/jair.2989", "10.1109/tvcg.2010.179", "10.1145/3303766"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.1043", "year": "2020", "title": "AutoCaption: An Approach to Generate Natural Language Description from Visualization Automatically", "conferenceName": "PacificVis", "authors": "Can Liu;Liwenhan Xie;Yun Han;Datong Wei;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.\nYuan, XR (Corresponding Author), Peking Univ, Sch EECS, Beijing, Peoples R China.\nYuan, XR (Corresponding Author), Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China.\nLiu, Can; Xie, Liwenhan; Han, Yun; Wei, Datong; Yuan, Xiaoru, Peking Univ, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.\nLiu, Can; Xie, Liwenhan; Han, Yun; Wei, Datong; Yuan, Xiaoru, Peking Univ, Sch EECS, Beijing, Peoples R China.\nYuan, Xiaoru, Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China.", "countries": "China", "abstract": "In this paper, we propose a novel approach to generate captions for visualization charts automatically. In the proposed method, visual marks and visual channels, together with the associated text information in the original charts, are first extracted and identified with a multilayer perceptron classifier. Meanwhile, data information can also be retrieved by parsing visual marks with extracted mapping relationships. Then a 1-D convolutional residual network is employed to analyze the relationship between visual elements, and recognize significant features of the visualization charts, with both data and visual information as input. In the final step, the full description of the visual charts can be generated through a template-based approach. The generated captions can effectively cover the main visual features of the visual charts and support major feature types in commons charts. We further demonstrate the effectiveness of our approach through several cases.", "keywords": "", "link": "https://doi.org/10.1109/PacificVis48177.2020.1043", "refList": ["10.1145/1414471.1414525", "10.1111/cgf.13193", "10.1109/tvcg.2018.2865145", "10.1109/is.2002.1044219", "10.1145/2470654.2481374", "10.1080/13614568.2010.534186", "10.1017/s1351324997001502", "10.1145/3035918.3035922", "10.1109/cvpr.2016.90", "10.1145/2047196.2047247", "10.1007/s11257-006-9002-9", "10.1109/cvpr.2015.7298935", "10.1145/1148170.1148270", "10.1109/72.279181", "10.1109/icsmc.2011.6084067"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/vast47406.2019.8986909", "title": "Origraph: Interactive Network Wrangling", "year": "2019", "conferenceName": "VAST", "authors": "Alex Bigelow;Carolina Nobre;Miriah D. Meyer;Alexander Lex", "citationCount": "2", "affiliation": "Bigelow, A (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bigelow, Alex; Nobre, Carolina; Meyer, Miriah; Lex, Alexander, Univ Utah, Salt Lake City, UT 84112 USA.", "countries": "USA", "abstract": "Networks are a natural way of thinking about many datasets. The data on which a network is based, however, is rarely collected in a form that suits the analysis process, making it necessary to create and reshape networks. Data wrangling is widely acknowledged to be a critical part of the data analysis pipeline, yet interactive network wrangling has received little attention in the visualization research community. In this paper, we discuss a set of operations that are important for wrangling network datasets and introduce a visual data wrangling tool, Origraph, that enables analysts to apply these operations to their datasets. Key operations include creating a network from source data such as tables, reshaping a network by introducing new node or edge classes, filtering nodes or edges, and deriving new node or edge attributes. Our tool, Origraph, enables analysts to execute these operations with little to no programming, and to immediately visualize the results. Origraph provides views to investigate the network model, a sample of the network, and node and edge attributes. In addition, we introduce interfaces designed to aid analysts in specifying arguments for sensible network wrangling operations. We demonstrate the usefulness of Origraph in two Use Cases: first, we investigate gender bias in the film industry, and then the influence of money on the political support for the war in Yemen.", "keywords": "Graph visualization,Data abstraction,Data wrangling,Human-centered computing [Information visualization],[Human-centered computing]: Visualization systems and tools,Information systems [Graph-based database models]", "link": "http://dx.doi.org/10.1109/VAST47406.2019.8986909", "refList": ["10.1101/gr.1239303", "10.1145/1054972.1055032", "10.1016/b978-0-12-382229-1.00002-3", "10.1109/tvcg.2017.2744843", "10.1109/tvcg.2013.154", "10.1007/978-1-4614-7163-9315-1", "10.1073/pnas.1607151113", "10.18637/jss.v040.i01", "10.1145/1124772.1124891", "10.1177/1473871611415994", "10.1109/tvcg.2018.2865149", "10.1145/2598153.2598175", "10.1177/1473871613488591", "10.1109/tvcg.2018.2859973", "10.1186/1471-2105-14-s19-s3", "10.1056/nejmsa066082", "10.1007/978-3-642-36763-2\\_48", "10.1016/j.socscimed.2016.01.049", "10.1111/j.1467-8659.2008.01231.x", "10.1002/cne.24084", "10.2138/am-2017-6104ccbyncnd", "10.1109/tvcg.2014.2346248", "10.1111/j.1467-8659.2009.01710.x", "10.1007/978-3-642-03658-3\\_47", "10.1007/978-3-319-06793-3\\_5", "10.3390/genes9110519", "10.1145/3290605.3300356", "10.1111/cgf.12883", "10.1177/1473871612462152", "10.1016/j.jelectrocard.2010.09.003", "10.1111/cgf.13610", "10.1109/vast.2010.5652520", "10.1111/evo.13318", "10.1109/tvcg.2018.2811488", "10.1109/tvcg.2009.111", "10.1111/cgf.13184", "10.1109/tvcg.2009.116", "10.1109/biovis.2012.6378600"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030462", "title": "Table Scraps: An Actionable Framework for Multi-Table Data Wrangling From An Artifact Study of Computational Journalism", "year": "2020", "conferenceName": "InfoVis", "authors": "Stephen Kasica;Charles Berret;Tamara Munzner", "citationCount": "0", "affiliation": "Kasica, S (Corresponding Author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada. Kasica, Stephen; Munzner, Tamara, Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada. Berret, Charles, Univ British Columbia, Sch Journalism Writing \\& Media, Vancouver, BC, Canada.", "countries": "Canada", "abstract": "For the many journalists who use data and computation to report the news, data wrangling is an integral part of their work. Despite an abundance of literature on data wrangling in the context of enterprise data analysis, little is known about the specific operations, processes, and pain points journalists encounter while performing this tedious, time-consuming task. To better understand the needs of this user group, we conduct a technical observation study of 50 public repositories of data and analysis code authored by 33 professional journalists at 26 news organizations. We develop two detailed and cross-cutting taxonomies of data wrangling in computational journalism, for actions and for processes. We observe the extensive use of multiple tables, a notable gap in previous wrangling analyses. We develop a concise, actionable framework for general multi-table data wrangling that includes wrangling operations documented in our taxonomy that are without clear parallels in other work. This framework, the first to incorporate tables as first-class objects, will support future interactive wrangling tools for both computational journalism and general-purpose use. We assess the generative and descriptive power of our framework through discussion of its relationship to our set of taxonomies.", "keywords": "Computational journalism,Data journalism,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030462", "refList": ["10.1145/1378773.1378792", "10.1109/tvcg.2012.219", "10.1109/vast47406.2019.8986909", "10.1145/1084805.1084812", "10.1007/s00778-008-0098-x", "10.1016/j.websem.2008.09.005", "10.18637/jss.v040.i01", "10.1145/989863.989865", "10.1109/tvcg.2015.2467551", "10.5281/zenodo.3509134", "10.1109/tvcg.2019.2934539", "10.1109/tvcg.2019.2934593", "10.1109/tse.2018.2796554", "10.17349/jmc117309", "10.1109/2945.981851", "10.1109/vast.2011.6102440", "10.1177/1473871611415994", "10.1145/2001269.2001288"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2020.3030355", "title": "Guidelines For Pursuing and Revealing Data Abstractions", "year": "2020", "conferenceName": "InfoVis", "authors": "Alex Bigelow;Katy Williams;Katherine E. Isaacs", "citationCount": "0", "affiliation": "Bigelow, A (Corresponding Author), Univ Arizona, Tucson, AZ 85721 USA. Bigelow, Alex; Williams, Katy; Isaacs, Katherine E., Univ Arizona, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "Many data abstraction types, such as networks or set relationships, remain unfamiliar to data workers beyond the visualization research community. We conduct a survey and series of interviews about how people describe their data, either directly or indirectly. We refer to the latter as latent data abstractions. We conduct a Grounded Theory analysis that (1) interprets the extent to which latent data abstractions exist, (2) reveals the far-reaching effects that the interventionist pursuit of such abstractions can have on data workers, (3) describes why and when data workers may resist such explorations, and (4) suggests how to take advantage of opportunities and mitigate risks through transparency about visualization research perspectives and agendas. We then use the themes and codes discovered in the Grounded Theory analysis to develop guidelines for data abstraction in visualization projects. To continue the discussion, we make our dataset open along with a visual interface for further exploration.", "keywords": "Data abstraction,Grounded theory,Survey design,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030355", "refList": ["10.1080/2159676x.2016.1251701", "10.1109/infvis.2000.885092", "10.1145/2702123.2702298", "10.4135/9781848607941.n14", "10.1007/978-1-4939", "10.1109/tvcg.2014.2346331", "10.1109/tvcg.2017.2744843", "10.1177/1473871613510429", "10.1007/978-1-4939-0378-8\\_2", "10.1145/2598153.2598175", "10.1109/tvcg.2019.2934285", "10.1177/1473871613488591", "10.1145/2501105.2501106", "10.1109/tvcg.2019.2934538", "10.1109/tvcg.2019.2934539", "10.1017/s1049096510990781", "10.1145/3025453.3025837", "10.1145/3290605.3300474", "10.1145/3290605.3300356", "10.1002/nur.1025", "10.1145/2993901.2993916", "10.1145/3392826", "10.1086/269268", "10.1109/tvcg.2018.2865241", "10.1145/2998181.2998331", "10.1145/291224.291229", "10.1057/ivs.2009.13", "10.1145/2047196.2047205", "10.1109/tvcg.2012.213", "10.1145/3274405", "10.1109/tvcg.2013.145", "10.1016/0040-6031(92)85160-w", "10.1109/iv.2013.45", "10.1109/tvcg.2009.111", "10.1109/mcg.2019.2914844", "10.1109/tvcg.2009.116"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14035", "year": "2020", "title": "Survey on the Analysis of User Interactions and Visualization Provenance", "conferenceName": "EuroVis", "authors": "Kai Xu;Alvitta Ottley;Conny Walchshofer;Marc Streit;Remco Chang;John E. Wenskovitch", "citationCount": "0", "affiliation": "Xu, K (Corresponding Author), Middlesex Univ, London, England.\nXu, Kai, Middlesex Univ, London, England.\nOttley, Alvitta, Washington Univ, St Louis, MO 63110 USA.\nWalchshofer, Conny; Streit, Marc, Johannes Kepler Univ Linz, Linz, Austria.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.\nWenskovitch, John, Virginia Tech, Blacksburg, VA USA.", "countries": "USA;England;Austria", "abstract": "There is fast-growing literature on provenance-related research, covering aspects such as its theoretical framework, use cases, and techniques for capturing, visualizing, and analyzing provenance data. As a result, there is an increasing need to identify and taxonomize the existing scholarship. Such an organization of the research landscape will provide a complete picture of the current state of inquiry and identify knowledge gaps or possible avenues for further investigation. In this STAR, we aim to produce a comprehensive survey of work in the data visualization and visual analytics field that focus on the analysis of user interaction and provenance data. We structure our survey around three primary questions: (1) WHY analyze provenance data, (2) WHAT provenance data to encode and how to encode it, and (3) HOW to analyze provenance data. A concluding discussion provides evidence-based guidelines and highlights concrete opportunities for future development in this emerging area. The survey and papers discussed can be explored online interactively at https://provenance-survey.caleydo.org.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14035", "refList": ["10.1145/3186266", "10.1145/3185524", "10.1109/tvcg.2014.2346575", "10.1109/tvcg.2016.2598471", "10.1109/tvcg.2016.2598446", "10.1145/2856767.2856779", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2015.2467871", "10.1109/tvcg.2019.2934668", "10.1145/3301275.3302307", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1111/cgf.12895", "10.1145/1084805.1084812", "10.1145/2983923", "10.1007/978-1-4419-5874-7\\_12", "10.1109/mcg.2010.18", "10.1109/tvcg.2015.2467153", "10.1109/tvcg.2013.211", "10.1145/3172944.3172964", "10.1145/3290605.3300360", "10.1109/tvcg.2009.199", "10.1109/vast.2016.7883515", "10.1145/2207676.2208412", "10.1145/1979742.1979570", "10.1145/2207676.2208565", "10.1109/tvcg.2017.2745078", "10.1109/tvcg.2013.226", "10.1145/3301275.3302270", "10.1145/2882903.2882919", "10.1109/tvcg.2013.132", "10.1007/978-1-4614-3223-4\\_6", "10.1007/978-1-4899-7993-3\\_80747-1", "10.1145/2449396.2449439", "10.4230/dagrep.8.11.35", "10.1111/cgf.13424", "10.1109/tvcg.2015.2467613", "10.1109/mcse.2007.106", "10.1109/vast.2014.7042486", "10.1145/3126594.3126653", "10.1145/2591510", "10.1109/vast.2017.8585665", "10.1109/tvcg.2017.2744684", "10.1109/vast.2009.5333564", "10.1111/cgf.12631", "10.1145/2702123.2702262", "10.1111/cgf.13717", "10.2312/evs.20191181", "10.1111/cgf.12925", "10.1145/2702123.2702590", "10.1109/tvcg.2015.2467551", "10.1145/3025171.3025187", "10.1145/3316416.3316418", "10.1109/tvcg.2015.2468078", "10.1109/mcg.2014.73", "10.1109/tvcg.2017.2744479", "10.1109/tvcg.2018.2859969", "10.1109/tvcg.2014.2346321", "10.1109/tvcg.2007.70589", "10.1007/s13218-012-0167-6", "10.1111/cgf.13670", "10.1145/2807442.2807478", "10.1111/cgf.13715", "10.1109/tvcg.2012.23", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2013.164", "10.1109/vast.2008.4677365", "10.1145/3301275.3302291", "10.1109/tvcg.2012.260", "10.1109/tvcg.2010.177", "10.1109/tvcg.2018.2865024", "10.1109/mcg.2015.51", "10.1145/2240236.2240260", "10.1109/tvcg.2016.2599030.2", "10.1109/tvcg.2007.70515", "10.1109/tvcg.2012.175", "10.1109/mcg.2019.2941856", "10.1109/tvcg.2008.137", "10.1016/j.visinf.2018.09.003", "10.4304/jmm.9.5.635-643", "10.1109/tvcg.2017.2744843", "10.1111/cgf.13405", "10.1145/2633043", "10.1109/tvcg.2009.129", "10.1109/tvcg.2019.2934609", "10.1111/cgf.12924", "10.1145/2702123.2702376", "10.1109/vast.2017.8585669", "10.1145/1502650.1502695", "10.1111/cgf.13730", "10.1109/tvcg.2013.124", "10.1109/tvcg.2017.2744805", "10.1109/mcg.2009.49", "10.1109/vast.2015.7347625", "10.1145/3009973", "10.1145/2470654.2470723", "10.1109/vast.2016.7883520", "10.1109/vast.2014.7042492", "10.1145/2984511.2984588", "10.1111/cgf.12391", "10.1561/1900000006", "10.1007/s00778-017-0486-1", "10.1109/vast.2009.5333020", "10.1145/1926385.1926423", "10.1145/1057977.1057978", "10.1145/3290605.3300892", "10.1111/j.1467-8659.2011.01928.x", "10.1109/tvcg.2013.188", "10.1109/tvcg.2015.2467191", "10.1109/iccicct.2014.6993023", "10.1145/3290605.3300874", "10.1145/2557500.2557524", "10.1109/mcg.2015.91", "10.1109/vast.2012.6400494", "10.1109/tvcg.2013.220", "10.1109/mcg.2019.2945378", "10.1109/vast.2012.6400486", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/1142473.1142574", "10.1177/1555343416672782", "10.1109/vast.2011.6102449", "10.1111/cgf.12090", "10.1109/vast.2016.7883518", "10.1111/cgf.13678", "10.1109/mcg.2009.53", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2016.2598797", "10.1111/cgf.13400", "10.1109/tvcg.2014.2346573", "10.1080/01431160600746456", "10.1145/2642918.2647378", "10.1109/mcg.2019.2945720", "10.1145/2207676.2207741", "10.1145/3025171.3025189", "10.1145/634067.634292", "10.1109/tvcg.2015.2467611", "10.1109/tit.1982.1056489", "10.1109/tvcg.2018.2865117", "10.1109/vast.2009.5333023", "10.1145/3332165.3347866", "10.1109/mcg.2019.2933419", "10.1145/3184900", "10.1109/tvcg.2012.273", "10.1109/vast.2010.5652885", "10.1109/vast.2015.7347627", "10.1145/3290605.3300803", "10.1109/tvcg.2012.258", "10.1109/mcg.2009.87", "10.1109/tvcg.2019.2934556", "10.1145/1869397.1869399", "10.1109/mcg.2015.50", "10.1145/3172944.3172979", "10.1111/cgf.13208", "10.1111/cgf.12619", "10.1145/3290605.3300358", "10.1109/vast.2008.4677352", "10.1109/tvcg.2016.2598468", "10.1109/vast.2016.7883519", "10.1109/mcse.2008.79"], "wos": 1, "children": [], "len": 1}], "len": 21}, "index": 697, "embedding": [3.497415065765381, 1.7092000246047974, -1.059349536895752, -0.5460559129714966, -0.6495316028594971, 0.16650904715061188, -0.7182635068893433, 1.0277044773101807, -0.4013857841491699, 1.1943981647491455, 0.551750898361206, 1.2426055669784546, -0.1336275339126587, 2.372398614883423, -0.2813386917114258, 3.9789631366729736, 1.5684905052185059, 4.302638530731201, -0.659038782119751, 4.146782398223877, -0.06630208343267441, 1.5773236751556396, 1.7472753524780273, 2.7760748863220215, -2.532860279083252, 2.1522107124328613, -0.5399249196052551, 1.630368709564209, 2.1785552501678467, 0.2577931582927704, 1.0693069696426392, 2.2143595218658447], "projection": [-0.06840071827173233, 10.054349899291992], "size": 11, "height": 3, "width": 5}