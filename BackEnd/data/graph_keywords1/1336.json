{"data": {"doi": "10.1111/cgf.12891", "year": "2016", "title": "Visualizing Co-occurrence of Events in Populations of Viral Genome Sequences", "conferenceName": "EuroVis", "authors": "Alper Sarikaya;M. Correli;Joao M. Dinis;David H. O'Connor;Michael Gleicher", "citationCount": "3", "affiliation": "Sarikaya, A (Corresponding Author), Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.\nSarikaya, A.; Gleicher, M., Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.\nCorrell, M., Univ Washington, Dept Comp Sci \\& Engn, Seattle, WA 98195 USA.\nDinis, J. M., Univ Wisconsin, Dept Pathobiol Sci, Madison, WI 53706 USA.\nO'Connor, D. H., Univ Wisconsin, Dept Pathol \\& Lab Med, Madison, WI USA.\nO'Connor, D. H., Wisconsin Natl Primate Res Ctr, Madison, WI USA.", "countries": "USA", "abstract": "Virologists are not only interested in point mutations in a genome, but also in relationships between mutations. In this work, we present a design study to support the discovery of correlated mutation events (called co-occurrences) in populations of viral genomes. The key challenge is to identify potentially interesting pairs of events within the vast space of event combinations. In our work, we identify analyst requirements and develop a prototype through a participatory process. The key ideas of our approach are to use interest metrics to create dynamic filtering that guides the viewer to interesting and relevant correlations of genome mutations, and to provide visual encodings designed to fit scientists' mental map of the data, along with dynamic filtering techniques. We demonstrate the strength of our approach in virology-situated case studies, and offer suggestions for extending our strategy to other sequence-based domains.", "keywords": "", "link": "https://doi.org/10.1111/cgf.12891", "refList": ["10.1038/nature10831", "10.1109/tvcg.2013.214", "10.1038/nature07423", "10.2307/2683294", "10.1128/jvi.05511-11", "10.1109/tvcg.2011.185", "10.1038/nmeth.1422", "10.1093/bioinformatics/18.suppl\\_2.s231", "10.1109/tvcg.2009.175", "10.1093/bioinformatics/btq332", "10.1109/biovis.2013.6664342", "10.1093/bioinformatics/btm293", "10.1101/gr.229102", "10.1109/infvis.2005.1532139", "10.1109/tvcg.2014.2346682", "10.1038/ncomms3636", "10.1093/bioinformatics/bts199", "10.1109/tvcg.2015.2467911", "10.1145/2470654.2466444", "10.1109/tvcg.2009.191", "10.1093/bioinformatics/btp352", "10.1109/infvis.2004.1", "10.1101/gr.134635.111", "10.1109/ices.2013.6613276", "10.1093/bioinformatics/btv407", "10.1109/tvcg.2006.160", "10.1109/tvcg.2012.213", "10.1128/jvi.00694-10", "10.1109/tvcg.2010.162", "10.1145/568522.568523", "10.1559/152304003100010929", "10.1038/nbt.1754", "10.1109/biovis.2013.6664340"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2017.2744199", "title": "Considerations for Visualizing Comparison", "year": "2017", "conferenceName": "InfoVis", "authors": "Michael Gleicher", "citationCount": "27", "affiliation": "Gleicher, M (Corresponding Author), Univ Wisconsin, Madison, WI 53706 USA. Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Supporting comparison is a common and diverse challenge in visualization. Such support is difficult to design because solutions must address both the specifics of their scenario as well as the general issues of comparison. This paper aids designers by providing a strategy for considering those general issues. It presents four considerations that abstract comparison. These considerations identify issues and categorize solutions in a domain independent manner. The first considers how the common elements of comparison-a target set of items that are related and an action the user wants to perform on that relationship-are present in an analysis problem. The second considers why these elements lead to challenges because of their scale, in number of items, complexity of items, or complexity of relationship. The third considers what strategies address the identified scaling challenges, grouping solutions into three broad categories. The fourth considers which visual designs map to these strategies to provide solutions for a comparison analysis problem. In sequence, these considerations provide a process for developers to consider support for comparison in the design of visualization tools. Case studies show how these considerations can help in the design and evaluation of visualization solutions for comparison problems.", "keywords": "Information Visualization,Comparison,Taxonomies,Visualization Models,Task Analysis", "link": "http://dx.doi.org/10.1109/TVCG.2017.2744199", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2007.70615", "10.1109/tvcg.2009.167", "10.1109/tvcg.2014.2346578", "10.1109/tvcg.2008.165", "10.1109/tvcg.2015.2467204", "10.1111/cgf.12380", "10.1109/biovis.2011.6094058", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1111/j.1467-8659.2011.01959.x", "10.1109/tvcg.2009.128", "10.1109/iv.2011.49", "10.1109/tvcg.2012.237", "10.1109/pacificvis.2013.6596128", "10.1109/tvcg.2013.213", "10.1109/visual.1990.146375", "10.1111/cgf.12891", "10.1109/cse.2011.100", "10.1089/big.2016.0007", "10.1109/cmv.2007.20", "10.1109/tvcg.2013.122", "10.1037/0096-1523.4.1.1", "10.1109/mcg.2010.100", "10.1109/tvcg.2013.124", "10.1109/tvcg.2015.2413774", "10.1145/2783258.2783311", "10.1037/0096-1523.24.3.719", "10.1109/mcg.2006.88", "10.1109/iv.2009.108", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2004.39", "10.1109/iv.2013.55", "10.1214/ss/1177012100", "10.1111/cgf.12375", "10.1109/tvcg.2011.194", "10.1167/16.5.11", "10.1109/tvcg.2006.147", "10.1111/j.1467-8659.2009.01441.x", "10.2307/2288400", "10.1109/tvcg.2015.2467618", "10.1109/vast.2014.7042503", "10.1068/p3318", "10.1145/1133265.1133319", "10.1111/cgf.12918", "10.1109/tvcg.2014.2346426", "10.1145/2470654.2470724", "10.1093/bioinformatics/btl193", "10.1093/bioinformatics/btv407", "10.1109/pacificvis.2012.6183556", "10.1111/j.1467-8659.2012.03114.x", "10.1145/882262.882291", "10.1109/tvcg.2012.284", "10.1109/tvcg.2011.232", "10.1109/tvcg.2014.2346298", "10.1109/tvcg.2010.78", "10.1093/oxfordhb/9780195376746.013.0010", "10.1057/ivs.2009.29", "10.1109/vast.2015.7347634", "10.1109/tvcg.2010.177", "10.1109/tvcg.2010.162", "10.1109/vast.2014.7042491", "10.1111/j.1467-8659.2012.03112.x", "10.1109/tvcg.2013.183", "10.1037/0033-2909.98.3.419", "10.1109/tvcg.2009.111", "10.1111/cgf.12653", "10.1146/annurev.psych.53.100901.135125", "10.1109/tvcg.2013.120", "10.1037/0003-066x.60.2.170", "10.1109/vast.2012.6400486", "10.1016/j.ijhcs.2013.08.004"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2864526", "title": "Duet: Helping Data Analysis Novices Conduct Pairwise Comparisons by Minimal Specification", "year": "2018", "conferenceName": "VAST", "authors": "Po-Ming Law;Rahul C. Basole;Yanhong Wu", "citationCount": "6", "affiliation": "Law, PM (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA. Law, Po-Ming; Basole, Rahul C., Georgia Inst Technol, Atlanta, GA 30332 USA. Wu, Yanhong, Visa Res, Palo Alto, CA USA.", "countries": "USA", "abstract": "Data analysis novices often encounter barriers in executing low-level operations for pairwise comparisons. They may also run into barriers in interpreting the artifacts (e.g., visualizations) created as a result of the operations. We developed Duet, a visual analysis system designed to help data analysis novices conduct pairwise comparisons by addressing execution and interpretation barriers. To reduce the barriers in executing low-level operations during pairwise comparison, Duet employs minimal specification: when one object group (i.e. a group of records in a data table) is specified, Duet recommends object groups that are similar to or different from the specified one; when two object groups are specified, Duet recommends similar and different attributes between them. To lower the barriers in interpreting its recommendations, Duet explains the recommended groups and attributes using both visualizations and textual descriptions. We conducted a qualitative evaluation with eight participants to understand the effectiveness of Duet. The results suggest that minimal specification is easy to use and Duet's explanations are helpful for interpreting the recommendations despite some usability issues.", "keywords": "Pairwise comparison,novices,data analysis,automatic insight generation", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864526", "refList": ["10.1145/506443.506619", "10.1145/2501988.2502040", "10.1145/1842993.1843029", "10.1057/ivs.2008.27", "10.1177/0272989x10373805", "10.1109/tvcg.2015.2467195", "10.14778/2831360.2831371", "10.1145/3025171.3025227", "10.1145/1502650.1502695", "10.1145/2678025.2701399", "10.1145/2207676.2207738", "10.1109/tvcg.2017.2744684", "10.1007/978-3-540-24670-1\\_3", "10.2307/3001913", "10.1109/tvcg.2017.2745219", "10.1109/tvcg.2017.2744199", "10.1109/cvpr.2000.854761", "10.1145/2984511.2984588", "10.1145/2807442.2807459", "10.1145/3035918.3035922", "10.1109/tvcg.2011.188", "10.1145/2807442.2807478", "10.1109/tvcg.2010.164", "10.1109/vast.2011.6102435", "10.1145/2213836", "10.1109/tpami.2003.1195991", "10.1109/tvcg.2015.2467191", "10.1145/985692.985712", "10.1109/tvcg.2005.63", "10.1109/tvcg.2016.2598468", "10.1145/3025453.3025777"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934264", "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;David S. Ebert;Arif Ghafoor", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Karimzadeh, Morteza; Ebert, David S.; Ghafoor, Arif, Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.", "keywords": "Summative evaluation,usefulness,evaluation process,taxonomy,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934264", "refList": ["10.1109/tvcg.2017.2744478", "10.1109/tvcg.2018.2865025", "10.1109/tvcg.2006.85", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2017.2745181", "10.1111/cgf.13677", "10.1109/tvcg.2018.2864844", "10.1109/tvcg.2013.126", "10.1109/tvcg.2018.2864811", "10.1109/infvis.2005.1532147", "10.1177/0956797613504966", "10.1145/2669557.2669579", "10.1109/mcg.2005.102", "10.1109/visual.2003.1250426", "10.1136/bmj.39489.470347.ad", "10.1109/tvcg.2017.2744080", "10.1109/mcg.2009.53", "10.1111/j.1467-8527.2005.00307.x", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2864886", "10.1109/tvcg.2018.2864843", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2018.2865051", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2018.2865026", "10.1007/978-3-540-71080-6\\_6", "10.1109/tvcg.2018.2865020", "10.1177/1473871611407399", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2018.2864526", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2864905", "10.1049/sej.1991.0040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2711030", "10.1109/tvcg.2011.279", "10.1109/vast.2017.8585505", "10.1147/jrd.2010.2042914", "10.1016/s0378-7206(98)00044-5", "10.1145/2993901.2993913", "10.1109/tvcg.2018.2865041", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744758", "10.1145/1168149.1168158", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2012.213", "10.1109/tvcg.2017.2744738", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2018.2864826", "10.1145/1377966.1377974", "10.1109/apec.2009.4802646", "10.1145/1168149.1168152", "10.1016/j.jss.2008.03.059", "10.1109/vast.2017.8585484", "10.1109/tvcg.2017.2744818", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2865042", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030388", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "year": "2020", "conferenceName": "SciVis", "authors": "Pepe Eulzer;Sabine Bauer;Francis Kilian;Kai Lawonn", "citationCount": "0", "affiliation": "Eulzer, P (Corresponding Author), Univ Jena, Jena, Germany. Eulzer, Pepe; Lawonn, Kai, Univ Jena, Jena, Germany. Bauer, Sabine, Univ Koblenz Landau, Koblenz, Germany. Kilian, Francis, Cath Clin Koblenz Montabaur, Dept Spine Surg, Koblenz, Germany.", "countries": "Germany", "abstract": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. $\\mathrm{By}$ linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. $\\mathrm{in}$ a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.", "keywords": "Medical visualization,bioinformatics,coordinated views,focus and context,biomechanical simulation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030388", "refList": ["10.1109/tvcg.2018.2864903", "10.1177/1473871613510429", "10.1093/ehjqcco/qcz052", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2018.2865076", "10.1055/s-0039-1687862", "10.1109/visual.1990.146375", "10.1109/tvcg.2017.2744198", "10.1016/j.ijmedinf.2014.10.001", "10.1109/tvcg.2013.124", "10.1016/j.jacc", "10.1111/cgf.13167", "10.17705/1thci.00055", "10.1136/bmjqs.2009.037895", "10.1109/tvcg.2013.238", "10.1109/tvcg.2018.2865240", "10.1186/1471-2261-6-34", "10.1109/tvcg.2019.2934264", "10.1109/tvcg.2013.200", "10.1109/tvcg.2011.209", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467091", "10.1136/bmjopen-2019-033208", "10.1109/beliv.2018.8634027", "10.1109/tvcg.2012.213", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2015.2467325", "10.1145/2133806.2133821", "10.1145/1806799.1806866", "10.1108/02635570610688869", "10.1002/hbm.20701", "10.1561/1100000039", "10.1145/3025453.3025645", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2020.3029413", "title": "A Design Space of Vision Science Methods for Visualization Research", "year": "2020", "conferenceName": "InfoVis", "authors": "Madison A. Elliott;Christine Nothelfer;Cindy Xiong;Danielle Albers Szafir", "citationCount": "0", "affiliation": "Elliott, MA (Corresponding Author), Univ British Columbia, Vancouver, BC, Canada. Elliott, Madison A., Univ British Columbia, Vancouver, BC, Canada. Nothelfer, Christine, Northwestern Univ, Evanston, IL 60208 USA. Xiong, Cindy, Univ Massachusetts, Amherst, MA 01003 USA. Szafir, Danielle Albers, Univ Colorado, Boulder, CO 80309 USA.", "countries": "Canada;USA", "abstract": "A growing number of efforts aim to understand what people see when using a visualization. These efforts provide scientific grounding to complement design intuitions, leading to more effective visualization practice. However, published visualization research currently reflects a limited set of available methods for understanding how people process visualized data. Alternative methods from vision science offer a rich suite of tools for understanding visualizations, but no curated collection of these methods exists in either perception or visualization research. We introduce a design space of experimental methods for empirically investigating the perceptual processes involved with viewing data visualizations to ultimately inform visualization design guidelines. This paper provides a shared lexicon for facilitating experimental visualization research. We discuss popular experimental paradigms, adjustment types, response types, and dependent measures used in vision science research, rooting each in visualization examples. We then discuss the advantages and limitations of each technique. Researchers can use this design space to create innovative studies and progress scientific understanding of design choices and evaluations in visualization. We highlight a history of collaborative success between visualization and vision science research and advocate for a deeper relationship between the two fields that can elaborate on and extend the methodological design space for understanding visualization and vision.", "keywords": "Perception,human vision,empirical research,evaluation,HCI", "link": "http://dx.doi.org/10.1109/TVCG.2020.3029413", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1177/0886109909354981", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1093/bioinformatics/btq110", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1177/1744987107081254", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1177/174498710501000305", "10.1017/s1049096513001789", "10.1109/tvcg.2012.213", "10.1093/nar/gkz239", "10.1093/sysbio/sys062", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030405", "title": "Insights From Experiments With Rigor in an EvoBio Design Study", "year": "2020", "conferenceName": "InfoVis", "authors": "Jennifer Rogers;Austin H. Patton;Luke Harmon;Alexander Lex;Miriah D. Meyer", "citationCount": "0", "affiliation": "Rogers, J (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Rogers, Jen; Lex, Alexander; Meyer, Miriah, Univ Utah, Salt Lake City, UT 84112 USA. Patton, Austin H., Washington State Univ, Pullman, WA 99164 USA. Harmon, Luke, Univ Idaho, Moscow, ID 83843 USA.", "countries": "USA", "abstract": "Design study is an established approach of conducting problem-driven visualization research. The academic visualization community has produced a large body of work for reporting on design studies, informed by a handful of theoretical frameworks, and applied to a broad range of application areas. The result is an abundance of reported insights into visualization design, with an emphasis on novel visualization techniques and systems as the primary contribution of these studies. In recent work we proposed a new, interpretivist perspective on design study and six companion criteria for rigor that highlight the opportunities for researchers to contribute knowledge that extends beyond visualization idioms and software. In this work we conducted a year-long collaboration with evolutionary biologists to develop an interactive tool for visual exploration of multivariate datasets and phylogenetic trees. During this design study we experimented with methods to support three of the rigor criteria: ABUNDANT, REFLEXIVE, and TRANSPARENT. As a result we contribute two novel visualization techniques for the analysis of multivariate phylogenetic datasets, three methodological recommendations for conducting design studies drawn from reflections over our process of experimentation, and two writing devices for reporting interpretivist design study. We offer this work as an example for implementing the rigor criteria to produce a diverse range of knowledge contributions.", "keywords": "Methodologies,Application Motivated Visualization,Guidelines,Life Sciences Visualization,Health,Medicine,Biology,Bioinformatics,Genomics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030405", "refList": ["10.1109/tvcg.2019.2934790", "10.1177/146879410200200205", "10.2312/eurovisshort", "10.1109/tvcg.2015.2467811", "10.1111/j.1467-6486.2009.00859.x", "10.1177/1473871613510429", "10.1109/tvcg.2018.2864836", "10.1109/tvcg.2018.2865076", "10.1109/tvcg.2013.231", "10.1002/cphy.c100079", "10.1109/tvcg.2018.2865149", "10.1080/1750984x.2017.1317357", "10.1007/978-3-7643-8472-2\\_6", "10.1111/cgf.13728", "10.2307/1511837", "10.1109/tvcg.2019.2934539", "10.3233/efi-2004-22201", "10.1080/17493460802276893", "10.1002/chp.1340180402", "10.1111/2041-210x.12034", "10.1111/j.2041-210x.2011.00169.x", "10.1109/tvcg.2015.2467452", "10.1002/chp", "10.1177/1077800410383121", "10.1002/cpbi.96", "10.1109/tvcg.2018.2864526", "10.1109/beliv.2018.8634026", "10.2307/1511637", "10.1109/tvcg.2014.2346431", "10.1111/cgf.12883", "10.1109/tvcg.2019.2934281", "10.1145/2993901.2993916", "10.2307/3178066", "10.1145/2993901.2993913", "10.1145/1182475.1182476", "10.1016/j.destud.2004.06.002", "10.1177/1609406918763214", "10.2312/eurovisshort.20151137", "10.1145/882262.882291", "10.1109/tvcg.2012.213", "10.1109/tvcg.2019.2898186", "10.1109/tvcg.2018.2811488", "10.1007/s11135-006-9044-4", "10.1109/tvcg.2009.111", "10.1111/2041-210x.12066", "10.1109/mcg.2018.2874523", "10.1177/1609406920909938"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.1028", "year": "2020", "title": "A Radial Visualisation for Model Comparison and Feature Identification", "conferenceName": "PacificVis", "authors": "Jianlong Zhou;Weidong Huang;Fang Chen", "citationCount": "0", "affiliation": "Zhou, JL (Corresponding Author), Univ Technol Sydney, Data Sci Inst, Sydney, NSW, Australia.\nZhou, Jianlong; Chen, Fang, Univ Technol Sydney, Data Sci Inst, Sydney, NSW, Australia.\nHuang, Weidong, Univ Technol Sydney, Fac Transdisciplinary Innovat, Sydney, NSW, Australia.", "countries": "Australia", "abstract": "Machine Learning (ML) plays a key role in various intelligent systems, and building an effective ML model for a data set is a difficult task involving various steps including data cleaning, feature definition and extraction, ML algorithms development, model training and evaluation as well as others. One of the most important steps in the process is to compare generated substantial amounts of ML models to find the optimal one for the deployment. It is challenging to compare such models with dynamic number of features. This paper proposes a novel visualisation approach based on a radial net to compare ML models trained with a different number of features of a given data set while revealing implicit dependent relations. In the proposed approach, ML models and features are represented by lines and arcs respectively. The dependence of ML models with dynamic number of features is encoded into the structure of visualisation, where ML models and their dependent features are directly revealed from related line connections. ML model performance information is encoded with colour and line width in the innovative visualisation. Together with the structure of visualization, feature importance can be directly discerned to help to understand ML models.", "keywords": "Machine learning; performance; comparison; visualisation", "link": "https://doi.org/10.1109/PacificVis48177.2020.1028", "refList": ["10.1109/tvcg.2012.215", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2018.2864526", "10.1016/j.neucom.2011.10.038", "10.1063/1.3122936", "10.1145/2687924", "10.1109/tvcg.2009.23", "10.1016/j.compmedimag.2007.10.007", "10.1007/978-3-319-90403-0", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2018.2864499", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}], "len": 13}, {"doi": "10.1109/tvcg.2018.2864769", "title": "Visual Analytics for Topic Model Optimization based on User-Steerable Speculative Execution", "year": "2018", "conferenceName": "VAST", "authors": "Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel A. Keim;Christopher Collins", "citationCount": "7", "affiliation": "El-Assady, M (Corresponding Author), Univ Konstanz, Constance, Germany. El-Assady, M (Corresponding Author), Univ Ontario Inst Technol, Oshawa, ON, Canada. El-Assady, Mennatallah; Sperrle, Fabian; Deussen, Oliver; Keim, Daniel, Univ Konstanz, Constance, Germany. El-Assady, Mennatallah; Collins, Christopher, Univ Ontario Inst Technol, Oshawa, ON, Canada.", "countries": "Canada;Germany", "abstract": "To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision-making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-Ioop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.", "keywords": "User-Steerable Topic Modeling,Speculative Execution,Mixed-Initiative Visual Analytics,Explainable Machine Learning", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864769", "refList": ["10.1109/vast.2014.7042493", "10.1145/2133806.2133826", "10.1007/bf00114265", "10.1109/tvcg.2014.2346578", "10.5555/2145432.2145462", "10.1145/775047.775110", "10.1016/j.csda.2008.01.011", "10.1109/tvcg.2017.2743959", "10.1109/tkde.2004.58", "10.1109/tvcg.2016.2598445", "10.1007/978-1-4614-3223-4\\_4", "10.1109/tvcg.2013.231", "10.1016/j.ijhcs.2017.03.007", "10.1111/cgf.12924", "10.1007/978-3-540-70956-5", "10.1007/s10994-013-5413-0", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2013.232", "10.1145/219717.219748", "10.1109/tvcg.2013.212", "10.1145/2207676.2207738", "10.1145/1835804.1835827", "10.1145/1189769.1189779", "10.1186/s12859-015-0564-6", "10.1145/1143844.1143859", "10.1109/tvcg.2013.162", "10.1109/tvcg.2017.2745080", "10.1109/tvcg.2017.2744199", "10.1198/016214506000000302", "10.1007/978-3-319-09259-1\\_7", "10.1145/860435.860485", "10.1109/caia.1988.196114", "10.1145/2212776.2223772", "10.1145/1667053.1667056", "10.3115/1690219.1690287", "10.1145/882262.882291", "10.1162/jmlr.2003.3.4-5.993", "10.1145/2678025.2701370", "10.1057/palgrave.ivs.9500157", "10.1109/vast.2011.6102461", "10.1016/j.visinf.2017.01.005", "10.1177/0165551515617393", "10.1145/1273496.1273576", "10.1007/978-1-4614-3223-4", "10.1016/0004-3702(89)90046-5", "10.2312/eurovisstar.20151113", "10.1016/j.nima.2010.11.062"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934654", "title": "Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections", "year": "2019", "conferenceName": "VAST", "authors": "Mennatallah El-Assady;Rebecca Kehlbeck;Christopher Collins;Daniel A. Keim;Oliver Deussen", "citationCount": "3", "affiliation": "El-Assady, M (Corresponding Author), Univ Konstanz, Constance, Germany. El-Assady, M (Corresponding Author), Ontario Tech Univ, Oshawa, ON, Canada. El-Assady, Mennatallah; Kehlbeck, Rebecca; Keim, Daniel; Deussen, Oliver, Univ Konstanz, Constance, Germany. El-Assady, Mennatallah; Collins, Christopher, Ontario Tech Univ, Oshawa, ON, Canada.", "countries": "Canada;Germany", "abstract": "We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decision-making process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.", "keywords": "Topic Model Optimization,Word Embedding,Mixed-Initiative Refinement,Guided Visual Analytics,Semantic Mapping", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934654", "refList": ["10.1109/vast.2014.7042493", "10.1145/2133806.2133826", "10.1016/j.visinf.2018.09.003", "10.1007/978-3-319-67008-9\\_26", "10.1109/tvcg.2013.126", "10.1162/tacl\\_a\\_00140", "10.1007/s13202-018-0509-5", "10.1109/bdva.2018.8534018", "10.1108/eb026526", "10.1007/s10994-013-5413-0", "10.1145/564376.564421", "10.1016/j.visinf.2017.01.006", "10.3115/v1/p14-2050", "10.1007/bf00288933", "10.1109/tvcg.2013.212", "10.1145/2207676.2207741", "10.1109/tvcg.2013.162", "10.1109/tvcg.2016.2515592", "10.1109/tvcg.2017.2745080", "10.1109/mcg.2013.53", "10.1109/tvcg.2017.2744199", "10.1145/3091108", "10.18653/v1/p17-4009", "10.1162/jmlr.2003.3.4-5.951", "10.1109/vast.2017.8585498", "10.1109/tvcg.2017.2723397", "10.1109/tvcg.2018.2864769", "10.1007/s10618-005-0361-3", "10.3115/v1/d14-1167", "10.1007/bf01840357", "10.1162/jmlr.2003.3.4-5.993", "10.1145/2678025.2701370", "10.1016/j.ins.2016.06.040", "10.1109/tvcg.2017.2746018", "10.1109/vast.2011.6102461", "10.1111/cgf.13092", "10.3115/1117729.1117730", "10.1109/mcg.2015.91", "10.1145/2669557.2669572"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/vast47406.2019.8986922", "title": "TopicSifter: Interactive Search Space Reduction through Targeted Topic Modeling", "year": "2019", "conferenceName": "VAST", "authors": "Hannah Kim;Dongjin Choi;Barry L. Drake;Alex Endert;Haesun Park", "citationCount": "0", "affiliation": "Kim, H (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA. Kim, Hannah; Choi, Dongjin; Endert, Alex; Park, Haesun, Georgia Inst Technol, Atlanta, GA 30332 USA. Drake, Barry, Georgia Tech Res Inst, Atlanta, GA 30332 USA.", "countries": "USA", "abstract": "Topic modeling is commonly used to analyze and understand large document collections. However, in practice, users want to focus on specific aspects or \u201ctargets\u201d rather than the entire corpus. For example, given a large collection of documents, users may want only a smaller subset which more closely aligns with their interests, tasks, and domains. In particular, our paper focuses on large-scale document retrieval with high recall where any missed relevant documents can be critical. A simple keyword matching search is generally not effective nor efficient as 1) it is difficult to find a list of keyword queries that can cover the documents of interest before exploring the dataset, 2) some documents may not contain the exact keywords of interest but may still be highly relevant, and 3) some words have multiple meanings, which would result in irrelevant documents included in the retrieved subset. In this paper, we present TopicSifter, a visual analytics system for interactive search space reduction. Our system utilizes targeted topic modeling based on nonnegative matrix factorization and allows users to give relevance feedback in order to refine their target and guide the topic modeling to the most relevant results.", "keywords": "Human-centered computing,Visualization,Visualization application domains,Visual analytics,Information systems,Information retrieval,Users and interactive retrieval,Search interfaces", "link": "http://dx.doi.org/10.1109/VAST47406.2019.8986922", "refList": ["10.1145/2505515.2505644", "10.1007/s10898-014-0247-2", "10.1017/s1351324909005129", "10.1145/3070616", "10.1145/1871437.1871535", "10.1109/wi.2005.158", "10.1109/tvcg.2016.2598445", "10.1109/mcg.2004.22", "10.1111/j.1467-8659.2012.03108.x", "10.1145/3132847.3132968", "10.1017/s0269888903000638", "10.1109/tvcg.2009.176", "10.1109/wi.2006.6", "10.1109/tvcg.2006.142", "10.1109/infvis.2001.963287", "10.1109/tvcg.2013.212", "10.1109/tvcg.2013.242", "10.1145/2939672.2939743", "10.1177/1473871618757228", "10.1007/s10898-017-0515-z", "10.1007/978-3-319-60585-2\\_16", "10.1145/2656334", "10.1145/3178876.3186069", "10.1162/153244303322533223", "10.1007/s00799-004-0111-y", "10.1109/tvcg.2018.2864769", "10.1057/palgrave.ivs.9500023", "10.1145/2600428.2609618", "10.1016/j.cag.2016.12.004", "10.1162/jmlr.2003.3.4-5.993", "10.1145/2678025.2701370", "10.1109/tvcg.2012.260", "10.3115/v1/d14-1162", "10.1109/tvcg.2010.154", "10.1108/eb046814", "10.1109/tvcg.2014.2346433"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/vast47406.2019.8986917", "title": "VIANA: Visual Interactive Annotation of Argumentation", "year": "2019", "conferenceName": "VAST", "authors": "Fabian Sperrle;Rita Sevastjanova;Rebecca Kehlbeck;Mennatallah El-Assady", "citationCount": "0", "affiliation": "Sperrle, F (Corresponding Author), Univ Konstanz, Constance, Germany. Sperrle, Fabian; Sevastjanova, Rita; Kehlbeck, Rebecca; El-Assady, Mennatallah, Univ Konstanz, Constance, Germany.", "countries": "Germany", "abstract": "Argumentation Mining addresses the challenging tasks of identifying boundaries of argumentative text fragments and extracting their relationships. Fully automated solutions do not reach satisfactory accuracy due to their insufficient incorporation of semantics and domain knowledge. Therefore, experts currently rely on time-consuming manual annotations. In this paper, we present a visual analytics system that augments the manual annotation process by automatically suggesting which text fragments to annotate next. The accuracy of those suggestions is improved over time by incorporating linguistic knowledge and language modeling to learn a measure of argument similarity from user interactions. Based on a long-term collaboration with domain experts, we identify and model five high-level analysis tasks. We enable close reading and note-taking, annotation of arguments, argument reconstruction, extraction of argument relations, and exploration of argument graphs. To avoid context switches, we transition between all views through seamless morphing, visually anchoring all text- and graph-based layers. We evaluate our system with a two-stage expert user study based on a corpus of presidential debates. The results show that experts prefer our system over existing solutions due to the speedup provided by the automatic suggestions and the tight integration between text and graph views.", "keywords": "Argumentation annotation,machine learning,user interaction,layered interfaces,semantic transitions", "link": "http://dx.doi.org/10.1109/VAST47406.2019.8986917", "refList": ["10.1007/978-3-642-40624-9\\_1", "10.3233/978-1-61499-436-7-185", "10.1145/371920.372071", "10.3233/978-1-61499-906-5-4", "10.1109/tvcg.2015.2467759", "10.18653/v1/d15-1050", "10.2307/2529310", "10.1109/mic.2003.1167344", "10.1145/312624.312682", "10.1145/2850417", "10.18653/v1/p17-2039", "10.1016/j.eswa.2016.02.013", "10.1007/978-0-387-85820-3\\_3", "10.1007/s11412-009-9080-x", "10.1109/tvcg.2008.127", "10.1145/2207676.2207741", "10.1109/cit.2012.217", "10.3233/aac-170022", "10.1109/tvcg.2017.2745080", "10.1007/978-3-319-44039-2\\_6", "10.1145/3290605.3300233", "10.1007/978-94-017-0431-1", "10.3233/978-1-61499-906-5-313", "10.1142/s0218213004001922", "10.1109/tvcg.2012.262", "10.1177/001316446002000104", "10.1109/tvcg.2018.2834341", "10.3233/978-1-61499-436-7-463", "10.1145/1772690.1772773", "10.1109/tvcg.2014.2346677", "10.1145/2523813", "10.1162/153244303322533223", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2015.2467531", "10.1109/tvcg.2018.2864769", "10.1007/978-3-319-90092-6\\_14", "10.1137/1.9781611972801.19", "10.1109/vast.2012.6400485", "10.1007/s10579-013-9215-6", "10.3115/v1/d14-1162", "10.1111/cgf.13446", "10.1109/tvcg.2006.156", "10.1111/cgf.13092", "10.1145/2645710.2645759", "10.1109/bigdata.2017.8258140", "10.1145/2669557.2669572", "10.2312/eurovisstar.20151113"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2018.2864884", "title": "Face to Face: Evaluating Visual Comparison", "year": "2018", "conferenceName": "InfoVis", "authors": "Brian D. Ondov;Nicole Jardine;Niklas Elmqvist;Steven Franconeri", "citationCount": "11", "affiliation": "Ondov, B (Corresponding Author), NIH, Bldg 10, Bethesda, MD 20892 USA. Ondov, B (Corresponding Author), Univ Maryland, College Pk, MD 20742 USA. Ondov, Brian, NIH, Bldg 10, Bethesda, MD 20892 USA. Ondov, Brian; Elmqvist, Niklas, Univ Maryland, College Pk, MD 20742 USA. Jardine, Nicole; Franconeri, Steven, Northwestern Univ, Evanston, IL USA.", "countries": "USA", "abstract": "Data are often viewed as a single set of values, but those values frequently must be compared with another set. The existing evaluations of designs that facilitate these comparisons tend to be based on intuitive reasoning, rather than quantifiable measures. We build on this work with a series of crowdsourced experiments that use low-level perceptual comparison tasks that arise frequently in comparisons within data visualizations (e.g., which value changes the most between the two sets of data?). Participants completed these tasks across a variety of layouts: overlaid, two arrangements of juxtaposed small multiples, mirror-symmetric small multiples, and animated transitions. A staircase procedure sought the difficulty level (e.g., value change delta) that led to equivalent accuracy for each layout. Confirming prior intuition, we observe high levels of performance for overlaid versus standard small multiples. However, we also find performance improvements for both mirror symmetric small multiples and animated transitions. While some results are incongruent with common wisdom in data visualization, they align with previous work in perceptual psychology, and thus have potentially strong implications for visual comparison designs.", "keywords": "Graphical perception,visual perception,visual comparison,crowdsourced evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864884", "refList": ["10.1101/084715", "10.1109/infvis.2000.885091", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2015.2489649", "10.1177/1473871611416549", "10.1109/tvcg.2014.2346424", "10.1109/tvcg.2014.2346979", "10.2307/2980460", "10.3758/bf03203266", "10.1109/mcg,2007.323435", "10.1177/0956797611418346", "10.1016/0042-6989(85)90171-3", "10.1016/0042-6989(79)90154-8", "10.1186/1471-2105-12-385", "10.1109/tvcg.2011.185", "10.1006/ijhc.2002.1017", "10.1016/s0042-6989(99)00163-7", "10.1037/0033-295x.96.3.433", "10.1145/3025453.3025912", "10.1109/tvcg.2017.2744198", "10.1109/tvcg.2017.2745140", "10.1006/ijhc.1017", "10.3758/s13414-016-1116-5", "10.1016/s1364-6613(97)01105-4", "10.1080/135062800394658", "10.1093/acprof:osobl/9780199734337.003.0030", "10.1109/infvis.2005.1532136", "10.1109/tvcg.2017.2744199", "10.1111/j.1467-8659.2009.01710.x", "10.1167/16.5.11", "10.1016/j.cag.2010.11.015", "10.2307/2288400", "10.1002/wcs.1328", "10.1016/0010-0285(80)90005-5", "10.1111/j.1467-8306.2006.00514.x", "10.1177/1473871613513228", "10.1145/102377.115768", "10.1145/2909132.2909255", "10.1167/11.5.4", "10.1109/tvcg.2008.125", "10.1109/tvcg.2007.70539", "10.1177/0956797615585002", "10.20380/gi2017.06", "10.1145/2993901.2993910", "10.1038/s41562-017-0058", "10.1109/tvcg.2018.2810918", "10.3390/sym2031510", "10.1109/pacificvis.2012.6183556", "10.1186/gb-2011-12-5-r50", "10.1093/oxfordhb/9780195376746.013.0010", "10.1109/tvcg.2010.78", "10.1109/tvcg.2005.63", "10.1109/tvcg.2010.162", "10.1371/journal.pone.0158261", "10.1109/tvcg.2015.2466971", "10.1111/j.1467-8659.2009.01694.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934535", "title": "BarcodeTree: Scalable Comparison of Multiple Hierarchies", "year": "2019", "conferenceName": "InfoVis", "authors": "Guozheng Li 0002;Yu Zhang 0043;Yu Dong;Jie Liang 0004;Jinson Zhang;Jinsong Wang;Michael J. McGuffin;Xiaoru Yuan", "citationCount": "2", "affiliation": "Li, GZ (Corresponding Author), Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China. Li, Guozheng; Yuan, Xiaoru, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China. Li, Guozheng; Yuan, Xiaoru, Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Zhang, Yu, Univ Oxford, Oxford, England. Zhang, Yu, Peking Univ, Beijing, Peoples R China. Dong, Yu; Liang, Jie; Zhang, Jinson, Univ Technol Sydney, Sydney, NSW, Australia. Wang, Jinsong, Southwest Elect \\& Telecom Engn Inst, Mianyang, Peoples R China. McGuffin, Michael J., Ecole Technol Super, Montreal, PQ, Canada.", "countries": "Canada;China;England;Australia", "abstract": "We propose BarcodeTree (BCT), a novel visualization technique for comparing topological structures and node attribute values of multiple trees. BCT can provide an overview of one hundred shallow and stable trees simultaneously, without aggregating individual nodes. Each BCT is shown within a single row using a style similar to a barcode, allowing trees to be stacked vertically with matching nodes aligned horizontally to ease comparison and maintain space efficiency. We design several visual cues and interactive techniques to help users understand the topological structure and compare trees. In an experiment comparing two variants of BCT with icicle plots, the results suggest that BCTs make it easier to visually compare trees by reducing the vertical distance between different trees. We also present two case studies involving a dataset of hundreds of trees to demonstrate BCT's utility.", "keywords": "tree visualization,comparison,multiple trees", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934535", "refList": ["10.1207/s15327051hci0701\\_3", "10.1109/vlhcc.2014.6883017", "10.1109/infvis.2000.885091", "10.1109/tvcg.2012.226", "10.1109/tvcg.2008.141", "10.1177/1473871611416549", "10.1109/tvcg.2007.70529", "10.1109/tvcg.2015.2467733", "10.1109/tse.1981.234519", "10.1111/j.1467-8659.2011.01963.x", "10.1111/j.1467-8659.2008.01214.x", "10.1109/infvis.2002.1173151", "10.1109/iv.2003.1218020", "10.1057/ivs.2009.4", "10.1109/tvcg.2013.231", "10.1109/tvcg.2007.70556", "10.1109/vast.2011.6102439", "10.1109/visual.1991.175815", "10.1109/infvis.2001.963290", "10.1109/tvcg.2017.2744080", "10.1111/j.1467-8659.2008.01205.x", "10.1109/mcg.2011.103", "10.1109/vl.1996.545307", "10.1109/infvis.2002.1173150", "10.1145/1294948.1294971", "10.1109/tvcg.2010.79", "10.1109/infvis.2001.963283", "10.1007/bf01908061", "10.1080/10635150590946961", "10.1177/1473871612455983", "10.1109/tvcg.2018.2865265", "10.1109/iv.2004.1320261", "10.1109/tvcg.2011.193", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2016.2598469", "10.1109/infvis.2002.1173148", "10.1145/102377.115768", "10.1145/633292.633484", "10.1109/tvcg.2015.2507595", "10.1109/infvis.2004.70", "10.1109/tvcg.2018.2864884", "10.1145/882262.882291", "10.1057/ivs.2009.29", "10.2307/2685881", "10.1057/palgrave.ivs.9500157", "10.1111/cgf.13164", "10.3897/bdj.4.e9787", "10.2312/vissym/eurovis05/053-060", "10.1109/infvis.2003.1249028", "10.1109/tvcg.2014.2346433"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13989", "year": "2020", "title": "Quantitative Comparison of Time-Dependent Treemaps", "conferenceName": "EuroVis", "authors": "Eduardo Faccin Vernier;Max Sondag;Jo{\\~{a}}o Luiz Dihl Comba;Bettina Speckmann;Alexandru Telea;Kevin Verbeek", "citationCount": "0", "affiliation": "Vernier, E (Corresponding Author), Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil.\nVernier, E (Corresponding Author), Univ Groningen, Groningen, Netherlands.\nSondag, M.; Speckmann, B.; Verbeek, K., TU Eindhoven, Eindhoven, Netherlands.\nVernier, E.; Comba, J., Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil.\nTelea, A., Univ Utrecht, Utrecht, Netherlands.\nVernier, E., Univ Groningen, Groningen, Netherlands.", "countries": "Netherlands;Brazil", "abstract": "Rectangular treemaps are often the method of choice to visualize large hierarchical datasets. Nowadays such datasets are available over time, hence there is a need for (a) treemaps that can handle time-dependent data, and (b) corresponding quality criteria that cover both a treemap's visual quality and its stability over time. In recent years a wide variety of (stable) treemapping algorithms has been proposed, with various advantages and limitations. We aim to provide insights to researchers and practitioners to allow them to make an informed choice when selecting a treemapping algorithm for specific applications and data. To this end, we perform an extensive quantitative evaluation of rectangular treemaps for time-dependent data. As part of this evaluation we propose a novel classification scheme for time-dependent datasets. Specifically, we observe that the performance of treemapping algorithms depends on the characteristics of the datasets used. We identify four potential representative features that characterize time-dependent hierarchical datasets and classify all datasets used in our experiments accordingly. We experimentally test the validity of this classification on more than 2000 datasets, and analyze the relative performance of 14 state-of-the-art rectangular treemapping algorithms across varying features. Finally, we visually summarize our results with respect to both visual quality and stability to aid users in making an informed choice among treemapping algorithms. All datasets, metrics, and algorithms are openly available to facilitate reuse and further comparative studies.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13989", "refList": ["10.1037/0033-295x.94.2.115", "10.1016/0022-2496(73)90003-5", "10.1109/tvcg.2007.70529", "10.1109/infvis.2005.1532145", "10.1109/tvcg.2013.231", "10.1109/sibgrapi.2018.00027", "10.1561/0600000017", "10.1007/s00371-017-1373-x", "10.1109/tvcg.2017.2745140", "10.1109/tvcg.2006.200", "10.1109/tvcg.2010.79", "10.1016/j.infsof.2016.10.003", "10.1109/tvcg.2010.186", "10.1109/infvis.2001.963283", "10.1109/tvcg.2018.2865265", "10.1016/j.comgeo.2013.12.008", "10.1145/571647.571649", "10.1109/tvcg.2019.2934535", "10.1109/vissoft.2018.00018", "10.5220/0006117500880095", "10.1137/110834032", "10.1109/cvpr.1994.323794", "10.1093/sysbio/syu085", "10.1145/102377.115768", "10.1109/tvcg.2012.108", "10.1145/1056018.1056041", "10.1088/1742-6596/787/1/012007", "10.1057/ivs.2009.29", "10.1111/cgf.13164", "10.1109/iswcs.2014.6933406", "10.1016/j.cor.2013.11.015", "10.1016/j.dam.2006.08.005", "10.1007/978-3-319-57336-6\\_14", "10.1145/2827872"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/tvcg.2019.2934208", "title": "Evaluating Perceptual Bias During Geometric Scaling of Scatterplots", "year": "2019", "conferenceName": "VAST", "authors": "Yating Wei;Honghui Mei;Ying Zhao;Shuyue Zhou;Bingru Lin;Haojing Jiang;Wei Chen", "citationCount": "5", "affiliation": "Chen, W (Corresponding Author), Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou 310058, Zhejiang, Peoples R China. Zhao, Y (Corresponding Author), Cent South Univ, Sch Comp Sci \\& Engn, Changsha 410083, Hunan, Peoples R China. Wei, Yating; Mei, Honghui; Zhou, Shuyue; Lin, Bingru; Chen, Wei, Zhejiang Univ, State Key Lab CAD \\& CG, Hangzhou 310058, Zhejiang, Peoples R China. Zhao, Ying; Jiang, Haojing, Cent South Univ, Sch Comp Sci \\& Engn, Changsha 410083, Hunan, Peoples R China.", "countries": "China", "abstract": "Scatterplots are frequently scaled to fit display areas in multi-view and multi-device data analysis environments. A common method used for scaling is to enlarge or shrink the entire scatterplot together with the inside points synchronously and proportionally. This process is called geometric scaling. However, geometric scaling of scatterplots may cause a perceptual bias, that is, the perceived and physical values of visual features may be dissociated with respect to geometric scaling. For example, if a scatterplot is projected from a laptop to a large projector screen, then observers may feel that the scatterplot shown on the projector has fewer points than that viewed on the laptop. This paper presents an evaluation study on the perceptual bias of visual features in scatterplots caused by geometric scaling. The study focuses on three fundamental visual features (i.e., numerosity, correlation, and cluster separation) and three hypotheses that are formulated on the basis of our experience. We carefully design three controlled experiments by using well-prepared synthetic data and recruit participants to complete the experiments on the basis of their subjective experience. With a detailed analysis of the experimental results, we obtain a set of instructive findings. First, geometric scaling causes a bias that has a linear relationship with the scale ratio. Second, no significant difference exists between the biases measured from normally and uniformly distributed scatterplots. Third, changing the point radius can correct the bias to a certain extent. These findings can be used to inspire the design decisions of scatterplots in various scenarios.", "keywords": "Evaluation,scatterplot,geometric scaling,bias,perceptual consistency", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934208", "refList": ["10.2307/2288843", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2015.2467732", "10.1109/tvcg.2014.2346979", "10.1126/science.216.4550.1138", "10.1109/tvcg.2017.2744138", "10.1111/j.1467-8659.2009.01467.x", "10.1145/2491568.2491577", "10.1109/mwc.2018.1700325", "10.1145/2449396.2449439", "10.1109/tvcg.2011.127", "10.1109/tvcg.2011.229", "10.1167/15.5.4", "10.1073/pnas.1113195108", "10.1145/2702123.2702545", "10.1109/vast.2009.5332628", "10.1109/tvcg.2018.2800013", "10.1007/s12650-018-0530-2", "10.1016/s0042-6989(97)00340-4", "10.1109/vast.2010.5652460", "10.1109/mcg.2018.2879067", "10.1109/tvcg.2018.2864912", "10.1109/pacificvis.2010.5429604", "10.1109/tcst.2018.2819965", "10.1167/10.2.10", "10.1145/2470654.2481318", "10.1145/1842993.1843002", "10.1167/12.6.8", "10.1109/tvcg.2013.124", "10.1057/ivs.2008.13", "10.1109/tvcg.2007.70596", "10.1109/tvcg.2018.2865266", "10.1145/3173574.3173664", "10.1109/tvcg.2015.2467671", "10.1016/j.cag.2017.07.004", "10.1177/0956797613501520", "10.1109/tvcg.2018.2865020", "10.1111/j.1467-8659.2012.03125.x", "10.3758/bf03205986", "10.3758/s13423-016-1174-7", "10.1109/tvcg.2017.2680452", "10.1109/mc.2006.109", "10.1109/tvcg.2017.2744098", "10.1038/srep32810", "10.1016/j.visres.2013.06.006", "10.1002/jhbs.20078", "10.1109/tvcg.2006.163", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1016/j.cognition.2007.10.009", "10.1109/tvcg.2018.2865142", "10.3758/app.72.7.1839", "10.1109/tvcg.2018.2810918", "10.1016/j.jvlc.2017.10.001", "10.1145/2682623", "10.1109/tvcg.2018.2864884", "10.1145/3025453.3025984", "10.1109/tvcg.2006.184", "10.1109/tvcg.2013.153", "10.1016/j.jvlc.2018.08.003", "10.1109/tvcg.2016.2520921", "10.1111/cgf.13446", "10.1017/s0022381612000187", "10.1145/2702123.2702406", "10.1109/vast.2012.6400487", "10.1109/tvcg.2013.183", "10.1177/1473871611415997", "10.1145/2702123.2702585", "10.1145/2993901.2993903", "10.1109/tvcg.2013.120", "10.1111/cgf.12632", "10.1145/1385569.1385602", "10.1109/tvcg.2017.2754480", "10.1111/j.1467-8659.2009.01694.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030456", "title": "Cartographic Relief Shading with Neural Networks", "year": "2020", "conferenceName": "InfoVis", "authors": "Bernhard Jenny;Magnus Heitzler;Dilpreet Singh;Marianna Farmakis-Serebryakova;Jeffery Chieh Liu;Lorenz Hurni", "citationCount": "4", "affiliation": "Jenny, B (Corresponding Author), Monash Univ, Melbourne, Vic, Australia. Jenny, Bernhard; Singh, Dilpreet; Liu, Jeffery Chieh, Monash Univ, Melbourne, Vic, Australia. Heitzler, Magnus; Farmakis-Serebryakova, Marianna; Hurni, Lorenz, Swiss Fed Inst Technol, Inst Cartog \\& Geoinformat, Zurich, Switzerland.", "countries": "Switzerland;Australia", "abstract": "Shaded relief is an effective method for visualising terrain on topographic maps, especially when the direction of illumination is adapted locally to emphasise individual terrain features. However, digital shading algorithms are unable to fully match the expressiveness of hand-crafted masterpieces, which are created through a laborious process by highly specialised cartographers. We replicate hand-drawn relief shading using U-Net neural networks. The deep neural networks are trained with manual shaded relief images of the Swiss topographic map series and terrain models of the same area. The networks generate shaded relief that closely resemble hand-drawn shaded relief art. The networks learn essential design principles from manual relief shading such as removing unnecessary terrain details, locally adjusting the illumination direction to accentuate individual terrain features, and varying brightness to emphasise larger landforms. Neural network shadings are generated from digital elevation models in a few seconds, and a study with 18 relief shading experts found that they are of high quality.", "keywords": "Relief shading,shaded relief,hillshade,neural rendering,illustrative visualisation,image-to-image translation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030456", "refList": ["10.1145/1456650.1456652", "10.1145/1145/1556262.1556270", "10.1145/345513.345271", "10.1109/tvcg.2019.2934803", "10.1145/3180658", "10.1109/tridui.2006.1618264", "10.3389/fict.2018.00015", "10.1109/vr.2018.8447558", "10.1518/hfes.45.1.160.27234", "10.1145/3290605.3300377", "10.1089/cpb.2006.9.157", "10.1007/s00779-011-0500-3", "10.1109/vl.1996.545307", "10.1109/tvcg.2019.2934415", "10.1109/5.726791", "10.1145/3290605.3300288", "10.1109/tvcg.2017.2745941", "10.1109/vr.2001.913779", "10.1145/586081.586086", "10.18637/jss.v067.i01", "10.1145/1502800.1502805", "10.1145/1165734.1165736", "10.1145/3126594.3126613", "10.1109/tvcg.2008.109", "10.1109/tvcg.2017.2744184", "10.1016/j.cola.2019.100937", "10.1109/visual.2019.8933545", "10.1145/3290605.3300555", "10.1111/cgf.13431", "10.1109/tvcg.2019.2934395", "10.1109/vr.2019.8798340", "10.1145/3025453.3026046", "10.1109/vr.2019.8797871", "10.1145/3290605.3300752", "10.1109/tvcg.2016.2520921", "10.1109/tvcg.2018.2865191", "10.1145/1970378.1970384", "10.1109/tvcg.2019.2934208", "10.18637/jss.v069.i01", "10.1162/105474698565659", "10.1145/1124772.1124775", "10.1109/vrais.1997.583043"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030443", "title": "CAVA: A Visual Analytics System for Exploratory Columnar Data Augmentation Using Knowledge Graphs", "year": "2020", "conferenceName": "VAST", "authors": "Dylan Cashman;Shenyu Xu;Subhajit Das;Florian Heimerl;Cong Liu;Shah Rukh Humayoun;Michael Gleicher;Alex Endert;Remco Chang", "citationCount": "0", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Liu, Cong; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Xu, Shenyu; Das, Subhajit; Endert, Alex, Georgia Tech, Atlanta, GA USA. Heimerl, Florian; Gleicher, Michael, Univ Wisconsin, Madison, WI 53706 USA. Humayoun, Shah Rukh, San Francisco State Univ, San Francisco, CA 94132 USA.", "countries": "USA", "abstract": "Most visual analytics systems assume that all foraging for data happens before the analytics process; once analysis begins, the set of data attributes considered is fixed. Such separation of data construction from analysis precludes iteration that can enable foraging informed by the needs that arise in-situ during the analysis. The separation of the foraging loop from the data analysis tasks can limit the pace and scope of analysis. In this paper, we present CAVA, a system that integrates data curation and data augmentation with the traditional data exploration and analysis tasks, enabling information foraging in-situ during analysis. Identifying attributes to add to the dataset is difficult because it requires human knowledge to determine which available attributes will be helpful for the ensuing analytical tasks. CAVA crawls knowledge graphs to provide users with a a broad set of attributes drawn from external data to choose from. Users can then specify complex operations on knowledge graphs to construct additional attributes. CAVA shows how visual analytics can help users forage for attributes by letting users visually explore the set of available data, and by serving as an interface for query construction. It also provides visualizations of the knowledge graph itself to help users understand complex joins such as multi-hop aggregations. We assess the ability of our system to enable users to perform complex data combinations without programming in a user study over two datasets. We then demonstrate the generalizability of CAVA through two additional usage scenarios. The results of the evaluation confirm that CAVA is effective in helping the user perform data foraging that leads to improved analysis outcomes, and offer evidence in support of integrating data augmentation as a part of the visual analytics pipeline.", "keywords": "Visual Analytics,Information Foraging,Data Augmentation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030443", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/tvcg.2018.2875702", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1109/icde.2016.7498287", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2019.2945960", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030432", "title": "Evaluation of Sampling Methods for Scatterplots", "year": "2020", "conferenceName": "VAST", "authors": "Jun Yuan;Shouxing Xiang;Jiazhi Xia;Lingyun Yu;Shixia Liu", "citationCount": "0", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, BNRist, Beijing, Peoples R China. Yuan, Jun; Xiang, Shouxing; Liu, Shixia, Tsinghua Univ, BNRist, Beijing, Peoples R China. Xia, Jiazhi, Cent South Univ, Changsha, Peoples R China. Yu, Lingyun, Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.", "countries": "China", "abstract": "Given a scatterplot with tens of thousands of points or even more, a natural question is which sampling method should be used to create a small but \u201cgood\u201d scatterplot for a better abstraction. We present the results of a user study that investigates the influence of different sampling strategies on multi-class scatterplots. The main goal of this study is to understand the capability of sampling methods in preserving the density, outliers, and overall shape of a scatterplot. To this end, we comprehensively review the literature and select seven typical sampling strategies as well as eight representative datasets. We then design four experiments to understand the performance of different strategies in maintaining: 1) region density; 2) class density; 3) outliers; and 4) overall shape in the sampling results. The results show that: 1) random sampling is preferred for preserving region density; 2) blue noise sampling and random sampling have comparable performance with the three multi-class sampling strategies in preserving class density; 3) outlier biased density based sampling, recursive subdivision based sampling, and blue noise sampling perform the best in keeping outliers; and 4) blue noise sampling outperforms the others in maintaining the overall shape of a scatterplot.", "keywords": "Scatterplot,data sampling,empirical evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030432", "refList": ["10.1057/palgrave.ivs.9500122", "10.1109/tvcg.2016.2598867", "10.1109/tvcg.2015.2467591", "10.1111/cgf.13708", "10.1109/tvcg.2019.2934799", "10.1109/tvcg.2019.2934541", "10.1109/tvcg.2013.65", "10.1109/iv.2004.1320207", "10.1068/p260471", "10.1145/1778765.1778816", "10.1109/tvcg.2018.2808489", "10.1109/tvcg.2018.2864912", "10.1016/j.apgeog.2015.12.006", "10.1111/j.1467-8659.2011.01960.x", "10.1109/tvcg.2018.2864843", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2014.2346898", "10.1109/5.726791", "10.1177/1475090214540874", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/vast.2012.6400489", "10.1145/1056808.1056914", "10.1016/j.neucom.2014.09.063", "10.1145/7529.8927", "10.1109/tvcg.2016.2607204", "10.1109/vast47406.2019.8986943", "10.3758/bf03205986", "10.1109/infvis.2005.1532142", "10.1145/1150402.1150479", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2017.2674999", "10.1109/tvcg.2011.279", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1111/cgf.12876", "10.1109/1011101.2019.2945960", "10.1007/s4095-020-0191-7", "10.1007/s11390-015-1535-0", "10.1111/cgf.12640", "10.1109/tvcg.2016.2598667", "10.1111/cgf.13683", "10.1109/tvcg.2013.153", "10.1109/tvcg.2019.2934208", "10.1109/tvcg.2019.2934655", "10.1111/cgf.12655", "10.1007/s11023-010-9221-z", "10.1109/vast.2012.6400487", "10.1145/2702123.2702585", "10.1007/bf00310175", "10.1109/tvcg.2017.2744378", "10.1103/physreve.64.061907", "10.1109/ldav.2017.8231848", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934786", "title": "The Perceptual Proxies of Visual Comparison", "year": "2019", "conferenceName": "InfoVis", "authors": "Nicole Jardine;Brian D. Ondov;Niklas Elmqvist;Steven Franconeri", "citationCount": "4", "affiliation": "Jardine, N (Corresponding Author), Northwestern Univ, Evanston, IL 60208 USA. Jardine, Nicole; Franconeri, Steven, Northwestern Univ, Evanston, IL 60208 USA. Jardine, Nicole, Cook Cty Assessors Off, Chicago, IL USA. Ondov, Brian D., NIH, Bldg 10, Bethesda, MD 20892 USA. Ondov, Brian D.; Elmqvist, Niklas, Univ Maryland, College Pk, MD 20742 USA.", "countries": "USA", "abstract": "Perceptual tasks in visualizations often involve comparisons. Of two sets of values depicted in two charts, which set had values that were the highest overall? Which had the widest range? Prior empirical work found that the performance on different visual comparison tasks (e.g., \u201cbiggest delta\u201d, \u201cbiggest correlation\u201d) varied widely across different combinations of marks and spatial arrangements. In this paper, we expand upon these combinations in an empirical evaluation of two new comparison tasks: the \u201cbiggest mean\u201d and \u201cbiggest range\u201d between two sets of values. We used a staircase procedure to titrate the difficulty of the data comparison to assess which arrangements produced the most precise comparisons for each task. We find visual comparisons of biggest mean and biggest range are supported by some chart arrangements more than others, and that this pattern is substantially different from the pattern for other tasks. To synthesize these dissonant findings, we argue that we must understand which features of a visualization are actually used by the human visual system to solve a given task. We call these perceptual proxies. For example, when comparing the means of two bar charts, the visual system might use a \u201cMean length\u201d proxy that isolates the actual lengths of the bars and then constructs a true average across these lengths. Alternatively, it might use a \u201cHull Area\u201d proxy that perceives an implied hull bounded by the bars of each chart and then compares the areas of these hulls. We propose a series of potential proxies across different tasks, marks, and spatial arrangements. Simple models of these proxies can be empirically evaluated for their explanatory power by matching their performance to human performance across these marks, arrangements, and tasks. We use this process to highlight candidates for perceptual proxies that might scale more broadly to explain performance in visual comparison.", "keywords": "Graphical perception,visual perception,visual comparison,crowdsourced evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934786", "refList": ["10.1177/1473871611416549", "10.1109/tvcg.2014.2346979", "10.1016/0010-0285(77)90012-3", "10.1037/a0026284", "10.1016/s1364-6613(97)01105-4", "10.3758/bf03201236", "10.1093/acprof:osobl/9780199734337.003.0030", "10.1109/infvis.2005.1532136", "10.3758/bf03204219", "10.1109/tvcg.2017.2744199", "10.1016/s0042-6989(99)00029-2", "10.1167/16.5.11", "10.1109/34.730558", "10.1145/2046684.2046699", "10.3758/s13414-012-0322-z", "10.1177/0956797615585002", "10.1109/tvcg.2018.2810918", "10.1109/tvcg.2018.2864884", "10.1016/0042-6989(85)90208-1", "10.1016/j.tics.2011.01.003", "10.1146/annurev-psych-010416-044232", "10.1109/tvcg.2010.162", "10.1109/tvcg.2015.2466971", "10.1109/tvcg.2007.70515", "10.1111/j.1467-8659.2009.01694.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030454", "title": "Objective Observer-Relative Flow Visualization in Curved Spaces for Unsteady 2D Geophysical Flows", "year": "2020", "conferenceName": "SciVis", "authors": "Peter Rautek;Matej Mlejnek;Johanna Beyer;Jakob Troidl;Hanspeter Pfister;Thomas Theu\u00dfl;Markus Hadwiger", "citationCount": "0", "affiliation": "Rautek, P (Corresponding Author), King Abdullah Univ Sci \\& Technol KAUST, Visual Comp Ctr, Thuwal 239556900, Saudi Arabia. Rautek, Peter; Mlejnek, Matej; Troidl, Jakob; Hadwiger, Markus, King Abdullah Univ Sci \\& Technol KAUST, Visual Comp Ctr, Thuwal 239556900, Saudi Arabia. Beyer, Johanna; Pfister, Hanspeter, Harvard Univ, Cambridge, MA 02138 USA. Troidl, Jakob, TU Wien, Vienna, Austria. Theussl, Thomas, King Abdullah Univ Sci \\& Technol KAUST, Core Labs, Thuwal 239556000, Saudi Arabia.", "countries": "USA;Arabia;Austria", "abstract": "Computing and visualizing features in fluid flow often depends on the observer, or reference frame, relative to which the input velocity field is given. A desired property of feature detectors is therefore that they are objective, meaning independent of the input reference frame. However, the standard definition of objectivity is only given for Euclidean domains and cannot be applied in curved spaces. We build on methods from mathematical physics and Riemannian geometry to generalize objectivity to curved spaces, using the powerful notion of symmetry groups as the basis for definition. From this, we develop a general mathematical framework for the objective computation of observer fields for curved spaces, relative to which other computed measures become objective. An important property of our framework is that it works intrinsically in 2D, instead of in the 3D ambient space. This enables a direct generalization of the 2D computation via optimization of observer fields in flat space to curved domains, without having to perform optimization in 3D. We specifically develop the case of unsteady 2D geophysical flows given on spheres, such as the Earth. Our observer fields in curved spaces then enable objective feature computation as well as the visualization of the time evolution of scalar and vector fields, such that the automatically computed reference frames follow moving structures like vortices in a way that makes them appear to be steady.", "keywords": "Flow visualization,observer fields,frames of reference,objectivity,symmetry groups,intrinsic covariant derivatives", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030454", "refList": ["10.1001/archpediatrics.2011.97", "10.1037/0033-2909.115.2.228", "10.1057/s41267-019-00289-7", "10.1097/00001888-199805000-00024", "10.1145/3173574.3173718", "10.1126/science.7455683", "10.1109/tvcg.2012.199", "10.2307/1914185", "10.1371/journal.pone.0142444", "10.1177/0956797613504966", "10.1198/0003130032369", "10.1037/0033-2909.111.2.361", "10.1037/a0014474", "10.1186/s41235-019-0182-3", "10.1016/0041-5553(76)90154-3", "10.1037/a0025185", "10.3758/s13423-013-0572-3", "10.1111/1467-985x.00120", "10.1098/rsta.1895.0010", "10.1109/tvcg.2019.2934786", "10.1037/h0046162", "10.1177/2059799116672879", "10.1167/16.5.11", "10.1111/j.1740-9713.2013.00636.x", "10.1037/a0023265", "10.1073/pnas.1722389115", "10.2307/2288400", "10.3389/fnins.2012.00001", "10.1016/j.ijforecast.2012.02.006", "10.1186/s13639-018-0087-0", "10.1037/1082-989x.10.4.389", "10.1006/cogp.1998.0710", "10.1177/0963721413481473", "10.1037/0033-295x.102.4.684", "10.1126/science.185.4157.1124", "10.1109/tvcg.2014.2346298", "10.1145/3313831.3376454", "10.1037/0033-295x.107.3.500", "10.1037/h0042769", "10.1037/0003-066x.60.2.170", "10.3389/fpsyg.2019.00813", "10.1111/rssa.12378"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030429", "title": "Revealing Perceptual Proxies with Adversarial Examples", "year": "2020", "conferenceName": "InfoVis", "authors": "Brian D. Ondov;Fumeng Yang;Matthew Kay;Niklas Elmqvist;Steven Franconeri", "citationCount": "0", "affiliation": "Ondov, BD (Corresponding Author), NIH, Bldg 10, Bethesda, MD 20892 USA. Ondov, BD (Corresponding Author), Univ Maryland, College Pk, MD 20742 USA. Ondov, Brian D., NIH, Bldg 10, Bethesda, MD 20892 USA. Ondov, Brian D.; Elmqvist, Niklas, Univ Maryland, College Pk, MD 20742 USA. Yang, Fumeng, Brown Univ, Providence, RI 02912 USA. Kay, Matthew, Univ Michigan, Ann Arbor, MI 48109 USA. Franconeri, Steven, Northwestern Univ, Evanston, IL USA.", "countries": "USA", "abstract": "Data visualizations convert numbers into visual marks so that our visual system can extract data from an image instead of raw numbers. Clearly, the visual system does not compute these values as a computer would, as an arithmetic mean or a correlation. Instead, it extracts these patterns using perceptual proxies; heuristic shortcuts of the visual marks, such as a center of mass or a shape envelope. Understanding which proxies people use would lead to more effective visualizations. We present the results of a series of crowdsourced experiments that measure how powerfully a set of candidate proxies can explain human performance when comparing the mean and range of pairs of data series presented as bar charts. We generated datasets where the correct answer-the series with the larger arithmetic mean or range-was pitted against an \u201cadversarial\u201d series that should be seen as larger if the viewer uses a particular candidate proxy. We used both Bayesian logistic regression models and a robust Bayesian mixed-effects linear model to measure how strongly each adversarial proxy could drive viewers to answer incorrectly and whether different individuals may use different proxies. Finally, we attempt to construct adversarial datasets from scratch, using an iterative crowdsourcing procedure to perform black-box optimization.", "keywords": "Perceptual proxies,vision science,crowdsourced evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030429", "refList": ["10.3389/fpsyg.2012.00054", "10.3758/bf03209345", "10.1167/14.8.23", "10.1109/tvcg.2014.2346979", "10.1111/j.1467-8659.2009.01694.x", "10.1109/mcg.2007.323435", "10.1080/00223980.1947.9917350", "10.1016/j.tics.2009.07.001", "10.1007/bf00308884", "10.1016/s0020-7373(86)80019-0", "10.1037/h0043158", "10.1038/nn.2706", "10.1037/0278-7393.25.4.986", "10.1177/0956797617709814", "10.1109/6.736450", "10.2307/1131314", "10.1016/j.cub.2014.07.030", "10.1109/tvcg.2014.2346320", "10.1109/tvcg.2019.2934786", "10.1111/cgf.12888", "10.1109/tvcg.2018.2865240", "10.1109/tvcg.2015.2467671", "10.1037/0096-1523.16.4.683", "10.1037/h0046162", "10.1037/h0044417", "10.1037/0096-3445.106.4.341", "10.2307/2288400", "10.1109/tvcg.2018.2810918", "10.1002/wcs.26", "10.1016/j.visres.2014.02.006", "10.1016/0010-0277(92)90002-y", "10.1017/s0142716400009796", "10.1037/0033-295x.107.3.500", "10.1038/s41598-018-37610-7", "10.1016/j.actpsy.2011.09.006"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030335", "title": "Visual Reasoning Strategies for Effect Size Judgments and Decisions", "year": "2020", "conferenceName": "InfoVis", "authors": "Alex Kale;Matthew Kay;Jessica Hullman", "citationCount": "0", "affiliation": "Kale, A (Corresponding Author), Univ Washington, Seattle, WA 98195 USA. Kale, Alex, Univ Washington, Seattle, WA 98195 USA. Kay, Matthew, Univ Michigan, Ann Arbor, MI 48109 USA. Hullman, Jessica, Northwestern Univ, Evanston, IL 60208 USA.", "countries": "USA", "abstract": "Uncertainty visualizations often emphasize point estimates to support magnitude estimates or decisions through visual comparison. However, when design choices emphasize means, users may overlook uncertainty information and misinterpret visual distance as a proxy for effect size. We present findings from a mixed design experiment on Mechanical Turk which tests eight uncertainty visualization designs: 95% containment intervals, hypothetical outcome plots, densities, and quantile dotplots, each with and without means added. We find that adding means to uncertainty visualizations has small biasing effects on both magnitude estimation and decision-making, consistent with discounting uncertainty. We also see that visualization designs that support the least biased effect size estimation do not support the best decision-making, suggesting that a chart user's sense of effect size may not necessarily be identical when they use the same information for different tasks. In a qualitative analysis of users' strategy descriptions, we find that many users switch strategies and do not employ an optimal strategy when one exists. Uncertainty visualizations which are optimally designed in theory may not be the most effective in practice because of the ways that users satisfice with heuristics, suggesting opportunities to better understand visualization effectiveness by modeling sets of potential strategies.", "keywords": "Uncertainty visualization,graphical perception,data cognition", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030335", "refList": ["10.1001/archpediatrics.2011.97", "10.1037/0033-2909.115.2.228", "10.1109/tvcg.2017.2744683", "10.1057/s41267-019-00289-7", "10.1097/00001888-199805000-00024", "10.1145/3173574.3173718", "10.1155/2019/2318680", "10.1109/tvcg.2012.199", "10.3389/fpsyg", "10.2307/1914185", "10.1371/journal.pone.0142444", "10.1177/0956797613504966", "10.1198/0003130032369", "10.1037/a0014474", "10.1186/s41235-019-0182-3", "10.1016/0041-5553(76)90154-3", "10.1037/a0025185", "10.3758/s13423-013-0572-3", "10.1111/1467-985x.00120", "10.1098/rsta.1895.0010", "10.1109/tvcg.2019.2934786", "10.1037/h0046162", "10.1177/2059799116672879", "10.3389/fnins.2012.00007", "10.1167/16.5.11", "10.1111/j.1740-9713.2013.00636.x", "10.1037/a0023265", "10.1073/pnas.1722389115", "10.2307/2288400", "10.1371/journal.pone.0087357", "10.1016/j.ijforecast.2012.02.006", "10.1186/s13639-018-0087-0", "10.1037/1082-989x.10.4.389", "10.1006/cogp.1998.0710", "10.1177/0963721413481473", "10.1037/0033-295x.102.4.684", "10.1126/science.185.4157.1124", "10.1109/tvcg.2014.2346298", "10.1145/3313831.3376454", "10.1037/0033-295x.107.3.500", "10.1037/h0042769", "10.1037/0003-066x.60.2.170", "10.1126/science.7455683", "10.1111/rssa.12378"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030419", "title": "Comparative Layouts Revisited: Design Space, Guidelines, and Future Directions", "year": "2020", "conferenceName": "InfoVis", "authors": "Sehi L'Yi;Jaemin Jo;Jinwook Seo", "citationCount": "0", "affiliation": "L'Yi, S (Corresponding Author), Harvard Med Sch, Boston, MA 02115 USA. L'Yi, Sehi, Harvard Med Sch, Boston, MA 02115 USA. Jo, Jaemin, Sungkyunkwan Univ, Seoul, South Korea. Seo, Jinwook, Seoul Natl Univ, Seoul, South Korea.", "countries": "USA;Korea", "abstract": "We present a systematic review on three comparative layouts-juxtaposition, superposition, and explicit-encoding-which are information visualization (InfoVis) layouts designed to support comparison tasks. For the last decade, these layouts have served as fundamental idioms in designing many visualization systems. However, we found that the layouts have been used with inconsistent terms and confusion, and the lessons from previous studies are fragmented. The goal of our research is to distill the results from previous studies into a consistent and reusable framework. We review 127 research papers, including 15 papers with quantitative user studies, which employed comparative layouts. We first alleviate the ambiguous boundaries in the design space of comparative layouts by suggesting lucid terminology (e.g., chart-wise and item-wise juxtaposition). We then identify the diverse aspects of comparative layouts, such as the advantages and concerns of using each layout in the real-world scenarios and researchers' approaches to overcome the concerns. Building our knowledge on top of the initial insights gained from the Gleicher et al.'s survey [19], we elaborate on relevant empirical evidence that we distilled from our survey (e.g., the actual effectiveness of the layouts in different study settings) and identify novel facets that the original work did not cover (e.g., the familiarity of the layouts to people). Finally, we show the consistent and contradictory results on the performance of comparative layouts and offer practical implications for using the layouts by suggesting trade-offs and seven actionable guidelines.", "keywords": "Comparative layout,visual comparison,literature review,juxtaposition,superposition,explicit-encoding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030419", "refList": ["10.1109/tvcg.2013.233", "10.1111/cgf.12380", "10.1177/1473871611416549", "10.1145/2702123.2702419", "10.1109/tvcg.2014.2322363", "10.1111/cgf.12791", "10.1145/2702123.2702130", "10.1145/2702123.2702217", "10.1109/tvcg.2012.237", "10.1177/1473871613480062", "10.1109/mcg.2017.377152546", "10.1109/tvcg.2013.213", "10.1111/cgf.12369", "10.1109/tvcg.2017.2744198", "10.1145/3139295.3139309", "10.1109/tvcg.2019.2934801", "10.1109/tvcg.2013.122", "10.1109/tvcg.2017.2747545", "10.1109/tvcg.2015.2413774", "10.1037/0096-1523.24.3.719", "10.1109/tvcg.2014.2346320", "10.1109/tvcg.2007.70535", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1145/2556288.2557141", "10.1109/tvcg.2013.149", "10.1145/1165734.1165736", "10.5220/0006127502170224", "10.1109/tvcg.2017.2745298", "10.1177/1473871617692841", "10.1190/int-2017-0083.1", "10.1190/int-2014-0283.1", "10.1109/tvcg.2016.2598796", "10.1111/cgf.13401", "10.1016/j.cag.2017.05.005", "10.1177/1473871616667632", "10.1145/3025453.3025882", "10.1145/2470654.2470724", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2010.164", "10.1145/3103010.3103013", "10.1109/pacificvis.2016.7465266", "10.1109/pacificvis.2012.6183556", "10.1109/tvcg.2015.2467751", "10.1109/tvcg.2018.2796557", "10.1111/cgf.13531", "10.1109/tvcg.2013.161", "10.1109/iv.2018.00051", "10.1109/tvcg.2010.162", "10.1109/tvcg.2018.2864510", "10.1109/iv.2017.30", "10.1109/tvcg.2007.70623"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030426", "title": "Introducing Layers of Meaning (LoM): A Framework to Reduce Semantic Distance of Visualization In Humanistic Research", "year": "2020", "conferenceName": "InfoVis", "authors": "Houda Lamqaddam;Andrew Vande Moere;Vero Vanden Abeele;Koenraad Brosens;Katrien Verbert", "citationCount": "0", "affiliation": "Lamqaddam, H (Corresponding Author), Katholieke Univ Leuven, Leuven, Belgium. Lamqaddam, Houda; Vande Moere, Andrew; Vanden Abeele, Vero; Brosens, Koenraad; Verbert, Katrien, Katholieke Univ Leuven, Leuven, Belgium.", "countries": "Belgium", "abstract": "Information visualization (infovis) is a powerful tool for exploring rich datasets. Within humanistic research, rich qualitative data and domain culture make traditional infovis approaches appear reductive and disconnected, leading to low adoption. In this paper, we use a multi-step approach to scrutinize the relationship between infovis and the humanities and suggest new directions for it. We first look into infovis from the humanistic perspective by exploring the humanistic literature around infovis. We validate and expand those findings though a co-design workshop with humanist and infovis experts. Then, we translate our findings into guidelines for designers and conduct a design critique exercise to explore their effect on the perception of humanist researchers. Based on these steps, we introduce Layers of Meaning, a framework to reduce the semantic distance between humanist researchers and visualizations of their research material, by grounding infovis tools in time and space, physicality, terminology, nuance, and provenance.", "keywords": "Infovis,Humanities,Digital Humanities", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030426", "refList": ["10.1177/1473871611416549", "10.1109/tvcg.2014.2346979", "10.1016/j.jesp.2013.03.013", "10.1109/tvcg.2018.2864909", "10.1145/2858036.2858465", "10.1145/1553374.1553527", "10.1037/1076-898x.14.1.36", "10.1109/tvcg.2011.185", "10.2200/s00371ed1v01y201107aim013", "10.1145/3025453.3025912", "10.3758/s13423-018-1525-7", "10.1109/tvcg.2017.2743898", "10.1037/a0029146", "10.1109/infvis.2005.1532136", "10.1109/tvcg.2015.2467671", "10.1109/tvcg.2018.2864907", "10.3758/s13423-016-1174-7", "10.1136/bmj.312.7047.1654", "10.2307/2288400", "10.1085/jgp.7.2.235", "10.3758/s13423-017-1323-7", "10.1109/tvcg.2018.2810918", "10.1080/13803390500205718", "10.1016/s0364-0213(87)80026-5", "10.1109/tvcg.2018.2864884", "10.2307/1419876", "10.1109/tvcg.2010.161", "10.5281/zenodo", "10.1109/tvcg.2005.63", "10.1177/001872089203400503", "10.1126/science.220.4598.671", "10.2307/2289447", "10.1006/jesp.1996.0009", "10.1007/s10898-012-9951-y", "10.3758/s13423-017-1343-3", "10.1109/tvcg.2007.70515", "10.21105/joss.01686", "10.18637/jss.v080.i01", "10.1111/j.1467-8659.2009.01694.x", "10.1145/2702123.2702608"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030421", "title": "LineSmooth: An Analytical Framework for Evaluating the Effectiveness of Smoothing Techniques on Line Charts", "year": "2020", "conferenceName": "VAST", "authors": "Paul Rosen;Ghulam Jilani Quadri", "citationCount": "0", "affiliation": "Rosen, P (Corresponding Author), Univ S Florida, Tampa, FL 33620 USA. Rosen, Paul; Quadri, Ghulam Jilani, Univ S Florida, Tampa, FL 33620 USA.", "countries": "USA", "abstract": "We present a comprehensive framework for evaluating line chart smoothing methods under a variety of visual analytics tasks. Line charts are commonly used to visualize a series of data samples. When the number of samples is large, or the data are noisy, smoothing can be applied to make the signal more apparent. However, there are a wide variety of smoothing techniques available, and the effectiveness of each depends upon both nature of the data and the visual analytics task at hand. To date, the visualization community lacks a summary work for analyzing and classifying the various smoothing methods available. In this paper, we establish a framework, based on 8 measures of the line smoothing effectiveness tied to 8 low-level visual analytics tasks. We then analyze 12 methods coming from 4 commonly used classes of line chart smoothing-rank filters, convolutional filters, frequency domain filters, and subsampling. The results show that while no method is ideal for all situations, certain methods, such as Gaussian filters and TOPOLOGY-based subsampling, perform well in general. Other methods, such as low-pass CUTOFF filters and Douglas-peucker subsampling, perform well for specific visual analytics tasks. Almost as importantly, our framework demonstrates that several methods, including the commonly used UNIFORM subsampling, produce low-quality results, and should, therefore, be avoided, if possible.", "keywords": "Line chart,data smoothing,time-series", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030421", "refList": ["10.1109/tvcg.2017.2744359", "10.1109/tip.2007.902329", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1002/acp.2350030302", "10.1109/tvcg.2018.2829750", "10.1080/03610927708827533", "10.1109/tvcg.2017.2653106", "10.3138/fm57-6770-u75u-7727", "10.2466/pms.104.3.707-721", "10.1016/s0146-664x(72", "10.1002/0471691852", "10.1093/ptj/74.8.768", "10.1016/j.rse.2015.12.023", "10.1145/2207676.2208556", "10.1109/tvcg.2018.2865264", "10.1109/tvcg.2018.2865077", "10.3758/bf03201236", "10.1145/3064175", "10.1145/2858036.2858300", "10.1109/infvis.2005.1532136", "10.1109/infvis.2005.1532144", "10.1021/ac60214a047", "10.1109/tvcg.2015.2467671", "10.1007/s11634-011-0102-y", "10.1109/tvcg.2017.2787113", "10.1002/cta.4490080205", "10.2312/evs.20201053", "10.1109/ncvpripg.2011.34", "10.1145/3025453.3025922", "10.1016/s0146-664x(72)80017-0", "10.1109/tvcg.2018.2864884", "10.1016/0734-189x(83)90054-3", "10.1109/tvcg.2013.234", "10.1109/tvcg.2016.2598592", "10.1109/tvcg.2010.162", "10.1109/jrproc.1949.232969", "10.1119/1.14057", "10.2307/2003354", "10.2312/evs", "10.1109/tvcg.2018.2864914", "10.1109/34.211471", "10.1145/2702123.2702608"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030352", "title": "StackGenVis: Alignment of Data, Algorithms, and Models for Stacking Ensemble Learning Using Performance Metrics", "year": "2020", "conferenceName": "VAST", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Kostiantyn Kucher;Andreas Kerren", "citationCount": "0", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Vaxjo, Sweden. Chatzimparmpas, Angelos; Martins, Rafael M.; Kucher, Kostiantyn; Kerren, Andreas, Linnaeus Univ, Vaxjo, Sweden.", "countries": "Sweden", "abstract": "In machine learning (ML), ensemble methods-such as bagging, boosting, and stacking-are widely-established approaches that regularly achieve top-notch predictive performance. Stacking (also called \u201cstacked generalization\u201d) is an ensemble method that combines heterogeneous base models, arranged in at least one layer, and then employs another metamodel to summarize the predictions of those models. Although it may be a highly-effective approach for increasing the predictive performance of ML, generating a stack of models from scratch can be a cumbersome trial-and-error process. This challenge stems from the enormous space of available solutions, with different sets of data instances and features that could be used for training, several algorithms to choose from, and instantiations of these algorithms using diverse parameters (i.e., models) that perform differently according to various metrics. In this work, we present a knowledge generation model, which supports ensemble learning with the use of visualization, and a visual analytics system for stacked generalization. Our system, StackGenVis, assists users in dynamically adapting performance metrics, managing data instances, selecting the most important features for a given data set, choosing a set of top-performant and diverse algorithms, and measuring the predictive performance. In consequence, our proposed tool helps users to decide between distinct models and to reduce the complexity of the resulting stack by removing overpromising and underperforming models. The applicability and effectiveness of StackGenVis are demonstrated with two use cases: a real-world healthcare data set and a collection of data related to sentiment/stance detection in texts. Finally, the tool has been evaluated through interviews with three ML experts.", "keywords": "Stacking,stacked generalization,ensemble learning,visual analytics,visualization", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030352", "refList": ["10.1109/tvcg.2017.2744359", "10.1109/tip.2007.902329", "10.1109/tvcg.2014.2346979", "10.1090/mbk/069", "10.1002/acp.2350030302", "10.1109/tvcg.2018.2829750", "10.1080/03610927708827533", "10.1109/tvcg.2017.2653106", "10.3138/fm57-6770-u75u-7727", "10.2466/pms.104.3.707-721", "10.1002/0471691852", "10.1093/ptj/74.8.768", "10.1016/j.rse.2015.12.023", "10.1145/2207676.2208556", "10.1109/tvcg.2018.2865264", "10.1109/tvcg.2018.2865077", "10.3758/bf03201236", "10.1145/2858036.2858300", "10.1109/infvis.2005.1532136", "10.1109/infvis.2005.1532144", "10.1021/ac60214a047", "10.1109/tvcg.2015.2467671", "10.1007/s11634-011-0102-y", "10.1109/tvcg.2017.2787113", "10.1002/cta.4490080205", "10.2312/evs.20201053", "10.1109/ncvpripg.2011.34", "10.1145/3025453.3025922", "10.1109/tvcg.2018.2864884", "10.1016/0734-189x(83)90054-3", "10.1109/tvcg.2013.234", "10.1109/tvcg.2016.2598592", "10.1109/tvcg.2010.162", "10.1109/jrproc.1949.232969", "10.1119/1.14057", "10.2307/2003354", "10.2312/evs", "10.1109/34.211471", "10.1145/2702123.2702608"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.1028", "year": "2020", "title": "A Radial Visualisation for Model Comparison and Feature Identification", "conferenceName": "PacificVis", "authors": "Jianlong Zhou;Weidong Huang;Fang Chen", "citationCount": "0", "affiliation": "Zhou, JL (Corresponding Author), Univ Technol Sydney, Data Sci Inst, Sydney, NSW, Australia.\nZhou, Jianlong; Chen, Fang, Univ Technol Sydney, Data Sci Inst, Sydney, NSW, Australia.\nHuang, Weidong, Univ Technol Sydney, Fac Transdisciplinary Innovat, Sydney, NSW, Australia.", "countries": "Australia", "abstract": "Machine Learning (ML) plays a key role in various intelligent systems, and building an effective ML model for a data set is a difficult task involving various steps including data cleaning, feature definition and extraction, ML algorithms development, model training and evaluation as well as others. One of the most important steps in the process is to compare generated substantial amounts of ML models to find the optimal one for the deployment. It is challenging to compare such models with dynamic number of features. This paper proposes a novel visualisation approach based on a radial net to compare ML models trained with a different number of features of a given data set while revealing implicit dependent relations. In the proposed approach, ML models and features are represented by lines and arcs respectively. The dependence of ML models with dynamic number of features is encoded into the structure of visualisation, where ML models and their dependent features are directly revealed from related line connections. ML model performance information is encoded with colour and line width in the innovative visualisation. Together with the structure of visualization, feature importance can be directly discerned to help to understand ML models.", "keywords": "Machine learning; performance; comparison; visualisation", "link": "https://doi.org/10.1109/PacificVis48177.2020.1028", "refList": ["10.1109/tvcg.2012.215", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2018.2864526", "10.1016/j.neucom.2011.10.038", "10.1063/1.3122936", "10.1145/2687924", "10.1109/tvcg.2009.23", "10.1016/j.compmedimag.2007.10.007", "10.1007/978-3-319-90403-0", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2018.2864499", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13709", "year": "2019", "title": "Designing Animated Transitions to Convey Aggregate Operations", "conferenceName": "EuroVis", "authors": "Younghoon Kim;Michael Correll;Jeffrey Heer", "citationCount": "1", "affiliation": "Kim, Y (Corresponding Author), Univ Washington, Seattle, WA 98195 USA.\nKim, Younghoon; Heer, Jeffrey, Univ Washington, Seattle, WA 98195 USA.\nCorrell, Michael, Tableau Res, Washington, DC USA.", "countries": "USA", "abstract": "Data can be aggregated in many ways before being visualized in charts, profoundly affecting what a chart conveys. Despite this importance, the type of aggregation is often communicated only via axis titles. In this paper, we investigate the use of animation to disambiguate different types of aggregation and communicate the meaning of aggregate operations. We present design rationales for animated transitions depicting aggregate operations and present the results of an experiment assessing the impact of these different transitions on identification tasks. We find that judiciously staged animated transitions can improve subjects' accuracy at identifying the aggregation performed, though sometimes with longer response times than with static transitions. Through an analysis of participants' rankings and qualitative responses, we find a consistent preference for animation over static transitions and highlight visual features subjects report relying on to make their judgments. We conclude by extending our animation designs to more complex charts of aggregated data such as box plots and bootstrapped confidence intervals.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13709", "refList": ["10.1111/cgf.13408", "10.1006/ijhc.1017", "10.1109/tvcg.2014.2346424", "10.1109/tvcg.2013.227", "10.1109/tvcg.2017.2785807", "10.1109/tvcg.2011.175", "10.1109/tvcg.2017.2750689", "10.1109/tvcg.2008.125", "10.1006/ijhc.2002.1017", "10.1109/tvcg.2011.185", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2018.2864884", "10.1006/s1045-926x(02)00028-9", "10.1109/infvis.1999.801854"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030396", "title": "What Makes a Data-GIF Understandable?", "year": "2020", "conferenceName": "InfoVis", "authors": "Xinhuan Shu;Aoyu Wu;Junxiu Tang;Benjamin Bach;Yingcai Wu;Huamin Qu", "citationCount": "1", "affiliation": "Wu, YC (Corresponding Author), Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Shu, Xinhuan; Tang, Junxiu; Wu, Yingcai, Zhejiang Univ, State Key Lab CAD\\&CG, Hangzhou, Peoples R China. Shu, Xinhuan; Wu, Aoyu; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Shu, Xinhuan, Zhejiang Univ, Hangzhou, Peoples R China. Bach, Benjamin, Univ Edinburgh, Edinburgh, Midlothian, Scotland.", "countries": "Scotland;China", "abstract": "GIFs are enjoying increasing popularity on social media as a format for data-driven storytelling with visualization; simple visual messages are embedded in short animations that usually last less than 15 seconds and are played in automatic repetition. In this paper, we ask the question, \u201cWhat makes a data-GIF understandable?\u201d While other storytelling formats such as data videos, infographics, or data comics are relatively well studied, we have little knowledge about the design factors and principles for \u201cdata-GIFs\u201d. To close this gap, we provide results from semi-structured interviews and an online study with a total of 118 participants investigating the impact of design decisions on the understandability of data-GIFs. The study and our consequent analysis are informed by a systematic review and structured design space of 108 data-GIFs that we found online. Our results show the impact of design dimensions from our design space such as animation encoding, context preservation, or repetition on viewers understanding of the GIF's core message. The paper concludes with a list of suggestions for creating more effective Data-GIFs.", "keywords": "Data-GIFs,Data-driven Storytelling,Evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030396", "refList": ["10.1109/tvcg.2016.2598647", "10.1016/j.visinf.2020.07.001", "10.1177/1473871615594652", "10.1109/tvcg.2014.2346424", "10.1111/cgf.13195", "10.1109/vlsicircuits18222.2020.9162811", "10.1109/tvcg.2018.2864909", "10.1109/tvcg.2018.2864903", "10.1145/2702123.2702431", "10.1016/j.visinf.2019.12.002", "10.1145/3274349", "10.1111/cgf.13444", "10.1145/3206505.3206552", "10.1145/3290605.3300280", "10.1109/jstqe.2020.3021589", "10.1145/2647868.2656408", "10.1006/ijhc.1017", "10.1145/3290605.3300335", "10.1007/s12650-020-00689-0", "10.1145/2818048.2819936", "10.1111/cgf.13325", "10.1145/2858036.2858387", "10.1145/3027063.3053139", "10.1145/3290605.3300474", "10.1109/tvcg.2016.2598920", "10.1109/tvcg.2018.2864899", "10.1145/3290605.3300483", "10.1145/3173574.3173612", "10.1145/2909132.2909255", "10.1109/tvcg.2016.2598620", "10.1016/j.learninstruc.2007.09.013", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2008.125", "10.1145/3173574.3173909", "10.1109/tvcg.2019.2934397", "10.1111/cgf.13709", "10.1109/tvcg.2013.234", "10.1109/tvcg.2019.2934401", "10.1145/2858036.2858532", "10.1016/j.visinf.2020.08.001", "10.1109/tvcg.2010.179", "10.1109/cicc48029.2020.9075900"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14005", "year": "2020", "title": "Canis: A High-Level Language for Data-Driven Chart Animations", "conferenceName": "EuroVis", "authors": "T. Ge;Y. Zhao;B. Lee;D. Ren;B. Chen;Y. Wang", "citationCount": "0", "affiliation": "Wang, Y (Corresponding Author), Shandong Univ, Qingdao, Peoples R China.\nGe, T.; Zhao, Y.; Wang, Y., Shandong Univ, Qingdao, Peoples R China.\nLee, B., Microsoft Res, Redmond, WA USA.\nRen, D., Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.\nChen, B., Peking Univ, Beijing, Peoples R China.", "countries": "USA;China", "abstract": "In this paper, we introduce Canis, a high-level domain-specific language that enables declarative specifications of data-driven chart animations. By leveraging data-enriched SVG charts, its grammar of animations can be applied to the charts created by existing chart construction tools. With Canis, designers can select marks from the charts, partition the selected marks into mark units based on data attributes, and apply animation effects to the mark units, with the control of when the effects start. The Canis compiler automatically synthesizes the Lottie animation JSON files {[}Aira], which can be rendered natively across multiple platforms. To demonstrate Canis' expressiveness, we present a wide range of chart animations. We also evaluate its scalability by showing the effectiveness of our compiler in reducing the output specification size and comparing its performance on different platforms against D3.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14005", "refList": ["10.1109/tvcg.2016.2598647", "10.1198/jcgs.2009.07098", "10.1111/cgf.13709", "10.1109/tvcg.2010.78", "10.1109/tvcg.2014.2346424", "10.1145/3173574.3173697", "10.1111/cgf.13178", "10.1006/ijhc.1017", "10.1145/3025453.3025942", "10.1145/2702123.2702431", "10.1109/tvcg.2008.125", "10.1006/ijhc.2002.1017", "10.1109/tvcg.2011.185", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2016.2599030.2", "10.1145/2642918.2647411", "10.1109/tvcg.2009.174"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.14002", "year": "2020", "title": "v-plots: Designing Hybrid Charts for the Comparative Analysis of Data Distributions", "conferenceName": "EuroVis", "authors": "Michael Blumenschein;Luka J. Debbeler;Nadine C. Lages;Britta Renner;Daniel A. Keim;Mennatallah El{-}Assady", "citationCount": "0", "affiliation": "Blumenschein, M (Corresponding Author), Univ Konstanz, Constance, Germany.\nBlumenschein, Michael; Debbeler, Luka J.; Lages, Nadine C.; Renner, Britta; Keim, Daniel A.; El-Assady, Mennatallah, Univ Konstanz, Constance, Germany.", "countries": "Germany", "abstract": "Comparing data distributions is a core focus in descriptive statistics, and part of most data analysis processes across disciplines. In particular, comparing distributions entails numerous tasks, ranging from identifying global distribution properties, comparing aggregated statistics (e.g., mean values), to the local inspection of single cases. While various specialized visualizations have been proposed (e.g., box plots, histograms, or violin plots), they are not usually designed to support more than a few tasks, unless they are combined. In this paper, we present the v-plot designer; a technique for authoring custom hybrid charts, combining mirrored bar charts, difference encodings, and violin-style plots. v-plots are customizable and enable the simultaneous comparison of data distributions on global, local, and aggregation levels. Our system design is grounded in an expert survey that compares and evaluates 20 common visualization techniques to derive guidelines for the task-driven selection of appropriate visualizations. This knowledge externalization step allowed us to develop a guiding wizard that can tailor v-plots to individual tasks and particular distribution properties. Finally, we confirm the usefulness of our system design and the userguiding process by measuring the fitness for purpose and applicability in a second study with four domain and statistic experts.", "keywords": "", "link": "https://doi.org/10.1111/cgf.14002", "refList": ["10.2307/2683468", "10.1186/1475-2891-12-1", "10.18637/jss.v028.c01", "10.1109/tvcg.2018.2865158", "10.1007/s00267-011-9692-6", "10.1177/1473871611416549", "10.1016/s0043-1354(98)00138-9", "10.1186/1471-2458-12-556", "10.1201/9781351072304", "10.1002/2014wr016367", "10.1201/9781315140919.2", "10.1111/cgf.12634", "10.1109/tvcg.2017.2744018", "10.1111/j.1467-8659.2009.01677.x", "10.1029/2007gl031764", "10.1145/3025453.3025912", "10.1109/tvcg.2018.2859973", "10.1016/j.scitotenv.2018.06.190", "10.1111/j.1752-1688.2011.00540.x", "10.1201/9781351072304.5", "10.1002/9781118575574", "10.2312/conf/eg2013/stars/039-063", "10.2307/2685478", "10.1109/tvcg.2018.2865240", "10.1177/0013916513515239", "10.1145/3125571.3125585", "10.1109/tvcg.2018.2864907", "10.1201/9781315140919", "10.1109/tvcg.2015.2467752", "10.1111/cgf.12391", "10.1109/tvcg.2015.2467091", "10.1109/tvcg.2018.2864884", "10.1016/j.jhydrol.2005.06.008", "10.2307/2682899", "10.2307/2685133", "10.1001/archpediatrics.2011.83", "10.1109/tvcg.2014.2346298", "10.1016/j.scitotenv.2009.06.031", "10.1109/tvcg.2015.2467191", "10.1016/b978-0-12-079050-0.50020-5", "10.1016/j.csda.2007.11.008", "10.1017/s1368980015003559", "10.1145/2669557.2669572", "10.1016/j.appet.2017.03.039"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030336", "title": "Visual cohort comparison for spatial single-cell omics-data", "year": "2020", "conferenceName": "VAST", "authors": "Antonios Somarakis;Marieke E. Ijsselsteijn;Sietse J. Luk;Boyd Kenkhuis;Noel F. C. C. de Miranda;Boudewijn P. F. Lelieveldt;Thomas H\u00f6llt", "citationCount": "0", "affiliation": "Hollt, T (Corresponding Author), Delft Univ Technol, Comp Graph \\& Visualizat Grp, Delft, Netherlands. Hollt, T (Corresponding Author), Leiden Univ, Med Ctr, Leiden Computat Biol Ctr, Leiden, Netherlands. Somarakis, Antonios; Lelieveldt, Boudewijn P. F., Leiden Univ, Med Ctr, Dept Radiol, Div Image Proc, Leiden, Netherlands. Ijsselsteijn, Marieke E.; de Miranda, Noel F. C. C., Leiden Univ, Med Ctr, Dept Pathol, Immunogen Grp, Leiden, Netherlands. Luk, Sietse J., Leiden Univ, Med Ctr, Hematol Dept, Leiden, Netherlands. Kenkhuis, Boyd, Leiden Univ, Med Ctr, Human Genet Dept, Leiden, Netherlands. Hollt, Thomas, Delft Univ Technol, Comp Graph \\& Visualizat Grp, Delft, Netherlands. Hollt, Thomas, Leiden Univ, Med Ctr, Leiden Computat Biol Ctr, Leiden, Netherlands.", "countries": "Netherlands", "abstract": "Spatially-resolved omics-data enable researchers to precisely distinguish cell types in tissue and explore their spatial interactions, enabling deep understanding of tissue functionality. To understand what causes or deteriorates a disease and identify related biomarkers, clinical researchers regularly perform large-scale cohort studies, requiring the comparison of such data at cellular level. In such studies, with little a-priori knowledge of what to expect in the data, explorative data analysis is a necessity. Here, we present an interactive visual analysis workflow for the comparison of cohorts of spatially-resolved omics-data. Our workflow allows the comparative analysis of two cohorts based on multiple levels-of-detail, from simple abundance of contained cell types over complex co-localization patterns to individual comparison of complete tissue images. As a result, the workflow enables the identification of cohort-differentiating features, as well as outlier samples at any stage of the workflow. During the development of the workflow, we continuously consulted with domain experts. To show the effectiveness of the workflow, we conducted multiple case studies with domain experts from different application areas and with different data modalities.", "keywords": "Visual analytics,Imaging Mass Cytometry,Vectra,spatially-resolved data,single-cell omics-data,Visual comparison", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030336", "refList": ["10.1038/nmeth.2869", "10.18637/jss.v028.c01", "10.1177/1473871611416549", "10.1016/j.celrep.2020.107523", "10.1602/neurorx.1.2.182", "10.1038/nprot.2014.191", "10.1136/jnnp-2011-300403", "10.1126/sciadv.aax5851", "10.2312/pe.vmv.vmv13.105-112", "10.1038/s43018-020-0031-9", "10.1109/tvcg.2013.213", "10.1093/jnci/92.8.613", "10.1109/52.329404", "10.1109/tvcg.2017.2785271", "10.1007/s11548-013-0820-z", "10.1109/tvcg.2019.2934547", "10.1002/cjp2.113", "10.1080/2162402x.2018.1507600", "10.1109/tvcg.2013.124", "10.1016/s0140-6736(14)60958-2", "10.1038/s41467-017-01689-9", "10.1016/j.cell.2018.07.010", "10.1007/978-3-319-24523-2", "10.1038/nrg3832", "10.2352/j.imagingsci.technol.2017.61.6.060404", "10.1109/tvcg.2018.2864907", "10.1111/cgf.13413", "10.1007/978-3-319-67979-2\\_4", "10.1145/2836034.2836040", "10.1038/nmeth.2563", "10.1101/2020.03.27.001834", "10.1016/0377-0427(87)90125-7", "10.1016/j.immuni.2016.04.014", "10.1002/cyto.a.22702", "10.12688/wellcomeopenres.15191.1", "10.1109/2945.981851", "10.1038/s43018-020-0026-6", "10.1038/s41586-019-1876-x", "10.1109/tvcg.2019.2931299", "10.1109/annes.1995.499469", "10.1145/2133806.2133821", "10.1109/tvcg.2013.161", "10.1007/978-3-319-24523-210", "10.1126/scitranslmed.3004330", "10.1038/nmeth.4391", "10.1111/cgf.14002", "10.1559/152304003100010929", "10.1126/science.280.5363.585", "10.1007/978-3-319-24523-2\\_10", "10.1109/tvcg.2016.2598587", "10.1038/s41597-019-0258-4"], "wos": 1, "children": [], "len": 1}], "len": 3}], "len": 45}, {"doi": "10.1109/tvcg.2018.2864886", "title": "MAQUI: Interweaving Queries and Pattern Mining for Recursive Event Sequence Exploration", "year": "2018", "conferenceName": "VAST", "authors": "Po-Ming Law;Zhicheng Liu;Sana Malik;Rahul C. Basole", "citationCount": "4", "affiliation": "Law, PM (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA. Law, Po-Ming; Basole, Rahul C., Georgia Inst Technol, Atlanta, GA 30332 USA. Liu, Zhicheng; Malik, Sana, Adobe Res, San Jose, CA USA.", "countries": "USA", "abstract": "Exploring event sequences by defining queries alone or by using mining algorithms alone is often not sufficient to support analysis. Analysts often interweave querying and mining in a recursive manner during event sequence analysis: sequences extracted as query results are used for mining patterns, patterns generated are incorporated into a new query for segmenting the sequences, and the resulting segments are mined or queried again. To support flexible analysis, we propose a framework that describes the process of interwoven querying and mining. Based on this framework, we developed MAQUI, a Mining And Querying User Interface that enables recursive event sequence exploration. To understand the efficacy of MAQUI, we conducted two case studies with domain experts. The findings suggest that the capability of interweaving querying and mining helps the participants articulate their questions and gain novel insights from their data.", "keywords": "Sequential pattern mining,temporal query,event sequence exploration", "link": "http://dx.doi.org/10.1109/TVCG.2018.2864886", "refList": ["10.1109/tvcg.2015.2467622", "10.1145/2470654.2481325", "10.1007/978-3-319-06483-3\\_8", "10.1177/1473871611416549", "10.1109/vast.2009.5332595", "10.1145/846183.846188", "10.1109/tvcg.2014.2346452", "10.1109/vast.2016.7883512", "10.1109/vast.2015.7347682", "10.1177/1473871614526077", "10.1016/j.jbi.2014.01.007", "10.1145/2856767.2856779", "10.1109/tvcg.2016.2598797", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2009.117", "10.1109/tvcg.2013.200", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2016.2539960", "10.1145/2557500.2557508", "10.1145/319950.320010", "10.1145/775047.775109", "10.1109/tvcg.2014.2346682", "10.1016/j.intcom.2012.01.003", "10.1016/j.jbi.2014.09.003", "10.1109/vlhcc.2009.5295262", "10.1109/tvcg.2017.2745083", "10.1007/s40273-015-0333-4", "10.1109/tvcg.2014.2346574", "10.1111/cgf.13208", "10.1145/3025453.3025777"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934264", "title": "The Validity, Generalizability and Feasibility of Summative Evaluation Methods in Visual Analytics", "year": "2019", "conferenceName": "VAST", "authors": "Mosab Khayat;Morteza Karimzadeh;David S. Ebert;Arif Ghafoor", "citationCount": "1", "affiliation": "Khayat, M (Corresponding Author), Purdue Univ, W Lafayette, IN 47907 USA. Khayat, Mosab; Karimzadeh, Morteza; Ebert, David S.; Ghafoor, Arif, Purdue Univ, W Lafayette, IN 47907 USA. Karimzadeh, Morteza, Univ Colorado, Boulder, CO 80309 USA.", "countries": "USA", "abstract": "Many evaluation methods have been used to assess the usefulness of Visual Analytics (VA) solutions. These methods stem from a variety of origins with different assumptions and goals, which cause confusion about their proofing capabilities. Moreover, the lack of discussion about the evaluation processes may limit our potential to develop new evaluation methods specialized for VA. In this paper, we present an analysis of evaluation methods that have been used to summatively evaluate VA solutions. We provide a survey and taxonomy of the evaluation methods that have appeared in the VAST literature in the past two years. We then analyze these methods in terms of validity and generalizability of their findings, as well as the feasibility of using them. We propose a new metric called summative quality to compare evaluation methods according to their ability to prove usefulness, and make recommendations for selecting evaluation methods based on their summative quality in the VA domain.", "keywords": "Summative evaluation,usefulness,evaluation process,taxonomy,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934264", "refList": ["10.1109/tvcg.2017.2744478", "10.1109/tvcg.2018.2865025", "10.1109/tvcg.2006.85", "10.1109/tvcg.2014.2346331", "10.1109/beliv.2018.8634420", "10.1109/tvcg.2017.2745181", "10.1111/cgf.13677", "10.1109/tvcg.2018.2864844", "10.1109/tvcg.2013.126", "10.1109/tvcg.2018.2864811", "10.1109/infvis.2005.1532147", "10.1177/0956797613504966", "10.1145/2669557.2669579", "10.1109/mcg.2005.102", "10.1109/visual.2003.1250426", "10.1136/bmj.39489.470347.ad", "10.1109/tvcg.2017.2744080", "10.1109/mcg.2009.53", "10.1111/j.1467-8527.2005.00307.x", "10.1109/tvcg.2010.132", "10.1109/tvcg.2018.2864886", "10.1109/tvcg.2018.2864843", "10.1109/tvcg.2018.2865028", "10.1109/tvcg.2018.2865051", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2018.2865026", "10.1007/978-3-540-71080-6\\_6", "10.1109/tvcg.2018.2865020", "10.1177/1473871611407399", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2018.2864526", "10.1109/tvcg.2005.53", "10.1109/tvcg.2018.2864905", "10.1049/sej.1991.0040", "10.1109/tvcg.2015.2513410", "10.1109/tvcg.2017.2711030", "10.1109/tvcg.2011.279", "10.1109/vast.2017.8585505", "10.1147/jrd.2010.2042914", "10.1016/s0378-7206(98)00044-5", "10.1145/2993901.2993913", "10.1109/tvcg.2018.2865041", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2017.2744758", "10.1145/1168149.1168158", "10.1109/tvcg.2017.2745279", "10.1109/tvcg.2012.213", "10.1109/tvcg.2017.2744738", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2018.2864826", "10.1145/1377966.1377974", "10.1109/apec.2009.4802646", "10.1145/1168149.1168152", "10.1016/j.jss.2008.03.059", "10.1109/vast.2017.8585484", "10.1109/tvcg.2017.2744818", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2865042", "10.1109/tvcg.2017.2744898"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030388", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "year": "2020", "conferenceName": "SciVis", "authors": "Pepe Eulzer;Sabine Bauer;Francis Kilian;Kai Lawonn", "citationCount": "0", "affiliation": "Eulzer, P (Corresponding Author), Univ Jena, Jena, Germany. Eulzer, Pepe; Lawonn, Kai, Univ Jena, Jena, Germany. Bauer, Sabine, Univ Koblenz Landau, Koblenz, Germany. Kilian, Francis, Cath Clin Koblenz Montabaur, Dept Spine Surg, Koblenz, Germany.", "countries": "Germany", "abstract": "We propose a visualization application, designed for the exploration of human spine simulation data. Our goal is to support research in biomechanical spine simulation and advance efforts to implement simulation-backed analysis in surgical applications. Biomechanical simulation is a state-of-the-art technique for analyzing load distributions of spinal structures. Through the inclusion of patient-specific data, such simulations may facilitate personalized treatment and customized surgical interventions. Difficulties in spine modelling and simulation can be partly attributed to poor result representation, which may also be a hindrance when introducing such techniques into a clinical environment. Comparisons of measurements across multiple similar anatomical structures and the integration of temporal data make commonly available diagrams and charts insufficient for an intuitive and systematic display of results. Therefore, we facilitate methods such as multiple coordinated views, abstraction and focus and context to display simulation outcomes in a dedicated tool. $\\mathrm{By}$ linking the result data with patient-specific anatomy, we make relevant parameters tangible for clinicians. Furthermore, we introduce new concepts to show the directions of impact force vectors, which were not accessible before. We integrated our toolset into a spine segmentation and simulation pipeline and evaluated our methods with both surgeons and biomechanical researchers. When comparing our methods against standard representations that are currently in use, we found increases in accuracy and speed in data exploration tasks. $\\mathrm{in}$ a qualitative review, domain experts deemed the tool highly useful when dealing with simulation result data, which typically combines time-dependent patient movement and the resulting force distributions on spinal structures.", "keywords": "Medical visualization,bioinformatics,coordinated views,focus and context,biomechanical simulation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030388", "refList": ["10.1109/tvcg.2018.2864903", "10.1177/1473871613510429", "10.1093/ehjqcco/qcz052", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2018.2865076", "10.1055/s-0039-1687862", "10.1109/visual.1990.146375", "10.1109/tvcg.2017.2744198", "10.1016/j.ijmedinf.2014.10.001", "10.1109/tvcg.2013.124", "10.1016/j.jacc", "10.1111/cgf.13167", "10.17705/1thci.00055", "10.1136/bmjqs.2009.037895", "10.1109/tvcg.2013.238", "10.1109/tvcg.2018.2865240", "10.1186/1471-2261-6-34", "10.1109/tvcg.2019.2934264", "10.1109/tvcg.2013.200", "10.1109/tvcg.2011.209", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467091", "10.1136/bmjopen-2019-033208", "10.1109/beliv.2018.8634027", "10.1109/tvcg.2012.213", "10.1109/tvcg.2015.2467191", "10.1109/tvcg.2015.2467325", "10.1145/2133806.2133821", "10.1145/1806799.1806866", "10.1108/02635570610688869", "10.1002/hbm.20701", "10.1561/1100000039", "10.1145/3025453.3025645", "10.1109/tvcg.2009.111", "10.1109/tvcg.2013.120", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2019.2934661", "title": "Visual Analysis of High-Dimensional Event Sequence Data via Dynamic Hierarchical Aggregation", "year": "2019", "conferenceName": "VAST", "authors": "David Gotz;Jonathan Zhang;Wenyuan Wang;Joshua Shrestha;David Borland", "citationCount": "3", "affiliation": "Gotz, D (Corresponding Author), Univ N Carolina, Sch Informat \\& Lib Sci, Chapel Hill, NC 27515 USA. Gotz, David; Wang, Wenyuan, Univ N Carolina, Sch Informat \\& Lib Sci, Chapel Hill, NC 27515 USA. Zhang, Jonathan, Univ N Carolina, Dept Biostat, Chapel Hill, NC 27515 USA. Shrestha, Joshua, Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27515 USA. Borland, David, Univ N Carolina, RENCI, Chapel Hill, NC 27515 USA.", "countries": "USA", "abstract": "Temporal event data are collected across a broad range of domains, and a variety of visual analytics techniques have been developed to empower analysts working with this form of data. These techniques generally display aggregate statistics computed over sets of event sequences that share common patterns. Such techniques are often hindered, however, by the high-dimensionality of many real-world event sequence datasets which can prevent effective aggregation. A common coping strategy for this challenge is to group event types together prior to visualization, as a pre-process, so that each group can be represented within an analysis as a single event type. However, computing these event groupings as a pre-process also places significant constraints on the analysis. This paper presents a new visual analytics approach for dynamic hierarchical dimension aggregation. The approach leverages a predefined hierarchy of dimensions to computationally quantify the informativeness, with respect to a measure of interest, of alternative levels of grouping within the hierarchy at runtime. This information is then interactively visualized, enabling users to dynamically explore the hierarchy to select the most appropriate level of grouping to use at any individual step within an analysis. Key contributions include an algorithm for interactively determining the most informative set of event groupings for a specific analysis context, and a scented scatter-plus-focus visualization design with an optimization-based layout algorithm that supports interactive hierarchical exploration of alternative event type groupings. We apply these techniques to high-dimensional event sequence data from the medical domain and report findings from domain expert interviews.", "keywords": "Temporal event sequence visualization,visual analytics,hierarchical aggregation,medical informatics", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934661", "refList": ["10.1145/937549.937550", "10.1109/infvis.2000.885091", "10.1145/2468356.2468434", "10.1016/j.jbi.2012.01.009", "10.1136/amiajnl-2014-002747", "10.1109/tvcg.2009.108", "10.1145/2702123.2702419", "10.3233/978-1-61499-289-9-1224", "10.13063/2327-9214", "10.1109/vast.2016.7883512", "10.1111/j.1467-8659.2011.01898.x", "10.1109/infvis.2005.1532152", "10.1177/1473871614526077", "10.1016/j.jbi.2014.01.007", "10.1145/2856767.2856779", "10.1038/nrg3208", "10.1001/jama.2014.4228", "10.1109/tvcg.2012.238", "10.1109/tvcg.2017.2745278", "10.1109/tvcg.2018.2864886", "10.2307/2983604", "10.1145/3009973", "10.1109/tvcg.2013.200", "10.1109/tvcg.2016.2539960", "10.1200/jco.2010.28.5478", "10.1145/2557500.2557508", "10.1109/tvcg.2014.2346682", "10.1111/cgf.12883", "10.1145/2678025.2701407", "10.1109/vast.2011.6102443", "10.1109/tvcg.2009.84", "10.1109/tvcg.2007.70589", "10.1109/tvcg.2017.2745320", "10.3233/978-1-61499-830-3-1327", "10.1136/jamia.2009.000893", "10.1186/2047-2501-2-3", "10.1145/2890478", "10.2307/2685881", "10.1109/vast.2014.7042487", "10.1561/1100000039", "10.1109/mcg.2016.59", "10.1109/tvcg.2017.2744686", "10.1109/vds.2017.8573439", "10.1016/j.otohns.2010.05.007", "10.1109/tvcg.2014.2346433"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2018.2865141", "title": "A Declarative Rendering Model for Multiclass Density Maps", "year": "2018", "conferenceName": "InfoVis", "authors": "Jaemin Jo;Fr\u00e9d\u00e9ric Vernier;Pierre Dragicevic;Jean-Daniel Fekete", "citationCount": "9", "affiliation": "Jo, J (Corresponding Author), Seoul Natl Univ, Seoul, South Korea. Jo, Jaemin, Seoul Natl Univ, Seoul, South Korea. Vernier, Frederic, Univ Paris Saclay, Univ Paris Sud, CNRS, LIMSI, Paris, France. Dragicevic, Pierre; Fekete, Jean-Daniel, INRIA, Rocquencourt, France.", "countries": "Korea;France", "abstract": "Multiclass maps are scatterplots, multidimensional projections, or thematic geographic maps where data points have a categorical attribute in addition to two quantitative attributes. This categorical attribute is often rendered using shape or color, which does not scale when overplotting occurs. When the number of data points increases, multiclass maps must resort to data aggregation to remain readable. We present multiclass density maps: multiple 2D histograms computed for each of the category values. Multiclass density maps are meant as a building block to improve the expressiveness and scalability of multiclass map visualization. In this article, we first present a short survey of aggregated multiclass maps, mainly from cartography. We then introduce a declarative model-a simple yet expressive JSON grammar associated with visual semantics-that specifies a wide design space of visualizations for multiclass density maps. Our declarative model is expressive and can be efficiently implemented in visualization front-ends such as modern web browsers. Furthermore, it can be reconfigured dynamically to support data exploration tasks without recomputing the raw data. Finally, we demonstrate how our model can be used to reproduce examples from the past and support exploring data at scale.", "keywords": "Scalability,multiclass scatterplots,density maps,aggregation,declarative specification,visualization grammar", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865141", "refList": ["10.1057/palgrave.ivs.9500122", "10.2307/2980460", "10.2307/2683294", "10.1111/cgf.12129", "10.1109/tvcg.2013.65", "10.1109/tvcg.2011.185", "10.1111/cgf.12878", "10.1109/tvcg.2009.175", "10.1145/2669557.2669559", "10.1145/3173574.3173991", "10.1109/pacificvis.2016.7465244", "10.1109/infvis.2002.1173156", "10.1117/12.2041200", "10.1179/000870403235002042", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2017.2674978", "10.1109/tvcg.2013.130", "10.1145/2872427.2883041", "10.1145/2669557.2669578", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2013.179", "10.1109/tvcg.2011.197", "10.1145/1476589.1476628", "10.1559/152304000783547786", "10.1037/xhp0000255", "10.1080/10618600.1994.10474656", "10.1145/2702123.2702585", "10.3138/cart.52.4.2016-0007", "10.1057/ivs.2009.34", "10.1109/tvcg.2007.70623"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934807", "title": "A Comparison of Visualizations for Identifying Correlation over Space and Time", "year": "2019", "conferenceName": "InfoVis", "authors": "Vanessa Pe\u00f1a Araya;Emmanuel Pietriga;Anastasia Bezerianos", "citationCount": "0", "affiliation": "Pena-Araya, V (Corresponding Author), Univ Paris Saclay, Univ Paris Sud, INRIA, CNRS, Paris, France. Pena-Araya, Vanessa; Pietriga, Emmanuel; Bezerianos, Anastasia, Univ Paris Saclay, Univ Paris Sud, INRIA, CNRS, Paris, France.", "countries": "France", "abstract": "Observing the relationship between two or more variables over space and time is essential in many domains. For instance, looking, for different countries, at the evolution of both the life expectancy at birth and the fertility rate will give an overview of their demographics. The choice of visual representation for such multivariate data is key to enabling analysts to extract patterns and trends. Prior work has compared geo-temporal visualization techniques for a single thematic variable that evolves over space and time, or for two variables at a specific point in time. But how effective visualization techniques are at communicating correlation between two variables that evolve over space and time remains to be investigated. We report on a study comparing three techniques that are representative of different strategies to visualize geo-temporal multivariate data: either juxtaposing all locations for a given time step, or juxtaposing all time steps for a given location; and encoding thematic attributes either using symbols overlaid on top of map features, or using visual channels of the map features themselves. Participants performed a series of tasks that required them to identify if two variables were correlated over time and if there was a pattern in their evolution. Tasks varied in granularity for both dimensions: time (all time steps, a subrange of steps, one step only) and space (all locations, locations in a subregion, one location only). Our results show that a visualization's effectiveness depends strongly on the task to be carried out. Based on these findings we present a set of design guidelines about geo-temporal visualization techniques for communicating correlation.", "keywords": "geo-temporal data,bivariate maps,correlation,controlled study,bar chart,Dorling cartogram,small multiples", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934807", "refList": ["10.1111/j.1467-9671.2010.01194.x", "10.3390/ijgi2030817", "10.1109/tvcg.2018.2865141", "10.3390/ijgi7080288", "10.1080/00045608.2010.485449", "10.1109/pacificvis.2014.13", "10.1109/tvcg.2014.2346979", "10.1080/00045608.2015.1064510", "10.1111/cgf.12647", "10.1111/j.1467-8659.2009.01694.x", "10.1109/mcg.2003.1242376", "10.1080/09585192.2016.1278253", "10.1177/0956797613504966", "10.1109/tvcg.2011.185", "10.1006/ijhc.2002.1017", "10.1111/j.1467-8306.1994.tb01869.x", "10.1080/00045608.2011.577364", "10.1109/tvcg.2007.70623", "10.1111/cgf.12932", "10.1109/iv.2014.69", "10.1006/ijhc.1017", "10.1109/iv.2004.1320136", "10.1145/2801040.2801062", "10.1007/s00371-017-1461-y", "10.1111/j.1467-8659.2012.03093.x", "10.1109/tvcg.2013.66", "10.1007/s00779-018-1120-y", "10.1109/tvcg.2015.2467199", "10.1109/tvcg.2016.2642109", "10.1109/tvcg.2015.2467671", "10.1109/tvcg.2016.2598862", "10.1109/tvcg.2011.194", "10.3758/s13423-016-1174-7", "10.1109/tvcg.2017.2765330", "10.1109/iv.2004.1320137", "10.1038/nmeth.2659", "10.2312/cgvc.20181221", "10.1109/tvcg.2013.130", "10.3138/carto.42.4.349", "10.1111/j.1467-8306.2006.00514.x", "10.1117/12.912192", "10.1109/tvcg.2015.2467091", "10.1109/iv.2005.3", "10.1559/15230406384350", "10.1145/2909132.2909255", "10.1109/iv.2018.00056", "10.1109/tvcg.2008.125", "10.1109/2945.537309", "10.1109/tvcg.2018.2810918", "10.3390/ijgi6060180", "10.1145/3025453.3025801", "10.1080/14498596.2018.1440649", "10.1179/000870409x12525737905169", "10.2307/2284077", "10.1109/tvcg.2006.84", "10.1002/9780470979587.ch33", "10.1007/978-3-319-26633-6\\_13", "10.1136/bmj.316.7139.1236", "10.1145/2801040.2801061", "10.1037/0003-066x.60.2.170", "10.1145/989863.989940"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934811", "title": "Winglets: Visualizing Association with Uncertainty in Multi-class Scatterplots", "year": "2019", "conferenceName": "InfoVis", "authors": "Min Lu;Shuaiqi Wang;Joel Lanir;Noa Fish;Yang Yue;Daniel Cohen-Or;Hui Huang 0004", "citationCount": "1", "affiliation": "Huang, H (Corresponding Author), Shenzhen Univ, Shenzhen, Peoples R China. Lu, Min; Wang, Shuaiqi; Yue, Yang; Cohen-Or, Daniel; Huang, Hui, Shenzhen Univ, Shenzhen, Peoples R China. Lanir, Joel, Univ Haifa, Haifa, Israel. Fish, Noa, Tel Aviv Univ, Tel Aviv, Israel.", "countries": "Israel;China", "abstract": "This work proposes Winglets, an enhancement to the classic scatterplot to better perceptually pronounce multiple classes by improving the perception of association and uncertainty of points to their related cluster. Designed as a pair of dual-sided strokes belonging to a data point, Winglets leverage the Gestalt principle of Closure to shape the perception of the form of the clusters, rather than use an explicit divisive encoding. Through a subtle design of two dominant attributes, length and orientation, Winglets enable viewers to perform a mental completion of the clusters. A controlled user study was conducted to examine the efficiency of Winglets in perceiving the cluster association and the uncertainty of certain points. The results show Winglets form a more prominent association of points into clusters and improve the perception of associating uncertainty.", "keywords": "Scatterplot,Gestalt laws,Association,Uncertainty", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934811", "refList": ["10.1109/tvcg.2018.2865141", "10.1214/aoms/1177728190", "10.1111/j.1467-8659.2009.01467.x", "10.2307/2289444", "10.1109/tvcg.2013.65", "10.1037/a0029334", "10.1109/vast.2009.5332628", "10.1007/bf00410640", "10.1109/vast.2010.5652460", "10.1145/2590349", "10.1109/pacificvis.2010.5429604", "10.1109/tvcg.2013.20", "10.1002/9781118575574", "10.2312/conf/eg2013/stars/039-063", "10.1109/tvcg.2009.122", "10.1109/tvcg.2017.2750689", "10.1023/a:1024454423670", "10.1515/secm-2017-0114", "10.1016/0377-0427(87)90125-7", "10.1016/0010-0285(92)90014-s", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2017.2744184", "10.1016/b978-044481862-1.50075-3", "10.1007/s10851-006-9176-0", "10.1145/1842993.1842999", "10.1145/37402.37422", "10.1037/0033-295x.115.1.131", "10.1109/tvcg.2013.153", "10.1007/978-3-642-04898-2\\_455", "10.1145/2702123.2702585", "10.1109/tvcg.2013.183", "10.1559/152304003100010929", "10.1057/ivs.2009.34"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030373", "title": "Multi-Perspective, Simultaneous Embedding", "year": "2020", "conferenceName": "InfoVis", "authors": "Md. Iqbal Hossain;Vahan Huroyan;Stephen G. Kobourov;Raymundo Navarrete", "citationCount": "0", "affiliation": "Hossain, MI (Corresponding Author), Univ Arizona, Dept Comp Sci, Tucson, AZ 85721 USA. Hossain, Md Iqbal; Kobourov, Stephen, Univ Arizona, Dept Comp Sci, Tucson, AZ 85721 USA. Huroyan, Vahan; Navarrete, Raymundo, Univ Arizona, Dept Math, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "We describe MPSE: a Multi-Perspective Simultaneous Embedding method for visualizing high-dimensional data, based on multiple pairwise distances between the data points. Specifically, MPSE computes positions for the points in 3D and provides different views into the data by means of 2D projections (planes) that preserve each of the given distance matrices. We consider two versions of the problem: fixed projections and variable projections. MPSE with fixed projections takes as input a set of pairwise distance matrices defined on the data points, along with the same number of projections and embeds the points in 3D so that the pairwise distances are preserved in the given projections. MPSE with variable projections takes as input a set of pairwise distance matrices and embeds the points in 3D while also computing the appropriate projections that preserve the pairwise distances. The proposed approach can be useful in multiple scenarios: from creating simultaneous embedding of multiple graphs on the same set of vertices, to reconstructing a 3D object from multiple 2D snapshots, to analyzing data from multiple points of view. We provide a functional prototype of MPSE that is based on an adaptive and stochastic generalization of multi-dimensional scaling to multiple distances and multiple variable projections. We provide an extensive quantitative evaluation with datasets of different sizes and using different number of projections, as well as several examples that illustrate the quality of the resulting solutions.", "keywords": "Graph visualization,Dimensionality reduction,Multidimensional scaling,Mental map preservation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030373", "refList": ["10.1016/j.cag.2014.01.006", "10.1109/tvcg.2007.70443", "10.1007/978-3-642-04843-2\\_36", "10.1111/cgf.12639", "10.1109/tvcg.2019.2944182", "10.1109/tvcg.2019.2934811", "10.1007/978-1-4757-2261-1", "10.1145/3173574.3174209", "10.1016/j.neucom.2006.11.018", "10.1109/tpami.2013.57", "10.2307/2334381", "10.1109/tpami.2012.88", "10.1109/tvcg.2017.2661309", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1111/j.1467-8659.2010.01835.x", "10.1109/vl.1996.545307", "10.1109/iv.2009.25", "10.1109/msp.2010.939739", "10.1063/1.3067728", "10.2312/pe/eurovast/eurovast10/013-018", "10.1109/vast.2010.5652460", "10.25365/thesis.51534", "10.21105/joss.00861", "10.1145/1007730.1007731", "10.1109/tvcg.2013.20", "10.1109/tvcg.2011.220", "10.1037/h0071325", "10.1109/vast.2012.6400488", "10.1109/tvcg.2007.70535", "10.1198/jasa.2009.0111", "10.1109/tvcg.2016.2598495", "10.1109/tvcg.2017.2698041", "10.1109/tvcg.2015.2467717", "10.1111/j", "10.1016/j.neucom.2014.07.071", "10.1126/science.290.5500.2323", "10.1109/tvcg.2013.153", "10.2312/pe/eurovast", "10.1109/tvcg.2018.2846735", "10.1007/bf02289530", "10.1111/j.1745-3984.2003.tb01108.x", "10.1177/1473871615600010", "10.5555/3053814.3053816", "10.1109/tvcg.2018.2865194"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2020.3030430", "title": "Competing Models: Inferring Exploration Patterns and Information Relevance via Bayesian Model Selection", "year": "2020", "conferenceName": "VAST", "authors": "Shayan Monadjemi;Roman Garnett;Alvitta Ottley", "citationCount": "0", "affiliation": "Monadjemi, S (Corresponding Author), Washington Univ, St Louis, MO 14263 USA. Monadjemi, Shayan; Garnett, Roman; Ottley, Alvitta, Washington Univ, St Louis, MO 14263 USA.", "countries": "USA", "abstract": "Analyzing interaction data provides an opportunity to learn about users, uncover their underlying goals, and create intelligent visualization systems. The first step for intelligent response in visualizations is to enable computers to infer user goals and strategies through observing their interactions with a system. Researchers have proposed multiple techniques to model users, however, their frameworks often depend on the visualization design, interaction space, and dataset. Due to these dependencies, many techniques do not provide a general algorithmic solution to user exploration modeling. In this paper, we construct a series of models based on the dataset and pose user exploration modeling as a Bayesian model selection problem where we maintain a belief over numerous competing models that could explain user interactions. Each of these competing models represent an exploration strategy the user could adopt during a session. The goal of our technique is to make high-level and in-depth inferences about the user by observing their low-level interactions. Although our proposed idea is applicable to various probabilistic model spaces, we demonstrate a specific instance of encoding exploration patterns as competing models to infer information relevance. We validate our technique's ability to infer exploration bias, predict future interactions, and summarize an analytic session using user study datasets. Our results indicate that depending on the application, our method outperforms established baselines for bias detection and future interaction prediction. Finally, we discuss future research directions based on our proposed modeling paradigm and suggest how practitioners can use this method to build intelligent visualization systems that understand users' goals and adapt to improve the exploration process.", "keywords": "User Interaction Modeling,Bayesian Machine Learning", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030430", "refList": ["10.1109/tvcg.2018.2865141", "10.1145/355588.365140", "10.1007/978-0-387-98141-3\\_1", "10.1145/2882903.2882919", "10.1111/cgf.13708", "10.1145/3183713.3183738", "10.1111/cgf.12129", "10.1109/tvcg.2014.2346452", "10.1109/tvcg.2013.65", "10.1109/tvcg.2011.185", "10.1109/infvis.2003.1249018", "10.1109/tvcg.2017.2671341", "10.1109/infvis.2002.1173156", "10.1109/38.974515", "10.1109/tvcg.2016.2598624", "10.1109/infvis.2003.1249019", "10.1145/1556262.1556289", "10.1109/tvcg.2007.70535", "10.1109/tvcg.2006.161", "10.1109/ldav.2015.7348077", "10.1109/iv.2004.1320190", "10.1145/3139958.3140037", "10.1109/vlhcc.2005.11", "10.1145/98524.98564", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2009.84", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2013.179", "10.1016/s0097-8493(02)00283-2", "10.14778/2428536.2428538", "10.1109/icde.2014.6816720", "10.1109/tvcg.2017.2785807", "10.1198/jcgs.2009.07098", "10.1109/tvcg.2017.2754480"], "wos": 1, "children": [], "len": 1}], "len": 9}, {"doi": "10.1109/vast.2018.8802509", "title": "Analyzing the Noise Robustness of Deep Neural Networks", "year": "2018", "conferenceName": "VAST", "authors": "Mengchen Liu;Shixia Liu;Hang Su 0006;Kelei Cao;Jun Zhu", "citationCount": "9", "affiliation": "Liu, SX (Corresponding Author), Tsinghua Univ, State Key Lab Intell Tech Sys, TNList Lab, Sch Software, Beijing, Peoples R China. Liu, Mengchen; Liu, Shixia; Cao, Kelei, Tsinghua Univ, State Key Lab Intell Tech Sys, TNList Lab, Sch Software, Beijing, Peoples R China. Su, Hang; Zhu, Jun, Tsinghua Univ, CBICR Ctr, State Key Lab Intell Tech Sys, Dept Comp Sci Tech,TNList Lab, Beijing, Peoples R China.", "countries": "China", "abstract": "Deep neural networks (DNNs) are vulnerable to maliciously generated adversarial examples. These examples are intentionally designed by making imperceptible perturbations and often mislead a DNN into making an incorrect prediction. This phenomenon means that there is significant risk in applying DNNs to safety-critical applications, such as driverless cars. To address this issue, we present a visual analytics approach to explain the primary cause of the wrong predictions introduced by adversarial examples. The key is to analyze the datapaths of the adversarial examples and compare them with those of the normal examples. A datapath is a group of critical neurons and their connections. To this end, we formulate the datapath extraction as a subset selection problem and approximately solve it based on back-propagation. A multi-level visualization consisting of a segmented DAG (layer level), an Euler diagram (feature map level), and a heat map (neuron level), has been designed to help experts investigate datapaths from the high-level layers to the detailed neuron activations. Two case studies are conducted that demonstrate the promise of our approach in support of explaining the working mechanism of adversarial examples.", "keywords": "Deep neural networks,robustness,adversarial examples,back propagation,multi-level visualization.", "link": "http://dx.doi.org/10.1109/VAST.2018.8802509", "refList": ["10.1109/tvcg.2011.183", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2598838", "10.2312/eurovisstar.20141170", "10.1006/cogp.2001.0755", "10.1109/tvcg.2017.2744938", "10.1007/s00371-013-0892-3", "10.1109/cvpr.2015.7298640", "10.1109/tvcg.2017.2744018", "10.1109/cvpr.2015.7298594", "10.1109/tvcg.2010.210", "10.1007/s11263-015-0816-y", "10.2307/2686111", "10.1007/978-3-319-27857-5\\_77", "10.1109/tvcg.2016.2515592", "10.23915/distill.00010", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2016.2598831", "10.1109/tvcg.2011.209", "10.1007/978-3-319-10590-1\\_53", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2014.2346431", "10.1109/cvpr.2017.17", "10.1016/0010-0285(92)90010-y", "10.1109/cvpr.2016.485", "10.1109/tvcg.2013.196", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2017.2744718", "10.1109/tsmc.1981.4308636", "10.1109/cvpr.2016.308", "10.1016/j.intcom.2007.05.004", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2011.186", "10.1109/cvpr.2016.90", "10.1109/tvcg.2016.2598496", "10.1364/boe.8.000579", "10.1109/cvpr.2016.282", "10.1109/tpami.2013.50", "10.1109/cvpr.2017.243", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2014.2346433", "10.1109/tvcg.2016.2614803"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028888", "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes", "year": "2020", "conferenceName": "VAST", "authors": "Yuxin Ma;Arlen Fan;Jingrui He;Arun Reddy Nelakurthi;Ross Maciejewski", "citationCount": "0", "affiliation": "Ma, YX (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Ma, Yuxin; Fan, Arlen; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. He, Jingrui, Univ Illinois, Champaign, IL USA. Nelakurthi, Arun Reddy, Samsung Res Amer, Mountain View, CA USA.", "countries": "USA", "abstract": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.", "keywords": "Transfer learning,deep learning,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028888", "refList": ["10.1109/tvcg.2014.2346578", "10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1109/tpami.2018.2868685", "10.1145/2702123.2702509", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2016.2598828", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2013.65", "10.1145/2976749.2978318", "10.1007/978-3-030-01424-7\\_27", "10.1109/tvcg.2019.2934261", "10.1007/s11704-016-6028-y", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/iccv.2015.279", "10.1109/mci.2018.2840738", "10.1109/tvcg.2019.2892483", "10.1109/vast.2018.8802509", "10.1109/tvcg.2013.124", "10.1186/s40537-016-0043-6", "10.1109/tvcg.2018.2864475", "10.1145/3200489", "10.1109/tvcg.2018.2865044", "10.1111/cgf.13210", "10.1109/tvcg.2018.2816223", "10.23915/distill.00007", "10.1109/tvcg.2017.2744199", "10.1109/tkde.2018.2876857", "10.1109/tvcg.2019.2934631", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2011.188", "10.1007/978-3-642-15561-1\\_16", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1109/tvcg.2017.2744718", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2016.2598541", "10.1109/tkde.2009.191", "10.1145/3065386", "10.1016/j.ins.2016.03.021", "10.1109/tvcg.2019.2903943", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/iccv.2017.74", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2012.207", "10.1111/cgf.13092", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2017.2754480", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030418", "title": "CNN Explainer: Learning Convolutional Neural Networks with Interactive Visualization", "year": "2020", "conferenceName": "VAST", "authors": "Zijie J. Wang;Robert Turko;Omar Shaikh;Haekyu Park;Nilaksh Das;Fred Hohman;Minsuk Kahng;Duen Horng Chau", "citationCount": "0", "affiliation": "Wang, ZJ (Corresponding Author), Georgia Tech, Atlanta, GA 30332 USA. Wang, Zijie J.; Turko, Robert; Shaikh, Omar; Park, Haekyu; Das, Nilaksh; Hohman, Fred; Chau, Duen Horng (Polo), Georgia Tech, Atlanta, GA 30332 USA. Kahng, Minsuk, Oregon State Univ, Corvallis, OR 97331 USA.", "countries": "USA", "abstract": "Deep learning's great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN's structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern deep learning techniques.", "keywords": "Deep learning,machine learning,convolutional neural networks,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030418", "refList": ["10.1016/j.cag.2018.09.018", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2017.2744938", "10.1037/0022-0663.83.4.484", "10.1016/j.patcog.2017.10.013", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2011.185", "10.1016/s0360-1315(99)00023-8", "10.1080/07380569.2012.651422", "10.1006/s1045-926x(02)00027-7", "10.1145/1821996.1821997", "10.1109/vast.2018.8802509", "10.1109/vl.2000.874346", "10.1007/978-3-319-27857-5\\_77", "10.1145/1227504.1227384", "10.1038/nature14539", "10.1109/tvcg.2018.2816223", "10.23915/distill.00016", "10.1162/neco.1997.9.8.1735", "10.1109/tvcg.2016.2640960", "10.1006/ijhc.2000.0409", "10.1109/tvcg.2017.2744718", "10.1006/s1045-926x(02)00028-9", "10.1111/cgf.13720", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1145/782941.782998", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 11}, {"doi": "10.1109/tvcg.2019.2934261", "title": "Ablate, Variate, and Contemplate: Visual Analytics for Discovering Neural Architectures", "year": "2019", "conferenceName": "VAST", "authors": "Dylan Cashman;Adam Perer;Remco Chang;Hendrik Strobelt", "citationCount": "1", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Perer, Adam, Carnegie Mellon Univ, Pittsburgh, PA 15213 USA. Strobelt, Hendrik, MIT IBM Watson AI Lab, Cambridge, MA USA.", "countries": "USA", "abstract": "The performance of deep learning models is dependent on the precise configuration of many layers and parameters. However, there are currently few systematic guidelines for how to configure a successful model. This means model builders often have to experiment with different configurations by manually programming different architectures (which is tedious and time consuming) or rely on purely automated approaches to generate and train the architectures (which is expensive). In this paper, we present Rapid Exploration of Model Architectures and Parameters, or REMAP, a visual analytics tool that allows a model builder to discover a deep learning model quickly via exploration and rapid experimentation of neural network architectures. In REMAP, the user explores the large and complex parameter space for neural network architectures using a combination of global inspection and local experimentation. Through a visual overview of a set of models, the user identifies interesting clusters of architectures. Based on their findings, the user can run ablation and variation experiments to identify the effects of adding, removing, or replacing layers in a given architecture and generate new models accordingly. They can also handcraft new models using a simple graphical interface. As a result, a model builder can build deep learning models quickly, efficiently, and without manual programming. We inform the design of REMAP through a design study with four deep learning model builders. Through a use case, we demonstrate that REMAP allows users to discover performant neural network architectures efficiently using visual exploration and user-defined semi-automated searches through the model space.", "keywords": "visual analytics,neural networks,parameter space exploration", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934261", "refList": ["10.1109/mcg.2018.2878902", "10.1111/cgf.12639", "10.1109/tvcg.2017.2744938", "10.1117/12.2007316", "10.1109/cvpr.2014.81", "10.1016/j.csda.2008.02.031", "10.1080/00994480.2000.10748487", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2013.125", "10.1109/cvpr.2015.7298594", "10.1111/j.1467-8659.2009.01475.x", "10.1109/tvcg.2012.65", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2017.2744805", "10.1145/2487575.2487629", "10.1109/tvcg.2018.2865044", "10.23915/distill.00010", "10.1109/72.279181", "10.1109/tvcg.2017.2744199", "10.1007/s13398-014-0173-7.2", "10.1109/tvcg.2018.2864504", "10.1109/vast.2012.6400490", "10.1007/978-3-319-10590-1\\_53", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2014.2346321", "10.1094/pdis-11-11-0999-pdn", "10.1109/ijcnn.2015.7280767", "10.1109/tvcg.2017.2744878", "10.5555/3326943.3327130", "10.1109/tvcg.2017.2744718", "10.1109/iccv.2015.169", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2014.223", "10.1109/cvpr.2016.90", "10.1109/vast.2010.5652443", "10.1111/cgf.13681", "10.1109/tvcg.2016.2598831", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028888", "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes", "year": "2020", "conferenceName": "VAST", "authors": "Yuxin Ma;Arlen Fan;Jingrui He;Arun Reddy Nelakurthi;Ross Maciejewski", "citationCount": "0", "affiliation": "Ma, YX (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Ma, Yuxin; Fan, Arlen; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. He, Jingrui, Univ Illinois, Champaign, IL USA. Nelakurthi, Arun Reddy, Samsung Res Amer, Mountain View, CA USA.", "countries": "USA", "abstract": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.", "keywords": "Transfer learning,deep learning,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028888", "refList": ["10.1109/tvcg.2014.2346578", "10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1109/tpami.2018.2868685", "10.1145/2702123.2702509", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2016.2598828", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2013.65", "10.1145/2976749.2978318", "10.1007/978-3-030-01424-7\\_27", "10.1109/tvcg.2019.2934261", "10.1007/s11704-016-6028-y", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/iccv.2015.279", "10.1109/mci.2018.2840738", "10.1109/tvcg.2019.2892483", "10.1109/vast.2018.8802509", "10.1109/tvcg.2013.124", "10.1186/s40537-016-0043-6", "10.1109/tvcg.2018.2864475", "10.1145/3200489", "10.1109/tvcg.2018.2865044", "10.1111/cgf.13210", "10.1109/tvcg.2018.2816223", "10.23915/distill.00007", "10.1109/tvcg.2017.2744199", "10.1109/tkde.2018.2876857", "10.1109/tvcg.2019.2934631", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2011.188", "10.1007/978-3-642-15561-1\\_16", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1109/tvcg.2017.2744718", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2016.2598541", "10.1109/tkde.2009.191", "10.1145/3065386", "10.1016/j.ins.2016.03.021", "10.1109/tvcg.2019.2903943", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/iccv.2017.74", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2012.207", "10.1111/cgf.13092", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2017.2754480", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 9}, {"doi": "10.1109/tvcg.2019.2934535", "title": "BarcodeTree: Scalable Comparison of Multiple Hierarchies", "year": "2019", "conferenceName": "InfoVis", "authors": "Guozheng Li 0002;Yu Zhang 0043;Yu Dong;Jie Liang 0004;Jinson Zhang;Jinsong Wang;Michael J. McGuffin;Xiaoru Yuan", "citationCount": "2", "affiliation": "Li, GZ (Corresponding Author), Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China. Li, Guozheng; Yuan, Xiaoru, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China. Li, Guozheng; Yuan, Xiaoru, Peking Univ, Natl Engn Lab Big Data Anal \\& Applicat, Beijing, Peoples R China. Zhang, Yu, Univ Oxford, Oxford, England. Zhang, Yu, Peking Univ, Beijing, Peoples R China. Dong, Yu; Liang, Jie; Zhang, Jinson, Univ Technol Sydney, Sydney, NSW, Australia. Wang, Jinsong, Southwest Elect \\& Telecom Engn Inst, Mianyang, Peoples R China. McGuffin, Michael J., Ecole Technol Super, Montreal, PQ, Canada.", "countries": "Canada;China;England;Australia", "abstract": "We propose BarcodeTree (BCT), a novel visualization technique for comparing topological structures and node attribute values of multiple trees. BCT can provide an overview of one hundred shallow and stable trees simultaneously, without aggregating individual nodes. Each BCT is shown within a single row using a style similar to a barcode, allowing trees to be stacked vertically with matching nodes aligned horizontally to ease comparison and maintain space efficiency. We design several visual cues and interactive techniques to help users understand the topological structure and compare trees. In an experiment comparing two variants of BCT with icicle plots, the results suggest that BCTs make it easier to visually compare trees by reducing the vertical distance between different trees. We also present two case studies involving a dataset of hundreds of trees to demonstrate BCT's utility.", "keywords": "tree visualization,comparison,multiple trees", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934535", "refList": ["10.1207/s15327051hci0701\\_3", "10.1109/vlhcc.2014.6883017", "10.1109/infvis.2000.885091", "10.1109/tvcg.2012.226", "10.1109/tvcg.2008.141", "10.1177/1473871611416549", "10.1109/tvcg.2007.70529", "10.1109/tvcg.2015.2467733", "10.1109/tse.1981.234519", "10.1111/j.1467-8659.2011.01963.x", "10.1111/j.1467-8659.2008.01214.x", "10.1109/infvis.2002.1173151", "10.1109/iv.2003.1218020", "10.1057/ivs.2009.4", "10.1109/tvcg.2013.231", "10.1109/tvcg.2007.70556", "10.1109/vast.2011.6102439", "10.1109/visual.1991.175815", "10.1109/infvis.2001.963290", "10.1109/tvcg.2017.2744080", "10.1111/j.1467-8659.2008.01205.x", "10.1109/mcg.2011.103", "10.1109/vl.1996.545307", "10.1109/infvis.2002.1173150", "10.1145/1294948.1294971", "10.1109/tvcg.2010.79", "10.1109/infvis.2001.963283", "10.1007/bf01908061", "10.1080/10635150590946961", "10.1177/1473871612455983", "10.1109/tvcg.2018.2865265", "10.1109/iv.2004.1320261", "10.1109/tvcg.2011.193", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2016.2598469", "10.1109/infvis.2002.1173148", "10.1145/102377.115768", "10.1145/633292.633484", "10.1109/tvcg.2015.2507595", "10.1109/infvis.2004.70", "10.1109/tvcg.2018.2864884", "10.1145/882262.882291", "10.1057/ivs.2009.29", "10.2307/2685881", "10.1057/palgrave.ivs.9500157", "10.1111/cgf.13164", "10.3897/bdj.4.e9787", "10.2312/vissym/eurovis05/053-060", "10.1109/infvis.2003.1249028", "10.1109/tvcg.2014.2346433"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13989", "year": "2020", "title": "Quantitative Comparison of Time-Dependent Treemaps", "conferenceName": "EuroVis", "authors": "Eduardo Faccin Vernier;Max Sondag;Jo{\\~{a}}o Luiz Dihl Comba;Bettina Speckmann;Alexandru Telea;Kevin Verbeek", "citationCount": "0", "affiliation": "Vernier, E (Corresponding Author), Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil.\nVernier, E (Corresponding Author), Univ Groningen, Groningen, Netherlands.\nSondag, M.; Speckmann, B.; Verbeek, K., TU Eindhoven, Eindhoven, Netherlands.\nVernier, E.; Comba, J., Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil.\nTelea, A., Univ Utrecht, Utrecht, Netherlands.\nVernier, E., Univ Groningen, Groningen, Netherlands.", "countries": "Netherlands;Brazil", "abstract": "Rectangular treemaps are often the method of choice to visualize large hierarchical datasets. Nowadays such datasets are available over time, hence there is a need for (a) treemaps that can handle time-dependent data, and (b) corresponding quality criteria that cover both a treemap's visual quality and its stability over time. In recent years a wide variety of (stable) treemapping algorithms has been proposed, with various advantages and limitations. We aim to provide insights to researchers and practitioners to allow them to make an informed choice when selecting a treemapping algorithm for specific applications and data. To this end, we perform an extensive quantitative evaluation of rectangular treemaps for time-dependent data. As part of this evaluation we propose a novel classification scheme for time-dependent datasets. Specifically, we observe that the performance of treemapping algorithms depends on the characteristics of the datasets used. We identify four potential representative features that characterize time-dependent hierarchical datasets and classify all datasets used in our experiments accordingly. We experimentally test the validity of this classification on more than 2000 datasets, and analyze the relative performance of 14 state-of-the-art rectangular treemapping algorithms across varying features. Finally, we visually summarize our results with respect to both visual quality and stability to aid users in making an informed choice among treemapping algorithms. All datasets, metrics, and algorithms are openly available to facilitate reuse and further comparative studies.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13989", "refList": ["10.1037/0033-295x.94.2.115", "10.1016/0022-2496(73)90003-5", "10.1109/tvcg.2007.70529", "10.1109/infvis.2005.1532145", "10.1109/tvcg.2013.231", "10.1109/sibgrapi.2018.00027", "10.1561/0600000017", "10.1007/s00371-017-1373-x", "10.1109/tvcg.2017.2745140", "10.1109/tvcg.2006.200", "10.1109/tvcg.2010.79", "10.1016/j.infsof.2016.10.003", "10.1109/tvcg.2010.186", "10.1109/infvis.2001.963283", "10.1109/tvcg.2018.2865265", "10.1016/j.comgeo.2013.12.008", "10.1145/571647.571649", "10.1109/tvcg.2019.2934535", "10.1109/vissoft.2018.00018", "10.5220/0006117500880095", "10.1137/110834032", "10.1109/cvpr.1994.323794", "10.1093/sysbio/syu085", "10.1145/102377.115768", "10.1109/tvcg.2012.108", "10.1145/1056018.1056041", "10.1088/1742-6596/787/1/012007", "10.1057/ivs.2009.29", "10.1111/cgf.13164", "10.1109/iswcs.2014.6933406", "10.1016/j.cor.2013.11.015", "10.1016/j.dam.2006.08.005", "10.1007/978-3-319-57336-6\\_14", "10.1145/2827872"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1109/tvcg.2019.2934538", "title": "Data Changes Everything: Challenges and Opportunities in Data Visualization Design Handoff", "year": "2019", "conferenceName": "InfoVis", "authors": "Jagoda Walny;Christian Frisson;Mieka West;Doris Kosminsky;S\u00f8ren Knudsen;Sheelagh Carpendale;Wesley Willett", "citationCount": "7", "affiliation": "Walny, J (Corresponding Author), Univ Calgary, Calgary, AB, Canada. Walny, Jagoda; Frisson, Christian; West, Mieka; Kosminsky, Doris; Knudsen, Soren; Carpendale, Sheelagh; Willett, Wesley, Univ Calgary, Calgary, AB, Canada. Frisson, Christian, McGill Univ, Montreal, PQ, Canada. Kosminsky, Doris, Univ Fed Rio de Janeiro, Rio De Janeiro, Brazil. Knudsen, Soren, Univ Copenhagen, Copenhagen, Denmark. Carpendale, Sheelagh, Simon Fraser Univ, Vancouver, BC, Canada.", "countries": "Canada;Brazil;Denmark", "abstract": "Complex data visualization design projects often entail collaboration between people with different visualization-related skills. For example, many teams include both designers who create new visualization designs and developers who implement the resulting visualization software. We identify gaps between data characterization tools, visualization design tools, and development platforms that pose challenges for designer-developer teams working to create new data visualizations. While it is common for commercial interaction design tools to support collaboration between designers and developers, creating data visualizations poses several unique challenges that are not supported by current tools. In particular, visualization designers must characterize and build an understanding of the underlying data, then specify layouts, data encodings, and other data-driven parameters that will be robust across many different data values. In larger teams, designers must also clearly communicate these mappings and their dependencies to developers, clients, and other collaborators. We report observations and reflections from five large multidisciplinary visualization design projects and highlight six data-specific visualization challenges for design specification and handoff. These challenges include adapting to changing data, anticipating edge cases in data, understanding technical challenges, articulating data-dependent interactions, communicating data mappings, and preserving the integrity of data mappings across iterations. Based on these observations, we identify opportunities for future tools for prototyping, testing, and communicating data-driven designs, which might contribute to more successful and collaborative data visualization design.", "keywords": "Information visualization,design handoff,data mapping,design process", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934538", "refList": ["10.1145/2145204.2145261", "10.1177/1473871611416549", "10.1145/2627341", "10.1145/2998181.2998190", "10.1145/958160.958184", "10.1145/3310276", "10.1027/1866-5888/a000009", "10.1109/infvis.2000.885086", "10.1109/tvcg.2011.185", "10.1109/tvcg.2018.2864836", "10.3102/0013189x030003013", "10.1145/2598153.2598175", "10.1109/tvcg.2016.2598609", "10.1109/tvcg.2012.219", "10.1145/3025453.3025912", "10.1109/icccbda.2018.8386459", "10.1007/978-1-84628-061-010", "10.1109/pacificvis.2017.8031599", "10.1145/2317956.2318034", "10.1145/1753326.1753400", "10.1109/tvcg.2009.162", "10.1109/mcg.2013.101", "10.1109/tvcg.2014", "10.1145/1753326.1753707", "10.1037/1076-8998.6.3.196", "10.1109/tvcg.2017.2744199", "10.1145/3125571.3125585", "10.1002/wics.118", "10.1109/tvcg.2018.2802520", "10.1145/2642918.2642920", "10.1109/tvcg.2016.2598620", "10.1016/j.jvlc.2017.10.001", "10.1109/mcg.2018.2874523", "10.1080/13678860701723802", "10.1145/2642918.2647411", "10.1145/257089.257396", "10.1177/1473871611415996", "10.1145/985692.985715", "10.1109/tvcg.2012.213", "10.1145/3170427.3186539", "10.1145/3196709.3196781", "10.1145/175276.175288", "10.1145/2399016.2399121", "10.1145/2807442.2807455", "10.1109/tvcg.2016.2599030", "10.1179/000870403235002042"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030355", "title": "Guidelines For Pursuing and Revealing Data Abstractions", "year": "2020", "conferenceName": "InfoVis", "authors": "Alex Bigelow;Katy Williams;Katherine E. Isaacs", "citationCount": "0", "affiliation": "Bigelow, A (Corresponding Author), Univ Arizona, Tucson, AZ 85721 USA. Bigelow, Alex; Williams, Katy; Isaacs, Katherine E., Univ Arizona, Tucson, AZ 85721 USA.", "countries": "USA", "abstract": "Many data abstraction types, such as networks or set relationships, remain unfamiliar to data workers beyond the visualization research community. We conduct a survey and series of interviews about how people describe their data, either directly or indirectly. We refer to the latter as latent data abstractions. We conduct a Grounded Theory analysis that (1) interprets the extent to which latent data abstractions exist, (2) reveals the far-reaching effects that the interventionist pursuit of such abstractions can have on data workers, (3) describes why and when data workers may resist such explorations, and (4) suggests how to take advantage of opportunities and mitigate risks through transparency about visualization research perspectives and agendas. We then use the themes and codes discovered in the Grounded Theory analysis to develop guidelines for data abstraction in visualization projects. To continue the discussion, we make our dataset open along with a visual interface for further exploration.", "keywords": "Data abstraction,Grounded theory,Survey design,Data wrangling", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030355", "refList": ["10.1080/2159676x.2016.1251701", "10.1109/infvis.2000.885092", "10.1145/2702123.2702298", "10.4135/9781848607941.n14", "10.1007/978-1-4939", "10.1109/tvcg.2014.2346331", "10.1109/tvcg.2017.2744843", "10.1177/1473871613510429", "10.1007/978-1-4939-0378-8\\_2", "10.1145/2598153.2598175", "10.1109/tvcg.2019.2934285", "10.1177/1473871613488591", "10.1145/2501105.2501106", "10.1109/tvcg.2019.2934538", "10.1109/tvcg.2019.2934539", "10.1017/s1049096510990781", "10.1145/3025453.3025837", "10.1145/3290605.3300474", "10.1145/3290605.3300356", "10.1002/nur.1025", "10.1145/2993901.2993916", "10.1145/3392826", "10.1086/269268", "10.1109/tvcg.2018.2865241", "10.1145/2998181.2998331", "10.1145/291224.291229", "10.1057/ivs.2009.13", "10.1145/2047196.2047205", "10.1109/tvcg.2012.213", "10.1145/3274405", "10.1109/tvcg.2013.145", "10.1016/0040-6031(92)85160-w", "10.1109/iv.2013.45", "10.1109/tvcg.2009.111", "10.1109/mcg.2019.2914844", "10.1109/tvcg.2009.116"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030423", "title": "MobileVisFixer: Tailoring Web Visualizations for Mobile Phones Leveraging an Explainable Reinforcement Learning Framework", "year": "2020", "conferenceName": "InfoVis", "authors": "Aoyu Wu;Wai Tong;Tim Dwyer;Bongshin Lee;Petra Isenberg;Huamin Qu", "citationCount": "0", "affiliation": "Wu, AY (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Wu, Aoyu; Tong, Wai; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China. Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Lee, Bongshin, Microsoft Res, Redmond, WA USA. Isenberg, Petra, INRIA, Le Chesnay Rocquencourt, France.", "countries": "USA;China;France;Australia", "abstract": "We contribute MobileVisFixer, a new method to make visualizations more mobile-friendly. Although mobile devices have become the primary means of accessing information on the web, many existing visualizations are not optimized for small screens and can lead to a frustrating user experience. Currently, practitioners and researchers have to engage in a tedious and time-consuming process to ensure that their designs scale to screens of different sizes, and existing toolkits and libraries provide little support in diagnosing and repairing issues. To address this challenge, MobileVisFixer automates a mobile-friendly visualization re-design process with a novel reinforcement learning framework. To inform the design of MobileVisFixer, we first collected and analyzed SVG-based visualizations on the web, and identified five common mobile-friendly issues. MobileVisFixer addresses four of these issues on single-view Cartesian visualizations with linear or discrete scales by a Markov Decision Process model that is both generalizable across various visualizations and fully explainable. MobileVisFixer deconstructs charts into declarative formats, and uses a greedy heuristic based on Policy Gradient methods to find solutions to this difficult, multi-criteria optimization problem in reasonable time. In addition, MobileVisFixer can be easily extended with the incorporation of optimization algorithms for data visualizations. Quantitative evaluation on two real-world datasets demonstrates the effectiveness and generalizability of our method.", "keywords": "Mobile visualization,Responsive visualization,Machine learning for visualizations,Reinforcement learning", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030423", "refList": ["10.1109/mcg.2014.82", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2012.196", "10.1145/3092703.3092726", "10.1109/pimrc.2015.7343276", "10.1109/tvcg.2014.48", "10.1145/3025453.3025957", "10.1145/2047196.2047247", "10.1145/3173574.3174168", "10.1109/icst.2019.00027", "10.1109/tvcg.2007.70594", "10.1145/3197517.3201311", "10.1145/3126594.3126653", "10.1109/mcg.2019.2924636", "10.1145/3180155.3180262", "10.1109/tvcg.2017.2659744", "10.1145/3123266.3123274", "10.1109/tvcg.2019.2934538", "10.1109/cvpr.2018.00592", "10.1109/tvcg.2018.2865138", "10.1109/tsmcc.2012.2218595", "10.1109/tvcg.2018.2865240", "10.1111/cgf.13193", "10.1145/2775441.2775478", "10.1145/3313831.3376777", "10.1109/mc.2006.109", "10.1111/cgf.13686", "10.1109/tvcg.2015.2467091", "10.4230/dagrep.9.7.78", "10.1007/978-3-319-71249-9\\_9", "10.1007/s00778-019-00588-3", "10.1109/ase.2015.31", "10.1007/bf02124742", "10.1109/tvcg.2019.2934397", "10.1109/adprl.2007.368196", "10.1109/mcg.2020.2968244", "10.1145/3092703.3092712", "10.1109/tvcg.2017.2744320", "10.1023/a:1017992615625", "10.1145/2642918", "10.1145/2858036.2858558", "10.1145/3290605.3300358", "10.1109/tvcg.2019.2934431", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030406", "title": "Palettailor: Discriminable Colorization for Categorical Data", "year": "2020", "conferenceName": "InfoVis", "authors": "Kecheng Lu;Mi Feng;Xin Chen;Michael Sedlmair;Oliver Deussen;Dani Lischinski;Zhanglin Cheng;Yunhai Wang", "citationCount": "0", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Jinan, Peoples R China. Cheng, ZL (Corresponding Author), SIAT, Shenzhen VesuCA Key Lab, Shenzhen, Peoples R China. Lu, Kecheng; Chen, Xin; Wang, Yunhai, Shandong Univ, Jinan, Peoples R China. Feng, Mi, Twitter Inc, San Francisco, CA USA. Lu, Kecheng; Deussen, Oliver; Cheng, Zhanglin, SIAT, Shenzhen VesuCA Key Lab, Shenzhen, Peoples R China. Sedlmair, Michael, Univ Stuttgart, VISUS, Stuttgart, Germany. Deussen, Oliver, Konstanz Univ, Constance, Germany. Lischinski, Dani, Hebrew Univ Jerusalem, Jerusalem, Israel.", "countries": "USA;Israel;Germany;China", "abstract": "We present an integrated approach for creating and assigning color palettes to different visualizations such as multi-class scatterplots, line, and bar charts. While other methods separate the creation of colors from their assignment, our approach takes data characteristics into account to produce color palettes, which are then assigned in a way that fosters better visual discrimination of classes. To do so, we use a customized optimization based on simulated annealing to maximize the combination of three carefully designed color scoring functions: point distinctness, name difference, and color discrimination. We compare our approach to state-of-the-art palettes with a controlled user study for scatterplots and line charts, furthermore we performed a case study. Our results show that Palettailor, as a fully-automated approach, generates color palettes with a higher discrimination quality than existing approaches. The efficiency of our optimization allows us also to incorporate user modifications into the color selection process.", "keywords": "Color Palette,Discriminability,Multi-Class Scatterplot,Line Chart,Bar Chart", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030406", "refList": ["10.1109/mcg.2014.82", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2012.196", "10.1145/3092703.3092726", "10.1109/tvcg.2014.48", "10.1145/3025453.3025957", "10.1145/2047196.2047247", "10.1145/3173574.3174168", "10.1109/icst.2019.00027", "10.1109/tvcg.2007.70594", "10.1145/3197517.3201311", "10.1145/3126594.3126653", "10.1109/mcg.2019.2924636", "10.1145/3180155.3180262", "10.1109/tvcg.2017.2659744", "10.1145/3123266.3123274", "10.1109/tvcg.2018.2865234", "10.1109/tvcg.2019.2934538", "10.1109/cvpr.2018.00592", "10.1145/2815833.2816956", "10.1109/tvcg.2018.2865138", "10.1109/tsmcc.2012.2218595", "10.1109/tvcg.2018.2865240", "10.1111/cgf.13193", "10.1145/2775441.2775478", "10.1145/3313831.3376777", "10.1109/mc.2006.109", "10.1111/cgf.13686", "10.1109/tvcg.2015.2467091", "10.4230/dagrep.9.7.78", "10.1007/978-3-319-71249-9\\_9", "10.1007/s00778-019-00588-3", "10.1109/ase.2015.31", "10.1007/bf02124742", "10.1145/3025453.3025768", "10.1109/tvcg.2019.2934397", "10.1109/adprl.2007.368196", "10.1109/mcg.2020.2968244", "10.1145/3092703.3092712", "10.1109/tvcg.2017.2744320", "10.1023/a:1017992615625", "10.1145/2642918", "10.1145/2858036.2858558", "10.1145/3290605.3300358", "10.1109/tvcg.2019.2934431", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2019.2934280", "title": "Motion Browser: Visualizing and Understanding Complex Upper Limb Movement Under Obstetrical Brachial Plexus Injuries", "year": "2019", "conferenceName": "VAST", "authors": "Gromit Yeuk-Yin Chan;Luis Gustavo Nonato;Alice Chu;Preeti Raghavan;Viswanath Aluru;Cl\u00e1udio T. Silva", "citationCount": "0", "affiliation": "Chan, GYY (Corresponding Author), NYU, New York, NY 10003 USA. Chan, Gromit Yeuk-Yin; Silva, Claudio T., NYU, New York, NY 10003 USA. Nonato, Luis Gustavo, Univ Sao Paulo, Sao Paulo, Brazil. Chu, Alice, Rutgers New Jersey Med Sch, Newark, NJ USA. Raghavan, Preeti; Aluru, Viswanath, NYU Langone Med Ctr, New York, NY USA.", "countries": "USA;Brazil", "abstract": "The brachial plexus is a complex network of peripheral nerves that enables sensing from and control of the movements of the arms and hand. Nowadays, the coordination between the muscles to generate simple movements is still not well understood, hindering the knowledge of how to best treat patients with this type of peripheral nerve injury. To acquire enough information for medical data analysis, physicians conduct motion analysis assessments with patients to produce a rich dataset of electromyographic signals from multiple muscles recorded with joint movements during real-world tasks. However, tools for the analysis and visualization of the data in a succinct and interpretable manner are currently not available. Without the ability to integrate, compare, and compute multiple data sources in one platform, physicians can only compute simple statistical values to describe patient's behavior vaguely, which limits the possibility to answer clinical questions and generate hypotheses for research. To address this challenge, we have developed Motion Browser, an interactive visual analytics system which provides an efficient framework to extract and compare muscle activity patterns from the patient's limbs and coordinated views to help users analyze muscle signals, motion data, and video information to address different tasks. The system was developed as a result of a collaborative endeavor between computer scientists and orthopedic surgery and rehabilitation physicians. We present case studies showing physicians can utilize the information displayed to understand how individuals coordinate their muscles to initiate appropriate treatment and generate new hypotheses for future research.", "keywords": "Medical Data Visualization,Visual Analytics Application,Time Series Data,Multimodal Data,Brachial Plexus Injuries", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934280", "refList": ["10.1109/tvcg.2006.85", "10.1111/j.1467-8659.2009.01681.x", "10.1145/2659766.2659772", "10.1007/s00402-015-2180-3", "10.1145/2254556.2254639", "10.1117/12.2080001", "10.1016/j.jse.2015.12.006", "10.1111/j.1467-8659.2011.01919.x", "10.1109/vl.1996.545307", "10.1109/tvcg.2013.178", "10.1080/001401398186063", "10.1109/tvcg.2011.195", "10.2519/jospt.2013.4659", "10.1177/1753193409348185", "10.1179/000870403235002042", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2016.2539960", "10.1109/tvcg.2010.193", "10.1145/2470654.2481374", "10.5220/0006127502170224", "10.1109/tvcg.2016.2598618", "10.1109/tvcg.2015.2468292", "10.1109/vast.2015.7347624", "10.1111/cgf.12390", "10.1109/tvcg.2012.110", "10.1145/1168149.1168158", "10.1109/pacificvis.2012.6183556", "10.2106/jbjs.j.01348", "10.1111/j.1467-8659.2012.03092.x", "10.1145/203356.203365", "10.1109/tvcg.2010.162", "10.1111/dmcn.12490", "10.1109/tvcg.2008.139", "10.1109/tvcg.2017.2744319", "10.1145/3011077.3011095", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934654", "title": "Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections", "year": "2019", "conferenceName": "VAST", "authors": "Mennatallah El-Assady;Rebecca Kehlbeck;Christopher Collins;Daniel A. Keim;Oliver Deussen", "citationCount": "3", "affiliation": "El-Assady, M (Corresponding Author), Univ Konstanz, Constance, Germany. El-Assady, M (Corresponding Author), Ontario Tech Univ, Oshawa, ON, Canada. El-Assady, Mennatallah; Kehlbeck, Rebecca; Keim, Daniel; Deussen, Oliver, Univ Konstanz, Constance, Germany. El-Assady, Mennatallah; Collins, Christopher, Ontario Tech Univ, Oshawa, ON, Canada.", "countries": "Canada;Germany", "abstract": "We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decision-making process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.", "keywords": "Topic Model Optimization,Word Embedding,Mixed-Initiative Refinement,Guided Visual Analytics,Semantic Mapping", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934654", "refList": ["10.1109/vast.2014.7042493", "10.1145/2133806.2133826", "10.1016/j.visinf.2018.09.003", "10.1007/978-3-319-67008-9\\_26", "10.1109/tvcg.2013.126", "10.1162/tacl\\_a\\_00140", "10.1007/s13202-018-0509-5", "10.1109/bdva.2018.8534018", "10.1108/eb026526", "10.1007/s10994-013-5413-0", "10.1145/564376.564421", "10.1016/j.visinf.2017.01.006", "10.3115/v1/p14-2050", "10.1007/bf00288933", "10.1109/tvcg.2013.212", "10.1145/2207676.2207741", "10.1109/tvcg.2013.162", "10.1109/tvcg.2016.2515592", "10.1109/tvcg.2017.2745080", "10.1109/mcg.2013.53", "10.1109/tvcg.2017.2744199", "10.1145/3091108", "10.18653/v1/p17-4009", "10.1162/jmlr.2003.3.4-5.951", "10.1109/vast.2017.8585498", "10.1109/tvcg.2017.2723397", "10.1109/tvcg.2018.2864769", "10.1007/s10618-005-0361-3", "10.3115/v1/d14-1167", "10.1007/bf01840357", "10.1162/jmlr.2003.3.4-5.993", "10.1145/2678025.2701370", "10.1016/j.ins.2016.06.040", "10.1109/tvcg.2017.2746018", "10.1109/vast.2011.6102461", "10.1111/cgf.13092", "10.3115/1117729.1117730", "10.1109/mcg.2015.91", "10.1145/2669557.2669572"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934289", "title": "STBins: Visual Tracking and Comparison of Multiple Data Sequences Using Temporal Binning", "year": "2019", "conferenceName": "VAST", "authors": "Ji Qi;Vincent Bloemen;Shihan Wang;Jarke J. van Wijk;Huub van de Wetering", "citationCount": "0", "affiliation": "Qi, J (Corresponding Author), Eindhoven Univ Technol, Dept Math \\& Comp Sci, Eindhoven, Netherlands. Qi, Ji; van Wijk, Jarke; van de Wetering, Huub, Eindhoven Univ Technol, Dept Math \\& Comp Sci, Eindhoven, Netherlands. Bloemen, Vincent, Univ Twente, Fac Elect Engn Math Comp Sci, Enschede, Netherlands. Wang, Shihan, Univ Amsterdam, Amsterdam, Netherlands.", "countries": "Netherlands", "abstract": "While analyzing multiple data sequences, the following questions typically arise: how does a single sequence change over time, how do multiple sequences compare within a period, and how does such comparison change over time. This paper presents a visual technique named STBins to answer these questions. STBins is designed for visual tracking of individual data sequences and also for comparison of sequences. The latter is done by showing the similarity of sequences within temporal windows. A perception study is conducted to examine the readability of alternative visual designs based on sequence tracking and comparison tasks. Also, two case studies based on real-world datasets are presented in detail to demonstrate usage of our technique.", "keywords": "Visualization,time series data,data sequence", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934289", "refList": ["10.1109/tvcg.2012.189", "10.1177/1473871611416549", "10.1145/2702123.2702419", "10.1145/2483977.2483989", "10.1109/vast.2016.7883512", "10.1109/tvcg.2016.2598797", "10.1016/j.eswa.2016.03.050", "10.1109/tvcg.2014.2346433", "10.1109/tvcg.2013.124", "10.1111/j.1469-8137.1912.tb05611.x", "10.1186/1753-6561-8-s2-s9", "10.1145/2851141.2851161", "10.1109/tvcg.2011.239", "10.1109/tvcg.2018.2864885", "10.2307/3001913", "10.1007/978-0-85729-079-3", "10.1109/tvcg.2009.117", "10.1109/tvcg.2013.200", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2016.2539960", "10.1145/2557500.2557508", "10.1109/tvcg.2012.225", "10.1186/1753-6561-8-s2-s8", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1016/j.intcom.2012.01.003", "10.1111/cgf.13264", "10.1109/tvcg.2011.232", "10.1111/cgf.12653", "10.1002/asi.21489", "10.1109/tvcg.2014.2346919", "10.1145/2254556.2254670"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2019.2934786", "title": "The Perceptual Proxies of Visual Comparison", "year": "2019", "conferenceName": "InfoVis", "authors": "Nicole Jardine;Brian D. Ondov;Niklas Elmqvist;Steven Franconeri", "citationCount": "4", "affiliation": "Jardine, N (Corresponding Author), Northwestern Univ, Evanston, IL 60208 USA. Jardine, Nicole; Franconeri, Steven, Northwestern Univ, Evanston, IL 60208 USA. Jardine, Nicole, Cook Cty Assessors Off, Chicago, IL USA. Ondov, Brian D., NIH, Bldg 10, Bethesda, MD 20892 USA. Ondov, Brian D.; Elmqvist, Niklas, Univ Maryland, College Pk, MD 20742 USA.", "countries": "USA", "abstract": "Perceptual tasks in visualizations often involve comparisons. Of two sets of values depicted in two charts, which set had values that were the highest overall? Which had the widest range? Prior empirical work found that the performance on different visual comparison tasks (e.g., \u201cbiggest delta\u201d, \u201cbiggest correlation\u201d) varied widely across different combinations of marks and spatial arrangements. In this paper, we expand upon these combinations in an empirical evaluation of two new comparison tasks: the \u201cbiggest mean\u201d and \u201cbiggest range\u201d between two sets of values. We used a staircase procedure to titrate the difficulty of the data comparison to assess which arrangements produced the most precise comparisons for each task. We find visual comparisons of biggest mean and biggest range are supported by some chart arrangements more than others, and that this pattern is substantially different from the pattern for other tasks. To synthesize these dissonant findings, we argue that we must understand which features of a visualization are actually used by the human visual system to solve a given task. We call these perceptual proxies. For example, when comparing the means of two bar charts, the visual system might use a \u201cMean length\u201d proxy that isolates the actual lengths of the bars and then constructs a true average across these lengths. Alternatively, it might use a \u201cHull Area\u201d proxy that perceives an implied hull bounded by the bars of each chart and then compares the areas of these hulls. We propose a series of potential proxies across different tasks, marks, and spatial arrangements. Simple models of these proxies can be empirically evaluated for their explanatory power by matching their performance to human performance across these marks, arrangements, and tasks. We use this process to highlight candidates for perceptual proxies that might scale more broadly to explain performance in visual comparison.", "keywords": "Graphical perception,visual perception,visual comparison,crowdsourced evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934786", "refList": ["10.1177/1473871611416549", "10.1109/tvcg.2014.2346979", "10.1016/0010-0285(77)90012-3", "10.1037/a0026284", "10.1016/s1364-6613(97)01105-4", "10.3758/bf03201236", "10.1093/acprof:osobl/9780199734337.003.0030", "10.1109/infvis.2005.1532136", "10.3758/bf03204219", "10.1109/tvcg.2017.2744199", "10.1016/s0042-6989(99)00029-2", "10.1167/16.5.11", "10.1109/34.730558", "10.1145/2046684.2046699", "10.3758/s13414-012-0322-z", "10.1177/0956797615585002", "10.1109/tvcg.2018.2810918", "10.1109/tvcg.2018.2864884", "10.1016/0042-6989(85)90208-1", "10.1016/j.tics.2011.01.003", "10.1146/annurev-psych-010416-044232", "10.1109/tvcg.2010.162", "10.1109/tvcg.2015.2466971", "10.1109/tvcg.2007.70515", "10.1111/j.1467-8659.2009.01694.x"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030454", "title": "Objective Observer-Relative Flow Visualization in Curved Spaces for Unsteady 2D Geophysical Flows", "year": "2020", "conferenceName": "SciVis", "authors": "Peter Rautek;Matej Mlejnek;Johanna Beyer;Jakob Troidl;Hanspeter Pfister;Thomas Theu\u00dfl;Markus Hadwiger", "citationCount": "0", "affiliation": "Rautek, P (Corresponding Author), King Abdullah Univ Sci \\& Technol KAUST, Visual Comp Ctr, Thuwal 239556900, Saudi Arabia. Rautek, Peter; Mlejnek, Matej; Troidl, Jakob; Hadwiger, Markus, King Abdullah Univ Sci \\& Technol KAUST, Visual Comp Ctr, Thuwal 239556900, Saudi Arabia. Beyer, Johanna; Pfister, Hanspeter, Harvard Univ, Cambridge, MA 02138 USA. Troidl, Jakob, TU Wien, Vienna, Austria. Theussl, Thomas, King Abdullah Univ Sci \\& Technol KAUST, Core Labs, Thuwal 239556000, Saudi Arabia.", "countries": "USA;Arabia;Austria", "abstract": "Computing and visualizing features in fluid flow often depends on the observer, or reference frame, relative to which the input velocity field is given. A desired property of feature detectors is therefore that they are objective, meaning independent of the input reference frame. However, the standard definition of objectivity is only given for Euclidean domains and cannot be applied in curved spaces. We build on methods from mathematical physics and Riemannian geometry to generalize objectivity to curved spaces, using the powerful notion of symmetry groups as the basis for definition. From this, we develop a general mathematical framework for the objective computation of observer fields for curved spaces, relative to which other computed measures become objective. An important property of our framework is that it works intrinsically in 2D, instead of in the 3D ambient space. This enables a direct generalization of the 2D computation via optimization of observer fields in flat space to curved domains, without having to perform optimization in 3D. We specifically develop the case of unsteady 2D geophysical flows given on spheres, such as the Earth. Our observer fields in curved spaces then enable objective feature computation as well as the visualization of the time evolution of scalar and vector fields, such that the automatically computed reference frames follow moving structures like vortices in a way that makes them appear to be steady.", "keywords": "Flow visualization,observer fields,frames of reference,objectivity,symmetry groups,intrinsic covariant derivatives", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030454", "refList": ["10.1001/archpediatrics.2011.97", "10.1037/0033-2909.115.2.228", "10.1057/s41267-019-00289-7", "10.1097/00001888-199805000-00024", "10.1145/3173574.3173718", "10.1126/science.7455683", "10.1109/tvcg.2012.199", "10.2307/1914185", "10.1371/journal.pone.0142444", "10.1177/0956797613504966", "10.1198/0003130032369", "10.1037/0033-2909.111.2.361", "10.1037/a0014474", "10.1186/s41235-019-0182-3", "10.1016/0041-5553(76)90154-3", "10.1037/a0025185", "10.3758/s13423-013-0572-3", "10.1111/1467-985x.00120", "10.1098/rsta.1895.0010", "10.1109/tvcg.2019.2934786", "10.1037/h0046162", "10.1177/2059799116672879", "10.1167/16.5.11", "10.1111/j.1740-9713.2013.00636.x", "10.1037/a0023265", "10.1073/pnas.1722389115", "10.2307/2288400", "10.3389/fnins.2012.00001", "10.1016/j.ijforecast.2012.02.006", "10.1186/s13639-018-0087-0", "10.1037/1082-989x.10.4.389", "10.1006/cogp.1998.0710", "10.1177/0963721413481473", "10.1037/0033-295x.102.4.684", "10.1126/science.185.4157.1124", "10.1109/tvcg.2014.2346298", "10.1145/3313831.3376454", "10.1037/0033-295x.107.3.500", "10.1037/h0042769", "10.1037/0003-066x.60.2.170", "10.3389/fpsyg.2019.00813", "10.1111/rssa.12378"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030429", "title": "Revealing Perceptual Proxies with Adversarial Examples", "year": "2020", "conferenceName": "InfoVis", "authors": "Brian D. Ondov;Fumeng Yang;Matthew Kay;Niklas Elmqvist;Steven Franconeri", "citationCount": "0", "affiliation": "Ondov, BD (Corresponding Author), NIH, Bldg 10, Bethesda, MD 20892 USA. Ondov, BD (Corresponding Author), Univ Maryland, College Pk, MD 20742 USA. Ondov, Brian D., NIH, Bldg 10, Bethesda, MD 20892 USA. Ondov, Brian D.; Elmqvist, Niklas, Univ Maryland, College Pk, MD 20742 USA. Yang, Fumeng, Brown Univ, Providence, RI 02912 USA. Kay, Matthew, Univ Michigan, Ann Arbor, MI 48109 USA. Franconeri, Steven, Northwestern Univ, Evanston, IL USA.", "countries": "USA", "abstract": "Data visualizations convert numbers into visual marks so that our visual system can extract data from an image instead of raw numbers. Clearly, the visual system does not compute these values as a computer would, as an arithmetic mean or a correlation. Instead, it extracts these patterns using perceptual proxies; heuristic shortcuts of the visual marks, such as a center of mass or a shape envelope. Understanding which proxies people use would lead to more effective visualizations. We present the results of a series of crowdsourced experiments that measure how powerfully a set of candidate proxies can explain human performance when comparing the mean and range of pairs of data series presented as bar charts. We generated datasets where the correct answer-the series with the larger arithmetic mean or range-was pitted against an \u201cadversarial\u201d series that should be seen as larger if the viewer uses a particular candidate proxy. We used both Bayesian logistic regression models and a robust Bayesian mixed-effects linear model to measure how strongly each adversarial proxy could drive viewers to answer incorrectly and whether different individuals may use different proxies. Finally, we attempt to construct adversarial datasets from scratch, using an iterative crowdsourcing procedure to perform black-box optimization.", "keywords": "Perceptual proxies,vision science,crowdsourced evaluation", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030429", "refList": ["10.3389/fpsyg.2012.00054", "10.3758/bf03209345", "10.1167/14.8.23", "10.1109/tvcg.2014.2346979", "10.1111/j.1467-8659.2009.01694.x", "10.1109/mcg.2007.323435", "10.1080/00223980.1947.9917350", "10.1016/j.tics.2009.07.001", "10.1007/bf00308884", "10.1016/s0020-7373(86)80019-0", "10.1037/h0043158", "10.1038/nn.2706", "10.1037/0278-7393.25.4.986", "10.1177/0956797617709814", "10.1109/6.736450", "10.2307/1131314", "10.1016/j.cub.2014.07.030", "10.1109/tvcg.2014.2346320", "10.1109/tvcg.2019.2934786", "10.1111/cgf.12888", "10.1109/tvcg.2018.2865240", "10.1109/tvcg.2015.2467671", "10.1037/0096-1523.16.4.683", "10.1037/h0046162", "10.1037/h0044417", "10.1037/0096-3445.106.4.341", "10.2307/2288400", "10.1109/tvcg.2018.2810918", "10.1002/wcs.26", "10.1016/j.visres.2014.02.006", "10.1016/0010-0277(92)90002-y", "10.1017/s0142716400009796", "10.1037/0033-295x.107.3.500", "10.1038/s41598-018-37610-7", "10.1016/j.actpsy.2011.09.006"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030335", "title": "Visual Reasoning Strategies for Effect Size Judgments and Decisions", "year": "2020", "conferenceName": "InfoVis", "authors": "Alex Kale;Matthew Kay;Jessica Hullman", "citationCount": "0", "affiliation": "Kale, A (Corresponding Author), Univ Washington, Seattle, WA 98195 USA. Kale, Alex, Univ Washington, Seattle, WA 98195 USA. Kay, Matthew, Univ Michigan, Ann Arbor, MI 48109 USA. Hullman, Jessica, Northwestern Univ, Evanston, IL 60208 USA.", "countries": "USA", "abstract": "Uncertainty visualizations often emphasize point estimates to support magnitude estimates or decisions through visual comparison. However, when design choices emphasize means, users may overlook uncertainty information and misinterpret visual distance as a proxy for effect size. We present findings from a mixed design experiment on Mechanical Turk which tests eight uncertainty visualization designs: 95% containment intervals, hypothetical outcome plots, densities, and quantile dotplots, each with and without means added. We find that adding means to uncertainty visualizations has small biasing effects on both magnitude estimation and decision-making, consistent with discounting uncertainty. We also see that visualization designs that support the least biased effect size estimation do not support the best decision-making, suggesting that a chart user's sense of effect size may not necessarily be identical when they use the same information for different tasks. In a qualitative analysis of users' strategy descriptions, we find that many users switch strategies and do not employ an optimal strategy when one exists. Uncertainty visualizations which are optimally designed in theory may not be the most effective in practice because of the ways that users satisfice with heuristics, suggesting opportunities to better understand visualization effectiveness by modeling sets of potential strategies.", "keywords": "Uncertainty visualization,graphical perception,data cognition", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030335", "refList": ["10.1001/archpediatrics.2011.97", "10.1037/0033-2909.115.2.228", "10.1109/tvcg.2017.2744683", "10.1057/s41267-019-00289-7", "10.1097/00001888-199805000-00024", "10.1145/3173574.3173718", "10.1155/2019/2318680", "10.1109/tvcg.2012.199", "10.3389/fpsyg", "10.2307/1914185", "10.1371/journal.pone.0142444", "10.1177/0956797613504966", "10.1198/0003130032369", "10.1037/a0014474", "10.1186/s41235-019-0182-3", "10.1016/0041-5553(76)90154-3", "10.1037/a0025185", "10.3758/s13423-013-0572-3", "10.1111/1467-985x.00120", "10.1098/rsta.1895.0010", "10.1109/tvcg.2019.2934786", "10.1037/h0046162", "10.1177/2059799116672879", "10.3389/fnins.2012.00007", "10.1167/16.5.11", "10.1111/j.1740-9713.2013.00636.x", "10.1037/a0023265", "10.1073/pnas.1722389115", "10.2307/2288400", "10.1371/journal.pone.0087357", "10.1016/j.ijforecast.2012.02.006", "10.1186/s13639-018-0087-0", "10.1037/1082-989x.10.4.389", "10.1006/cogp.1998.0710", "10.1177/0963721413481473", "10.1037/0033-295x.102.4.684", "10.1126/science.185.4157.1124", "10.1109/tvcg.2014.2346298", "10.1145/3313831.3376454", "10.1037/0033-295x.107.3.500", "10.1037/h0042769", "10.1037/0003-066x.60.2.170", "10.1126/science.7455683", "10.1111/rssa.12378"], "wos": 1, "children": [], "len": 1}], "len": 7}, {"doi": "10.1109/tvcg.2020.3028888", "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes", "year": "2020", "conferenceName": "VAST", "authors": "Yuxin Ma;Arlen Fan;Jingrui He;Arun Reddy Nelakurthi;Ross Maciejewski", "citationCount": "0", "affiliation": "Ma, YX (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Ma, Yuxin; Fan, Arlen; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. He, Jingrui, Univ Illinois, Champaign, IL USA. Nelakurthi, Arun Reddy, Samsung Res Amer, Mountain View, CA USA.", "countries": "USA", "abstract": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.", "keywords": "Transfer learning,deep learning,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028888", "refList": ["10.1109/tvcg.2014.2346578", "10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1109/tpami.2018.2868685", "10.1145/2702123.2702509", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2016.2598828", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2013.65", "10.1145/2976749.2978318", "10.1007/978-3-030-01424-7\\_27", "10.1109/tvcg.2019.2934261", "10.1007/s11704-016-6028-y", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/iccv.2015.279", "10.1109/mci.2018.2840738", "10.1109/tvcg.2019.2892483", "10.1109/vast.2018.8802509", "10.1109/tvcg.2013.124", "10.1186/s40537-016-0043-6", "10.1109/tvcg.2018.2864475", "10.1145/3200489", "10.1109/tvcg.2018.2865044", "10.1111/cgf.13210", "10.1109/tvcg.2018.2816223", "10.23915/distill.00007", "10.1109/tvcg.2017.2744199", "10.1109/tkde.2018.2876857", "10.1109/tvcg.2019.2934631", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2011.188", "10.1007/978-3-642-15561-1\\_16", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1109/tvcg.2017.2744718", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2016.2598541", "10.1109/tkde.2009.191", "10.1145/3065386", "10.1016/j.ins.2016.03.021", "10.1109/tvcg.2019.2903943", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/iccv.2017.74", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2012.207", "10.1111/cgf.13092", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2017.2754480", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030357", "title": "CcNav: Understanding Compiler Optimizations in Binary Code", "year": "2020", "conferenceName": "VAST", "authors": "Sabin Devkota;Pascal Aschwanden;Adam Kunen;Matthew P. LeGendre;Katherine E. Isaacs", "citationCount": "0", "affiliation": "Devkota, S (Corresponding Author), Univ Arizona, Tucson, AZ 85721 USA. Devkota, Sabin; Isaacs, Katherine E., Univ Arizona, Tucson, AZ 85721 USA. Aschwanden, Pascal; Kunen, Adam; Legendre, Matthew, LLNL, Livermore, CA USA.", "countries": "USA", "abstract": "Program developers spend significant time on optimizing and tuning programs. During this iterative process, they apply optimizations, analyze the resulting code, and modify the compilation until they are satisfied. Understanding what the compiler did with the code is crucial to this process but is very time-consuming and labor-intensive. Users need to navigate through thousands of lines of binary code and correlate it to source code concepts to understand the results of the compilation and to identify optimizations. We present a design study in collaboration with program developers and performance analysts. Our collaborators work with various artifacts related to the program such as binary code, source code, control flow graphs, and call graphs. Through interviews, feedback, and pair-analytics sessions, we analyzed their tasks and workflow. Based on this task analysis and through a human-centric design process, we designed a visual analytics system Compilation Navigator (CcNav) to aid exploration of the effects of compiler optimizations on the program. CcNav provides a streamlined workflow and a unified context that integrates disparate artifacts. CcNav supports consistent interactions across all the artifacts making it easy to correlate binary code with source code concepts. CcNav enables users to navigate and filter large binary code to identify and summarize optimizations such as inlining, vectorization, loop unrolling, and code hoisting. We evaluate CcNav through guided sessions and semi-structured interviews. We reflect on our design process, particularly the immersive elements, and on the transferability of design studies through our experience with a previous design study on program analysis.", "keywords": "Design study,program analysis,compilation,binary code,transferability,immersion", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030357", "refList": ["10.1109/icse.2015.210", "10.1109/icsme.2014.48", "10.1109/icsme.2014.101", "10.1109/msr.2009.5069475", "10.1109/tse.2002.995435", "10.1145/3238147.3238169", "10.1145/1062455.1062514", "10.1109/visual.1991.175815", "10.1109/vissoft.2019.00010", "10.1109/icse.2012.6227122", "10.1109/tse.2007.70731", "10.1145/2568225.2568233", "10.1109/asew.2008.4686322", "10.1109/botse.2019.00018", "10.1109/msr.2016.020", "10.1016/s0377-2217(97)00147-1", "10.1109/issre.2014.17", "10.1109/icpc.2011.39", "10.1109/icpc.2019.00017", "10.1109/icse.2004.1317461", "10.1109/hicss.2014.231", "10.1109/msr.2012.6224299", "10.1109/tvcg.2017.2744199", "10.1109/32.177365", "10.1109/icsme.2016.39", "10.1109/icsm.2015.7332446", "10.1007/978-3-319-22698-9\\_1", "10.1191/1478088706qp063oa", "10.1002/smr.1936", "10.1145/2635868.2661676", "10.1109/vissoft.2014.28", "10.1006/ijhc.2000.0420", "10.1109/vlhcc.2013.6645254", "10.1109/vissoft.2015.7332421", "10.1109/icsme.2018.00060", "10.1145/2786805.2786855", "10.1109/saner.2019.8668034", "10.1109/vissoft.2018.00009", "10.1145/1743546.1743567", "10.1145/2901739.2901768", "10.1109/vissoft.2016.6", "10.1109/tvcg.2017.2745078", "10.1191/1478088706qp0630a"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030419", "title": "Comparative Layouts Revisited: Design Space, Guidelines, and Future Directions", "year": "2020", "conferenceName": "InfoVis", "authors": "Sehi L'Yi;Jaemin Jo;Jinwook Seo", "citationCount": "0", "affiliation": "L'Yi, S (Corresponding Author), Harvard Med Sch, Boston, MA 02115 USA. L'Yi, Sehi, Harvard Med Sch, Boston, MA 02115 USA. Jo, Jaemin, Sungkyunkwan Univ, Seoul, South Korea. Seo, Jinwook, Seoul Natl Univ, Seoul, South Korea.", "countries": "USA;Korea", "abstract": "We present a systematic review on three comparative layouts-juxtaposition, superposition, and explicit-encoding-which are information visualization (InfoVis) layouts designed to support comparison tasks. For the last decade, these layouts have served as fundamental idioms in designing many visualization systems. However, we found that the layouts have been used with inconsistent terms and confusion, and the lessons from previous studies are fragmented. The goal of our research is to distill the results from previous studies into a consistent and reusable framework. We review 127 research papers, including 15 papers with quantitative user studies, which employed comparative layouts. We first alleviate the ambiguous boundaries in the design space of comparative layouts by suggesting lucid terminology (e.g., chart-wise and item-wise juxtaposition). We then identify the diverse aspects of comparative layouts, such as the advantages and concerns of using each layout in the real-world scenarios and researchers' approaches to overcome the concerns. Building our knowledge on top of the initial insights gained from the Gleicher et al.'s survey [19], we elaborate on relevant empirical evidence that we distilled from our survey (e.g., the actual effectiveness of the layouts in different study settings) and identify novel facets that the original work did not cover (e.g., the familiarity of the layouts to people). Finally, we show the consistent and contradictory results on the performance of comparative layouts and offer practical implications for using the layouts by suggesting trade-offs and seven actionable guidelines.", "keywords": "Comparative layout,visual comparison,literature review,juxtaposition,superposition,explicit-encoding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030419", "refList": ["10.1109/tvcg.2013.233", "10.1111/cgf.12380", "10.1177/1473871611416549", "10.1145/2702123.2702419", "10.1109/tvcg.2014.2322363", "10.1111/cgf.12791", "10.1145/2702123.2702130", "10.1145/2702123.2702217", "10.1109/tvcg.2012.237", "10.1177/1473871613480062", "10.1109/mcg.2017.377152546", "10.1109/tvcg.2013.213", "10.1111/cgf.12369", "10.1109/tvcg.2017.2744198", "10.1145/3139295.3139309", "10.1109/tvcg.2019.2934801", "10.1109/tvcg.2013.122", "10.1109/tvcg.2017.2747545", "10.1109/tvcg.2015.2413774", "10.1037/0096-1523.24.3.719", "10.1109/tvcg.2014.2346320", "10.1109/tvcg.2007.70535", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1145/2556288.2557141", "10.1109/tvcg.2013.149", "10.1145/1165734.1165736", "10.5220/0006127502170224", "10.1109/tvcg.2017.2745298", "10.1177/1473871617692841", "10.1190/int-2017-0083.1", "10.1190/int-2014-0283.1", "10.1109/tvcg.2016.2598796", "10.1111/cgf.13401", "10.1016/j.cag.2017.05.005", "10.1177/1473871616667632", "10.1145/3025453.3025882", "10.1145/2470654.2470724", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2010.164", "10.1145/3103010.3103013", "10.1109/pacificvis.2016.7465266", "10.1109/pacificvis.2012.6183556", "10.1109/tvcg.2015.2467751", "10.1109/tvcg.2018.2796557", "10.1111/cgf.13531", "10.1109/tvcg.2013.161", "10.1109/iv.2018.00051", "10.1109/tvcg.2010.162", "10.1109/tvcg.2018.2864510", "10.1109/iv.2017.30", "10.1109/tvcg.2007.70623"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030414", "title": "Githru: Visual Analytics for Understanding Software Development History Through Git Metadata Analysis", "year": "2020", "conferenceName": "VAST", "authors": "Youngtaek Kim;Jaeyoung Kim;Hyeon Jeon;Young-Ho Kim;Hyunjoo Song;Bo Hyoung Kim;Jinwook Seo", "citationCount": "0", "affiliation": "Kim, Y (Corresponding Author), Seoul Natl Univ, Seoul, South Korea. Kim, Youngtaek; Kim, Jaeyoung; Seo, Jinwook, Seoul Natl Univ, Seoul, South Korea. Kim, Youngtaek, Samsung Elect, Seoul, South Korea. Jeon, Hyeon, POSTECH, Pohang, South Korea. Kim, Young-Ho, Univ Maryland, College Pk, MD 20742 USA. Song, Hyunjoo, Soongsil Univ, Seoul, South Korea. Kim, Bohyoung, Hankuk Univ Foreign Studies, Seoul, South Korea.", "countries": "USA;Korea", "abstract": "Git metadata contains rich information for developers to understand the overall context of a large software development project. Thus it can help new developers, managers, and testers understand the history of development without needing to dig into a large pile of unfamiliar source code. However, the current tools for Git visualization are not adequate to analyze and explore the metadata: They focus mainly on improving the usability of Git commands instead of on helping users understand the development history. Furthermore, they do not scale for large and complex Git commit graphs, which can play an important role in understanding the overall development history. In this paper, we present Githru, an interactive visual analytics system that enables developers to effectively understand the context of development history through the interactive exploration of Git metadata. We design an interactive visual encoding idiom to represent a large Git graph in a scalable manner while preserving the topological structures in the Git graph. To enable scalable exploration of a large Git commit graph, we propose novel techniques (graph reconstruction, clustering, and Context-Preserving Squash Merge (CSM) methods) to abstract a large-scale Git commit graph. Based on these Git commit graph abstraction techniques, Githru provides an interactive summary view to help users gain an overview of the development history and a comparison view in which users can compare different clusters of commits. The efficacy of Githru has been demonstrated by case studies with domain experts using real-world, in-house datasets from a large software development team at a major international IT company. A controlled user study with 12 developers comparing Githru to previous tools also confirms the effectiveness of Githru in terms of task completion time.", "keywords": "git,history,exploration,overview,repository,visualization,cluster,DAG", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030414", "refList": ["10.1109/icse.2015.210", "10.1109/icsme.2014.48", "10.1109/icsme.2014.101", "10.1109/vissoft.2019.00019", "10.1109/msr.2009.5069475", "10.1109/tse.2002.995435", "10.1145/3238147.3238169", "10.1145/1062455.1062514", "10.1109/vissoft.2018.00015", "10.1109/vissoft.2014.30", "10.1109/visual.1991.175815", "10.1109/tvcg.2009.123", "10.1109/vissoft.2019.00010", "10.1109/vl.1996.545307", "10.1109/icse.2012.6227122", "10.1109/tse.2007.70731", "10.1145/2568225.2568233", "10.1109/asew.2008.4686322", "10.1145/3196321.3197546", "10.1109/botse.2019.00018", "10.1109/msr.2016.020", "10.1016/s0377-2217(97)00147-1", "10.1109/issre.2014.17", "10.1109/icpc.2011.39", "10.1109/icpc.2019.00017", "10.1109/icse.2004.1317461", "10.1109/hicss.2014.231", "10.1109/msr.2012.6224299", "10.1109/tvcg.2017.2744199", "10.1109/32.177365", "10.1109/vissoft.2019.00009", "10.1109/icsme.2016.39", "10.1145/3338906.3338979", "10.1109/icsm.2015.7332446", "10.1007/978-3-319-22698-9\\_1", "10.1191/1478088706qp063oa", "10.1002/smr.1936", "10.1145/2635868.2661676", "10.1109/vissoft.2014.28", "10.1006/ijhc.2000.0420", "10.1109/vlhcc.2013.6645254", "10.1109/vissoft.2015.7332421", "10.1109/icsme.2018.00060", "10.1145/2786805.2786855", "10.1145/1181775.1181779", "10.1109/saner.2019.8668034", "10.1109/vissoft.2018.00009", "10.1145/1743546.1743567", "10.1145/2901739.2901768", "10.1109/vissoft.2016.9", "10.1109/vissoft.2016.6", "10.1145/2597073.2597074", "10.1191/1478088706qp0630a"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/pacificvis48177.2020.1028", "year": "2020", "title": "A Radial Visualisation for Model Comparison and Feature Identification", "conferenceName": "PacificVis", "authors": "Jianlong Zhou;Weidong Huang;Fang Chen", "citationCount": "0", "affiliation": "Zhou, JL (Corresponding Author), Univ Technol Sydney, Data Sci Inst, Sydney, NSW, Australia.\nZhou, Jianlong; Chen, Fang, Univ Technol Sydney, Data Sci Inst, Sydney, NSW, Australia.\nHuang, Weidong, Univ Technol Sydney, Fac Transdisciplinary Innovat, Sydney, NSW, Australia.", "countries": "Australia", "abstract": "Machine Learning (ML) plays a key role in various intelligent systems, and building an effective ML model for a data set is a difficult task involving various steps including data cleaning, feature definition and extraction, ML algorithms development, model training and evaluation as well as others. One of the most important steps in the process is to compare generated substantial amounts of ML models to find the optimal one for the deployment. It is challenging to compare such models with dynamic number of features. This paper proposes a novel visualisation approach based on a radial net to compare ML models trained with a different number of features of a given data set while revealing implicit dependent relations. In the proposed approach, ML models and features are represented by lines and arcs respectively. The dependence of ML models with dynamic number of features is encoded into the structure of visualisation, where ML models and their dependent features are directly revealed from related line connections. ML model performance information is encoded with colour and line width in the innovative visualisation. Together with the structure of visualization, feature importance can be directly discerned to help to understand ML models.", "keywords": "Machine learning; performance; comparison; visualisation", "link": "https://doi.org/10.1109/PacificVis48177.2020.1028", "refList": ["10.1109/tvcg.2012.215", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2018.2864526", "10.1016/j.neucom.2011.10.038", "10.1063/1.3122936", "10.1145/2687924", "10.1109/tvcg.2009.23", "10.1016/j.compmedimag.2007.10.007", "10.1007/978-3-319-90403-0", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2018.2864499", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13401", "year": "2018", "title": "Towards Easy Comparison of Local Businesses Using Online Reviews", "conferenceName": "EuroVis", "authors": "Yong Wang;Hammad Haleem;Conglei Shi;Yanhong Wu;Xun Zhao;Siwei Fu;Huamin Qu", "citationCount": "0", "affiliation": "Wang, Y (Corresponding Author), Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.\nWang, Yong; Haleem, Hammad; Zhao, Xun; Fu, Siwei; Qu, Huamin, Hong Kong Univ Sci \\& Technol, Hong Kong, Peoples R China.\nShi, Conglei, Airbnb Inc, San Francisco, CA USA.\nWu, Yanhong, Vis Res, Palo Alto, CA USA.", "countries": "USA;China", "abstract": "With the rapid development of e-commerce, there is an increasing number of online review websites, such as Yelp, to help customers make better purchase decisions. Viewing online reviews, including the rating score and text comments by other customers, and conducting a comparison between different businesses are the key to making an optimal decision. However, due to the massive amount of online reviews, the potential difference of user rating standards, and the significant variance of review time, length, details and quality, it is difficult for customers to achieve a quick and comprehensive comparison. In this paper, we present E-Comp, a carefully-designed visual analytics system based on online reviews, to help customers compare local businesses at different levels of details. More specifically, intuitive glyphs overlaid on maps are designed for quick candidate selection. Grouped Sankey diagram visualizing the rating difference by common customers is chosen for more reliable comparison of two businesses. Augmented word cloud showing adjective-noun word pairs, combined with a temporal view, is proposed to facilitate in-depth comparison of businesses in terms of different time periods, rating scores and features. The effectiveness and usability of E-Comp are demonstrated through a case study and in-depth user interviews.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13401", "refList": ["10.1007/978-3-211-77280-5\\_4", "10.1177/1473871611416549", "10.1145/2702123.2702476", "10.1145/1014052.1014073", "10.1109/tvcg.2014.2346249", "10.1007/978-3-540-33037-08", "10.1109/tvcg.2007.70570", "10.1016/j.ijhm.2008.06.011", "10.1109/mic.2003.1167344", "10.1177/0047287513481274", "10.1109/iv.2013.5", "10.1111/j.1467-8659.2008.01205.x", "10.1561/1500000001", "10.1007/978-3-319-30319-2\\_13", "10.1109/tvcg.2013.254", "10.1007/978-3-540-33037-0\\_8", "10.1109/tvcg.2013.122", "10.1002/acp.2350050106", "10.1109/tvcg.2016.2598590", "10.2312/eurovisshort.20161167", "10.1007/s11002-013-9278-6", "10.1111/cgf.12888", "10.1145/1134707.1134743", "10.1109/tvcg.2015.2467691", "10.1109/tvcg.2010.183", "10.1109/vast.2009.5333919", "10.1109/tvcg.2017.2744199", "10.1111/cgf.13217", "10.1145/1060745.1060797", "10.1162/jmlr.2003.3.4-5.951", "10.1109/tvcg.2017.2723397", "10.2753/jec1086-4415170204", "10.1287/mksc.1110.0653", "10.1109/tvcg.2012.110", "10.1086/268567", "10.1109/tvcg.2017.2744738", "10.1007/978-3-540", "10.1109/tvcg.2014.2346912", "10.1109/mis.2013.36", "10.1109/tvcg.2013.173", "10.1109/tvcg.2006.76", "10.1109/tvcg.2017.2745298", "10.1559/152304009788188808"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030411", "title": "Co-Bridges: Pair-wise Visual Connection and Comparison for Multi-item Data Streams", "year": "2020", "conferenceName": "VAST", "authors": "Siming Chen;Natalia V. Andrienko;Gennady L. Andrienko;Jie Li 0006;Xiaoru Yuan", "citationCount": "0", "affiliation": "Yuan, XR (Corresponding Author), Peking Univ, Beijing, Peoples R China. Chen, Siming, Fudan Univ, Sch Data Sci, Shanghai, Peoples R China. Chen, Siming; Andrienko, Natalia; Andrienko, Gennady, Fraunhofer Inst IAIS, St Augustin, Germany. Andrienko, Natalia; Andrienko, Gennady, City Univ London, London, England. Li, Jie, Tianjin Univ, Tianjin, Peoples R China. Yuan, Xiaoru, Peking Univ, Beijing, Peoples R China.", "countries": "Germany;China;England", "abstract": "In various domains, there are abundant streams or sequences of multi-item data of various kinds, e.g. streams of news and social media texts, sequences of genes and sports events, etc. Comparison is an important and general task in data analysis. For comparing data streams involving multiple items (e.g., words in texts, actors or action types in action sequences, visited places in itineraries, etc.), we propose Co-Bridges, a visual design involving connection and comparison techniques that reveal similarities and differences between two streams. Co-Bridges use river and bridge metaphors, where two sides of a river represent data streams, and bridges connect temporally or sequentially aligned segments of streams. Commonalities and differences between these segments in terms of involvement of various items are shown on the bridges. Interactive query tools support the selection of particular stream subsets for focused exploration. The visualization supports both qualitative (common and distinct items) and quantitative (stream volume, amount of item involvement) comparisons. We further propose Comparison-of-Comparisons, in which two or more Co-Bridges corresponding to different selections are juxtaposed. We test the applicability of the Co-Bridges in different domains, including social media text streams and sports event sequences. We perform an evaluation of the users' capability to understand and use Co-Bridges. The results confirm that Co-Bridges is effective for supporting pair-wise visual comparisons in a wide range of applications.", "keywords": "Visual Comparison,Pair-wise Analysis,Multi-item Data Stream,Social Media", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030411", "refList": ["10.1109/tvcg.2014.2346753", "10.1109/pacificvis.2010.5429590", "10.1109/vast.2009.5333443", "10.1101/gr.2289704", "10.1177/1473871611416549", "10.1057/palgrave.ivs.9500099", "10.1109/vast.2017.8585638", "10.1109/vast.2014.7042496", "10.1109/tvcg.2017.2764459", "10.1109/tvcg.2013.221", "10.1109/vast.2011.6102439", "10.1109/tvcg.2013.213", "10.1109/tmm.2016.2614220", "10.1109/tvcg.2016.2598797", "10.1145/2207676.2208556", "10.1145/1835804.1835827", "10.1109/tvcg.2013.124", "10.2312/conf/eg2013/stars/039-063", "10.1111/cgf.13211", "10.1109/tvcg.2014.2346920", "10.1109/tvcg.2011.239", "10.1016/j.jvlc.2018.08.008", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934535", "10.1109/tvcg.2018.2864526", "10.1007/978-0-85729-436-4\\_9", "10.1109/vast.2016.7883511", "10.1109/tvcg.2014.2346682", "10.1109/tvcg.2015.2467618", "10.1145/2566486.2567977", "10.1109/tvcg.2017.2745320", "10.1080/136588199241247", "10.1111/cgf.13401", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2012.291", "10.1109/vast.2012.6400485", "10.1109/pacificvis.2016.7465266", "10.1109/tvcg.2011.232", "10.1109/tvcg.2017.2745083", "10.1109/tvcg.2012.253", "10.1007/s12650-014-0246-x", "10.1109/tvcg.2010.20", "10.1109/tvcg.2014.2346919", "10.1109/visual.2019.8933646", "10.1007/978-0-85729-079-3"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030419", "title": "Comparative Layouts Revisited: Design Space, Guidelines, and Future Directions", "year": "2020", "conferenceName": "InfoVis", "authors": "Sehi L'Yi;Jaemin Jo;Jinwook Seo", "citationCount": "0", "affiliation": "L'Yi, S (Corresponding Author), Harvard Med Sch, Boston, MA 02115 USA. L'Yi, Sehi, Harvard Med Sch, Boston, MA 02115 USA. Jo, Jaemin, Sungkyunkwan Univ, Seoul, South Korea. Seo, Jinwook, Seoul Natl Univ, Seoul, South Korea.", "countries": "USA;Korea", "abstract": "We present a systematic review on three comparative layouts-juxtaposition, superposition, and explicit-encoding-which are information visualization (InfoVis) layouts designed to support comparison tasks. For the last decade, these layouts have served as fundamental idioms in designing many visualization systems. However, we found that the layouts have been used with inconsistent terms and confusion, and the lessons from previous studies are fragmented. The goal of our research is to distill the results from previous studies into a consistent and reusable framework. We review 127 research papers, including 15 papers with quantitative user studies, which employed comparative layouts. We first alleviate the ambiguous boundaries in the design space of comparative layouts by suggesting lucid terminology (e.g., chart-wise and item-wise juxtaposition). We then identify the diverse aspects of comparative layouts, such as the advantages and concerns of using each layout in the real-world scenarios and researchers' approaches to overcome the concerns. Building our knowledge on top of the initial insights gained from the Gleicher et al.'s survey [19], we elaborate on relevant empirical evidence that we distilled from our survey (e.g., the actual effectiveness of the layouts in different study settings) and identify novel facets that the original work did not cover (e.g., the familiarity of the layouts to people). Finally, we show the consistent and contradictory results on the performance of comparative layouts and offer practical implications for using the layouts by suggesting trade-offs and seven actionable guidelines.", "keywords": "Comparative layout,visual comparison,literature review,juxtaposition,superposition,explicit-encoding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030419", "refList": ["10.1109/tvcg.2013.233", "10.1111/cgf.12380", "10.1177/1473871611416549", "10.1145/2702123.2702419", "10.1109/tvcg.2014.2322363", "10.1111/cgf.12791", "10.1145/2702123.2702130", "10.1145/2702123.2702217", "10.1109/tvcg.2012.237", "10.1177/1473871613480062", "10.1109/mcg.2017.377152546", "10.1109/tvcg.2013.213", "10.1111/cgf.12369", "10.1109/tvcg.2017.2744198", "10.1145/3139295.3139309", "10.1109/tvcg.2019.2934801", "10.1109/tvcg.2013.122", "10.1109/tvcg.2017.2747545", "10.1109/tvcg.2015.2413774", "10.1037/0096-1523.24.3.719", "10.1109/tvcg.2014.2346320", "10.1109/tvcg.2007.70535", "10.1109/mcg.2018.032421661", "10.1109/tvcg.2017.2744199", "10.1145/2556288.2557141", "10.1109/tvcg.2013.149", "10.1145/1165734.1165736", "10.5220/0006127502170224", "10.1109/tvcg.2017.2745298", "10.1177/1473871617692841", "10.1190/int-2017-0083.1", "10.1190/int-2014-0283.1", "10.1109/tvcg.2016.2598796", "10.1111/cgf.13401", "10.1016/j.cag.2017.05.005", "10.1177/1473871616667632", "10.1145/3025453.3025882", "10.1145/2470654.2470724", "10.1109/tvcg.2007.70539", "10.1109/tvcg.2018.2864884", "10.1109/tvcg.2010.164", "10.1145/3103010.3103013", "10.1109/pacificvis.2016.7465266", "10.1109/pacificvis.2012.6183556", "10.1109/tvcg.2015.2467751", "10.1109/tvcg.2018.2796557", "10.1111/cgf.13531", "10.1109/tvcg.2013.161", "10.1109/iv.2018.00051", "10.1109/tvcg.2010.162", "10.1109/tvcg.2018.2864510", "10.1109/iv.2017.30", "10.1109/tvcg.2007.70623"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13681", "year": "2019", "title": "A User-based Visual Analytics Workflow for Exploratory Model Analysis", "conferenceName": "EuroVis", "authors": "Dylan Cashman;Shah Rukh Humayoun;Florian Heimerl;Kendall Park;Subhajit Das;John Thompson;Bahador Saket;Abigail Mosca;John T. Stasko;Alex Endert;Michael Gleicher;Remco Chang", "citationCount": "6", "affiliation": "Cashman, D; Humayoun, SR (Corresponding Author), Tufts Univ, Medford, MA 02155 USA.\nCashman, Dylan; Humayoun, Shah Rukh; Mosca, Abigail; Chang, Remco, Tufts Univ, Medford, MA 02155 USA.\nHeimerl, Florian; Park, Kendall; Gleicher, Michael, Georgia Tech, Atlanta, GA USA.\nDas, Subhajit; Thompson, John; Saket, Bahador; Stasko, John; Endert, Alex, Univ Wisconsin, Madison, WI USA.", "countries": "USA", "abstract": "Many visual analytics systems allow users to interact with machine learning models towards the goals of data exploration and insight generation on a given dataset. However, in some situations, insights may be less important than the production of an accurate predictive model for future use. In that case, users are more interested in generating of diverse and robust predictive models, verifying their performance on holdout data, and selecting the most suitable model for their usage scenario. In this paper, we consider the concept of Exploratory Model Analysis (EMA), which is defined as the process of discovering and selecting relevant models that can be used to make predictions on a data source. We delineate the differences between EMA and the well-known term exploratory data analysis in terms of the desired outcome of the analytic process: insights into the data or a set of deployable models. The contributions of this work are a visual analytics system workflow for EMA, a user study, and two use cases validating the effectiveness of the workflow. We found that our system workflow enabled users to generate complex models, to assess them for various qualities, and to select the most relevant model for their task.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13681", "refList": ["10.1109/tvcg.2017.2744683", "10.1111/cgf.12639", "10.1007/s11390-016-1663-1", "10.1109/tvcg.2017.2744938", "10.1117/12.2007316", "10.1109/tvcg.2017.2745178", "10.1145/2702123.2702509", "10.1080/02701367.1992.10608764", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2011.185", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2013.125", "10.1109/mcse.2007.55", "10.1007/978-3-540-70956-5", "10.1109/vl.1996.545307", "10.1901/jeab.1979.31-433", "10.1111/j.1467-8659.2009.01475.x", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2012.65", "10.1145/1835804.1835827", "10.1109/tvcg.2017.2745085", "10.1145/2939672.2939778", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2017.2744805", "10.1145/2487575.2487629", "10.1109/tvcg.2013.157", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2011.209", "10.1109/vast.2012.6400490", "10.1109/tvcg.2015.2513410", "10.1007/978-3-540-79347-2\\_3", "10.1109/tvcg.2014.2346431", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2012.277", "10.1109/infvis.1998.729560", "10.1109/infvis.2004.64", "10.1109/mcg.2006.70", "10.1109/tvcg.2012.260", "10.1109/tvcg.2014.2346660", "10.1145/1743546.1743567", "10.1111/cgf.13417", "10.1109/tvcg.2014.2346325", "10.1109/tvcg.2018.2864838", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2003.1207445", "10.1109/vast.2010.5652443", "10.1145/2641190.2641198", "10.1109/tvcg.2016.2598830", "10.1109/tvcg.2017.2744378", "10.1111/cgf.13324", "10.1109/mcg.2009.22", "10.1109/tvcg.2016.2599030", "10.1109/vast.2012.6400486", "10.1109/tvcg.2016.2598831", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934261", "title": "Ablate, Variate, and Contemplate: Visual Analytics for Discovering Neural Architectures", "year": "2019", "conferenceName": "VAST", "authors": "Dylan Cashman;Adam Perer;Remco Chang;Hendrik Strobelt", "citationCount": "1", "affiliation": "Cashman, D (Corresponding Author), Tufts Univ, Medford, MA 02155 USA. Cashman, Dylan; Chang, Remco, Tufts Univ, Medford, MA 02155 USA. Perer, Adam, Carnegie Mellon Univ, Pittsburgh, PA 15213 USA. Strobelt, Hendrik, MIT IBM Watson AI Lab, Cambridge, MA USA.", "countries": "USA", "abstract": "The performance of deep learning models is dependent on the precise configuration of many layers and parameters. However, there are currently few systematic guidelines for how to configure a successful model. This means model builders often have to experiment with different configurations by manually programming different architectures (which is tedious and time consuming) or rely on purely automated approaches to generate and train the architectures (which is expensive). In this paper, we present Rapid Exploration of Model Architectures and Parameters, or REMAP, a visual analytics tool that allows a model builder to discover a deep learning model quickly via exploration and rapid experimentation of neural network architectures. In REMAP, the user explores the large and complex parameter space for neural network architectures using a combination of global inspection and local experimentation. Through a visual overview of a set of models, the user identifies interesting clusters of architectures. Based on their findings, the user can run ablation and variation experiments to identify the effects of adding, removing, or replacing layers in a given architecture and generate new models accordingly. They can also handcraft new models using a simple graphical interface. As a result, a model builder can build deep learning models quickly, efficiently, and without manual programming. We inform the design of REMAP through a design study with four deep learning model builders. Through a use case, we demonstrate that REMAP allows users to discover performant neural network architectures efficiently using visual exploration and user-defined semi-automated searches through the model space.", "keywords": "visual analytics,neural networks,parameter space exploration", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934261", "refList": ["10.1109/mcg.2018.2878902", "10.1111/cgf.12639", "10.1109/tvcg.2017.2744938", "10.1117/12.2007316", "10.1109/cvpr.2014.81", "10.1016/j.csda.2008.02.031", "10.1080/00994480.2000.10748487", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2013.125", "10.1109/cvpr.2015.7298594", "10.1111/j.1467-8659.2009.01475.x", "10.1109/tvcg.2012.65", "10.1109/tvcg.2017.2745085", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2017.2744805", "10.1145/2487575.2487629", "10.1109/tvcg.2018.2865044", "10.23915/distill.00010", "10.1109/72.279181", "10.1109/tvcg.2017.2744199", "10.1007/s13398-014-0173-7.2", "10.1109/tvcg.2018.2864504", "10.1109/vast.2012.6400490", "10.1007/978-3-319-10590-1\\_53", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2014.2346321", "10.1094/pdis-11-11-0999-pdn", "10.1109/ijcnn.2015.7280767", "10.1109/tvcg.2017.2744878", "10.5555/3326943.3327130", "10.1109/tvcg.2017.2744718", "10.1109/iccv.2015.169", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/cvpr.2014.223", "10.1109/cvpr.2016.90", "10.1109/vast.2010.5652443", "10.1111/cgf.13681", "10.1109/tvcg.2016.2598831", "10.1109/vast.2011.6102453"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3028888", "title": "A Visual Analytics Framework for Explaining and Diagnosing Transfer Learning Processes", "year": "2020", "conferenceName": "VAST", "authors": "Yuxin Ma;Arlen Fan;Jingrui He;Arun Reddy Nelakurthi;Ross Maciejewski", "citationCount": "0", "affiliation": "Ma, YX (Corresponding Author), Arizona State Univ, Tempe, AZ 85287 USA. Ma, Yuxin; Fan, Arlen; Maciejewski, Ross, Arizona State Univ, Tempe, AZ 85287 USA. He, Jingrui, Univ Illinois, Champaign, IL USA. Nelakurthi, Arun Reddy, Samsung Res Amer, Mountain View, CA USA.", "countries": "USA", "abstract": "Many statistical learning models hold an assumption that the training data and the future unlabeled data are drawn from the same distribution. However, this assumption is difficult to fulfill in real-world scenarios and creates barriers in reusing existing labels from similar application domains. Transfer Learning is intended to relax this assumption by modeling relationships between domains, and is often applied in deep learning applications to reduce the demand for labeled data and training time. Despite recent advances in exploring deep learning models with visual analytics tools, little work has explored the issue of explaining and diagnosing the knowledge transfer process between deep learning models. In this paper, we present a visual analytics framework for the multi-level exploration of the transfer learning processes when training deep neural networks. Our framework establishes a multi-aspect design to explain how the learned knowledge from the existing model is transferred into the new learning task when training deep neural networks. Based on a comprehensive requirement and task analysis, we employ descriptive visualization with performance measures and detailed inspections of model behaviors from the statistical, instance, feature, and model structure levels. We demonstrate our framework through two case studies on image classification by fine-tuning AlexNets to illustrate how analysts can utilize our framework.", "keywords": "Transfer learning,deep learning,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3028888", "refList": ["10.1109/tvcg.2014.2346578", "10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2016.2598838", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1109/tpami.2018.2868685", "10.1145/2702123.2702509", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/tvcg.2016.2598828", "10.1111/j.1467-8659.2011.01898.x", "10.1109/tvcg.2013.65", "10.1145/2976749.2978318", "10.1007/978-3-030-01424-7\\_27", "10.1109/tvcg.2019.2934261", "10.1007/s11704-016-6028-y", "10.1016/j.visinf.2017.01.006", "10.1145/2858036.2858529", "10.1109/iccv.2015.279", "10.1109/mci.2018.2840738", "10.1109/tvcg.2019.2892483", "10.1109/vast.2018.8802509", "10.1109/tvcg.2013.124", "10.1186/s40537-016-0043-6", "10.1109/tvcg.2018.2864475", "10.1145/3200489", "10.1109/tvcg.2018.2865044", "10.1111/cgf.13210", "10.1109/tvcg.2018.2816223", "10.23915/distill.00007", "10.1109/tvcg.2017.2744199", "10.1109/tkde.2018.2876857", "10.1109/tvcg.2019.2934631", "10.1109/tvcg.2018.2864504", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2014.2346594", "10.1109/tvcg.2011.188", "10.1007/978-3-642-15561-1\\_16", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1109/tvcg.2017.2744718", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2016.2598541", "10.1109/tkde.2009.191", "10.1145/3065386", "10.1016/j.ins.2016.03.021", "10.1109/tvcg.2019.2903943", "10.1007/s10994-009-5152-4", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2018.2864500", "10.1109/iccv.2017.74", "10.1109/tvcg.2017.2744158", "10.1109/tvcg.2012.207", "10.1111/cgf.13092", "10.1109/tvcg.2018.2865027", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2017.2754480", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}], "len": 9}, {"doi": "10.1111/cgf.14034", "year": "2020", "title": "The State of the Art in Enhancing Trust in Machine Learning Models with the Use of Visualizations", "conferenceName": "EuroVis", "authors": "Angelos Chatzimparmpas;Rafael Messias Martins;Ilir Jusufi;Kostiantyn Kucher;Fabrice Rossi;Andreas Kerren", "citationCount": "2", "affiliation": "Chatzimparmpas, A (Corresponding Author), Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nChatzimparmpas, A.; Martins, R. M.; Jusufi, I.; Kucher, K.; Kerren, A., Linnaeus Univ, Dept Comp Sci \\& Media Technol, Vaxjo, Sweden.\nRossi, F., PSL Univ, Univ Paris Dauphine, Ceremade, Paris, France.", "countries": "Sweden;France", "abstract": "Machine learning (ML) models are nowadays used in complex applications in various domains, such as medicine, bioinformatics, and other sciences. Due to their black box nature, however, it may sometimes be hard to understand and trust the results they provide. This has increased the demand for reliable visualization tools related to enhancing trust in ML models, which has become a prominent topic of research in the visualization community over the past decades. To provide an overview and present the frontiers of current research on the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in ML models with the use of interactive visualization. We define and describe the background of the topic, introduce a categorization for visualization techniques that aim to accomplish this goal, and discuss insights and opportunities for future research directions. Among our contributions is a categorization of trust against different facets of interactive ML, expanded and improved from previous research. Our results are investigated from different analytical perspectives: (a) providing a statistical overview, (b) summarizing key findings, (c) performing topic analyses, and (d) exploring the data sets used in the individual papers, all with the support of an interactive web-based survey browser. We intend this survey to be beneficial for visualization researchers whose interests involve making ML models more trustworthy, as well as researchers and practitioners from other disciplines in their search for effective visualization techniques suitable for solving their tasks with confidence and conveying meaning to their data.", "keywords": "trustworthy machine learning; visualization; interpretable machine learning; explainable machine learning", "link": "https://doi.org/10.1111/cgf.14034", "refList": ["10.1109/mcg.2018.2878902", "10.1109/tvcg.2008.153", "10.2312/mlvis.20181130", "10.1109/tvcg.2016.2598828", "10.1145/3290605.3300911", "10.1145/2993901.2993909", "10.2312/mlvis.20191158", "10.1109/tvcg.2016.2598446", "10.1111/j.1467-8659.2009.01475.x", "10.1145/3290605.3300809", "10.1109/pacificvis.2018.00031", "10.1055/s-0029-1216348", "10.1007/978-3-319-90403-0\\_13", "10.1109/tvcg.2017.2745085", "10.1111/j.1467-8659.2011.01938.x", "10.1007/978-3-319-54024-5\\_6", "10.1016/s0167-9473(01)00065-2", "10.1111/cgf.12895", "10.2312/eurovisshort.20141152.17", "10.2312/eurova.20151098.22", "10.1111/cgf.13667", "10.3115/1072228.1072378", "10.1109/tbdata.2018.2877350.16", "10.1109/lars.2009.5418323.25", "10.1109/tvcg.2017.2744878", "10.1016/j.combustflame.2005.09.018", "10.1016/j.cag.2014.01.006", "10.1109/tvcg.2016.2570755", "10.2312/mlvis.20191158.1,19", "10.1518/hfes.46.1.50.30392", "10.1145/3301275.3302280", "10.1145/3351095.3372834.1", "10.1016/s0377-2217(01)00264-8", "10.1111/cgf.13424", "10.1007/978-3-319-23485-4\\_53", "10.1007/978-94-007-0753-5\\_3623", "10.1109/tvcg.2013.101", "10.1111/cgf.12884", "10.1111/j.1467-8659.2010.01835.x", "10.1021/ci9901338", "10.4230/lipics.itcs:2017", "10.1109/mis.2013.24", "10.1109/tvcg.2014.2346482", "10.1109/tvcg.2018.2865230", "10.1073/pnas.1807180116", "10.1109/dsaa.2018.00018", "10.1109/tvcg.2009.153", "10.3115/1219840.1219855", "10.1109/tvcg.2018.2864812", "10.1109/tvcg.2018.2872577", "10.2312/trvis.20191183", "10.1109/cvpr:2004.383", "10.1109/vast.2008.4677350", "10.1145/302979.303001", "10.1109/tvcg.2018.2864499", "10.1111/cgf.13672", "10.1109/tvcg.2014.2346626", "10.23915/distill.00010.18", "10.1109/tvcg.2017.2744805", "10.2312/mlvis:20191160.18", "10.23915/distill:00016", "10.1109/pacificvis.2016.7465260", "10.1111/cgf.13683", "10.23915/distill.00016.19", "10.1145/32906053.300803.22", "10.1109/tvcg.2014.2346574", "10.1177/1473871615600010", "10.1109/tvcg.2016.2598831", "10.1145/3230623", "10.1109/visual.2019.8933744", "10.1145/2993901.2993915", "10.1016/j.visinf.2017.01.006", "10.1145/2993901.2993914", "10.1109/tvcg.2014.2346984", "10.1109/5.726791", "10.23915/distill.00010", "10.1109/tvcg.2018.2864825", "10.2307/2394164", "10.1109/tvcg.2017.2744458", "10.1145/2993901.2993917.28", "10.1111/cgf.13711", "10.1109/tvcg.2016.2598664", "10.2307/2530428", "10.1109/icdar.1997.620583", "10.1109/tvcg.2017.2744718", "10.1111/cgf.13425", "10.1109/tvcg.2019.2903943", "10.1145/1518701.1518895", "10.1109/tvcg.2018.2864838", "10.1145/62038.62043", "10.1111/j.1467-8659.2011.01920.x", "10.1145/3025171.3025208", "10.1145/355112.355124", "10.1109/vast.2010.5652398", "10.1109/tvcg.2014.2346578", "10.1145/3173574.3174209", "10.4178/epih/e2014008", "10.1109/beliv.2018.8634201.25,28", "10.1016/j.eswa.2007.12.020", "10.1111/cgf:13406", "10.1109/mcg.2011.103", "10.1631/fitee.1700808", "10.1109/tvcg.2017.2745158", "10.1073/pnas.0307752101", "10.1109/tvcg.2018.2816223", "10.1109/tvcg.2017.2745141", "10.1145/32232.32234", "10.1109/tvcg.2019.2934267", "10.1109/tvcg.2018.2864477", "10.1145/3132169", "10.2312/eurova.20151100.19", "10.1145/3172944.3172964", "10.1145/2601248.2601268", "10.1109/isai-nlp.2018.8693002.1", "10.1111/cgf.13404", "10.1007/s100320200071", "10.2312/trvis.20191183.16", "10.1111/cgf.12755", "10.1145/2702123.2702509", "10.1145/1401890.1402008.25", "10.1109/tvcg.2012.65", "10.1177/1473871617733996", "10.2312/trvis.20191187.7", "10.21236/ada273556", "10.1109/vast.2010.5652450", "10.1111/j.1467-8659.2011.01940.x", "10.1177/875647939000600106", "10.1109/tvcg.2017.2711030", "10.1016/j.immuni.2016.04.014", "10.1109/tvcg.2019.2934209", "10.1016/0095-0696(78)90006-2", "10.1016/j.jclinepi.2015.04.014", "10.1016/j.ress.2013.02.013", "10.1111/cgf.12640", "10.1145/1015330.1015388.25", "10.1111/j.1467-8659.2009.01468.x", "10.1145/1541880.1541882", "10.1109/vast.2011.6102453", "10.1016/s0008-8846(98)00165-3", "10.2312/trvis.20191186.7", "10.1007/s13748-013-0040-3", "10.1109/tvcg.2015.2467757", "10.1109/tbdata.2018.2877350", "10.1109/vast.2017.8585613", "10.1080/10556789208805504", "10.1109/vast.2012.6400484", "10.1145/3025171.3025181", "10.1007/978-3-319-90403-0\\_12", "10.1109/mcg.2019.2922592", "10.1021/ci000392t", "10.1109/vast.2007.4389000", "10.1111/cgf.13406.16", "10.1111/cgf.13684", "10.2312/eurovisshort.20161166.17", "10.2312/evs:20191168", "10.1145/3041021.3055135", "10.1145/3301275.3302265", "10.1613/jair.4135", "10.1109/tvcg.2018.2864500", "10.1109/tvcg.2017.2744158", "10.1109/ssci.2015.33", "10.1111/cgf.13453", "10.2312/pe/eurovast/eurova11/049-052", "10.1145/3025171.3025181.13", "10.1145/371920.372094", "10.1109/tvcg.2017.2744938", "10.1111/j.1467-8659.2012.03108.x", "10.1109/tvcg.2019.2934251", "10.1145/3290605.3300809.18", "10.1109/ldav.2016.7874305", "10.1109/vast.2016.7883514", "10.1109/iccv.2015.425", "10.1145/3301275.3302324", "10.1109/tnnls.2013.2292894", "10.1109/tvcg.2018.2865044", "10.1109/tvcg.2013.157", "10.1371/journal.pcbi.0020161", "10.1186/s12916-019-1426-2", "10.1109/10.867928", "10.1111/cgf.13217", "10.1109/mcg.2019.2919033", "10.1145/2993901.2993910", "10.1109/tvcg.2017.2744738", "10.2307/258792", "10.1111/cgf.12655", "10.1016/j.dss.2014.03.001", "10.1109/tvcg.2019.2904069", "10.1111/cgf.13205", "10.1002/mds.22340", "10.1109/tvcg.2019.2934595", "10.1287/inte.2018.0957", "10.1109/cvpr.2007.382974", "10.1109/tvcg.2012.280", "10.1145/2678025.2701399.13", "10.2312/mlvis.20181130.13", "10.2312/eurova.20181106.16", "10.1109/tvcg.2018.2864475", "10.2312/mlvis.20191160.18,22", "10.1007/978-1-4612-4026-6.25", "10.1109/cvpr.2006.42", "10.1145/3172944.3172950", "10.1109/cvpr.2004.383.25", "10.1109/tvcg.2019.2934812", "10.1609/aimag.v35i4.2513", "10.2312/trvis.20191185.7", "10.1111/j.1467-8659.2009.01684.x", "10.1371/journal.pone.0129126", "10.1111/cgf.13092", "10.1162/coli\\_a\\_00237", "10.1109/tvcg.2017.2744818", "10.1109/vast.2011.6102448", "10.1109/tvcg.2009.111", "10.1109/tvcg.2019.2934619", "10.1016/s0377-2217(01)00264-8.25", "10.1111/cgf.12639", "10.1109/access.2019.2919343", "10.1186/s12874-019-0681-4", "10.1109/pacificvis.2015.7156366", "10.1109/pacificvis.2018.00029", "10.1109/tvcg.2019.2934261", "10.1080/13506280444000102.http://www.uvt.nl/ticc", "10.1109/vast.2018.8802509", "10.1111/cgf.13212", "10.1145/3200489", "10.1111/cgf.13176", "10.1177/1473871620904671", "10.1007/978-3-319-10590-1\\_53", "10.1111/j.1467-8659.2012.03126.x", "10.1111/cgf.13172", "10.1145/3025171.3025208.6,21", "10.1111/cgf.12366", "10.1109/tvcg.2019.2934659", "10.1109/cvprw.2009.5206848", "10.1016/j.media.2014.11.010", "10.1109/tvcg.2017.2744683", "10.1109/tvcg.2019.2934262", "10.1016/j.neucom.2006.11.018", "10.1177/1473871615571951", "10.1371/journal.pcbi.0020161.25", "10.1177/1473871611415994", "10.2312/trvis20191187", "10.2312/eurova", "10.1145/2858036.2858529", "10.1145/2207676.2207738", "10.1007/s11042-009-0417-2", "10.1145/3301275.3302317", "10.1038/s41746-019-0148-3", "10.1145/3351095.3372834", "10.1080/10691898.2011.11889627", "10.1109/tvcg.2018.2864504", "10.1109/vast.2017.8585720", "10.1109/mcg.2018.053491732", "10.1007/s10994-010-5207-6", "10.1109/tvcg.2019.2934433", "10.1016/j.cag.2017.05.005", "10.1109/tvcg.2012.279", "10.1145/3231622.3231641.19,20", "10.1145/1562849.1562851", "10.1007/11564126\\_", "10.1109/tvcg.2018.2846735", "10.3115/1225403.1225421", "10.1109/vast.2012.6400486", "10.2312/eurova.20191116.22", "10.1109/tvcg.2007.70443", "10.1080/027868291009242", "10.2312/eurova.20151100", "10.1145/2939502.2939503.19", "10.1016/j.cag.2014.02.004", "10.1145/2939672.2939778", "10.1109/vast.2010.5652484", "10.1111/cgf.12641", "10.1145/3301275.3302289", "10.1109/visual.2019.8933619", "10.1109/tvcg.2017.2672987", "10.1109/vast.2016.7883517.20", "10.1109/igarss.2017.8127684", "10.1016/j.jcae.2010.04.002", "10.1109/vast.2010.5652885", "10.1016/j.visinf.2019.03.002", "10.1007/s00371-018-1500-3", "10.1109/mcg.2018.042731661", "10.1109/34.598228", "10.1109/tvcg.2018.2865027", "10.1016/j.neucom.2017.01.105", "10.1111/cgf.12389", "10.1109/tvcg.2019.2934591", "10.1109/vast.2010.5652392.16", "10.1111/cgf.13416", "10.1080/02701367.1992.10608764", "10.1109/vast.2017.8585721", "10.1109/tvcg.2018.2843369", "10.1109/sbes.2010.27", "10.1109/vast.2015.7347637", "10.1145/1401890.1402008", "10.1016/j.bpj.2010.04.064", "10.1109/tvcg.2013.125", "10.1109/isai-nlp.2018.8693002", "10.1371/journal.pone.0160127", "10.1145/2856767.2856779", "10.1145/2678025.2701399", "10.1109/vast.2011.6102451", "10.1109/mcg.2010.18", "10.1021/ci4000213", "10.1109/vast.2012.6400492", "10.1007/11564126-49.25", "10.2312/eurova.20171114", "10.1109/vast.2010.5652443", "10.1145/3134599", "10.2312/pe/eurovast/eurovast10/013-018.19", "10.1109/tvcg.2019.2903946", "10.1109/tvcg.2015.2467717", "10.1177/1473871612460526", "10.1016/j.cag.2018.09.018", "10.1109/tvcg.2016.2598838", "10.1145/3172944.3172964.14", "10.1109/vast.2009.5333428", "10.1109/vast.2014.7042480", "10.2312/pe/eurovast/eurovast10/013-018", "10.1177/0962280215588241", "10.1145/2993901.2993917", "10.1109/cvpr.2008:4587581", "10.1109/tvcg.2015.2467551", "10.1016/j.visinf.2018.09.001", "10.1126/science.290.5500.2319", "10.2312/eurova.20151107", "10.1109/visual.2019.8933695", "10.13140/2.1.2393.1847", "10.1197/jamia.m1929", "10.1109/cvpr.2008.4587581.25", "10.1145/1518701.1518895.16", "10.1109/vds.2017.8573444", "10.2312/trvis:20191185", "10.1145/3359786", "10.13140/2.1.1341.1520", "10.2312/pe/eurovast/eurova11/049-052.17", "10.1109/tvcg.2014.2346660", "10.1145/3185517", "10.1109/adprl.2011.5967372", "10.1109/tvcg.2015.2467591", "10.1111/j.1751-9004.2009.00232.x", "10.1136/qshc.2004.010033", "10.1109/vast.2012.6400493", "10.1109/tvcg.2019.2934266", "10.1145/2971763.2971775", "10.1111/cgf.13730", "10.1109/vast.2012.6400488", "10.1111/cgf.13210", "10.1109/tvcg.2019.2934631", "10.1145/3172944.3172965", "10.1057/ivs.2010.2", "10.1109/tvcg.2017.2745258", "10.1016/j.jbusres.2019.07.039", "10.1162/jmlr.2003.3.4-5.993", "10.1109/pacificvis.2019.00044", "10.2312/pe/eurovast/eurova12/037-041", "10.1109/tvcg.2019.2934629", "10.1177/0018720814547570", "10.1145/3292500.3330908", "10.1111/j.1467-8659.2009.01467.x", "10.2312/eurova.20151098", "10.1177/1473871617713337", "10.1109/lars:2009.5418323", "10.1109/vast.2011.6102437", "10.1111/cgf.12878", "10.1561/1500000001", "10.1145/3025171.3025222", "10.1007/s11704-016-6028-y", "10.1016/j.procs.2017.05.216", "10.1109/cvpr.2007.382974.25", "10.1080/10447318.2020.1741118", "10.1109/vast.2012.6400489", "10.1145/3025171.3025172", "10.1109/tvcg.2017.2744098", "10.1109/tvcg.2005.66", "10.1016/j.dss.2009.05.016", "10.1007/978-1-4612-4026-6", "10.2312/evs.20191168.16,20,28", "10.2312/pe/eurovast/eurova12/037-041.17", "10.1145/3290605.3300803", "10.1145/1015330.1015388", "10.1111/cgf.13197", "10.1016/b978-1-55860-377-6.50048-7", "10.1111/cgf.13681", "10.1109/tvcg.2017.2744378", "10.1109/tvcg.2017.2744358", "10.1109/vast.2008.4677352"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030368", "title": "Implicit Multidimensional Projection of Local Subspaces", "year": "2020", "conferenceName": "InfoVis", "authors": "Rongzheng Bian;Yumeng Xue;Liang Zhou;Jian Zhang 0070;Baoquan Chen;Daniel Weiskopf;Yunhai Wang", "citationCount": "1", "affiliation": "Wang, YH (Corresponding Author), Shandong Univ, Qingdao, Peoples R China. Zhou, L (Corresponding Author), Univ Utah, Salt Lake City, UT 84112 USA. Bian, Rongzheng; Xue, Yumeng; Wang, Yunhai, Shandong Univ, Qingdao, Peoples R China. Chen, Baoquan, Peking Univ, Beijing, Peoples R China. Zhang, Jian, Chinese Acad Sci, CNIC, Beijing, Peoples R China. Zhou, Liang, Univ Utah, Salt Lake City, UT 84112 USA. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany.", "countries": "USA;Germany;China", "abstract": "We propose a visualization method to understand the effect of multidimensional projection on local subspaces, using implicit function differentiation. Here, we understand the local subspace as the multidimensional local neighborhood of data points. Existing methods focus on the projection of multidimensional data points, and the neighborhood information is ignored. Our method is able to analyze the shape and directional information of the local subspace to gain more insights into the global structure of the data through the perception of local structures. Local subspaces are fitted by multidimensional ellipses that are spanned by basis vectors. An accurate and efficient vector transformation method is proposed based on analytical differentiation of multidimensional projections formulated as implicit functions. The results are visualized as glyphs and analyzed using a full set of specifically-designed interactions supported in our efficient web-based visualization tool. The usefulness of our method is demonstrated using various multi- and high-dimensional benchmark datasets. Our implicit differentiation vector transformation is evaluated through numerical comparisons; the overall method is evaluated through exploration examples and use cases.", "keywords": "High-dimensional data visualization,dimensionality reduction,local linear subspaces,user interaction", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030368", "refList": ["10.3844/jcssp.2018.613.622", "10.1016/j.aci.2018.08.003", "10.3390/info9030065", "10.1016/j.visinf.2018.09.003", "10.1371/journal.pone.0118432", "10.1016/s0893-6080(05)80023-1", "10.1109/tvcg.2020.2986996", "10.1109/icst.2012.56", "10.1007/s10844-013-0250-y", "10.1109/tbdata.2018", "10.1145/1125451.1125659", "10.1109/tvcg.2013.125", "10.1177/1473871611412817", "10.1145/3290605.3300911", "10.1111/cgf.14034", "10.1007/s13721-013-0034-x", "10.1016/j.procs.2017.05.216", "10.1016/j.ins.2009.08.025", "10.1109/tvcg.2013.124", "10.1109/tvcg.2018.2864499", "10.1109/tvcg.2018.2864475", "10.1080/10447318.2020.1741118", "10.1016/j.imu.2019.100203", "10.1109/tvcg.2018.2865240", "10.1186/s12864-019-6413-7", "10.1109/tvcg.2015.2467551", "10.1002/widm.1249", "10.1007/978-3-319-66429-3\\_29", "10.1109/tvcg.2014.2346481", "10.1109/tvcg.2018.2864825", "10.1177/1473871620904671", "10.1109/tvcg.2019.2934267", "10.1136/bmjopen-2016-012799", "10.1109/smartgridcomm.2017.8340682", "10.1007/s10664-015-9401-9", "10.1145/1143844.1143874", "10.1111/cgf.12389", "10.1109/tvcg.2018.2872577", "10.1007/978-981-13-5802-9\\_20", "10.1016/j.ipm.2009.03.002", "10.5220/0006431602160222", "10.1109/mcg.2015.50", "10.1109/tvcg.2014.2346574", "10.1007/bf02289565", "10.1109/tvcg.2017.2744378", "10.1016/j.patrec.2008.08.010", "10.1023/a:1010933404324", "10.9735/2229-3981"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030465", "title": "Visual Causality Analysis of Event Sequence Data", "year": "2020", "conferenceName": "VAST", "authors": "Zhuochen Jin;Shunan Guo;Nan Chen;Daniel Weiskopf;David Gotz;Nan Cao", "citationCount": "0", "affiliation": "Cao, N (Corresponding Author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Jin, Zhuochen; Guo, Shunan; Chen, Nan; Cao, Nan, Tongji Univ, IDVx Lab, Shanghai, Peoples R China. Weiskopf, Daniel, Univ Stuttgart, Stuttgart, Germany. Gotz, David, Univ N Carolina, Chapel Hill, NC 27515 USA.", "countries": "USA;Germany;China", "abstract": "Causality is crucial to understanding the mechanisms behind complex systems and making decisions that lead to intended outcomes. Event sequence data is widely collected from many real-world processes, such as electronic health records, web clickstreams, and financial transactions, which transmit a great deal of information reflecting the causal relations among event types. Unfortunately, recovering causalities from observational event sequences is challenging, as the heterogeneous and high-dimensional event variables are often connected to rather complex underlying event excitation mechanisms that are hard to infer from limited observations. Many existing automated causal analysis techniques suffer from poor explainability and fail to include an adequate amount of human knowledge. In this paper, we introduce a visual analytics method for recovering causalities in event sequence data. We extend the Granger causality analysis algorithm on Hawkes processes to incorporate user feedback into causal model refinement. The visualization system includes an interactive causal analysis framework that supports bottom-up causal exploration, iterative causal verification and refinement, and causal comparison through a set of novel visualizations and interactions. We report two forms of evaluation: a quantitative evaluation of the model improvements resulting from the user-feedback mechanism, and a qualitative evaluation through case studies in different application domains to demonstrate the usefulness of the system.", "keywords": "Event sequence data,causality analysis,visual analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030465", "refList": ["10.5555/1642718", "10.1109/tvcg.2018.2865022", "10.5555/2074094.2074151", "10.1109/tvcg.2016.2618797", "10.1073/pnas.1320645111", "10.1109/tvcg.2017.2744843", "10.1109/iv.2017.69", "10.1109/tvcg.2019.2934399", "10.1145/3172944.3173007", "10.5555/1795555", "10.1109/mcg.2005.102", "10.1145/381641.381653", "10.1162/153244301753344614", "10.1111/cgf.14034", "10.1111/cgf.13198", "10.1017/cbo9780511519857", "10.1007/s10462-016-9475-9", "10.1075/ssol.2.2.07bor", "10.1007/978-1-4614-3223-4\\_3", "10.1109/infvis.2003.1249025", "10.1007/978-94-007-6094-313", "10.1145/2858036.2858387", "10.1109/tvcg.2007.70528", "10.1109/tvcg.2017.2674958", "10.1147/sj.454.0801", "10.1109/tvcg.2013.119", "10.1145/3173574.3173711", "10.1016/j.erss.2017.06.034", "10.1109/visual.2019.8933695", "10.1007/s11665-016-2173-6", "10.1016/0377-0427(87)90125-7", "10.2307/3601125", "10.1016/b978-0-444-88738-2.50018-x", "10.1109/mcg.2006.70", "10.1109/tvcg.2018.2865145", "10.1017/s1351324997001502", "10.1109/tvcg.2010.179", "10.1214/aos/1176344552", "10.1109/mcg.2009.22", "10.1007/978-3-319-26633-6\\_13", "10.1080/10447318.2014.905422", "10.1016/j.visinf.2019.03.004", "10.1057/palgrave.ivs.9500074", "10.1007/978-1-4614-3223-4"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13970", "year": "2020", "title": "QUESTO: Interactive Construction of Objective Functions for Classification Tasks", "conferenceName": "EuroVis", "authors": "Subhajit Das;Shenyu Xu;Michael Gleicher;Remco Chang;Alex Endert", "citationCount": "0", "affiliation": "Das, S (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA.\nDas, Subhajit; Xu, Shenyu; Endert, Alex, Georgia Inst Technol, Atlanta, GA 30332 USA.\nGleicher, Michael, Univ Wisconsin, Madison, WI USA.\nChang, Remco, Tufts Univ, Medford, MA 02155 USA.", "countries": "USA", "abstract": "Building effective classifiers requires providing the modeling algorithms with information about the training data and modeling goals in order to create a model that makes proper tradeoffs. Machine learning algorithms allow for flexible specification of such meta-information through the design of the objective functions that they solve. However, such objective functions are hard for users to specify as they are a specific mathematical formulation of their intents. In this paper, we present an approach that allows users to generate objective functions for classification problems through an interactive visual interface. Our approach adopts a semantic interaction design in that user interactions over data elements in the visualization are translated into objective function terms. The generated objective functions are solved by a machine learning solver that provides candidate models, which can be inspected by the user, and used to suggest refinements to the specifications. We demonstrate a visual analytics system QUESTO for users to manipulate objective functions to define domain-specific constraints. Through a user study we show that QUESTO helps users create various objective functions that satisfy their goals.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13970", "refList": ["10.1016/j.neucom.2017.01.105", "10.1371/journal.pone.0050474", "10.1109/tvcg.2016.2598839", "10.1007/978-3-642-21530-8\\_14", "10.1109/tvcg.2016.2598828", "10.5555/2969442.2969547", "10.1007/s00371-015-1132-9", "10.1145/2851581.2856492", "10.1126/scirobotics.aao6760", "10.1109/vast.2011.6102453", "10.1109/vast.2011.6102449", "10.1088/1749-4699/8/1/014008", "10.1109/tvcg.2016.2598446", "10.1145/359784.360332", "10.1111/j.1467-8659.2009.01475.x", "10.1016/j.visinf.2017.01.006", "10.1109/tvcg.2016.2598460", "10.1109/tevc.2015.2472283", "10.1145/2207676.2207741", "10.1109/tvcg.2017.2745085", "10.1145/2939672.2939778", "10.1145/1866029.1866038", "10.1145/2487575.2487629", "10.1145/3077257.3077259", "10.2312/eurova.20171123", "10.1145/2983924", "10.1109/mcg.2013.53", "10.1109/tvcg.2015.2467615", "10.1109/vast.2014.7042492", "10.1145/2675133.2675214", "10.1109/tvcg.2014.2346482", "10.1145/3180308.3180362", "10.1109/tvcg.2014.2346321", "10.24963/ijcai.2017/202", "10.1109/tvcg.2014.2346291", "10.1109/tbme.2012.2212278", "10.1007/s40708-016-0042-6", "10.1609/aimag.v35i4.2513", "10.1016/j.ijhcs.2009.03.004", "10.1109/tevc.2012.2225064", "10.1016/s0890-6955(02)00074-3", "10.1109/icmlde.2018.00014", "10.1111/cgf.13681", "10.1109/tvcg.2013.173", "10.1145/3025171.3025208", "10.1145/3025453.3026044", "10.1109/cec.2017.7969334", "10.1109/vast.2012.6400486", "10.1109/tvcg.2016.2598831", "10.1109/tcyb.2014.2310651"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13972", "year": "2020", "title": "Boxer: Interactive Comparison of Classifier Results", "conferenceName": "EuroVis", "authors": "Michael Gleicher;Aditya Barve;Xinyi Yu;Florian Heimerl", "citationCount": "0", "affiliation": "Gleicher, M (Corresponding Author), Univ Wisconsin, Madison, WI 53706 USA.\nGleicher, Michael; Barve, Aditya; Yu, Xinyi; Heimerl, Florian, Univ Wisconsin, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Machine learning practitioners often compare the results of different classifiers to help select, diagnose and tune models. We present Boxer, a system to enable such comparison. Our system facilitates interactive exploration of the experimental results obtained by applying multiple classifiers to a common set of model inputs. The approach focuses on allowing the user to identify interesting subsets of training and testing instances and comparing performance of the classifiers on these subsets. The system couples standard visual designs with set algebra interactions and comparative elements. This allows the user to compose and coordinate views to specify subsets and assess classifier performance on them. The flexibility of these compositions allow the user to address a wide range of scenarios in developing and assessing classifiers. We demonstrate Boxer in use cases including model selection, tuning, fairness assessment, and data quality diagnosis.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13972", "refList": ["10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2016.2598838", "10.1007/s10618-014-0368-8", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1145/3287560.3287589", "10.1109/vast.2017.8585721", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2009.128", "10.1109/tvcg.2017.2744018", "10.1080/00994480.2000.10748487", "10.5555/3305890.3306024", "10.1109/iccv.2015.329", "10.1109/tvcg.2013.125", "10.1089/big.2016.0007", "10.1109/memsys.2019.8870817", "10.1145/2939672.2939778", "10.1007/s11104-019-04156-0", "10.1371/journal.pone.0181142", "10.1145/3301275.3302324", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2018.2865044", "10.1023/a:1010933404324", "10.1145/2487575.2487579", "10.1109/tvcg.2013.157", "10.1145/2783258.2788613", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934631", "10.1016/s0304-3800(02)00064-9", "10.1007/s10115-013-0679-x", "10.1109/tvcg.2019.2934267", "10.1007/978-3-319-10590-1\\_53", "10.1109/vast.2017.8585720", "10.1016/0004-3702(80)90021-1", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2009.84", "10.1007/s11263-016-0911-8", "10.1111/cgf.12918", "10.1111/cgf.12373", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1080/00207176808905715", "10.1002/er.3827", "10.1109/tvcg.2014.2346660", "10.1111/cgf.13417", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2017.2744158", "10.1016/b978-0-12-815849-4.00004-9", "10.1097/ede.0b013e3181c30fb2", "10.1111/cgf.13681", "10.1016/j.ejor.2006.04.051", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2016.2598468", "10.9735/2229-3981", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 21}, {"doi": "10.1111/cgf.13673", "year": "2019", "title": "Multiple Views: different meanings and collocated words", "conferenceName": "EuroVis", "authors": "Jonathan C. Roberts;Hayder Al{-}Maneea;Peter W. S. Butcher;Robert Lew;Geraint Rees;Nirwan Sharma;Ana Frankenberg{-}Garcia", "citationCount": "1", "affiliation": "Roberts, JC (Corresponding Author), Bangor Univ, Bangor, Gwynedd, Wales.\nRoberts, J. C.; Al-maneea, H.; Butcher, P. W. S.; Sharma, N., Bangor Univ, Bangor, Gwynedd, Wales.\nAl-maneea, H., Basrah Univ, Basrah, Iraq.\nLew, R.; Rees, G., Adam Mickiewicz Univ, Poznan, Poland.\nFrankenberg-Garcia, A., Univ Surrey, Guildford, Surrey, England.", "countries": "Poland;Wales;England;Iraq", "abstract": "We report on an in-depth corpus linguistic study on multiple views' terminology and word collocation. We take a broad interpretation of these terms, and explore the meaning and diversity of their use in visualisation literature. First we explore senses of the term multiple views' (e.g., multiple views' can mean juxtaposition, many viewport projections or several alternative opinions). Second, we investigate term popularity and frequency of occurrences, investigating usage of multiple' and view' (e.g., multiple views, multiple visualisations, multiple sets). Third, we investigate word collocations and terms that have a similar sense (e.g., multiple views, side-by-side, small multiples). We built and used several corpora, including a 6-million-word corpus of all IEEE Visualisation conference articles published in IEEE Transactions on Visualisation and Computer Graphics 2012 to 2017. We draw on our substantial experience from early work in coordinated and multiple views, and with collocation analysis develop several lists of terms. This research provides insight into term use, a reference for novice and expert authors in visualisation, and contributes a taxonomy of multiple view' terms.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13673", "refList": ["10.1109/tvcg.2017.2744359", "10.1109/38.31462", "10.1109/cmv.2003.1215002", "10.2307/2289444", "10.1016/j.ijhcs.2011.02.007", "10.1017/s0022112091210654", "10.1016/b978-008044531-1/50426-7", "10.1109/cmv.2003.1215001", "10.1109/tvcg.2013.134", "10.1117/12.378894", "10.1037/0096-1523.21.6.1494", "10.1057/palgrave.ivs.9500086", "10.1109/iv.2011.42", "10.1109/tvcg.2017.2744199", "10.4324/9780203810088", "10.1145/1321440.1321580", "10.1109/tvcg.2015.2467271", "10.1016/j.learninstruc.2006.03.001", "10.1145/345513.345282", "10.1007/s40607-014-0009-9", "10.1109/tvcg.2017.2743859", "10.1109/infvis.1997.636761", "10.1109/tvcg.2013.219", "10.1007/978-3-319-55627-7", "10.1109/tvcg.2009.111", "10.1109/iv.2008.21", "10.1109/infvis.2002.1173157", "10.1109/mcg.2014.82", "10.1145/3143699.3143717", "10.1109/tvcg.2012.226", "10.1145/345513.345271", "10.2478/jazcas-2018-0006", "10.1145/2556288.2556969", "10.1109/visual.1998.745282", "10.1145/1276377.1276427", "10.1109/infvis.2001.963283", "10.1007/978-1-4471-6497-5\\_1", "10.1109/visual.1994.346302", "10.1057/palgrave.ivs.9500068", "10.1109/tvcg.2006.160", "10.1109/visual.1990.146374", "10.1109/tvcg.2016.2614803", "10.1177/1473871611416549", "10.1109/tvcg.2017.2745878", "10.1109/tvcg.2006.69", "10.1109/tpami.1983.4767367", "10.1109/visual.1991.175794", "10.1109/tvcg.2017.2744159", "10.1109/tvcg.2017.2744198", "10.1109/32.328995", "10.1016/j.jeap.2018.07.003", "10.1016/j.geomorph.2012.08.021", "10.1109/tvcg.2014.2346920", "10.1109/2.917550", "10.1017/s0958344018000150", "10.1109/tvcg.2009.94", "10.1179/2051819613z.0000000003", "10.1109/vast.2008.4677370", "10.1117/12.309533", "10.1109/tvcg.2014.2346747", "10.1075/ijcl.13.4.06ray", "10.1007/s12650-015-0323-9", "10.1109/tvcg.2006.178", "10.1109/tvcg.2016.2615308", "10.2307/1269768", "10.2307/4132312", "10.1109/tvcg.2018.2864903", "10.1109/mis.2006.100", "10.1109/iv.1998.694193", "10.1007/s11192-015-1830-0", "10.1109/tvcg.2017.2744080", "10.1111/j.1467-9280.1997.tb00442.x", "10.1109/cmv.2007.20", "10.1109/infvis.2003.1249006", "10.1109/tvcg.2017.2745941", "10.1016/s1364-6613(99)01332-7", "10.1145/989863.989893", "10.1109/tse.1985.232211", "10.1109/tvcg.2017.2744184", "10.1109/infvis.2004.12", "10.1075/ijcl.17.3.04har", "10.1109/tvcg.2010.179", "10.1016/j.jss.2006.05.024", "10.1109/cmv.2003.1215005", "10.1109/tvcg.2017.2744358", "10.1145/2992154.2996365", "10.1109/tvcg.2016.2598827"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030338", "title": "Composition and Configuration Patterns in Multiple-View Visualizations", "year": "2020", "conferenceName": "InfoVis", "authors": "Xi Chen;Wei Zeng 0004;Yanna Lin;Hayder Al-Maneea;Jonathan Roberts 0002;Remco Chang", "citationCount": "0", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Xi; Zeng, Wei; Lin, Yanna, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. AI-maneea, Hayder Mahdi; Roberts, Jonathan, Bangor Univ, Bangor, Gwynedd, Wales. Chang, Remco, Tufts Univ, Medford, MA 02155 USA.", "countries": "USA;Wales;China", "abstract": "Multiple-view visualization (MV) is a layout design technique often employed to help users see a large number of data attributes and values in a single cohesive representation. Because of its generalizability, the MV design has been widely adopted by the visualization community to help users examine and interact with large, complex, and high-dimensional data. However, although ubiquitous, there has been little work to categorize and analyze MVs in order to better understand its design space. As a result, there has been little to no guideline in how to use the MV design effectively. In this paper, we present an in-depth study of how MVs are designed in practice. We focus on two fundamental measures of multiple-view patterns: composition, which quantifies what view types and how many are there; and configuration, which characterizes spatial arrangement of view layouts in the display space. We build a new dataset containing 360 images of MVs collected from IEEE VIS, EuroVis, and PacificVis publications 2011 to 2019, and make fine-grained annotations of view types and layouts for these visualization images. From this data we conduct composition and configuration analyses using quantitative metrics of term frequency and layout topology. We identify common practices around MVs, including relationship of view types, popular view layouts, and correlation between view types and layouts. We combine the findings into a MV recommendation system, providing interactive tools to explore the design space, and support example-based design.", "keywords": "Multiple views,design pattern,quantitative analysis,example-based design", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030338", "refList": ["10.1109/tvcg.2018.2865235", "10.1109/tvcg.2016.2615308", "10.1145/2642918.2647398", "10.1177/1473871611416549", "10.1109/tvcg.2019.2934810", "10.1109/tvcg.2014.48", "10.1109/tvcg.2018.2864903", "10.1145/345513.345271", "10.1109/iv.1998.694193", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2017.2744019", "10.1109/tvcg.2011.185", "10.1109/tvcg.2015.2467194", "10.1109/visual.1991.175815", "10.14778/2831360.2831371", "10.1109/mcg.2019.2924636", "10.1111/cgf.13673", "10.1111/cgf.12131", "10.1109/tvcg.2017.2744198", "10.1145/108360.108361", "10.1109/vl.1996.545307", "10.1109/tvcg.2017.2745140", "10.1109/cmv.2007.20", "10.1145/198366.198376", "10.1145/2508363.2508405", "10.1111/cgf.12114", "10.1109/icde.2018.00019", "10.1109/tvcg.2017.2787113", "10.1109/tvcg.2018.2865240", "10.1111/cgf.13380", "10.1111/cgf.12902", "10.1109/vast.2015.7347628", "10.2307/2288400", "10.1109/infvis.2004.12", "10.1145/102377.115768", "10.1109/tvcg.2009.179", "10.1109/2945.981851", "10.1109/tvcg.2007.70521", "10.1109/pacificvis.2012.6183556", "10.1145/2213836", "10.1109/tvcg.2013.234", "10.1109/iv.2008.87"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.13972", "year": "2020", "title": "Boxer: Interactive Comparison of Classifier Results", "conferenceName": "EuroVis", "authors": "Michael Gleicher;Aditya Barve;Xinyi Yu;Florian Heimerl", "citationCount": "0", "affiliation": "Gleicher, M (Corresponding Author), Univ Wisconsin, Madison, WI 53706 USA.\nGleicher, Michael; Barve, Aditya; Yu, Xinyi; Heimerl, Florian, Univ Wisconsin, Madison, WI 53706 USA.", "countries": "USA", "abstract": "Machine learning practitioners often compare the results of different classifiers to help select, diagnose and tune models. We present Boxer, a system to enable such comparison. Our system facilitates interactive exploration of the experimental results obtained by applying multiple classifiers to a common set of model inputs. The approach focuses on allowing the user to identify interesting subsets of training and testing instances and comparing performance of the classifiers on these subsets. The system couples standard visual designs with set algebra interactions and comparative elements. This allows the user to compose and coordinate views to specify subsets and assess classifier performance on them. The flexibility of these compositions allow the user to address a wide range of scenarios in developing and assessing classifiers. We demonstrate Boxer in use cases including model selection, tuning, fairness assessment, and data quality diagnosis.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13972", "refList": ["10.1109/tvcg.2019.2934629", "10.1109/tvcg.2017.2744359", "10.1109/tvcg.2016.2598838", "10.1007/s10618-014-0368-8", "10.1109/tvcg.2017.2744938", "10.1109/tvcg.2019.2934262", "10.1145/3287560.3287589", "10.1109/vast.2017.8585721", "10.1109/tvcg.2016.2598828", "10.1109/tvcg.2009.128", "10.1109/tvcg.2017.2744018", "10.1080/00994480.2000.10748487", "10.5555/3305890.3306024", "10.1109/iccv.2015.329", "10.1109/tvcg.2013.125", "10.1089/big.2016.0007", "10.1109/memsys.2019.8870817", "10.1145/2939672.2939778", "10.1007/s11104-019-04156-0", "10.1371/journal.pone.0181142", "10.1145/3301275.3302324", "10.1109/tvcg.2017.2745158", "10.1109/tvcg.2018.2865044", "10.1023/a:1010933404324", "10.1145/2487575.2487579", "10.1109/tvcg.2013.157", "10.1145/2783258.2788613", "10.1109/tvcg.2017.2744199", "10.1109/tvcg.2019.2934631", "10.1016/s0304-3800(02)00064-9", "10.1007/s10115-013-0679-x", "10.1109/tvcg.2019.2934267", "10.1007/978-3-319-10590-1\\_53", "10.1109/vast.2017.8585720", "10.1016/0004-3702(80)90021-1", "10.1109/tvcg.2015.2467618", "10.1109/tvcg.2018.2864477", "10.1109/tvcg.2009.84", "10.1007/s11263-016-0911-8", "10.1111/cgf.12918", "10.1111/cgf.12373", "10.1109/tvcg.2017.2744878", "10.1109/tvcg.2018.2864812", "10.1109/mcg.2019.2919033", "10.1080/00207176808905715", "10.1002/er.3827", "10.1109/tvcg.2014.2346660", "10.1111/cgf.13417", "10.1109/tvcg.2019.2934659", "10.1109/tvcg.2017.2744158", "10.1016/b978-0-12-815849-4.00004-9", "10.1097/ede.0b013e3181c30fb2", "10.1111/cgf.13681", "10.1016/j.ejor.2006.04.051", "10.1109/tvcg.2019.2934619", "10.1109/tvcg.2016.2598468", "10.9735/2229-3981", "10.1109/tvcg.2016.2598831"], "wos": 1, "children": [], "len": 1}], "len": 187}], "len": 189}, "index": 1336, "embedding": [-2.2008650302886963, 2.4218411445617676, -1.0434188842773438, -2.387450695037842, -0.6721880435943604, 0.16650904715061188, -0.07890406996011734, -0.9959407448768616, 2.2018492221832275, 6.000886917114258, -1.5249708890914917, 1.760757565498352, 4.921413898468018, -1.6473230123519897, 9.661967277526855, 0.9528248906135559, 1.0869715213775635, 2.0769052505493164, 1.714843511581421, -1.063811182975769, -0.06630208343267441, 3.186119318008423, 3.0086400508880615, 6.253750801086426, -2.424682378768921, 11.273521423339844, -0.5581337213516235, 3.0160036087036133, -0.5742089152336121, 2.565760612487793, 0.50951087474823, 3.045470952987671], "projection": [0.9865551590919495, 11.36264419555664], "size": 95, "height": 6, "width": 44}