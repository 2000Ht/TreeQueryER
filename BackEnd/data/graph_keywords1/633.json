{"data": {"doi": "10.1109/tvcg.2017.2743859", "title": "MyBrush: Brushing and Linking with Personal Agency", "year": "2017", "conferenceName": "InfoVis", "authors": "Philipp Koytek;Charles Perin;Jo Vermeulen;Elisabeth Andr\u00e9;Sheelagh Carpendale", "citationCount": "5", "affiliation": "Koytek, P (Corresponding Author), Univ Calgary, Calgary, AB, Canada. Koytek, P (Corresponding Author), Augsburg Univ, Augsburg, Germany. Koytek, Philipp; Perin, Charles; Vermeulen, Jo; Carpendale, Sheelagh, Univ Calgary, Calgary, AB, Canada. Koytek, Philipp; Andre, Elisabeth, Augsburg Univ, Augsburg, Germany. Perin, Charles, City Univ London, London, England.", "countries": "Canada;Germany;England", "abstract": "We extend the popular brushing and linking technique by incorporating personal agency in the interaction. We map existing research related to brushing and linking into a design space that deconstructs the interaction technique into three components: source (what is being brushed), link (the expression of relationship between source and target), and target (what is revealed as related to the source). Using this design space, we created MyBrush, a unified interface that offers personal agency over brushing and linking by giving people the flexibility to configure the source, link, and target of multiple brushes. The results of three focus groups demonstrate that people with different backgrounds leveraged personal agency in different ways, including performing complex tasks and showing links explicitly. We reflect on these results, paving the way for future research on the role of personal agency in information visualization.", "keywords": "Brushing,linking,personal agency,coordinated multiple views,interaction,design space,information visualization", "link": "http://dx.doi.org/10.1109/TVCG.2017.2743859", "refList": ["10.1109/tvcg.2011.183", "10.1057/palgrave.ivs.9500057", "10.1109/tvcg.2008.153", "10.2307/1269768", "10.1109/vast.2009.5333443", "10.1057/ivs.2009.22", "10.1109/tvcg.2014.2371858", "10.2307/1390772", "10.1145/345513.345271", "10.1109/tvcg.2013.154", "10.1145/1731903.1731936", "10.1109/iv.1998.694193", "10.1109/tvcg.2014.2346260", "10.1109/tvcg.2011.185", "10.1109/visual.1991.175794", "10.1111/cgf.12901", "10.1007/3-540-30790-7\\_18", "10.1057/palgrave.ivs.9500167", "10.1109/tvcg.2008.116", "10.1109/cmv.2007.20", "10.1109/tvcg.2009.162", "10.1109/cmv.2003.1215008", "10.1145/2207676.2208293", "10.1109/tvcg.2006.99", "10.1109/tvcg.2014.2346279", "10.1111/cgf.12902", "10.1109/2945.981847", "10.1109/tvcg.2011.201", "10.1109/infvis.1999.801858", "10.3389/fpsyg.2016.01272", "10.1109/tvcg.2006.147", "10.1109/visual.1994.346302", "10.1109/visual.2000.885739", "10.18637/jss.v007.i11", "10.1109/infvis.2004.12", "10.1023/a:1021271517844", "10.1109/tvcg.2010.138", "10.3389/fnhum.2013.00514", "10.1145/345513.345282", "10.1109/tvcg.2007.70521", "10.1109/infvis.2004.64", "10.1109/pacificvis.2010.5429609", "10.1109/pacificvis.2012.6183556", "10.1006/obhd.1998.2758", "10.1145/882262.882291", "10.1559/15230406384373", "10.1006/obhd.1998.2756", "10.1109/visual.1996.567800", "10.1080/13658810903214203", "10.1111/j.1467-8659.2012.03121.x", "10.1145/2207676.2208350", "10.1145/1054972.1055031", "10.1111/j.1467-8721.2009.01644.x", "10.1109/visual.1995.485139", "10.1109/infvis.2002.1173157"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2018.2865159", "title": "Dynamic Composite Data Physicalization Using Wheeled Micro-Robots", "year": "2018", "conferenceName": "InfoVis", "authors": "Mathieu Le Goc;Charles Perin;Sean Follmer;Jean-Daniel Fekete;Pierre Dragicevic", "citationCount": "3", "affiliation": "Le Goc, M (Corresponding Author), Stanford Univ, Stanford, CA 94305 USA. Le Goc, Mathieu; Follmer, Sean, Stanford Univ, Stanford, CA 94305 USA. Perin, Charles, Univ Victoria, Victoria, BC, Canada. Perin, Charles, City Univ London, London, England. Fekete, Jean-Daniel; Dragicevic, Pierre, INRIA, Saclay, France.", "countries": "Canada;USA;England;France", "abstract": "This paper introduces dynamic composite physicalizations, a new class of physical visualizations that use collections of self-propelled objects to represent data. Dynamic composite physicalizations can be used both to give physical form to well-known interactive visualization techniques, and to explore new visualizations and interaction paradigms. We first propose a design space characterizing composite physicalizations based on previous work in the fields of Information Visualization and Human Computer Interaction. We illustrate dynamic composite physicalizations in two scenarios demonstrating potential benefits for collaboration and decision making, as well as new opportunities for physical interaction. We then describe our implementation using wheeled micro-robots capable of locating themselves and sensing user input, before discussing limitations and opportunities for future work.", "keywords": "information visualization,data physicalization,tangible user interfaces", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865159", "refList": ["10.1145/1057237.1057242", "10.1145/2858036.2858058", "10.1109/tvcg.2008.153", "10.1109/tvcg.2014.2346424", "10.1145/2642918.2647377", "10.1145/2556288.2557379", "10.1145/2702123.2702275", "10.1007/978-3-319-67687-6\\_25", "10.1016/j.intcom.2006.03.006", "10.1559/152304086783900068", "10.1109/tvcg.2012.199", "10.1145/571985.572011", "10.1109/tvcg.2013.227", "10.1057/palgrave.ivs.9500099", "10.1109/mcg.2010.101", "10.1145/2702123.2702237", "10.1109/roman.2013.6628441", "10.1145/2501988.2502032", "10.1109/tvcg.2013.134", "10.1145/1413634.1413696", "10.1109/tvcg.2014.2346250", "10.1145/258549.258803", "10.1109/tvcg.2014.2346984", "10.1145/2807442.2807488", "10.1145/2207676.2208691", "10.1145/2556288.2557231", "10.1111/cgf.12935", "10.1109/tvcg.2014.2346279", "10.1109/tvcg.2014.2346292", "10.1109/tvcg.2016.2598920", "10.1145/3173574.3173728", "10.1145/3025453.3025512", "10.1007/s00779-009-0279-7", "10.1145/1226969.1226984", "10.1109/tvcg.2016.2598498", "10.2307/2288400", "10.1145/2702123.2702180", "10.1016/0004-3702(94)00017-u", "10.1109/tvcg.2007.70539", "10.1145/2858036.2858041", "10.1109/tvcg.2017.2743859", "10.1109/sibgrapi.2007.21", "10.1177/014662168000400305", "10.1145/2396636.2396675", "10.1145/1226969", "10.1145/2702123.2702604", "10.1145/2207676.2208350", "10.1007/s10551-008-9665-8", "10.1145/3130931", "10.1109/comcomap.2012.6154871", "10.1145/2470654.2481359", "10.1145/3025453.3025877", "10.1145/2984511.2984547", "10.1109/tvcg.2014.2359887"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934283", "title": "What is Interaction for Data Visualization?", "year": "2019", "conferenceName": "InfoVis", "authors": "Evanthia Dimara;Charles Perin", "citationCount": "1", "affiliation": "Dimara, E (Corresponding Author), Sorbonne Univ, Paris, France. Dimara, Evanthia, Sorbonne Univ, Paris, France. Perin, Charles, Univ Victoria, Victoria, BC, Canada.", "countries": "Canada;France", "abstract": "Interaction is fundamental to data visualization, but what \u201cinteraction\u201d means in the context of visualization is ambiguous and confusing. We argue that this confusion is due to a lack of consensual definition. To tackle this problem, we start by synthesizing an inclusive view of interaction in the visualization community \u2013 including insights from information visualization, visual analytics and scientific visualization, as well as the input of both senior and junior visualization researchers. Once this view takes shape, we look at how interaction is defined in the field of human-computer interaction (HCI). By extracting commonalities and differences between the views of interaction in visualization and in HCI, we synthesize a definition of interaction for visualization. Our definition is meant to be a thinking tool and inspire novel and bolder interaction design practices. We hope that by better understanding what interaction in visualization is and what it can be, we will enrich the quality of interaction in visualization systems and empower those who use them.", "keywords": "interaction,visualization,data,definition,human-computer interaction", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934283", "refList": ["10.1057/ivs.2009.22", "10.1515/icom-2017-0027", "10.1145/2493102.2493104", "10.1007/978-3-319-06793-3\\_6", "10.1080/03640210801898177", "10.1109/mcg.2010.30", "10.1109/tvcg.2013.134", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2018.2865237", "10.1109/2945.981847", "10.1145/2598784.2598806", "10.1109/tvcg.2013.130", "10.1109/tvcg.2007.70577", "10.1109/mic.2015.129", "10.1145/2909132.2909267", "10.1080/01449290500330331", "10.1109/tvcg.2018.2865233", "10.1109/tvcg.2015.2467831", "10.1109/iv.2015.34", "10.1109/tvcg.2009.111", "10.1145/3290605.3300565", "10.1145/3173574.3173797", "10.1145/948496.948514", "10.1145/3025453.3025765", "10.1109/tvcg.2015.2467613", "10.1145/2659796", "10.1109/tvcg.2014.2346311", "10.1145/3025453.3025524", "10.1080/17452759.2011.558588", "10.1145/3027063.3053113", "10.1109/infvis.2005.1532136", "10.1145/2470654.2481307", "10.1109/tvcg.2018.2864913", "10.1145/345513.345267", "10.3102/00028312005004437", "10.1109/tvcg.2007.70436", "10.1037/0033-295x.106.4.643", "10.1145/2133416.2146416", "10.1109/tvcg.2013.191", "10.1109/tvcg.2010.177", "10.1145/960201.957206", "10.1109/tvcg.2007.70515", "10.1109/infvis.2000.885092", "10.1145/2636240.2636844", "10.1037/h0055392", "10.1177/1473871611413180", "10.1109/vl.1996.545307", "10.1111/j.1471-1842.2009.00848.x", "10.1145/989863.989865", "10.1109/tvcg.2013.124", "10.1109/tvcg.2008.109", "10.1145/1166253.1166265", "10.1145/2702123.2702180", "10.1109/tvcg.2016.2598620", "10.1080/07370024.2016.1226139", "10.1145/3173574.3173909", "10.1109/pacificvis.2010.5429613", "10.1111/j.1467-6478.2006.00368.x", "10.1109/tvcg.2012.204", "10.1109/tvcg.2013.120", "10.1179/1743277412y.0000000019", "10.1145/1936652.1936684", "10.2307/1269768", "10.1109/infvis.1996.559213", "10.1109/tvcg.2016.2598839", "10.1145/2642918.2647360", "10.1057/palgrave.ivs.9500099", "10.1016/j.cag.2009.06.004", "10.1109/mc.2013.178", "10.1109/tvcg.2007.70541", "10.1109/vast.2011.6102473", "10.1145/358886.358895", "10.1109/tvcg.2014.2346573", "10.1109/tvcg.2018.2865159", "10.1145/2207676.2207741", "10.1109/tvcg.2015.2396062", "10.1145/2207676.2208572", "10.1109/tvcg.2016.2598608", "10.1057/ivs.2008.31", "10.1177/001316446002000104", "10.1109/tvcg.2017.2680452", "10.1109/tvcg.2006.80", "10.1145/2598510.2598566", "10.1037/0003-066x.51.4.355", "10.7146/dpb.v16i224.7586", "10.1109/infvis.1998.729560", "10.1162/leon\\_a\\_00011", "10.1109/tvcg.2010.157", "10.1109/tvcg.2014.2359887"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1109/tvcg.2018.2865235", "title": "Multiple Coordinated Views at Large Displays for Multiple Users: Empirical Findings on User Behavior, Movements, and Distances", "year": "2018", "conferenceName": "InfoVis", "authors": "Ricardo Langner;Ulrike Kister;Raimund Dachselt", "citationCount": "5", "affiliation": "Langner, R (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Langner, Ricardo; Kister, Ulrike; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany.", "countries": "Germany", "abstract": "Interactive wall-sized displays benefit data visualization. Due to their sheer display size, they make it possible to show large amounts of data in multiple coordinated views (MCV) and facilitate collaborative data analysis. In this work, we propose a set of important design considerations and contribute a fundamental input vocabulary and interaction mapping for MCV functionality. We also developed a fully functional application with more than 45 coordinated views visualizing a real-world, multivariate data set of crime activities, which we used in a comprehensive qualitative user study investigating how pairs of users behave. Most importantly, we found that flexible movement is essential and-depending on user goals-is connected to collaboration, perception, and interaction. Therefore, we argue that for future systems interaction from the distance is required and needs good support. We show that our consistent design for both direct touch at the large display and distant interaction using mobile phones enables the seamless exploration of large-scale MCV at wall-sized displays. Our MCV application builds on design aspects such as simplicity, flexibility, and visual consistency and, therefore, supports realistic workflows. We believe that in the future, many visual data analysis scenarios will benefit from wall-sized displays presenting numerous coordinated visualizations, for which our findings provide a valuable foundation.", "keywords": "Multiple coordinated views,wall-sized displays,mobile devices,distant interaction,physical navigation,user behavior,user movements,multi-user,collaborative data analysis", "link": "http://dx.doi.org/10.1109/TVCG.2018.2865235", "refList": ["10.1109/mcg.2014.82", "10.1111/j.1467-8659.2009.01444.x", "10.1016/j.cag.2007.01.029", "10.1109/vast.2010.5652880", "10.1109/tvcg.2013.166", "10.1145/3025453.3025594", "10.1109/tvcg.2012.275", "10.1145/2254556.2254652", "10.1145/1866029.1866034", "10.1145/1099203.1099209", "10.1145/2470654.2470695", "10.1109/tvcg.2017.2744019", "10.1109/tvcg.2012.237", "10.1145/3173574.3173593", "10.1109/tvcg.2013.134", "10.1109/vast.2016.7883506", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1111/cgf.12871", "10.1109/tvcg.2017.2744198", "10.1145/3025453.3026006", "10.1109/cmv.2007.20", "10.1145/2470654.2481318", "10.1145/2598153.2598195", "10.1109/tvcg.2009.162", "10.1145/3173574.3173747", "10.1145/2576099", "10.1145/2207676.2208691", "10.1145/2858036.2858039", "10.1145/2556288.2557231", "10.1145/2556288.2556956", "10.1109/tvcg.2017.2745219", "10.1177/1473871617725907", "10.1145/2557500.2557541", "10.1007/978-3-319-22698-9\\_31", "10.1145/2785830.2785849", "10.1145/2207676.2208690", "10.1007/s00779-013-0727-2", "10.1109/tvcg.2016.2592906", "10.1111/cgf.13206", "10.1145/2207676.2208639", "10.1145/2817721.2817726", "10.1145/2556288.2557020", "10.1145/2598153.2598163", "10.1145/2254556.2254708", "10.1145/2669485.2669507", "10.1109/tvcg.2006.184", "10.1109/tvcg.2017.2743859", "10.1109/tvcg.2012.204", "10.1145/2207676.2208297", "10.1145/1897239.1897250", "10.1145/2858036.2858118", "10.1145/2396636.2396675", "10.1145/1731903.1731926", "10.1145/2702123.2702406", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1145/1936652.1936693", "10.1177/1473871611415997", "10.1145/2556288.2557170", "10.1145/2992154.2992157", "10.1145/3206505.3206506", "10.1007/978-3-319-45853-3\\_5"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030338", "title": "Composition and Configuration Patterns in Multiple-View Visualizations", "year": "2020", "conferenceName": "InfoVis", "authors": "Xi Chen;Wei Zeng 0004;Yanna Lin;Hayder Al-Maneea;Jonathan Roberts 0002;Remco Chang", "citationCount": "0", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Xi; Zeng, Wei; Lin, Yanna, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. AI-maneea, Hayder Mahdi; Roberts, Jonathan, Bangor Univ, Bangor, Gwynedd, Wales. Chang, Remco, Tufts Univ, Medford, MA 02155 USA.", "countries": "USA;Wales;China", "abstract": "Multiple-view visualization (MV) is a layout design technique often employed to help users see a large number of data attributes and values in a single cohesive representation. Because of its generalizability, the MV design has been widely adopted by the visualization community to help users examine and interact with large, complex, and high-dimensional data. However, although ubiquitous, there has been little work to categorize and analyze MVs in order to better understand its design space. As a result, there has been little to no guideline in how to use the MV design effectively. In this paper, we present an in-depth study of how MVs are designed in practice. We focus on two fundamental measures of multiple-view patterns: composition, which quantifies what view types and how many are there; and configuration, which characterizes spatial arrangement of view layouts in the display space. We build a new dataset containing 360 images of MVs collected from IEEE VIS, EuroVis, and PacificVis publications 2011 to 2019, and make fine-grained annotations of view types and layouts for these visualization images. From this data we conduct composition and configuration analyses using quantitative metrics of term frequency and layout topology. We identify common practices around MVs, including relationship of view types, popular view layouts, and correlation between view types and layouts. We combine the findings into a MV recommendation system, providing interactive tools to explore the design space, and support example-based design.", "keywords": "Multiple views,design pattern,quantitative analysis,example-based design", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030338", "refList": ["10.1109/tvcg.2018.2865235", "10.1109/tvcg.2016.2615308", "10.1145/2642918.2647398", "10.1177/1473871611416549", "10.1109/tvcg.2019.2934810", "10.1109/tvcg.2014.48", "10.1109/tvcg.2018.2864903", "10.1145/345513.345271", "10.1109/iv.1998.694193", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2017.2744019", "10.1109/tvcg.2011.185", "10.1109/tvcg.2015.2467194", "10.1109/visual.1991.175815", "10.14778/2831360.2831371", "10.1109/mcg.2019.2924636", "10.1111/cgf.13673", "10.1111/cgf.12131", "10.1109/tvcg.2017.2744198", "10.1145/108360.108361", "10.1109/vl.1996.545307", "10.1109/tvcg.2017.2745140", "10.1109/cmv.2007.20", "10.1145/198366.198376", "10.1145/2508363.2508405", "10.1111/cgf.12114", "10.1109/icde.2018.00019", "10.1109/tvcg.2017.2787113", "10.1109/tvcg.2018.2865240", "10.1111/cgf.13380", "10.1111/cgf.12902", "10.1109/vast.2015.7347628", "10.2307/2288400", "10.1109/infvis.2004.12", "10.1145/102377.115768", "10.1109/tvcg.2009.179", "10.1109/2945.981851", "10.1109/tvcg.2007.70521", "10.1109/pacificvis.2012.6183556", "10.1145/2213836", "10.1109/tvcg.2013.234", "10.1109/iv.2008.87"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030460", "title": "Personal Augmented Reality for Information Visualization on Large Interactive Displays", "year": "2020", "conferenceName": "InfoVis", "authors": "Patrick Reipschl\u00e4ger;Tamara Flemisch;Raimund Dachselt", "citationCount": "0", "affiliation": "Reipschlager, P (Corresponding Author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund, Tech Univ Dresden, Interact Media Lab, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany. Dachselt, Raimund, Tech Univ Dresden, Cluster Excellence Phys Life, Dresden, Germany.", "countries": "Germany", "abstract": "In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.", "keywords": "Augmented Reality,Information Visualization,InfoVis,Large Displays,Immersive Analytics,Physical Navigation,Multiple Coordinated Views", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030460", "refList": ["10.1109/tvcg.2008.153", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1109/tvcg.2019.2934803", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1109/tvcg.2019.2934415", "10.1109/tvcg.2009.162", "10.1109/mcg.2019.2897927", "10.1109/tvcg.2017.2744199", "10.1145/3126594.3126613", "10.1145/3173574.3173759", "10.1145/3290605.3300360", "10.1145/2858036.2858158", "10.1145/642611.642695", "10.1145/2702123.2702312", "10.1109/3dui.2014.6798833", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1145/3343055.3359718", "10.1007/978-3-030-01388-2\\_1", "10.1145/2858036.2858524", "10.1145/3173574.3173610", "10.1145/2817721.2817735", "10.1145/1166253.1166280", "10.1109/vr46266.2020.1582298687237", "10.1145/2576099", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1080/15384047.2020.1806642", "10.1023/a:1021271517844", "10.1109/tvcg.2019.2903956", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.1016/s1071-5819(03)00021-1", "10.1145/2470654.2466431", "10.1109/vr.2019.8797733", "10.1177/1473871611416549", "10.1109/tvcg.2011.287", "10.2312/eurp.20191136", "10.1111/cgf.12871", "10.1145/3290605.3300288", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2013.197", "10.1007/s11071-020-05736-x", "10.1109/vr46266.2020.00-23", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1109/tvcg.2016.2640960", "10.1109/tvcg.2017.2745258", "10.1145/2817721.2817726", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/vr46266.2020.1581122519414", "10.1109/vr46266.2020.00-20", "10.1109/pacificvis.2019.00010", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1109/ismar-adjunct.2016.0030", "10.1145/2317956.2318025", "10.1109/cmv.2007.20", "10.1080/07370020902739429", "10.1109/visual.2019.8933673", "10.1109/tvcg.2016.2598608", "10.1145/2702123.2702331", "10.1109/tvcg.2017.2745941", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1145/3009939.3009945", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2017.2744184", "10.1145/2785830.2785871", "10.1109/tvcg.2012.251"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030334", "title": "Uplift: A Tangible and Immersive Tabletop System for Casual Collaborative Visual Analytics", "year": "2020", "conferenceName": "VAST", "authors": "Barrett Ens;Sarah Goodwin;Arnaud Prouzeau;Fraser Anderson;Florence Y. Wang;Samuel Gratzl;Zac Lucarelli;Brendan Moyle;Jim Smiley;Tim Dwyer", "citationCount": "0", "affiliation": "Ens, B (Corresponding Author), Monash Univ, Clayton, Vic, Australia. Ens, Barrett; Goodwin, Sarah; Prouzeau, Arnaud; Wang, Florence Y.; Gratzl, Samuel; Lucarelli, Zac; Moyle, Brendan; Smiley, Jim; Dwyer, Tim, Monash Univ, Clayton, Vic, Australia. Anderson, Fraser, Autodesk Res, Toronto, ON, Canada.", "countries": "Canada;Australia", "abstract": "Collaborative visual analytics leverages social interaction to support data exploration and sensemaking. These processes are typically imagined as formalised, extended activities, between groups of dedicated experts, requiring expertise with sophisticated data analysis tools. However, there are many professional domains that benefit from support for short 'bursts' of data exploration between a subset of stakeholders with a diverse breadth of knowledge. Such 'casual collaborative\u2019 scenarios will require engaging features to draw users' attention, with intuitive, 'walk-up and use\u2019 interfaces. This paper presents Uplift, a novel prototype system to support 'casual collaborative visual analytics' for a campus microgrid, co-designed with local stakeholders. An elicitation workshop with key members of the building management team revealed relevant knowledge is distributed among multiple experts in their team, each using bespoke analysis tools. Uplift combines an engaging 3D model on a central tabletop display with intuitive tangible interaction, as well as augmented-reality, mid-air data visualisation, in order to support casual collaborative visual analytics for this complex domain. Evaluations with expert stakeholders from the building management and energy domains were conducted during and following our prototype development and indicate that Uplift is successful as an engaging backdrop for casual collaboration. Experts see high potential in such a system to bring together diverse knowledge holders and reveal complex interactions between structural, operational, and financial aspects of their domain. Such systems have further potential in other domains that require collaborative discussion or demonstration of models, forecasts, or cost-benefit analyses to high-level stakeholders.", "keywords": "Data visualisation,tangible and embedded interaction,augmented reality,immersive analytics", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030334", "refList": ["10.1109/vr.2019.8797733", "10.1109/tvcg.2018.2865235", "10.1111/cgf.13213", "10.1109/mcg.2014.82", "10.1109/tvcg.2008.153", "10.1145/1240624.1240701", "10.1109/tvcg.2013.166", "10.1177/1473871611416549", "10.1145/3343055.3359718", "10.1109/pacificvis.2019.00010", "10.1109/tvcg.2012.275", "10.1109/vr46266.2020.00-20", "10.1109/tvcg.2011.287", "10.3788/co.20201301.0001", "10.1109/tvcg.2019.2934803", "10.1145/3173574.3173610", "10.1145/2858036.2858524", "10.1145/3359996.3364242", "10.1145/3173574.3173593", "10.1177/1473871611412817", "10.1109/tvcg.2017.2745958", "10.1111/cgf.12871", "10.1145/2817721.2817735", "10.1145/2317956.2318025", "10.1145/1166253.1166280", "10.1007/978-3-319-73207-7", "10.1109/tvcg.2019.2934415", "10.1109/cmv.2007.20", "10.1109/vr46266.2020.1582298687237", "10.1109/tvcg.2009.162", "10.1109/visual.2019.8933673", "10.1109/mcg.2019.2897927", "10.1145/2702123.2702331", "10.1145/2576099", "10.1145/3290605.3300288", "10.1145/3173574.3173664", "10.1109/infvis.2005.1532136", "10.1109/3dui.2014.6798842", "10.1145/302979.303113", "10.1145/3343055.3359709", "10.1109/tvcg.2016.2598608", "10.1109/tvcg.2017.2744199", "10.1145/1936652.1936676", "10.1117/12.2521648", "10.1080/15384047.2020.1806642", "10.1109/tvcg.2013.197", "10.1145/3009939.3009945", "10.1145/3126594.3126613", "10.1109/tvcg.2016.2592906", "10.1109/mcg.2019.2898856", "10.1111/cgf.13206", "10.1109/tvcg.2013.163", "10.1109/tvcg.2016.2640960", "10.1145/3173574.3173759", "10.1109/tvcg.2017.2744184", "10.1109/tvcg.2017.2745258", "10.1023/a:1021271517844", "10.1145/2817721.2817726", "10.1145/2858036.2858158", "10.1109/tvcg.2018.2865192", "10.1109/bigdata.2018.8622521", "10.1145/642611.642695", "10.1145/2785830.2785871", "10.1109/tvcg.2012.204", "10.1080/07370024.2019.1697697", "10.1109/tvcg.2019.2903956", "10.1109/tcyb.2020.2970556", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/2817721.2823505", "10.1145/3290605.3300521", "10.2312/eurp", "10.1145/2470654.2466431"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13995", "year": "2020", "title": "GTMapLens: Interactive Lens for Geo-Text Data Browsing on Map", "conferenceName": "EuroVis", "authors": "Chao Ma;Ye Zhao;Shamal Al{-}Dohuki;Jing Yang;Xinyue Ye;Farah Kamw;Md. Amiruzzaman", "citationCount": "0", "affiliation": "Zhao, Y (Corresponding Author), Kent State Univ, Kent, OH 44240 USA.\nMa, Chao; Zhao, Ye; Amiruzzaman, Md, Kent State Univ, Kent, OH 44240 USA.\nAl-Dohuki, Shamal, Univ Duhok, Duhok, Iraq.\nYang, Jing, Univ N Carolina, Charlotte, NC USA.\nYe, Xinyue, New Jersey Inst Technol, Newark, NJ 07102 USA.\nKamw, Farah, Concordia Univ, Ann Arbor, MI USA.", "countries": "USA;Iraq", "abstract": "Data containing geospatial semantics, such as geotagged tweets, travel blogs, and crime reports, associates natural language texts with geographical locations. This paper presents a lens-based visual interaction technique, GTMapLens, to flexibly browse the geo-text data on a map. It allows users to perform dynamic focus+context exploration by using movable lenses to browse geographical regions, find locations of interest, and perform comparative and drill-down studies. Geo-text data is visualized in a way that users can easily perceive the underlying geospatial semantics along with lens moving. Based on a requirement analysis with a cohort of multidisciplinary domain experts, a set of lens interaction techniques are developed including keywords control, path management, context visualization, and snapshot anchors. They allow users to achieve a guided and controllable exploration of geo-text data. A hierarchical data model enables the interactive lens operations by accelerated data retrieval from a geo-text database. Evaluation with real-world datasets is presented to show the usability and effectiveness of GTMapLens.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13995", "refList": ["10.1109/tvcg.2018.2865235", "10.1145/1936652.1936673", "10.1145/2598153.2598200", "10.1145/1456650.1456652", "10.1109/vast.2011.6102456", "10.1145/3290605.3300864", "10.1177/1473871611413180", "10.1109/iv.2011.43", "10.1109/tvcg.2015.2467971", "10.1109/iv.2016.62", "10.1080/13658816.2017.1325488", "10.1111/cgf.12871", "10.1111/cgf.12132", "10.1109/mc.2012.430", "10.1109/tvcg.2016.2598585", "10.1109/tvcg.2011.195", "10.1109/tvcg.2015.2467619", "10.1109/pacificvis.2013.6596122", "10.13140/rg.2.2.36636.59521", "10.1145/3170427.3188506", "10.1016/b978-155860915-0/50040-8", "10.1109/infvis.2003.1249008", "10.1109/tvcg.2018.2850781", "10.1016/j.datak.2006.01.013", "10.1109/tvcg.2015.2467991", "10.1111/gec3.12404", "10.1109/tvcg.2008.149", "10.1109/visual.1998.745317", "10.1109/pacificvis.2012.6183572", "10.1109/tvcg.2011.176", "10.1016/j.visinf.2018.04.006.2", "10.1111/cgf.13264", "10.1080/13658816.2010.508043", "10.1007/s41651-017-0002-6", "10.1109/tvcg.2009.65", "10.1109/vast.2011.6102498", "10.1145/3025453.3025777", "10.1109/tvcg.2006.138"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1111/cgf.13979", "year": "2020", "title": "Short-Contact Touch-Manipulation of Scatterplot Matrices on Wall Displays", "conferenceName": "EuroVis", "authors": "Patrick Riehmann;Gabriela Molina Le{\\'{o}}n;Joshua Reibert;Florian Echtler;Bernd Fr{\\\"{o}}hlich", "citationCount": "0", "affiliation": "Riehmann, P (Corresponding Author), Bauhaus Univ Weimar, Weimar, Germany.\nRiehmann, P.; Leon, G. Molina; Reibert, J.; Echtler, F.; Froehlich, B., Bauhaus Univ Weimar, Weimar, Germany.\nReibert, J., German Aerosp Ctr DLR, Inst Data Sci, Jena, Germany.\nLeon, G. Molina, Univ Bremen, Bremen, Germany.", "countries": "Germany", "abstract": "This paper presents a short-contact multitouch vocabulary for interacting with scatterplot matrices (SPLOMs) on wall-sized displays. Fling-based gestures overcome central interaction challenges of such large displays by avoiding long swipes on the typically blunt surfaces, frequent physical navigation by walking for accessing screen areas beyond arm's reach in the horizontal direction and uncomfortable postures for accessing screen areas in the vertical direction. Furthermore, we make use of the display's high resolution and large size by supporting the efficient specification of two-tiered focus + context regions which are consistently propagated across the SPLOM. These techniques are complemented by axis-centered and lasso-based selection techniques for specifying subsets of the data. An expert review as well as a user study confirmed the potential and general usability of our seamlessly integrated multitouch interaction techniques for SPLOMs on large vertical displays.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13979", "refList": ["10.1109/tvcg.2018.2865235", "10.1145/3152832.3152852", "10.1109/tvcg.2008.153", "10.1016/j.cag.2007.01.029", "10.1145/3025453.3025594", "10.1109/vast.2009.5332595", "10.1145/2971485.2971503", "10.1145/3173574.3173593", "10.1145/2817721.2817735", "10.1145/2670444.2670445", "10.1145/2470654.2481318", "10.1109/mcg.2005.88", "10.1145/1936652.1936707", "10.1145/2556288.2557231", "10.1111/cgf.12902", "10.1201/9781498710411", "10.1109/tvcg.2016.2592906", "10.1111/cgf.13206", "10.1145/1520340.1520467", "10.1109/tvcg.2010.130", "10.1145/2598153.2598163", "10.1145/2909132.2909258", "10.1002/hbm.20701", "10.1145/2396636.2396675", "10.1145/2702123.2702406", "10.1109/tvcg.2012.251", "10.2312/eurovisshort.20181088", "10.1145/2702123.2702312", "10.1177/1473871611415997", "10.1145/22339.22342", "10.1109/mcg.2013.24", "10.1145/2669485.2669507"], "wos": 1, "children": [], "len": 1}], "len": 11}, {"doi": "10.1111/cgf.13673", "year": "2019", "title": "Multiple Views: different meanings and collocated words", "conferenceName": "EuroVis", "authors": "Jonathan C. Roberts;Hayder Al{-}Maneea;Peter W. S. Butcher;Robert Lew;Geraint Rees;Nirwan Sharma;Ana Frankenberg{-}Garcia", "citationCount": "1", "affiliation": "Roberts, JC (Corresponding Author), Bangor Univ, Bangor, Gwynedd, Wales.\nRoberts, J. C.; Al-maneea, H.; Butcher, P. W. S.; Sharma, N., Bangor Univ, Bangor, Gwynedd, Wales.\nAl-maneea, H., Basrah Univ, Basrah, Iraq.\nLew, R.; Rees, G., Adam Mickiewicz Univ, Poznan, Poland.\nFrankenberg-Garcia, A., Univ Surrey, Guildford, Surrey, England.", "countries": "Poland;Wales;England;Iraq", "abstract": "We report on an in-depth corpus linguistic study on multiple views' terminology and word collocation. We take a broad interpretation of these terms, and explore the meaning and diversity of their use in visualisation literature. First we explore senses of the term multiple views' (e.g., multiple views' can mean juxtaposition, many viewport projections or several alternative opinions). Second, we investigate term popularity and frequency of occurrences, investigating usage of multiple' and view' (e.g., multiple views, multiple visualisations, multiple sets). Third, we investigate word collocations and terms that have a similar sense (e.g., multiple views, side-by-side, small multiples). We built and used several corpora, including a 6-million-word corpus of all IEEE Visualisation conference articles published in IEEE Transactions on Visualisation and Computer Graphics 2012 to 2017. We draw on our substantial experience from early work in coordinated and multiple views, and with collocation analysis develop several lists of terms. This research provides insight into term use, a reference for novice and expert authors in visualisation, and contributes a taxonomy of multiple view' terms.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13673", "refList": ["10.1109/tvcg.2017.2744359", "10.1109/38.31462", "10.1109/cmv.2003.1215002", "10.2307/2289444", "10.1016/j.ijhcs.2011.02.007", "10.1017/s0022112091210654", "10.1016/b978-008044531-1/50426-7", "10.1109/cmv.2003.1215001", "10.1109/tvcg.2013.134", "10.1117/12.378894", "10.1037/0096-1523.21.6.1494", "10.1057/palgrave.ivs.9500086", "10.1109/iv.2011.42", "10.1109/tvcg.2017.2744199", "10.4324/9780203810088", "10.1145/1321440.1321580", "10.1109/tvcg.2015.2467271", "10.1016/j.learninstruc.2006.03.001", "10.1145/345513.345282", "10.1007/s40607-014-0009-9", "10.1109/tvcg.2017.2743859", "10.1109/infvis.1997.636761", "10.1109/tvcg.2013.219", "10.1007/978-3-319-55627-7", "10.1109/tvcg.2009.111", "10.1109/iv.2008.21", "10.1109/infvis.2002.1173157", "10.1109/mcg.2014.82", "10.1145/3143699.3143717", "10.1109/tvcg.2012.226", "10.1145/345513.345271", "10.2478/jazcas-2018-0006", "10.1145/2556288.2556969", "10.1109/visual.1998.745282", "10.1145/1276377.1276427", "10.1109/infvis.2001.963283", "10.1007/978-1-4471-6497-5\\_1", "10.1109/visual.1994.346302", "10.1057/palgrave.ivs.9500068", "10.1109/tvcg.2006.160", "10.1109/visual.1990.146374", "10.1109/tvcg.2016.2614803", "10.1177/1473871611416549", "10.1109/tvcg.2017.2745878", "10.1109/tvcg.2006.69", "10.1109/tpami.1983.4767367", "10.1109/visual.1991.175794", "10.1109/tvcg.2017.2744159", "10.1109/tvcg.2017.2744198", "10.1109/32.328995", "10.1016/j.jeap.2018.07.003", "10.1016/j.geomorph.2012.08.021", "10.1109/tvcg.2014.2346920", "10.1109/2.917550", "10.1017/s0958344018000150", "10.1109/tvcg.2009.94", "10.1179/2051819613z.0000000003", "10.1109/vast.2008.4677370", "10.1117/12.309533", "10.1109/tvcg.2014.2346747", "10.1075/ijcl.13.4.06ray", "10.1007/s12650-015-0323-9", "10.1109/tvcg.2006.178", "10.1109/tvcg.2016.2615308", "10.2307/1269768", "10.2307/4132312", "10.1109/tvcg.2018.2864903", "10.1109/mis.2006.100", "10.1109/iv.1998.694193", "10.1007/s11192-015-1830-0", "10.1109/tvcg.2017.2744080", "10.1111/j.1467-9280.1997.tb00442.x", "10.1109/cmv.2007.20", "10.1109/infvis.2003.1249006", "10.1109/tvcg.2017.2745941", "10.1016/s1364-6613(99)01332-7", "10.1145/989863.989893", "10.1109/tse.1985.232211", "10.1109/tvcg.2017.2744184", "10.1109/infvis.2004.12", "10.1075/ijcl.17.3.04har", "10.1109/tvcg.2010.179", "10.1016/j.jss.2006.05.024", "10.1109/cmv.2003.1215005", "10.1109/tvcg.2017.2744358", "10.1145/2992154.2996365", "10.1109/tvcg.2016.2598827"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030338", "title": "Composition and Configuration Patterns in Multiple-View Visualizations", "year": "2020", "conferenceName": "InfoVis", "authors": "Xi Chen;Wei Zeng 0004;Yanna Lin;Hayder Al-Maneea;Jonathan Roberts 0002;Remco Chang", "citationCount": "0", "affiliation": "Zeng, W (Corresponding Author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. Chen, Xi; Zeng, Wei; Lin, Yanna, Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China. AI-maneea, Hayder Mahdi; Roberts, Jonathan, Bangor Univ, Bangor, Gwynedd, Wales. Chang, Remco, Tufts Univ, Medford, MA 02155 USA.", "countries": "USA;Wales;China", "abstract": "Multiple-view visualization (MV) is a layout design technique often employed to help users see a large number of data attributes and values in a single cohesive representation. Because of its generalizability, the MV design has been widely adopted by the visualization community to help users examine and interact with large, complex, and high-dimensional data. However, although ubiquitous, there has been little work to categorize and analyze MVs in order to better understand its design space. As a result, there has been little to no guideline in how to use the MV design effectively. In this paper, we present an in-depth study of how MVs are designed in practice. We focus on two fundamental measures of multiple-view patterns: composition, which quantifies what view types and how many are there; and configuration, which characterizes spatial arrangement of view layouts in the display space. We build a new dataset containing 360 images of MVs collected from IEEE VIS, EuroVis, and PacificVis publications 2011 to 2019, and make fine-grained annotations of view types and layouts for these visualization images. From this data we conduct composition and configuration analyses using quantitative metrics of term frequency and layout topology. We identify common practices around MVs, including relationship of view types, popular view layouts, and correlation between view types and layouts. We combine the findings into a MV recommendation system, providing interactive tools to explore the design space, and support example-based design.", "keywords": "Multiple views,design pattern,quantitative analysis,example-based design", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030338", "refList": ["10.1109/tvcg.2018.2865235", "10.1109/tvcg.2016.2615308", "10.1145/2642918.2647398", "10.1177/1473871611416549", "10.1109/tvcg.2019.2934810", "10.1109/tvcg.2014.48", "10.1109/tvcg.2018.2864903", "10.1145/345513.345271", "10.1109/iv.1998.694193", "10.1109/tvcg.2007.70594", "10.1109/tvcg.2017.2744019", "10.1109/tvcg.2011.185", "10.1109/tvcg.2015.2467194", "10.1109/visual.1991.175815", "10.14778/2831360.2831371", "10.1109/mcg.2019.2924636", "10.1111/cgf.13673", "10.1111/cgf.12131", "10.1109/tvcg.2017.2744198", "10.1145/108360.108361", "10.1109/vl.1996.545307", "10.1109/tvcg.2017.2745140", "10.1109/cmv.2007.20", "10.1145/198366.198376", "10.1145/2508363.2508405", "10.1111/cgf.12114", "10.1109/icde.2018.00019", "10.1109/tvcg.2017.2787113", "10.1109/tvcg.2018.2865240", "10.1111/cgf.13380", "10.1111/cgf.12902", "10.1109/vast.2015.7347628", "10.2307/2288400", "10.1109/infvis.2004.12", "10.1145/102377.115768", "10.1109/tvcg.2009.179", "10.1109/2945.981851", "10.1109/tvcg.2007.70521", "10.1109/pacificvis.2012.6183556", "10.1145/2213836", "10.1109/tvcg.2013.234", "10.1109/iv.2008.87"], "wos": 1, "children": [], "len": 1}], "len": 3}, {"doi": "10.1111/cgf.13718", "year": "2019", "title": "Investigating the Manual View Specification and Visualization by Demonstration Paradigms for Visualization Construction", "conferenceName": "EuroVis", "authors": "Bahador Saket;Alex Endert", "citationCount": "1", "affiliation": "Saket, B (Corresponding Author), Georgia Inst Technol, Sch Interact Comp, Atlanta, GA 30332 USA.\nSaket, Bahador; Endert, Alex, Georgia Inst Technol, Sch Interact Comp, Atlanta, GA 30332 USA.", "countries": "USA", "abstract": "Interactivity plays an important role in data visualization. Therefore, understanding how people create visualizations given different interaction paradigms provides empirical evidence to inform interaction design. We present a two-phase study comparing people's visualization construction processes using two visualization tools: one implementing the manual view specification paradigm (Polestar) and another implementing visualization by demonstration (VisExemplar). Findings of our study indicate that the choice of interaction paradigm influences the visualization construction in terms of: 1) the overall effectiveness, 2) how participants phrase their goals, and 3) their perceived control and engagement. Based on our findings, we discuss trade-offs and open challenges with these interaction paradigms.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13718", "refList": ["10.1111/cgf.12887", "10.1023/a:1008716330212", "10.1145/2470654.2466255", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2012.275", "10.1145/2493102.2493104", "10.1109/tvcg.2016.2598839", "10.1145/948496.948514", "10.1109/tvcg.2015.2467201", "10.2312/pe.eurovisshort.eurovisshort2013.019-023", "10.1109/tvcg.2007.70541", "10.1109/tvcg.2016.2598446", "10.1109/vl.1996.545307", "10.1109/tvcg.2014.2346250", "10.1145/3025453.3025942", "10.1145/2207676.2207741", "10.1145/3173574.3174212", "10.1109/tvcg.2015.2467615", "10.1145/2598784.2598806", "10.1109/tvcg.2017.2680452", "10.1145/2598510.2598566", "10.1109/tvcg.2007.70577", "10.1109/tvcg.2016.2598620", "10.2307/2530428", "10.1109/infvis.1998.729560", "10.1109/tvcg.2014.2346291", "10.1109/tvcg.2010.164", "10.1109/tvcg.2017.2743859", "10.1109/tvcg.2013.191", "10.1109/2.153286", "10.1109/tvcg.2015.2467191", "10.1145/3173574.3173697", "10.1145/1502650.1502667", "10.1109/tvcg.2007.70515"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2019.2934534", "title": "Investigating Direct Manipulation of Graphical Encodings as a Method for User Interaction", "year": "2019", "conferenceName": "InfoVis", "authors": "Bahador Saket;Samuel Huron;Charles Perin;Alex Endert", "citationCount": "0", "affiliation": "Saket, B (Corresponding Author), Georgia Tech, Atlanta, GA 30332 USA. Saket, Bahador; Endert, Alex, Georgia Tech, Atlanta, GA 30332 USA. Huron, Samuel, Univ Paris Saclay, Paris, France. Perin, Charles, Univ Victoria, Victoria, BC, Canada.", "countries": "Canada;USA;France", "abstract": "We investigate direct manipulation of graphical encodings as a method for interacting with visualizations. There is an increasing interest in developing visualization tools that enable users to perform operations by directly manipulating graphical encodings rather than external widgets such as checkboxes and sliders. Designers of such tools must decide which direct manipulation operations should be supported, and identify how each operation can be invoked. However, we lack empirical guidelines for how people convey their intended operations using direct manipulation of graphical encodings. We address this issue by conducting a qualitative study that examines how participants perform 15 operations using direct manipulation of standard graphical encodings. From this study, we 1) identify a list of strategies people employ to perform each operation, 2) observe commonalities in strategies across operations, and 3) derive implications to help designers leverage direct manipulation of graphical encoding as a method for user interaction.", "keywords": "Direct Manipulation,Data Visualization", "link": "http://dx.doi.org/10.1109/TVCG.2019.2934534", "refList": ["10.1145/2556288.2557379", "10.1109/tvcg.2016.2598839", "10.1145/2858036.2858435", "10.1145/2702123.2702237", "10.1111/j.1467-8659.2009.01678.x", "10.1109/tvcg.2011.185", "10.1109/mcg.2016.90", "10.1177/1473871611413180", "10.1111/cgf.13718", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2014.2346984", "10.1145/2207676.2207741", "10.1109/mcg.2019.2903711", "10.1109/tvcg.2018.2865075", "10.1109/tvcg.2014.2346279", "10.1109/iv.1999.781570", "10.1109/tvcg.2015.2467615", "10.1109/tvcg.2014.2346292", "10.1145/2984511.2984588", "10.1109/tvcg.2017.2680452", "10.1145/1166253.1166265", "10.1109/tvcg.2017.2745258", "10.1109/tvcg.2016.2598620", "10.1109/tvcg.2014.2346291", "10.2307/2530428", "10.1109/tvcg.2012.204", "10.1145/3173574.3173697", "10.1207/s15327051hci0104\\_2", "10.1109/vast.2012.6400486", "10.1109/tvcg.2017.2745078"], "wos": 1, "children": [], "len": 1}, {"doi": "10.1109/tvcg.2020.3030476", "title": "StructGraphics: Flexible Visualization Design through Data-Agnostic and Reusable Graphical Structures", "year": "2020", "conferenceName": "InfoVis", "authors": "Theophanis Tsandilas", "citationCount": "0", "affiliation": "Tsandilas, T (Corresponding Author), Univ Paris Saclay, INRIA, Gif Sur Yvette, France. Tsandilas, T (Corresponding Author), CNRS, F-75700 Paris, France. Tsandilas, Theophanis, Univ Paris Saclay, INRIA, Gif Sur Yvette, France. Tsandilas, Theophanis, CNRS, F-75700 Paris, France.", "countries": "France", "abstract": "Information visualization research has developed powerful systems that enable users to author custom data visualizations without textual programming. These systems can support graphics-driven practices by bridging lazy data-binding mechanisms with vector-graphics editing tools. Yet, despite their expressive power, visualization authoring systems often assume that users want to generate visual representations that they already have in mind rather than explore designs. They also impose a data-to-graphics workflow, where binding data dimensions to graphical properties is a necessary step for generating visualization layouts. In this paper, we introduce StructGraphics, an approach for creating data-agnostic and fully reusable visualization designs. StructGraphics enables designers to construct visualization designs by drawing graphics on a canvas and then structuring their visual properties without relying on a concrete dataset or data schema. In StructGraphics, tabular data structures are derived directly from the structure of the graphics. Later, designers can link these structures with real datasets through a spreadsheet user interface. StructGraphics supports the design and reuse of complex data visualizations by combining graphical property sharing, by-example design specification, and persistent layout constraints. We demonstrate the power of the approach through a gallery of visualization examples and reflect on its strengths and limitations in interaction with graphic designers and data visualization experts.", "keywords": "Visualization design,graphical structures,visualization grammars,layout constraints,infographics,flexible data binding", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030476", "refList": ["10.1145/344949.344959", "10.1057/ivs.2009.22", "10.1109/mcg.1987.277079", "10.1109/tvcg.2018.2865158", "10.1109/tvcg.2016.2598839", "10.1145/2858036.2858435", "10.1145/3173574.3173610", "10.1145/3173574.3174106", "10.1109/tvcg.2011.185", "10.1109/tvcg.2007.70515", "10.1109/iv.2008.66", "10.1111/cgf.13718", "10.1109/tvcg.2014.2346250", "10.1109/tvcg.2017.2744198", "10.1145/22949.22950", "10.1145/1925844.1926423", "10.1109/mcg.2019.2903711", "10.1006/cogp.1994.1010", "10.1109/tvcg.2018.2865240", "10.1145/3125571.3125585", "10.1145/3022671.2984020", "10.1111/cgf.12391", "10.1109/tvcg.2015.2467091", "10.1109/infvis.2004.12", "10.1109/tvcg.2016.2598620", "10.1016/j.jvlc.2017.10.001", "10.1109/tvcg.2014.2346291", "10.1109/2945.981851", "10.1073/pnas.1807184115", "10.1109/tvcg.2010.177", "10.1145/3173574.3173697", "10.1080/1369118x.2016.1153126", "10.1109/tvcg.2015.2414454", "10.1145/2740908.2742849", "10.1023/a:1025671410623", "10.1111/cgf.12903", "10.1207/s15327051hci0104\\_2", "10.1145/3290605.3300358", "10.1109/tvcg.2016.2599030"], "wos": 1, "children": [], "len": 1}], "len": 5}, {"doi": "10.1111/cgf.13976", "year": "2020", "title": "Many At Once: Capturing Intentions to Create And Use Many Views At Once In Large Display Environments", "conferenceName": "EuroVis", "authors": "Jillian Aurisano;Abhinav Kumar;Abeer Alsaiari;Barbara Di Eugenio;Andrew E. Johnson", "citationCount": "0", "affiliation": "Aurisano, J (Corresponding Author), Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.\nAurisano, J (Corresponding Author), Elect Visualizat Lab, Chicago, IL 60607 USA.\nAurisano, Jillian; Kumar, Abhinav; Alsaiari, Abeer; Di Eugenio, Barbara; Johnson, Andrew, Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.\nAurisano, Jillian; Alsaiari, Abeer; Johnson, Andrew, Elect Visualizat Lab, Chicago, IL 60607 USA.", "countries": "USA", "abstract": "This paper describes results from an observational, exploratory study of visual data exploration in a large, multi-view, flexible canvas environment. Participants were provided with a set of data exploration sub-tasks associated with a local crime dataset and were instructed to pose questions to a remote mediator who would respond by generating and organizing visualizations on the large display. We observed that participants frequently posed requests to cast a net around one or several subsets of the data or a set of data attributes. They accomplished this directly and by utilizing existing views in unique ways, including by requesting to copy and pivot a group of views collectively and posing a set of parallel requests on target views expressed in one command. These observed actions depart from multi-view flexible canvas environments that typically provide interfaces in support of generating one view at a time or actions that operate on one view at a time. We describe how participants used these `cast-a-net' requests for tasks that spanned more than one view and describe design considerations for multi-view environments that would support the observed multi-view generation actions.", "keywords": "", "link": "https://doi.org/10.1111/cgf.13976", "refList": ["10.1109/vast47406.2019.8986918", "10.1109/tvcg.2013.166", "10.1109/tvcg.2012.275", "10.1177/0011000006287390", "10.1109/tvcg.2014.2346293", "10.1111/cgf.12106", "10.1109/visual.2005.1532788", "10.1145/2399016.2399102", "10.1109/tvcg.2014.2346260", "10.1145/3173574.3173593", "10.1145/2559206.2581202", "10.1111/cgf.12131", "10.4108/icst.collaboratecom.2014.257337", "10.1145/2207676.2208293", "10.1145/2576099", "10.1145/1936652.1936676", "10.1109/tvcg.2013.163", "10.1109/mcg.2013.37", "10.1186/1471-2105-16-s11-s6", "10.1109/2945.981851", "10.1145/2807442.2807478", "10.1109/tvcg.2010.164", "10.1109/tvcg.2014.2337337", "10.1109/tvcg.2017.2743859", "10.1109/tvcg.2006.184", "10.1109/tvcg.2015.2467191", "10.1145/2702123.2702406", "10.1109/tvcg.2012.251", "10.1145/2702123.2702312", "10.1177/1473871611415997"], "wos": 1, "children": [], "len": 1}], "len": 29}, "index": 633, "embedding": [2.1201059818267822, 2.0408596992492676, -1.030107021331787, -2.273205041885376, -0.3538986146450043, 0.16650904715061188, -0.7323254346847534, 1.6023223400115967, 0.6399315595626831, 0.8771766424179077, 1.4964654445648193, 1.581524133682251, 1.7138527631759644, 2.160466194152832, -0.7001745700836182, 2.5937743186950684, 2.5248911380767822, 0.07072390615940094, 0.5381700396537781, 4.436172008514404, -0.06630208343267441, -0.5897487998008728, 0.2274302989244461, 3.0810656547546387, -2.405334949493408, 1.9889094829559326, -0.5452967882156372, 2.374844551086426, 2.5489065647125244, -1.3314732313156128, 1.683369517326355, 2.031749963760376], "projection": [0.6741462349891663, 9.239886283874512], "size": 15, "height": 3, "width": 9}