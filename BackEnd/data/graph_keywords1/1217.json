{"data": {"doi": "10.1109/vast47406.2019.8986948", "title": "FAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning", "year": "2019", "conferenceName": "VAST", "authors": "\u00c1ngel Alexander Cabrera;Will Epperson;Fred Hohman;Minsuk Kahng;Jamie Morgenstern;Duen Horng Chau", "citationCount": "4", "affiliation": "Cabrera, AA (Corresponding Author), Georgia Inst Technol, Atlanta, GA 30332 USA. Cabrera, Angel Alexander; Epperson, Will; Hohman, Fred; Kahng, Minsuk; Morgenstern, Jamie; Chau, Duen Horng (Polo), Georgia Inst Technol, Atlanta, GA 30332 USA.", "countries": "USA", "abstract": "The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FAIRVIS, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FAIRVIS, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FAIRVIS's coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FAIRVIS helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FAIRVIS demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.", "keywords": "Machine learning fairness,visual analytics,intersectional bias,subgroup discovery", "link": "http://dx.doi.org/10.1109/VAST47406.2019.8986948", "refList": ["10.1177/1473871611416549", "10.1109/vast.2017.8585720", "10.1145/2702123.2702509", "10.1007/978-3-540-79347-2\\_3", "10.2307/2346830", "10.1109/bigdata.2018.8622525", "10.1109/tvcg.2018.2843369", "10.1145/1007730.1007731", "10.1109/tvcg.2016.2598828", "10.15779/z38bg31", "10.1109/tvcg.2017.2744718", "10.1089/big.2016.0047", "10.1186/1471-2105-9-497"], "wos": 1, "children": [{"doi": "10.1109/tvcg.2020.3030383", "title": "II-20: Intelligent and pragmatic analytic categorization of image collections", "year": "2020", "conferenceName": "VAST", "authors": "Jan Zah\u00e1lka;Marcel Worring;Jarke J. van Wijk", "citationCount": "0", "affiliation": "Zahalka, J (Corresponding Author), Czech Tech Univ, Prague, Czech Republic. Zahalka, Jan, Czech Tech Univ, Prague, Czech Republic. Worring, Marcel, Univ Amsterdam, Amsterdam, Netherlands. van Wijk, Jarke J., Eindhoven Univ Technol, Eindhoven, Netherlands.", "countries": "Republic;Netherlands", "abstract": "In this paper, we introduce 11\u201320 (Image Insight 2020), a multimedia analytics approach for analytic categorization of image collections. Advanced visualizations for image collections exist, but they need tight integration with a machine model to support the task of analytic categorization. Directly employing computer vision and interactive learning techniques gravitates towards search. Analytic categorization, however, is not machine classification (the difference between the two is called the pragmatic gap): a human adds/redefines/deletes categories of relevance on the fly to build insight, whereas the machine classifier is rigid and non-adaptive. Analytic categorization that truly brings the user to insight requires a flexible machine model that allows dynamic sliding on the exploration-search axis, as well as semantic interactions: a human thinks about image data mostly in semantic terms. 11\u201320 brings three major contributions to multimedia analytics on image collections and towards closing the pragmatic gap. Firstly, a new machine model that closely follows the user's interactions and dynamically models her categories of relevance. II-20's machine model, in addition to matching and exceeding the state of the art's ability to produce relevant suggestions, allows the user to dynamically slide on the exploration-search axis without any additional input from her side. Secondly, the dynamic, 1-image-at-a-time Tetris metaphor that synergizes with the model. It allows a well-trained model to analyze the collection by itself with minimal interaction from the user and complements the classic grid metaphor. Thirdly, the fast-forward interaction, allowing the user to harness the model to quickly expand (\u201cfast-forward\u201d) the categories of relevance, expands the multimedia analytics semantic interaction dictionary. Automated experiments show that II-20's machine model outperforms the existing state of the art and also demonstrate the Tetris metaphor's analytic quality. User studies further confirm that II\u201320 is an intuitive, efficient, and effective multimedia analytics tool.", "keywords": "Multimedia analytics,image data,analytic categorization,pragmatic gap", "link": "http://dx.doi.org/10.1109/TVCG.2020.3030383", "refList": ["10.1109/tvcg.2008.137", "10.1145/2882903.2882919", "10.1145/2556647.2556657", "10.1145/1142473.1142574", "10.1109/tvcg.2016.2598471", "10.1109/vast.2017.8585669", "10.1109/tvcg.2014.2346573", "10.1111/j.0956-7976.2005.00782.x", "10.1007/978-3-540-89965-5\\_27", "10.1109/visual.2019.8933611", "10.1109/vast.2010.5653598", "10.1145/2207676.2208293", "10.1111/cgf.13402", "10.1109/tvcg.2012.271", "10.1109/mcg.2009.49", "10.1057/ivs.2008.31", "10.1111/cgf.12925", "10.1109/hicss.2016.183", "10.1109/tvcg.2016.2598466", "10.1109/tvcg.2015.2467551", "10.1109/tvcg.2018.2865117", "10.1109/vast47406.2019.8986948", "10.1016/s0950-7051(00)00101-5", "10.1109/tvcg.2016.2598594", "10.1109/tvcg.2006.101", "10.1111/cgf.12311", "10.1109/vast.2009.5333020", "10.1109/vast.2010.5652885", "10.1111/cgf.13670", "10.1109/wvl.1988.18020", "10.1145/2133806.2133821", "10.1109/vast.2012.6400486"], "wos": 1, "children": [], "len": 1}], "len": 3}, "index": 1217, "embedding": [0.5943491458892822, -0.27671948075294495, 0.23277908563613892, 0.27486729621887207, -0.3780239224433899, 0.16650904715061188, -0.6826844215393066, -0.9950615167617798, 0.009467952884733677, -0.5474514961242676, 0.18886925280094147, 0.03882044181227684, -0.12592144310474396, -0.1665761023759842, 1.3219338655471802, -0.5597680807113647, 0.020497551187872887, 0.06402745097875595, 0.8913590908050537, -1.0394032001495361, -0.06630208343267441, -0.6541746258735657, 0.14656192064285278, -0.13563132286071777, 0.4838313162326813, 0.09350619465112686, 0.05166451260447502, -0.16470037400722504, -0.5601260662078857, 1.718578577041626, 0.6312606930732727, -0.4619571268558502], "projection": [-3.251115560531616, 4.5204925537109375], "size": 2, "height": 2, "width": 1}